,claps,days_since_publication,fans,link,num_responses,publication,published_date,read_ratio,read_time,reads,started_date,tags,text,title,title_word_count,type,views,word_count,claps_per_word,editing_days,<tag>Education,<tag>Data Science,<tag>Towards Data Science,<tag>Machine Learning,<tag>Python
121,2,716.0538482379168,2,https://medium.com/p/screw-the-environment-but-consider-your-wallet-a4f7cd3d3161,0,None,2017-06-10 14:25:00,42.17,7,70,2017-06-10 14:24:00,"['Climate Change', 'Economics']","Screw the Environment, but Consider Your Wallet When the Health of the Environment and the Health of Your Wallet Align According to Malcolm Gladwell’s bestselling nonfiction work Outliers, it takes 10,000 hours to achieve world-class expertise in a field such as playing an instrument or becoming a professional athlete. Based on this logic, one LED lightbulb could accompany you through the process of becoming a concert-level piano player, learning to speak French like a native, and then halfway to making it as a professional tennis player. As incredible as it may seem, the average LED lightbulb is rated to last for a minimum of 25,000 hours, or in practical terms, 22.8 years at 3 hours of usage per day. If you want the same duration of light from incandescent bulbs with an average lifetime of 1,000 hours, you will have to interrupt your studying to swap bulbs 25 times. You have probably seen LED lightbulbs and even thought about making the switch on your last trip to the local hardware store. However, like many of us, you were probably put off by the high cost of LEDs and decided to stick with the same incandescent bulb that Thomas Edison would still recognize over 130 years after its invention (I have to mention that Edison did not invent the lightbulb but only figured out how to sell them in the classic example of the value of a good marketing campaign). Why pay up to 10 times the price for a bulb that produces the exact same amount of light? On my latest trip to Ace, I decided to work out if I was missing anything by not making the switch to LED. Quickly, I realized the enormous short-sighted error I was making. Take the humble and ubiquitous 60-watt lightbulb. My choice seemed relatively simple; I could pick up an incandescent for $0.50 (a 20-pack was just $10) or I could get a 60-watt equivalent LED bulb for $1.50 (4 for $6). The price discrepancy was much lower than I remembered from a couple years ago, but the LED was still 3 times the up-front price. Nevertheless, to get the full story, I did the simple math to work out the lifetime cost of each bulb. The LED, while putting out the exact same light intensity and color, uses only 9.5 watts which equates to $1.14 in electricity costs per year (based on 3 hours/day). Meanwhile, the incandescent bulb only converts 2.2% of the energy it receives into usable light (the rest is lost as heat) and consequently costs $7.23/year. Let’s see what that equates to over the lifetime of the bulb. I’ll go easy on the incandescent and assume that the LED lasts for only 20 years. The total cost to purchase and operate the 60-watt LED bulb for two decades works out to $24.30. The same light production from incandescent bulbs, which last for only a year of daily use, will require 20 bulbs and cost $154.60. (An interesting side note is that early incandescent lightbulb manufacturers formed a cartel and limited the lifetime of bulbs in order to sell larger quantities). That means a single LED bulb will save you over $130 over its two-decade lifetime. Standing in the hardware store, I had what can only be described as an epiphany as I realized how much cash I had been throwing away through short-term thinking. Now, perhaps that is not persuasive enough. What’s $130 spread out over 20 years worth to you? I knew that I could not stop my analysis at merely a single bulb. Based on surveys, the average American household has 45 lightbulbs in daily use (take a moment to go and count. I was astonished to discover this was actually the case). Assuming these average out to 60 watts each, switching to all LEDs will save $490 in energy costs each year. That works out to a staggering total of $5863.50 over the next two decades. If pocketing a free six grand is not an enticement, imagine having to replace all 900 of those incandescents compared to the one-time installation of the 45 LEDs. Taking matters as far as we can, let’s consider the 126 million households in the United States. If all incandescent bulbs were switched to LEDs today (perhaps we can have a one-time national lightbulb switching holiday?), over the next 20 years, US households would save almost $740 billion and 111 billion lightbulbs. Oftentimes, when it comes to talking about climate change, either the narratives are so depressing that people lose hope and believe their individual actions to be meaningless, or the audience is not responsive to environmental arguments and unwilling to make lifestyle changes for some intangible benefits at some point in the distant future. The light bulb example is not some abstract calculation involving megatons of CO2 reduced or inches of sea level rise prevented 150 years from now. It is about you being able to afford one (or perhaps two) more family vacations over the next 20 years because you made the simple step of changing your light bulbs. This is about real money staying in American pockets rather than lining the walls of a Saudi Arabian oil tycoon (or in reality buying another mansion for an Exxon CEO). Who is going to make the extra effort of recycling or incur the added cost of buying an electric car if the effects will not be felt for generations? However, when it comes to making personal or household decisions, you will find that the environmentally sound choice often aligns with the best interests of your wallet. Switching lightbulbs might be the most actionable example, but there are plenty of others from eating less meat to walking more (both of which will not only improve your health and that of the environment, but also cost far less over the long term than the alternative). The problem with these decisions is that, as in the case of LED lightbulbs, the upfront costs of implementing a change can obscure the benefits of a smart investment. Of course, I encourage anyone to consider the lasting impact to the Earth when making a purchasing decision or changing daily habits, but if that does not appeal to you, then I urge you to take the time to figure out the true economic cost of the options. More often than not, you will find that “saving the environment” aligns nicely with keeping your wallet “full of Benjamins.” Author’s Note: I was inspired to write this post after my experience a few months ago with a house owner from whom I was renting. I noticed that every single bulb in his house was incandescent and he kept them on almost constantly. After hearing him complain yet another time that he had to replace the darn things twice a year, I decide to make the polite suggestion that LEDs were a better choice for the environment. He scoffed and said (although not so politely): “Screw the environment. That’s not for me to worry about; I’m sure the polar bears can find somewhere else to live.” (He did refrain from calling me a socialist environmental-terrorist which I thought was a nice gesture). I took a while off from the topic and thought about my approach. The next time the issue came up when another two bulbs had burned out, I made the LED suggestion again, but this time I couched my recommendation in cold economic terms. I mentioned the cost-savings and the lifetime advantage of LEDs, and reluctantly he agreed to make the switch in several fixtures. After a month with the new bulbs, he was prepared to accept that they did indeed produce the same comforting light as incandescents and of course none had burned out. At that point, he decided to go full in (this was not a man to do anything halfway) and replaced every single bulb in his house with LEDs. After two months with no complaints and not a single replacement, I thought nothing more of the matter. Then, one day, he triumphantly pulled out his electricity bills and showed me the year-over-year comparisons by month. His electricity costs had decreased by an astounding $40 per month under the same exact conditions and with no conscious effort on his part to reduce usage. He said that he had even convinced many of his friends (all people to whom saving the environment was about as meaningful as taking a vacation to Jupiter) to do the same and he had heard only positive responses. At this point, it finally dawned on me, people may not care about conserving natural resources, but when it comes to an economic argument, there can be no reason to campaign for less money in American pockets. Now, any time an environmental subject arises, I keep all of my arguments in terms of proven economic facts. I have found that people from all backgrounds are receptive to this approach and it often leads to productive dialogue about addressing climate change to order to strengthen the economy (for example by reducing dependence on foreign energy sources). I am optimistic about the future of renewable energy not because people are suddenly going to become tree-hugging environmentalists, but because new installations of renewable energy can now compete on a level playing field with fossil fuels. Rick Perry did not make Texas into the US leader in wind energy because he wants to preserve the Earth for his grandchildren, he did it because it made economic sense. There are numerous challenges remaining in the switch to renewable energy, and I do not claim that technological progress will inevitably save us. Instead, I see reason to be optimistic that economic realities can accelerate the world’s move to sustainable energy production. Will Sources: Lightbulb data was compiled by the author from the Ace Hardware Store in Ames, Iowa. All other sources used are linked in the article itself. Math calculations follow below: Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator","Screw the Environment, but Consider Your Wallet",8,published,166,1859,0.0010758472296933835,0,0,0,0,0,0
132,18,708.735908961088,3,https://medium.com/p/the-vanquishing-of-war-plague-and-famine-b424ac967a41,0,None,2017-06-17 22:02:00,29.51,14,54,2017-06-17 22:02:00,"['Climate Change', 'Humanity', 'Optimism', 'History']","The Vanquishing of War, Plague and Famine Part 1 of the Optimist’s Guide to the 21st Century The next time you sit down to dinner, start off the conversation with the statement: “The world today is more peaceful, more prosperous, and healthier than at any point in human civilization.” Wait a few seconds for it to sink in, and then ride out the inevitable storm of protestations, most of which will begin with: “Well today on the news I heard…” or “I saw a picture in the paper the other day…” While the commotion roils, take solace in the knowledge that your statement is entirely correct. Once the uproar has subsided, pull out this article, and slowly begin to turn everyone’s world upside down. Indeed, any way you look at it, humans today are better off than at any other point in history. This is the first in an indeterminate number (I’ll put the bounds at one and infinity) of articles about the drastic improvement in the human condition throughout history. It is designed to get past the surface-level stories of constant strife we hear reported in the press and take a fact-based look at the current and historical state of human affairs. When two years ago, I began reading deeper into the large scale trends surrounding humanity, I expected to find a roller-coaster graph of human living conditions over time with large drops and rises, and the modern world in a particularly deep trough. What I discovered instead is that the story has been a gradual increase with a dramatic acceleration in the latter half of the 20th century (what is called exponential growth). These startling realities led to a change in my worldview and I gradually adopted a positive outlook for the future of humanity (I like to say I’m a short-term realist but a long-term optimist). My intention is not convert anyone with this series, but rather to encourage readers to take a deeper look at the state of our world beyond the headlines. Grand Scale Improvements Let’s start our examination on the grand scale of all of human history. As Yuval Harari writes in his non-fiction work Homo Deus: A Brief History of Tomorrow, for the entirety of human existence up until the last few decades, humans were occupied on a daily basis with the “Big Three” problems of famine, plague, and war. Yet, in the 21st century, we can look back and realize that we have, for the most part, vanquished all three of these issues. As Harari summarizes, “For the first time in history, more people die today from eating too much than from eating too little; more people die from old age than from infectious diseases; and more people commit suicide than are killed by soldiers, terrorists and criminals combined.” The average human today is far more likely to die from the problems of overabundance than from a wheat shortage, ISIS attack, or Ebola outbreak. However, bold claims such as these are meaningless by themselves. In order to give these statements value, it is time to take a journey through the numbers that demonstrate the extent to which we have solved our largest historical challenges. A nice dinner-table worthy opening point is plagues, the invisible killer that has decimated human populations again and again. (In this case, decimate is not a strong enough word as it comes from the Roman military practice of killing one of every ten soldiers as a form of discipline). Diseases spread by Europeans to the New World starting in the 1500s routinely killed up to 90% of the native inhabitants, notably in the Aztecs and Mayans in central America. As an example, the native population in Hawaii decreased from half a million to 70,000 individuals within 80 years of James Cook first setting foot in the islands. Native peoples had no biological defenses against diseases they could not see let alone comprehend. Humanity in the 21st century however, is no longer helpless in the face of pathogens. Modern medicine has allowed us to prevent numerous diseases, significantly mitigate the effects of the diseases that we do catch, and wipe some plagues off the face of the Earth. Smallpox, the disease that more than any other factor may have been responsible for the Spanish conquest of Central America, was eradicated by humanity in 1979 thanks to a monumental global vaccination campaign overseen by the World Health Organization. Measles and Polio are next on the way out. In 1988, when the campaign to eradicate polio was launched, there were 350,000 new cases per year. In 2016, there were 37. It is possible that in 2017, the world will see its last ever case of polio. The reduction in global infectious disease deaths has occurred even as the world has become more connected , seemingly predisposing us to more disease outbreaks. Nonetheless, the outbreaks that do occur today are on a much smaller scale and cause far less damage than any historical plagues. Take the Ebola outbreak that lasted for 18 months in 2014–2016 and was accompanied by hysterical cries of doom and calls to shut down international travel. The entire Ebola outbreak infected only 30,000 individuals (4 in the United States) and only 11,000 individuals perished. This is 1/3 of number of people killed in the United States in automobile accidents every year. The global response to the outbreak was incredibly effective, and in December 2016, a vaccine was released with 100% efficacy. This transformation of what once would have been a catastrophic event for entire human civilizations into a minor regional flare-up is a testament to the amazing capabilities of modern medicine and to the rapid international response enabled by collaboration between government agencies and non-governmental organizations (NGOs). As we move forward in the 21st century, we must remain vigilant about the possibility of plagues, but we can take solace that we are no longer helpless in the face of disease. The second scourge in the triumvirate of human problems is famine. The default state of nearly all human societies up until the last decades of the 20th century was food vulnerability. One bad season or poor planting decision could lead to the starvation of millions of individuals. Historically, famines killed significant percentages of even well-established agricultural societies. For example, from 1692–194 in France, nearly 15% of citizens perished in a famine that resulted from two years of uncooperative weather. The same conditions killed one-fifth of all individuals in 1695 in Estonia. Famines have repeatedly struck every type of society with devastating results, particularly for the poorest individuals. However, in the last 50 years, a global network of government agencies, NGOs, and private insurance agencies have constructed a robust food security network that has made it exceedingly unlikely that famines will effect large swaths of a population. A poor harvest, or even multiple seasons of unfavorable growing conditions no longer spell doom for a society. In fact, the greater problem in the modern world is one of overabundance. While starvation/malnutrition led to the deaths of slightly under 1 million individuals in 2010, obesity related diseases killed over 3 million people. The “green revolution” and the miracle of modern agriculture (including Genetically Modified Organisms which have allowed us to feed more people cheaper than ever before with less environmental impact) have allowed humanity to move away from the default state of food insecurity. Today, we live in a world where just 2% of the population is able to feed the other 98% and farming now produces 2.5 times the amount of food as in 1950 with fewer inputs. In fact, collectively, the world produces enough food to feed everyone on the planet and the issue remains only in the distribution of nutrition. As with plagues, we have not eradicated hunger completely, but there are numerous plans in action that will soon allow us to cross famine off humanity’s collective to-do list. Finally, we must address what is sure to be the most controversial point of this article: humanity has reduced the extent and impact of violent conflict to a new low. This may be nearly impossible to fathom in our seemingly conflict-obsessed society, but a careful examination of history proves that this is the case. Throughout all of human history, the accepted state between different groups of humans has been conflict. Yuval Harari describes this as the “Law of the Jungle”: even if two groups were temporarily at peace, war was always an acceptable option for resolving conflicts. Times of peace were viewed as temporary interludes between conflicts. Now however, the narrative has flipped and the 21st century is on track to be the most peaceful century in human existence. Moreover, the 20th century, even with 2 global conflicts and numerous regional conflicts, was the most peaceful century in all of history. That point bears repeating: The 20th century was the most peaceful century in human existence. As Steven Pinker details in his book The Better Angels of Our Nature: Why Violence Has Declined, war killed at least 15% of the population in antiquity, 5% of the population in the 20th century, and less than 1% of the population since the turn of the millennium. On every conceivable scale, from human civilizations to individuals, from millennium down to single years, violence is on the decline. The global death rate from violence has fallen from 500 per 100,000 individuals in prestate societies to less than 1 in developed nations. Homicide rates in most parts of the developed world have fallen by 100 times in the past six centuries. The image of peaceful native peoples in ancient times and brutish, ruthless modern humans is flipped. Since 2002, the proportion of youth involved in violent behavior has fallen by 29% according to a study from the American Journal of Public Health. Even the threat of terrorism is miniscule; terrorists from the seven nations included in the administration’s recent travel ban killed 0 Americans from 1975–2015. For Americans, fast-food restaurants pose a much greater danger to your health than al-Qaeda. As Harari bluntly puts it: “Sugar is now more dangerous than gunpowder.” Countries no longer turn to violence as the first method for solving problems, and democratic countries nearly never go to war with each other. Increasingly, the global economy has come to rely on trade between nations, and this connectedness means it does not make sense to invade your neighbor when they are the ones feeding your citizens. Posturing between some of the most powerful nations on Earth may at the moment seem frightening, but it is nothing more than bluffing. Countries and leaders have matured to the point (well with one notable exception) where resolutions are worked out in boardrooms with lawyers armed with expensive pens and large vocabularies rather than soldiers with well-honed swords. The massive improvements that humanity has collectively made, particularly in the past 50 years, raises a critical question: If the examination of reality demonstrates that we live in a more peaceful age than ever before, why does no one believe that to be the case? To address this query, think back to the dinner table situation I outlined at the beginning of the article. The first objection anyone will make to the claims of worldwide peace, prosperity, and health is that they have repeatedly heard the exact opposite from various sources. In our modern world, we are exposed to more news and information in a single day than our distant ancestors would have been over the course of their entire lifetime. Therefore, we should easily be able to ascertain the historical and present-day realities and come to the realization that life really is getting better for the majority of humanity. However, access to information does not mean that we are in fact better informed. (In the words of Mark Twain: “If you don’t read the newspaper, you’re uninformed. If you read the newspaper, you’re mis-informed.”) Unfortunately, bad news sells much better than good news. Humans, whether we like it or not, are evolutionarily wired to pay more attention to negative news and to adopt a pessimistic worldview. Our ancestors on the savannah who saw a lion hiding behind every rock were much more likely to survive and pass on their genes than the hominids who stopped to appreciate the beauty of nature. In addition to the evolutionary benefits of adopting a negative outlook, stories of tragedy and woe are more intriguing to us as social creatures. There is considerably more gossip to be derived from a break-up than from a happy marriage. (How many times have you started a conversation about a postive aspect of your co-workers?). It does not take a trained psychologist to notice these human tendencies, and media companies are well-aware of our predisposition to negative information. Therefore, when we turn on the television or the radio (people still do that right?), is it any surprise that we are inundated with stories of murder, conflict, and tragedy? It is not the number of violent incidents that are on the rise, but the coverage of these events. As humans are mere reflections of their environment, it is inevitable that we will come to adopt the negative attitude encouraged by these stories. Surveys repeatedly show that Americans believe violent crime is on the rise even as the rates of crime have dropped precipitously. The negativity cycle is self-perpetrating: we are naturally inclined to pay attention to bad news, the media shows us negative news stories, and our attitudes become even more pessimistic. Over time, we adopt false belief that the world is becoming a worse place to live. Indeed, ask Americans what they think about the world, and they will say that the country is not improving. This cycle is a prime example of what is known as the representativeness bias.(This is also referred to as the representativeness heuristic where a heuristic means a rule of thumb that we adopt as a mental shortcut when we are forced to make a rapid decision). The idea is that humans judge the likelihood of an event based on how easily they can call to mind examples of the event. Therefore, when people are asked to judge the chance that they will be murdered or die in a terrorist attack, they vastly overstate the true probability because of all the attention the media gives to these extremely rare events. Violence, poverty, famine, plague, and war continue to decrease on a yearly basis while conversely, the coverage and saturation of these events grows ever more prevalent. The other major reason for the public unwillingness to believe in the overall improvement in the human condition is that people need to believe they are engaged in a great struggle. It gives our lives meaning if we think that we are overcoming unimaginable odds to achieve success. In the modern United States, we do not do battle against war, plague, and famine on a daily basis. Instead, we are forced to create a mental image of a terrible and frightening world so we can feel like we are triumphing in the face of extreme difficulties. Humans have been in a constant struggle for survival for 99% of our existence, and now that the pressures of starvation and conflict have been lifted from our daily life, we feel lost. To compensate, we generate a fictional representative of the world in which we are still locked in a fight to make it through each day. To accept the truth that the world is actually getting better means to give up this fantasy and admit that our lives are easier than ever before. It requires that we no longer have the defining scourges of war, plague, and famine to do constant battle against. (This raises the interesting possibility that we could replace the “Big 3” with something equally worthwhile such as solving climate change, developing genetic engineering, and creating beneficial artificial intelligence). The statement “may you live in interesting times” is often presented as a curse, but I believe that many people instead see it as a blessing. People want to experience monumental events during their lifetime, even when those events are monumentally negative. In short, we believe that the world is on the decline because it gives us something to argue about at the dinner table. How Should We Respond? What should we do now that we are aware of this privileged information? What steps can we take in our life to make sure that the trend of general improvement in the human condition continues? The critical part is that we do not sit back and relax now that we know humanity is on the right track. All of these improvements, from the reduction in the extent of famine, to the eradication of diseases has not come about automatically. These problems have been systematically addressed through a combination of science, medicine, and evolution of society’s collective morals. The solutions have been achieved through the work of millions of individuals who took the long term view rather than trying to maximize short-term rewards. We must carry on in this tradition by focusing on our strengths (science, medicine, arts) and suppressing our weakness (pessimism, the desire for vengeance). We have to realize that our efforts, both as individuals and as nations really do make a difference. Moreover, by improving the lives and increasing the prosperity of citizens in all nations, we benefit ourselves. By eradicating and controlling disease in developing countries, we immunize ourselves against a global pandemic. By donating food to countries currently struggling with shortages, we can gain valuable trading partners far in the future. And by working to prevent violence in all countries, we stabilize the entire world and keep our citizens safe (even safer than we already are). When we view the world as one connected community, we realize that improving the life of even one individual in another country raises the standard of living for all of humanity. One practical step you can take with this information is simply to spread it. The next time you hear a negative news story or a relative brings up a tragedy they heard, gently remind them that these stories only make it into the public consciousness because they are anomalies. Remind them that since 1990, the global extreme poverty rate has been cut in half. One billion less people live in extreme poverty today than did 25 years ago. Furthermore, the childhood mortality rate has decreased by 50% since 1990 thanks to advances in infant care, education, and access to trained medical experts. 86% of children born around the world today receive basic vaccines, the highest figure ever. The next time someone raises an objection to foreign aid, inform them that every dollar the United States invests in vaccination returns $44 in economic benefits to our economy. (Preceding facts are taken from Bill Gate’s 2017 annual letter). I could go on for pages, but I will leave that exercise for another article. Communicate to those around you that we have conquered war, famine, and plague, and are making progress on all fronts towards a more prosperous, equitable, and sustainable worldwide civilization. Talk to people, make them aware of these facts, and then work to make the statistics even better. When politicians campaign on promises of decreasing foreign aid and diverting it to the military, citing the threat of conflict, remind your neighbor that in fact, armed conflict is decreasing thanks in no small part foreign aid. Remind your friends and your family that we have less to fear than ever before and as a country need we should reflect that in our investment and priorities. And finally, stop being a cog in the fear-mongering news cycle. I took a sabbatical from the traditional news two years ago and I have not gone back since. I diverted more time to reading about what is actually going on in the world rather than what the media presented to me. Yes, I may not be able to participate in all the recent gossip or bemoan the most recent tragedy, but I can discourse at length on the polio eradication campaign and the massive improvements in global agriculture production. I will delve into the full impacts of my news-free lifestyle in a future article, but for now, I will leave you with this: I believe that switching off the news has made me a better informed citizen and more aware of both my individual biases and the collective biases of our society. I am more empathetic, less anxious about the world at large, and I have more meaningful conversations with my fellow humans once we get past the surface level stories that dominate the headlines. At the end of the day, the narrative we tell ourselves about the world we inhabit is the only thing that matters. Would you rather live in a tragedy, or in a triumphant story that reflects the reality of the 21st century? Will Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator","The Vanquishing of War, Plague and Famine",8,published,183,3891,0.004626060138781804,0,0,0,0,0,0
119,52,696.116014442257,20,https://medium.com/p/capstone-project-mercedes-benz-greener-manufacturing-competition-4798153e2476,0,None,2017-06-30 12:55:00,19.98,42,224,2017-06-30 12:00:00,"['Machine Learning', 'Python', 'Udacity', 'Kaggle']","Capstone Project: Mercedes-Benz Greener Manufacturing Competition Udacity Machine Learning Engineer Nanodegree Author’s Note: This is the report I completed for my Udacity Machine Learning Engineer Nanodegree Capstone Project. All work is original and feel to use/expand upon/disseminate. [Numbers in brackets are citations to the sources listed in the references section]. I. Definition Safety and reliability testing is a crucial step in the automobile manufacturing process. Every new vehicle design must pass a thorough evaluation before it enters the consumer market. Testing can be time-consuming and cost-intensive as a full check of vehicle systems requires subjecting the car to all situations it will encounter in its intended use. Predicting the overall time for a vehicle to pass testing is difficult because each model requires a different test stand configuration.[1] Mercedes-Benz has been a pioneer of numerous vehicle safety and technology features and offers a range of custom options for each model. Every possible vehicle combination must undergo the same rigorous testing to ensure the vehicle is robust enough to keep occupants safe and withstand the rigors of daily use. The large array of options offered by Mercedes means a large number of tests for the company’s engineers to conduct. More tests result in more time spent on the test stand, increasing costs for Mercedes and generating carbon dioxide, a polluting greenhouse gas. Efforts by Mercedes Benz and other automakers to improve the efficiency of vehicle testing procedures have mainly focusing on developing automated test systems.[2][3] An automatic test system eliminates the variability inherent in human behavior, is safer than allowing humans in the driver’s seat, and results in an overall more efficient evaluation process. The Mercedes-Benz “Greener Manufacturing” Competition hosted by Kaggle[1] pursues a related approach to optimizing the vehicle testing process by encouraging the development of a machine learning model that is able to predict the testing duration based on a particular vehicle combination. The stated goal of the competition is to reduce the time vehicles spend on the test stand which consequently will decrease carbon dioxide emissions associated with the testing procedure. Although the reduction in carbon dioxide may not be noteworthy on a global scale,4 improvements to Mercedes’s process can be passed along to other automakers or even to other industries which could result in a significant decrease in carbon dioxide emissions. Moreover, one of the fundamental tenets of machine learning is that the efficiency of current systems can be improved through the use of the massive quantities of data now routinely collected by companies. Kaggle is a website dedicated to that proposition where companies create machine learning competitions with a stated objective and provide the public a dataset to apply to the problem. Competitions such as those offered on Kaggle, or the X-Prizes,[5] have been demonstrated to spur innovation [6] and help attract individuals and teams looking to hone their skills, participate in cutting-edge challenges, and perhaps win a modest prize. For this project, I created a model to participate in the Greener Manufacturing competition. All required data for the Greener Manufacturing competition was provided by Mercedes-Benz. The dataset was collected from thousands of safety and reliability tests run on a variety of Mercedes vehicles. The objective of the Mercedes-Benz Greener Manufacturing competition is to develop a machine learning model that can accurately predict the time a car will spend on the test bench based on the vehicle configuration. The vehicle configuration is defined as the set of customization options and features selected for the particular vehicle. The motivation behind the problem is that an accurate model will be able to reduce the total time spent testing vehicles by allowing cars with similar testing configurations to be run successively. This problem is an example of a machine learning regression task because it requires predicting a continuous target variable (the duration of the test) based on one or more explanatory variables [7] (the configuration of the vehicle). This problem is also a supervised task because the targets for the training data are known ahead of time and the model will learn based on labeled data. The steps to solving the problem are as follows: 1. Download the Mercedes vehicle testing data from Kaggle. 2. Prepare the data to feed into the machine learning model. 3. Select an appropriate algorithm/method for the problem. 4. Optimize the model using the labeled training data. 5. Make predictions on the unlabeled testing data and submit to the competition. 6. Iterate on 3–5 to improve the model to an acceptable standard. Mercedes-Benz will implement the best-performing model into the vehicle design and manufacturing process to increase the overall efficiency of the testing procedure while maintaining high safety and reliability standards. The evaluation metric for the competition is the R^2 measure, known as the coefficient of determination. R^2 is a measure of the quality of a model that is used to predict one continuous variable from a number of other variables. [8] It describes the amount of variation in the dependent variable, in this case the testing time of a vehicle in seconds, based on the independent variables, in this case the combination of vehicle custom features, that can be explained by the model. It is often interpreted as the percentage of the variation in the targets that is explained by the features. Thus, an R^2 value of 0.6 indicates that 60% of the variation in the testing time could be explained by the variation in the vehicle set-up. The remaining 40% of the variance is either not captured by the model, or is due to lurking variables that have not been included in the data. The coefficient of determination is expressed mathematically [9] as where n is the number of instances (vehicle tests), x is the prediction for the instance (the predicted test time in seconds), and y is the known truth value for the instance (the known testing time in seconds). An R^2 of 0 can be achieved by simply drawing a straight line through the data at the mean value of the target variable. The best possible coefficient of determination value is 1.0 which would indicate that the model explains all the of the variance in the response variable in terms of the input variables. The coefficient of determination is the appropriate metric for the problem because the goal, as defined by Mercedes-Benz, is to create a model that is able to determine the testing time of a vehicle. Mercedes is interested in why different vehicles take different times to test and how this can be represented in a machine learning model. Therefore, the algorithm that best explains the variation in testing times will be the optimal machine learning model for the task. Moreover, the evaluation metric used to determine the winner of the competition is the coefficient of determination. The coefficient of determination is a common metric used in regression tasks and is implemented in Scikit-Learn, where it is the default evaluation score for a regressor.10 The coefficient of determination for the training data can be ascertained during the model evaluation phase because the training data includes the known target values; however, R^2 for the testing data can only be found by submitting the predictions from the model to the competition. Five predictions are allowed per participant per day, which limits the amount of possible evaluation on the test set. II. Analysis Two data files are provided by Mercedes-Benz for use in the competition: a training dataset, and a testing dataset. Both files are provided in the comma separated value (CSV) format and are available for download on the Kaggle competition data page. [11] The training and testing data both contain 4209 vehicle tests obtained by Mercedes for a range of vehicle configurations. The training data also contains the target variable, or testing duration in seconds, for each vehicle test. No target is provided for the testing data as the testing durations are known only by Mercedes and are used to determine the winner of the competition. Each vehicle test is defined by the vehicle configuration, which is encoded in a set of features. Both the training and the testing dataset contain 376 different vehicle features with names such as ‘X0’, ‘X1’, ‘X2’ and so on. All of the features have been anonymized meaning that they do not come with a physical representation. The description of the data does indicate that the vehicle features are configuration options such as suspension setting, adaptive cruise control, all-wheel drive, and a number of different options that together define a car model. There are 8 categorical features, with values encoded as strings such as ‘a’, ‘b’, ‘c’, etc. The other 368 features are binary, meaning they either have a value of 1 or 0. Each vehicle test has also been assigned an ID which was not treated as a feature for this analysis. An image of a representative subset of the training data is shown below: A brief summary table of the testing and training dataset follows: Although the data has been cleaned by Mercedes prior to being made available for the competition and therefore there are no errors or missing entries, the data may still contain outliers with respect to vehicle testing time. These outliers could be valid data but are extreme enough to affect the performance of the model. The definition of a strong outlier is expressed mathematically [12] as: where the first quartile is the value that 25% of the numbers fall beneath, the third quartile is the value that 75% of the numbers fall beneath, and the interquartile range (IQR) is the different between the third and first quartile. Given this definition of an outlier, there were 4 vehicle tests that classified as extreme on the upper end of the vehicle testing time. The Exploratory Visualization section further examines the outliers within the dataset. In order for the machine learning model to process the categorical variables, they must be one-hot encoded. [7] This means that the unique categories contained in the categorical variable are transformed into a set of new variables. Each instance is assigned a one for the new variable corresponding to its original categorical variable and a zero in all other new variables. This is best illustrated by Figure 2. One-hot encoding transform all of the features to binary values. After both the testing and training data had been one-hot encoded, the two datasets were aligned in order to eliminate any features that were present in one dataset but not in the other. This is necessary because the model would not know how to respond it if encountered a feature in the testing set that it had not seen in the training data. After one-hot encoding and alignment of the training and testing data there were a total of 553 binary features in both datasets. The first aspect of the data to investigate was the target variable, the vehicle testing duration. The plot below shows all of the testing times arranged from shortest to longest from left to right. The four outliers can be seen at right with the highest value shown in red. Plotting the data demonstrates the extreme nature of this data point more effectively than examining the numbers. The testing times can also be visualized as a histogram. For this visualization, the outlying data point has been removed in order to better represent the majority of the data. There are several conclusions to be drawn from this histogram: · The majority of test durations are between 90 and 100 seconds · There are peaks in testing times around 97–98 seconds and near 108 seconds. · The testing times are bi-modal, with two distinct peaks. · This data is positively skewed, with a long tail stretching into the upper values. [14] Based on the target variable visualization, I concluded that I was justified in removing one outlier, the training data point with the greatest testing time. Although it is a valid data point, it is extreme enough that it will adversely affect the performance of the algorithm (as shown in the Evaluation section). The second half of the data exploration phase focused on the vehicle features. I began with the categorical variables by plotting the testing durations versus the unique category in boxplots. [8] This was done for each of the categorical variables on the training data. There are no discernible trends within the categorical variables with respect to vehicle testing duration. Moreover, it is difficult to draw intuitive conclusions from this exercise because the features have no physical representation. Further exploration of the binary features also proved inconclusive. The plot below shows the distribution of the binary features in both the training and testing set. The zeros, indicating that the car did not have the feature, are shown in orange, and the ones, representing that the car had the option, are shown in blue. With this particular problem, it was difficult to draw any conclusions from the visualizations of the explanatory variables. Figure 6 demonstrates that some of the binary features are shared by all of the vehicles and some are shared by none. Looking further into the numerical data for the testing set, I identified 12 binary variables where the values were either all 1 or 0. As there is no variation in these features, they have no predictive power and therefore, these 12 features may be removed. The data exploration has underscored the need for dimensionality reduction. Fortunately, one common technique for reducing the number of features, Principal Components Analysis (PCA), is an unsupervised technique that does not require an understanding of the physical representation of the features. [7] PCA played a crucial role in reducing the input number of features into the algorithm and as discussed in the Data Preprocessing section. The conclusions from the data exploration are as follows: · One outlier, as determined by vehicle testing duration, needs to be removed · 12 binary variables encode no information and should be removed · There are no noticeable trends within the categorical or binary variables · Unsupervised dimensionality reduction (PCA) will need to be performed on the data The final model created for this project combines a number of different machine learning techniques. At the highest level, the final model is a weighted vote between two intermediate models. The first intermediate model is an ensemble method known as extreme gradient boosting [15] that works by building numerous simple regressors on top of each other to create a final strong regressor. The second intermediate model is a stacked model in which another ensemble method, an extra trees forest [16], builds on top of a regularized linear regression. The architecture is shown below in Figure 7. The final model was derived based upon research from books, the Kaggle discussion forum, and papers discussing the benefits and drawbacks of various models. Nearly all of the top-performing models in the Kaggle competition used the XGBoost method averaged with or stacked on top of other ensemble methods. This is where I derived the top level architecture. From that point, it was a matter of determining which models best complimented one another. Based on the benchmark model, I saw that a Linear Regression performed quite well, but tended to overfit the training data. Therefore, my stacked model incorporated a Linear Regression with regularization to reduce the amount of overfitting. The choice of an Extra Trees forest was made by determining the performance of ensemble methods stacked on top of the Linear Regression. Based on the individual performance of each intermediate model, it was evident that combining the two predictions in a weighted average would help to improve the robustness of the model. The full process for designing the algorithms and optimizing them for the problem is discussed in the Implementation section. To explain how the model functions, it is best to start at the lowest level, the linear regression with elastic net regularization. The full workings of a linear regression are explained in the Benchmark Model section, and elastic net is one of many methods to regularize a linear model. Regularization can be thought of as constraining a model’s complexity in order to prevent overfitting on the training data. [7] Overfitting means that the model “memorizes” the training data leading to poor generalization on new instances the model has not seen before. A model that overfits the training data is said to have high variance and low bias. Regularization can reduce the variance of a model by reducing the degrees of freedom within the model and can be performed on a linear model by reducing the magnitude of the model parameters. Elastic Net regularization applies a penalty to each model parameter in the cost function which encourages the model to select smaller parameters during training. [7] Elastic Net is one of a number of regularization functions which differ in the type of penalty applied during training. The idea of a stacked model [17] is relatively simple: the outputs (predictions) of the first model are used as inputs into the second model. In this case, during training, the predictions (vehicle testing durations) from the Linear Regression with regularization are fed as inputs into the Extra Trees Forest regressor along with the known target labels. The Extra Trees regressor thus learns to make predictions based on the prediction from the previous model and the known true values. The Extra Trees regressor is what is known as an ensemble method. [7] It works by combining multiple simpler models into one complex model, thereby reducing the variance from a single model and generating better predictions. The Extra Trees regressor is built of many individual Decision Tree algorithms. A Decision Tree creates a flowchart (tree)of questions (generally in the form of thresholds for the features) during training to split the data points into ever smaller bins, each with a different predicted target value. During testing, the Decision Tree moves through the flowchart one node at a time and places the data point in the appropriate bin based on the features of the instance. A simple model of a decision tree is presented below. In this case, the x terms would be features, the θ terms would be thresholds established during training, and the letters would be the final predicted target. An Extra Trees regressor trains many decision trees and takes a vote of all the trees to determine the final prediction for each instance. The stacked model forms one half of the final model. The other half is composed of another ensemble method, called Extreme Gradient Boosting. The principal behind extreme gradient boosting is the same as that of the Extra Trees regressor except in this case, the individual decision trees are trained sequentially and each tree is trained on the previous tree’s residuals, or the difference between the tree’s prediction and the true value. By training on the residuals, or errors in the predictions, each successive tree becomes better at predicting the most difficult to predict instances (those with the largest residuals) and over time, the ensemble becomes more powerful than a single classifier. The final prediction is a weighted average over all of the individual predictions, with the more confident trees receiving a higher weighting. Gradient boosting has become one of the algorithms of choice for machine learning competitions because of its predictive power. Decision trees form the basis for both ensemble methods because they are relatively quick to train and have well-established default parameters. At the top level, the final model takes an average of the predictions from each intermediate model. The weighting given to each model can be determined through an iterative process of adjusting the weighting and determining the model’s performance. During training, the preprocessed data will be passed to both intermediate models. The Extreme Gradient model will learn the thresholds for each leaf in the forest of decision trees it trains. The stacked model will first pass the training data through the linear regression, where the model will learn the parameters (weighting) to apply to each feature, then the linear regression will make a prediction for each training point and pass that on as input to the Extra Trees Forest along with the known target values. The Extra Trees regressor will likewise form its own forest of decision trees with the thresholds for each split determined during training. When testing, each new instance will be passed to both intermediate models. In the case of the stacked model, a prediction will be made by the linear regression and then the Extra Trees Regressor will make a prediction based on the output from the linear regression. The Gradient Boosting model will generate its own prediction. The overall prediction will then be an average from the two intermediate models. The architecture of the model is relatively complex, but the fierce (yet friendly) competition on Kaggle encourages the development of unique models in order to achieve slight performance increases. A benchmark model for this regression task is a linear regression. Linear regression is a method for modeling the relationship between a target variable (also called the response variable) and one or more independent, explanatory variables. [7] When there are more than one explanatory variables, this process is called multiple linear regression. In this situation, the target variable is the testing time of the car and the explanatory variables are the customization options for the car. A linear regression model assumes that the response variable can be represented as a linear combination of the model parameters (coefficients) and the explanatory variables. The benchmark model in this situation is a basic linear regression that predicts the target for an instance based on the instance’s features multiplied by the model’s parameters (coefficients) and added to a bias term. This is shown as an equation below: where y is the target (predicted) variable, the c terms are the model parameters (coefficients) and the x terms are the feature values. When the model trains, it learns the set of parameters, and when it is asked to make a prediction, it takes the instance’s feature values and multiplies by the parameters to generate a prediction. The default method for Linear Regression in Scikit-learn is ordinary least squares, where the model calculates parameters that minimize the sum of the squared difference between the prediction and the known targets. [19] Initially, I used the entire set of raw features for creating a benchmark model. However, this resulted in a negative coefficient of determination, indicating that the model was performing worse than if it had drawn a constant line at the mean of the data. To develop a more reasonable benchmark, I decided to reduce the dimensionality of the data. I found the Pearson’s Correlation Coefficient, or R, between all of the individual features and the target variable. The correlation coefficient describes the strength and direction of the linear relationship between an independent and dependent variable. [20] A value of 1.0 would indicate a perfectly linear positive relationship and thus the Pearson coefficient can be used as one method to determine which features will be useful when predicting a target variable. Once I had identified the correlation coefficients, I trained a number of linear regressors using a range of the most positively correlated features. The top-scoring model used the 47 top correlated features. I selected this as the benchmark model because of the reasonable R^2 value which would take some effort to better. The final benchmark model scored a 0.56232 coefficient of determination on the testing set using five-fold cross validation (cross validation is discussed in the Implementation section). The predictions made on the testing set scored a 0.54549 coefficient of determination when submitted into the Kaggle competition. As of June 25, this score was only good enough for 2400 on the competition leaderboard [21] out of roughly 3000 competitors. The benchmark serves as a comparison for the final model. In order to declare the project a success, the final model will need to significantly outperform the benchmark model on the R^2 measure. The final model should score higher both on the training dataset, using five-fold cross validation, and on the testing set, when the predictions are submitted to the competition. III. Methodology There are several interrelated steps in the data preprocessing workflow [7]: · Obtaining and cleaning data · Data preparation · Feature engineering/ Dimensionality reduction The first step requires obtaining the data in a usable format. For this project, no “data wrangling” was required because the data was already provided in an organized format on the Kaggle competition page. Data preparation mainly consisted of one-hot encoding the categorical variables, removing the outliers, and removing features that do not encode any information. In the training data, there was a single outlier with a testing time far greater than that of the next highest value. This point was removed so that it would not skew the predictions of the regression. There were also 12 binary features with values that were either all 1’s or 0’s. These variables (they cannot even be called variables because they do not change, but should be referred to as constants) do not contain any information and were removed. After aligning the training and testing datasets to ensure they contained the same number of features, I was left with a training set of 4208 data points with labels and 531 features. The testing set has 4209 data points and 531 features without labels. The final step in the data preprocessing pipeline is feature engineering. This encompasses a range of operations including adding new features or removing features to reduce the dimensionality of the data. Based on the data exploration, I decided that I would not be able to create brand new features by combining existing features because I did not observe any trends within the variables. Therefore, I concentrated on reducing the number of features. One implementation of this is documented in the Benchmark Model section where I used a subset of features selected by ranking the variables in terms of correlation coefficient with respect to the target variable. Another method involves selecting the most relevant features for a given algorithm. Within Scikit-learn, there are a number of algorithms that will display the feature importances after training. Both of the ensemble methods explored in this report have a feature importances attribute. [22] However, I received substantially different results when comparing the weights given to each feature for both models. Consequently, I decided that selecting a smaller number of features based on feature importances was not the correct decision. After some experimentation, the dimensionality reduction method that I did implement is an unsupervised technique known as Principal Components Analysis (PCA). [23] PCA creates a set of basis vectors that represent the dimensions along which the original data has the greatest variation. In other words, the original data is projected onto a new axis with the greatest variance in order to reduce the dimensions while preserving the maximum amount of information. The first principal component explains the greatest variance in the data, while the second explains the second most and is orthogonal to the first and so on. A common technique is to keep the number of principal components that explain a given percentage of the variance within a dataset. After some trial and error, I decided to retain the Principal Components that explained 95% of the variation in the data. The meant keeping the first 140 principal components as can be seen in the graph below: The 140 principal components selected explain 95.31% of the variance in the data. The PCA algorithm was trained on the training features and then the training features are transformed by projecting the original features along the first 140 principal components. The testing data features are transformed by the already-trained PCA model. Applying PCA considerably reduces the number of dimensions within the data and should lead to improved performance by reducing the noise in the data. All of the code for this project was written in Python 2.7 and executed in an interactive Jupyter Notebook. The main Python library used for the project was Scikit-learn, a popular machine learning framework. Although the dataset is not large by machine learning standards, training the various models and performing hyperparameter optimization on my personal laptop in a timely manner was not feasible. Therefore, I ran the Jupyter Notebook on a Google Cloud server. The development of the final regression model can be split into two sections. I. Model Selection II. Hyperparameter tuning (discussed in the Refinement section) The first stage involved testing and evaluating a selection of algorithms. Different algorithms are suited for different tasks, and the best way to determine the correct algorithm is to try them out on the dataset. I began this process as usual by referring to the Scikit-learn algorithm selection flowchart. The relevant section of the chart for choosing a regression algorithm is shown below. As this was a regression task with fewer than 100,000 samples (training data points), there were several starting options. My approach was to evaluate a number of the simple algorithms recommended, and if none were sufficient, I would proceed to ensemble methods. The results of evaluating a number of simple classifiers are shown below. As is demonstrated in the table, the only simple model that outperformed the benchmark was the Support Vector regressor with an rbf kernel. All of the simple models were evaluated with the 140 principal components from the data preprocessing and were created with minimal hyperparameter tuning. The objective at this stage was simply to evaluate the models to determine if any were acceptable for the problem. I expected that I would have to move on to ensemble methods, but in some machine learning tasks, a simple model is able to capture all of the relationships within the data and can perform very well. Examining the performance of the simple regressors, it was clear I would have to employ a more sophisticated model to significantly improve on the benchmark. The next logical place to turn, as evidenced by the flowchart and on the discussion boards for the Kaggle Competition, [25] was ensemble methods. Ensemble methods work to create a model by combining multiple or building upon many simple models. [26] There are two commonly identified classes of ensemble methods: · Bagging · Boosting Bagging [27] (short for bootstrap aggregating) uses the “wisdom” of the crowd to create a better model than an individual regressor. If training a single model is like asking one business analyst to make a prediction given an entire dataset, bagging is like taking the average forecast of an entire roomful of analysts, each of whom have seen a small part of the data. For example, an Extra Trees regressor trains numerous Decision Trees, with each tree fit on a different subset of the data. In the bagging technique, these subsets are randomly chosen, and the final prediction is attained by averaging all of the predictions. The implementation used in this project set the Bootstrap parameter to false meaning that the subsets are not replaced after being used for training, a technique known as pasting [28]). Boosting is based on a similar idea, except each successive simple model is trained on the most “difficult” subset of the data. The first model is trained on the data, and then the data points with predictions that are the furthest away from the true values (as determined by the residuals, or the difference between the prediction and the known target value) are used to trained the second model and so on. Over time, the entire ensemble will become stronger by concentrating on the hardest to learn data. The final prediction is a weighted average of all the individual regressors with the most “confident” trees given the highest weighting. One boosting technique that is very popular for machine learning competitions is known as XGBoost, [29] which stands for extreme gradient boosting. Both of the ensemble methods used in this project are based on many individual Decision Trees. Decision Trees are a popular choice to use in an ensemble because of their well-established default hyperparameters and relatively rapid training time. Studies [26][30] have shown ensemble methods are more accurate than a single classifier because they reduce the variance that can arise in a single classifier. Bagging was observed to almost always have better performance than a single model, and boosting generally performed better depending on the dataset and the model hyperparameters. The final model used both an Extra Trees and an Extreme Gradient Boosting ensemble method in Scikit-learn. [31] The Extra Trees algorithm was part of a stacked model built on top of a Linear Regression with Elastic Net Regularization. There were three major problems that I encountered while developing the model: · Algorithm training and predicting time · Diminishing returns to algorithm optimization · Limited number of testing opportunities The first problem arose when I was running Grid Search to determine the best parameters for the algorithms. Many of the parameter grids contained over 100 models to test, and because I was using five-fold cross-validation, the total time to carry out the Grid Search was a significant obstacle. I soon realized I would have to invest time into learning how to use cloud-based computing services. I was successfully able to figure out how to run a Jupyter Notebook on a Google Cloud server, and I am sure that this skill will be invaluable moving forward as the size of datasets and the complexity of algorithms continue to increase and deep learning becomes commonplace in machine learning implementations. Implementing basic algorithms in Scikit-learn is relatively simple because of the high level of abstraction within the library. The actual theory behind the algorithms is hidden in favor of usability. Moreover, the default parameters are given reasonable values to allow a user to have a usable model up and running quickly. [32] In this project, I found it relatively simple to get the baseline model functioning with a decent coefficient of determination. However, as in most machine learning projects, the law of diminishing returns soon crept in with regards to hyperparameter tuning. [33] Improving the performance of any model even a few tenths of a percentage point required a significant time investment in tuning the algorithm. The full effect of these diminishing returns can be seen in the complexity of the final model. As the development of the model progressed, it became difficult to determine what steps to take to improve the performance. Mostly, I tried to combat this by using Grid Search to find the optimal parameters. This problem was an example of the effectiveness of data, [34] a phenomenon where the quality and preprocessing of the training data has much greater importance to the final model performance than the tuning of the algorithm. Finally, the structure of the Kaggle competition allowed for minimal evaluation on the testing set. The Kaggle competition is set up to only allow users to make five submissions per twenty-four hours. This meant that it was difficult to compare the relative performance of models as they were developed on the testing set. I repeatedly would test the algorithms using five-fold cross validation on the training set, but these results were not always in alignment with performance on the testing set when I would make my submissions. The models routinely scored lowered on the testing set than on the training set as expected, but there was not always a correlation between R^2 on the training and on the testing set. For example, some intermediate models scored a higher coefficient of regression on the training set but then scored lower on the testing set. The main way that I overcame this problem was by testing as much as possible on the training set using cross-validation, and then making submissions only on the models in which I had the most confidence. Once the structure of the final algorithm had been selected, the next step was to optimize the model. This mainly consisted of what is known as hyperparameter tuning. (The word ‘hyperparameter’ refers to an aspect of the algorithm set by the programmer before training, such as the number of trees in a Random Forest, or the number of neighbors to use in a Nearest Neighbors implementation. The word ‘parameters’ refers to the learned attributes of the model, such as the coefficients in a linear regression).7 I like to think of this step as adjusting the settings of the model to optimize them for the problem. This can be done manually, by changing one or more hyperparameters at a time and then checking the model performance. However, a more efficient method for parameter optimization is to perform a grid search. There are two primary aspects to this method in Scikit-learn with the GridSearchCV class: · Hyperparameter search · Cross validation The idea of grid search is to define a set of hyperparameters, known as a grid, and then allow an algorithm to test all of the possible combinations. Most of the default hyperparameters in Scikit-learn are reasonable, and my methodology when performing a grid search was therefore to try values for parameters both above and below the default value. Depending on which configuration returned the best score, I would then perform another search with the hyperparameters moving in the direction that increased scores. The CV in GridSearchCV stands for cross validation, which is a method to prevent overfitting on the training data. The concept behind cross-validation is that the data is first split into a training and testing set. Then, the training data is split into ’n’ smaller subsets, known as folds. Each iteration through the cross validation, the algorithm is trained on n-1 on these subsets and evaluated on the nth subset. The final score of the algorithm is the average score across all of the folds. This prevents overfitting whereby the model learns the training data very well, but then cannot generalize to new data that it has not seen before. After n iterations of cross-validation have been completed, the model is evaluated one final time on the test set to determine the performance on a previously unseen set of data. Cross-validation is combined with Grid Search to optimize the algorithm for the given problem while also ensuring that the variance of the model is not too high so that the model can generalize to new instances. GridSearchCV allows a wide range of models to be evaluated more efficiently than manually checking each option. The final hyperparameters for the constituent models that make up the final model are presented in Table 3. Any hyperparameters not specified in the table were set to the default value in Scikit-learn. GridSearchCV was performed on both intermediate models (the XGBoost model and the stacked model of an Extra Trees regressor stacked on a regularized Linear Regression) individually. The main hyperparameters adjusted for the XGBoost model were the number of decision trees used, the maximum depth of each tree, and the learning rate. The number of trees is simply the number of trees built by the algorithm; the maximum depth of each tree controls the number of levels in each tree and can be used to reduce overfitting or increase the complexity to better learn the relationships within the data; and the learning rate is used in gradient descent to determine how large a step the algorithm takes at each iteration. Compared to the defaults, the learning rate was decreased, the maximum depth of each tree was decreased, and the number of trees was increased. The main parameters adjusted for the other intermediate model were the l1 ratio and tolerance for the Linear Regression and the maximum features, minimum number of samples per leaf, and the number of estimators for the Extra Trees Regressor. The l1 ratio for Elastic Net controls the penalty assigned to the model parameters (it specifies the blend between Ridge and Lasso Regression) and the tolerance is a minimum step size for the algorithm to continue running. In terms of the Extra Trees regressor, the maximum features is the number of features each tree considers, the minimum number of samples per leaf is the minimum number of data points that must be in each leaf node, and the number of estimators is the number of decision trees in the forest. The l1_ratio was increased for Elastic Net which has the effect of increasing the penalty (if the ratio is 1.0, then Elastic Net is equivalent to Lasso Regression which tends to eliminate the least important features by setting the weights close to zero). The maximum number of features for the Extra Trees Regressor was decreased which means that the model did not need to use all the features. Both of these adjustments suggest to me that 140 principal components may have been too many because both models performed implicit feature selection through the hyperparameters. However, keeping too many principal components and then having some not used by the model is preferable to not having enough features and therefore discarding useful information. The final and most critical hyperparameter was the weighting for the averaging between the two intermediate models. I determined that the best method was to try a range of ratios and submit the resulting predictions to the competition. The final model placed more weight on the XGBoost predictions. However, increasing the contribution from the XGBoost regressor reached a point of diminishing returns above 0.75. Averaging the predictions from the XGBoost model and the stacked model resulted in a significant improvement in the coefficient of determination compared to each model individually as discussed in the next section. IV. Results The final model was optimized through a combination of Grid Search with cross validation and manually adjusting the final weighting attributed to each of the intermediate models. The Python code for the final model and all of the model hyperparameters can be viewed in the Appendix section of the paper. Both the stacked model and the XGBoost model were tuned using Grid Search cross validation with the training set. The relative weighting of each intermediate model was determined by varying the ratio and submitting the predictions made on the test set to the Kaggle competition. Although I am usually opposed to manually performing an operation, this was the only option for optimizing the weighting for the test data. I could have optimized the weighting for the training data, but as can be seen in the results tables, the coefficient of determination score on the training data did not necessarily correlate with that on the testing data. Moreover, through linear interpolation, it was possible to find the optimal weighting for the test data with a minimal number of submissions. The final model accomplishes the objective of the problem as stated by Mercedes-Benz. The model is able to explain 56.605% of the variance in the testing data which means that it can account for more than half of the variation in vehicle testing times based on the vehicle configuration. Although this percentage may seem low in absolute terms, in relative terms, it is reasonably high. The top scores on the Kaggle leaderboard21 as of June 25 are near 0.58005 indicating that there is an upper limit to the explanatory power of the data provided by Mercedes-Benz. No model will ever be able to capture all of the variance with the provided data, and if anything, this competition demonstrates that if Mercedes wants to improve predictions further, it will need to collect more and higher quality data. The performance of any machine learning model is limited by the quality of the data (again underscoring the “Effectiveness of Data”) [34] and in this problem, the data cannot to explain all of the difference in vehicle testing times. The final results of the model on the training and testing data are shown below in Table 4: There are number of important conclusions to be drawn from these metrics. The first is that the stacked model performs better on the training data but worse on the testing data than the XGBoost model. This suggests that the stacked model is overfitting the training data and the XGBoost model is better able to generalize to unseen data. In other words, the XGBoost model has a lower variance and higher bias than the stacked model.7 Therefore, averaging the two models means that the weaknesses of one are partly cancelled out by the strengths of the other. The final model does not perform as well on the training data as the stacked model, but it significantly outperforms both intermediate models on the testing data. The averaging of the two intermediate models is an acceptable solution to the problem because it leads to a model that is better equipped to handle novel vehicle configurations. The final model is also more robust to changes in the input because it is an average of two different models. In order to determine the sensitivity of the final model, I manipulated the training inputs and observed the change in coefficient of determination on the testing and training set. I trained with and without the outlier using 140 principal components, and then altered the number of principal components and again recorded the coefficient of determination. The results are presented in Table 5. Across all training inputs, the R^2 score is very consistent. The model is therefore robust to variations in changes in the training data. If the results were highly dependent on the training data, it would indicate that the model is not robust enough for the task which is not the observed situation. Final results from the model can be trusted as indicated by the coefficient of determination score on the testing set. Although the model does not explain all of the variation in vehicle testing times, it explains more than half the variance and performs in the top 25% of the 3000 models submitted to the competition. The model would not be able to predict vehicle testing times for other companies because it depends on the precise features measured by Mercedes. However, given the problem objective of predicting Mercedes-Benz vehicle testing durations based on vehicle configuration, the model is useful. The model outperforms the baseline on all relevant measures. The comparison between the baseline model and the final model are shown in Table 6. The final model outperforms the baseline model by 4.86% in terms of coefficient of determination on the training set and by 3.95% in the same metric on the testing set. While these numbers may not seem large in absolute terms, they represent a significant improvement. This can be seen in the jump in leaderboard spots from the baseline to the final model. The baseline model placed near the bottom of the competition leaderboard at the 5th percentile, while the final model placed at the 75th percentile as of June 25. These results again show the diminishing returns of model optimization and the need for more quality and quantity of data. Even with a vastly more complex model, the overall performance did not vastly outperform the baseline and if Mercedes-Benz wants to obtain a better model, it will need to concentrate on obtaining more data. Nonetheless, a 4% improvement over the baseline could represent millions of dollars saved for a company that tests tens of thousands of cars on a yearly basis. In conclusion, the final model accomplishes the problem objective more successfully than the baseline and would improve the Mercedes-Benz vehicle testing process if implemented. As there is diminishing returns with respect to model optimization, additional improvements in the problem realm will only come from using more data. V. Conclusion In order to visually demonstrate the benefit of averaging models, I graphed the difference between the known target values and the predictions made by the stacked intermediate model, the XGBoost intermediate model, and the combined model. The result is shown in Figure 11. This figure clearly illustrates the benefit of combining predictions from different models. The R^2 measure for the stacked model on the training data was much higher than that of the XGBoost model suggesting the stacked model overfits the training data. This can be seen by comparing the top graph of Figure 11 to the middle graph as the spread of differences between the known target values and the predicted target values is greater for the XGBoost model. The XGBoost model therefore has greater bias and less variance than the stacked model. Subsequently, the XGBoost model does not overfit the training data to the same degree as the stacked model, but it also might ignore some of the underlying relationships between the features and the target. While the stacked model does overfit to an extent, averaging in the prediction from the stacked model resulted in a better evaluation metric on the testing set. This indicates that the stacked model may be capturing a correlation on the training data that the XGBoost model did not incorporate while training. Overall, the final model is better able to handle new data because each weakness of the intermediate models is partially cancelled by averaging their predictions. The second summary visualization also displays the benefits of combining the predictions from two different models. Figure 12 shows the individual models’ and the final model’s prediction on the training data (graphed against the assigned index). The primary takeaway from this plot is that the stacked model has greater variance in its predictions on the new testing data in much the same way that it did on the training data. The spread of the stacked model predictions is much greater than that of the XGBoost model with the final model test set predictions nestled between those of the two intermediate models. Consequently, the final model is able to outperform both intermediate models. This visualization shows that combining models is beneficial and suggests that one possible method for further improvement would be to average in the predictions from additional different models. Each model will have its own biases and variance, but, much as averaging the predictions of a group of people is more accurate than that of a single individual, combining predictions from multiple machine learning model produces a higher-performing final model. This project represented the implementation of a typical machine learning workflow: 1. Data Exploration and Preparation 2. Data Preprocessing 3. Model Selection 4. Model Optimization 5. Model Testing and Evaluation 6. Reporting of Results Completing a project based on a Kaggle competition had a number of advantages as well as several disadvantages. The biggest benefit is the availability of a clean dataset. Typically, machine learning projects will involve a substantial time investment procuring data and wrangling it into a usable format. That was not the case for this project because Mercedes-Benz made both the training and testing data available for download in a convenient CSV format. The first real step in this project was therefore data exploration. One of the major difficulties of this project was the anonymity of the data. No variable was given a physical representation (except for the target, the vehicle testing time) and therefore it was impossible to have an intuitive sense of the problem. This difficulty in interpreting the data influenced my decisions when it came time to preprocess the data. I decided to perform principal components analysis because it is an unsupervised technique that does not require an understanding of what each feature means. There were a number of options I could have explored, but for the final model, I decided that a PCA retaining 95% of the variance in the data was an acceptable procedure. The second major difficulty of the project was deciding on the correct model. There are many more machine learning models than I would have thought possible at the onset of this project! I quickly realized that simple models such as a Support Vector Machine or a single Decision Tree were not going to be able to seriously compete. I therefore adjusted my approach and began researching ensemble methods, both in books, and in the Kaggle discussion forums. I read through many code examples that helped influence my final decision to go with an averaged model between two different models (with both intermediate novels incorporating ensemble methods). The final model I selected ended up being more complicated than I had initially thought, but it was justified in terms of the testing performance and the robustness offered by combining two different models. Optimizing the model was not overly difficult, but it did take a significant amount of research into the algorithms themselves and the meaning of various hyperparameters. I am grateful to the Scikit-learn community both for building such a user-friendly library and for providing the documentation needed to improve the models. Grid Search with cross validation proved to be an effective technique for bettering the algorithm scores and will always be a technique that I can rely upon when I do not intuitively know what the correct settings should be. The testing of the model also proved to be a challenge because of the limited opportunities to try out models on the testing data. This was addressed mainly by using cross-validation numerous times on the training set, but those scores did not necessarily translate well to the testing set. This was the largest drawback from the Kaggle competition structure and I hope that Mercedes will release the targets for the testing set at the end of the competition to allow me (and the machine learning community) to learn how the models could have been better designed. Finally, the report itself has been a thoroughly useful experience. It helped to clarify my thoughts and encouraged me to understand my model in greater detail so I could communicate it to others. I know this is the most crucial aspect of any machine learning project because a model is only useful if its results can be applied. This almost always means involving other people and being able to explain how the model functions so others can use and improve upon the model. Moreover, I enjoying competing in the Kaggle competition because it gave me a chance to ask questions and interact with a community with a collective experience level far greater than any one individual can ever hope to acquire. The final model is more complex than my expectations, but I believe it is appropriate for the problem. On future projects, I will approach the problem with the idea from the outset that a complex model might be required (although if the data is good enough, a simple model may be adequate) and I now have a better idea of what this type of framework looks like. I do not know if this exact model will generalize well to different problems, but the basic ideas behind it will. This project showed me the power of ensemble methods, particularly the XGBoost gradient boosting algorithm, and also the benefits of combining different models so that the weaknesses of one are cancelled by the strengths of the other. Overall, I thoroughly enjoyed this project and become much more involved with the community surrounding the competition than I had expected. I am eager to tackle more machine learning projects and it is encouraging to be in the midst of a rapidly-evolving field. The largest area for improvement is feature engineering. Most of the competitors at the top of the leaderboards used similar algorithms (some form of XGBoost stacked with other ensemble methods), and minor differences between scores were mostly the result of clever feature manipulation. [35] There were many approaches that were taken, including techniques such as Independent Components Analysis, Gaussian Random Project, Sparse Random Projection, and Truncated Singular Value Decomposition. I used only Principal Components Analysis because that was the technique with which I had the greatest familiarity. This is a poor excuse for not exploring other options, and that is my only regret with this project. I got too caught up with developing the model when I should have put greater emphasis on engineering the features. One common approach I saw among other competitors was adding the principal components to the existing features and then training on all of the features rather than reducing the number of features. This works because algorithms such as Random Forest and XGBoost learn which features are important during training by themselves, so it is not the job of the programmer to make that decision. My reduction of features was justified because I was still able to achieve a high coefficient of determination, but in the future, I may think about expanding the number of features rather than automatically trying to reduce them. Now that I know how to use Google Cloud, computing resources is not a significant impediment and I do not necessarily need to remove features in order to speed up training time. Moving forward, I will spend more time on feature engineering and less on perfecting the algorithm. Another area that I would like to explore is deep learning, particularly with neural networks. [36] Although neural networks generally require many more training instances (as in millions rather than thousands), perhaps a properly designed neural network would be better able to learn the underlying relationships in the data. There were several competitors who tested out neural networks, but the leaders all used some combination of traditional ensemble methods. In the future, I might try a neural network for the practice and then implement one if I am using a much larger dataset in an industry setting. In conclusion, I know that my model is not the best out there (as evidenced by my place on the leaderboard). I will continue to improve my model until the competition ends by redesigning the set of features and further adjusting the model hyperparameters. I am pleased with my performance and even more satisfied with the amount of knowledge that I was able to learn in throughout the process. In the future, I will be more prepared to spend a greater percentage of time on feature engineering and I know the level of model complexity that is required to perform well in these sorts of competitions. The Mercedes-Benz Greener Manufacturing Competition was difficult enough to challenge me but was also manageable for one getting started in the field. As an introduction to a complete machine learning project, I could not have picked a better problem to encourage me to continue moving forward in a field with great potential for improving our current systems. VI. References [1]. “Mercedes-Benz Greener Manufacturing Overview”, Kaggle.com, 2017. [Online]. Available: https://www.kaggle.com/c/mercedes-benz-greener-manufacturing. [Accessed: 20- Jun- 2017]. [2]. H. Schoner, S. Neads and N. Schretter, “Testing and Verification of Active Safety Systems with Coordinated Automated Driving”, 2017. [Online]. Available: https://pdfs.semanticscholar.org/ac7b/26a8df0609384ccf1593c7096665b2fd2e88.pdf [3]. M. Tatar and R. Schaich, “Automated Test of the AMG Speedshift DCT Control Software”, 2010. [Online]. Available: https://www.qtronic.de/doc/TestWeaver_CTI_2010_paper.pdf [4]. “Global Greenhouse Gas Emissions Data | US EPA”, US EPA, 2017. [Online]. Available: https://www.epa.gov/ghgemissions/global-greenhouse-gas-emissions-data. [Accessed: 20- Jun- 2017]. [5]. “About The Prize”, XPRIZE, 2017. [Online]. Available: http://www.xprize.org/about. [Accessed: 20- Jun- 2017]. [6]. L. Kay, “The effect of inducement prizes on innovation: evidence from the Ansari XPrize and the Northrop Grumman Lunar Lander Challenge”, R&D Management, vol. 41, no. 4, pp. 360–377, 2011. [Online]. Available: http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9310.2011.00653.x/abstract [7]. A. Géron, Hands-On machine learning with Scikit-learn and TensorFlow. O’Reilly, 2017. [8]. D. Freedman and D. Freedman, Statistical Models: Theory and Practice, 2nd ed. Cambridge: Cambridge University Press, 2009. [9]. “Finding R Squared / The Coefficient of Determination”, statisticshowto.com, 2017. [Online]. Available: http://www.statisticshowto.com/what-is-a-coefficient-of-determination/ [10]. “sklearn.metrics.r2_score — scikit-learn 0.18.1 documentation”, Scikit-learn.org, 2017. [Online]. Available: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html. [Accessed: 20- Jun- 2017]. [11]. “Mercedes-Benz Greener Manufacturing Data”, Kaggle.com, 2017. [Online]. Available: https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/data. [Accessed: 19- Jun- 2017]. [12]. D. Moore and G. McCabe, Introduction to the Practice of Statistics, W.H. Freeman, 2002. [13]. “One-hot (dummy) encoding of categorical data in Excel”, Stackoverflow.com, Posted by user Chichi, 2015. [Online]. Available: https://stackoverflow.com/questions/34104422/one-hot-dummy-encoding-of-categorical-data-in-excel. [Accessed: 22- Jun- 2017]. [14]. “Skewed Distribution: Definition, Examples”, statisticshowto.com, 2017. [Online]. Available: http://www.statisticshowto.com/skewed-distribution/ [15]. L. Breiman, “Arcing the Edge”, University of California, Berkeley, CA, 1997. Available: https://www.stat.berkeley.edu/~breiman/arcing-the-edge.pdf [16]. P. Geurts, D. Ernst and L. Wehenkel, “Extremely randomized trees”, Machine Learning, vol. 63, no. 1, pp. 3–42, 2006. Available: https://pdfs.semanticscholar.org/336a/165c17c9c56160d332b9f4a2b403fccbdbfb.pdf [17]. D. Wolpert, “Stacked generalization”, Neural Networks, vol. 5, no. 2, pp. 241–259, 1992. Available: http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=BDDE4D399F09DA1275DAF5E444736A8F?doi=10.1.1.56.1533&rep=rep1&type=pdf [18]. A. Charan, “What is a Decision Tree?”, Quora, 2016. [Online]. Available: https://www.quora.com/What-is-decision-tree. [Accessed: 27- Jun- 2017]. [19]. “sklearn.linear_model.LinearRegression — scikit-learn 0.18.1 documentation”, Scikit-learn.org, 2017. [Online]. Available: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html [Accessed: 20- Jun- 2017]. [20]. “Pearson Correlation: Definition and Easy Steps for Use”, Statistics How To, 2017. [Online]. Available: http://www.statisticshowto.com/what-is-the-pearson-correlation-coefficient/. [Accessed: 23- Jun- 2017]. [21]. “Mercedes-Benz Greener Manufacturing Leaderboard”, Kaggle.com, 2017. [Online]. Available: https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/leaderboard. [Accessed: 20- Jun- 2017]. [22]. “Feature importances with forests of trees — scikit-learn 0.18.2 documentation”, Scikit-learn.org, 2017. [Online]. Available: http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html. [Accessed: 23- Jun- 2017]. [23]. L. Smith, A Tutorial on Principal Components Analysis. 2002. [Online]. Available: http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf [24]. “Choosing the right estimator — scikit-learn 0.18.2 documentation”, Scikit-learn.org, 2017. [Online]. Available: http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html. [Accessed: 23- Jun- 2017]. [25]. “Mercedes-Benz Greener Manufacturing Discussion”, Kaggle.com, 2017. [Online]. Available: https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion. [Accessed: 20- Jun- 2017]. [26]. T. Dietterich, “Ensemble Methods in Machine Learning”, Multiple Classifier Systems, pp. 1–15, 2000. Available: https://link.springer.com/chapter/10.1007/3-540-45014-9_1 [27]. L. Breiman, “Bagging Predictors”, Machine Learning, vol. 24, no. 2, pp. 123–140, 1996. Available: https://link.springer.com/content/pdf/10.1023%2FA%3A1018054314350.pdf [28]. L. Breiman, “Pasting Small Votes for Classification in Large Databases and On-Line”, Machine Learning, vol. 36, no. 12, pp. 85–103, 1999. Available: https://link.springer.com/content/pdf/10.1023%2FA%3A1007563306331.pdf [29]. “XGBoost Documentation”, xgboost.com, [Online]. Available: http://xgboost.readthedocs.io/en/latest/model.html [30]. D. Opitz and R. Maclin, “Popular Ensemble Methods: An Empirical Study”, Journal of Artificial Intelligence Research, 1999. Available: http://www.jair.org/media/614/live-614-1812-jair.pdf [31]. “1.11. Ensemble methods — scikit-learn 0.18.1 documentation”, Scikit-learn.org, 2017. [Online]. Available: http://scikit-learn.org/stable/modules/ensemble.html. [Accessed: 20- Jun- 2017]. [32]. L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa and A. Muller, “API Design for Machine Learning Software: Experiences from the Scikit-learn Project”, European Conference on Machine Learning and Principals and Practices of Knowledge Discovery in Databases, 2013. Available: https://arxiv.org/pdf/1309.0238.pdf [33]. “Diminishing returns and Machine Learning: Increasing Gains | MIT EECS”, Eecs.mit.edu, 2016. [Online]. Available: https://www.eecs.mit.edu/node/6508. [Accessed: 27- Jun- 2017]. [34]. A. Halevy, P. Norvig and F. Pereira, “The Unreasonable Effectiveness of Data”, IEEE Intelligent Systems, vol. 24, no. 2, pp. 8–12, 2009. Available: http://clair.si.umich.edu/si767/papers/Week06/TextRepresentation/Halevy.pdf [35]. “Winning Tips on Machine Learning Competitions by Kazanova, Current Kaggle #3 Tutorials & Notes | Machine Learning | HackerEarth”, HackerEarth, 2017. [Online]. Available: https://www.hackerearth.com/practice/machine-learning/advanced-techniques/winning-tips-machine-learning-competitions-kazanova-current-kaggle-3/tutorial/. [Accessed: 27- Jun- 2017]. [36]. Y. LeCun, Y. Bengio and G. Hinton, “Deep learning”, Nature, vol. 521, no. 7553, pp. 436–444, 2015. Available: http://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf Appendix Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",Capstone Project: Mercedes-Benz Greener Manufacturing Competition,7,published,1121,12025,0.004324324324324324,0,0,0,0,1,1
130,0,695.2734209005672,0,https://medium.com/p/home-of-the-scared-5af0fe5eab40,0,None,2017-07-01 09:08:00,35.85,9,19,2017-06-30 18:21:00,"['Politics', 'Books', 'News', 'Media Criticism']","Home of the Scared A review of A Culture of Fear: Why Americans are Afraid of the Wrong Things One Sentence Summary: At the safest and most prosperous time in US history, Americans have been driven into a state of perpetual fear by a constant stream of dire news and politicians eager to take advantage of a wary populace. In the midst of the McCarthy communism scare, pioneering American journalist Edward Murrow told the nation on See It Now, “We will not be driven by fear into an age of unreason, if we dig deep in our history and our doctrine, and remember that we are not descended from fearful men.” Murrow’s words seem to have been all but forgotten today, with daily news shows providing us with a litany of scares to keep us up at night and politicians running exclusively on fear-driven platforms — fear of outsiders, fear of the future, fear of the upper class, fear of the lower class, and most concerning, fear of each other. Yet, even as Americans report feeling less secure, the country is demonstrably becoming a safer place to live. (One typical example: In 2014, over 60% of Americans estimated that crime was increasing when actually, violent crime has dropped 50% since 1991.) What has created and maintained this misconception of a more dangerous world that runs counter to all statistics? In A Culture of Fear, Barry Glassner attempts to answer this question with an illuminating trip through a number of recent scares created and spread throughout the American consciousness. A Culture of Fear was first published in 1999 and as a result, the majority of the focus is on panic “epidemics” from the 1990s, including violent children, teen moms, rap music, and plane crashes. While some of these fears remain prevalent (CNN’s theme seems to be “never let a good plane crash go unreported”), I was struck by how little I had heard of some of the others. Multiple Chemical Sensitivity (MCS) and Road Rage, bogus afflictions said to be affecting large swathes of the American population two decades ago, have barely crossed my radar and never been presented as serious issues. Gradually, I realized that this only served to underscore the fleeting nature of these manufactured fears. They have disappeared because they never existed in the first place. Once the public eventually catches on, rather than retract the story, the media simply moves on to the next event sure to “end civilization as we know it.” The motto of modern news organizations is best described as “alarm, confuse, manipulate.” Alarm the public about a false danger, confuse them with misleading statistics or none at all, and manipulate them into consuming ever-greater amounts of coverage and supporting whichever political party the media happens to be aligned with. Although most of the fears described may not be salient today, the way they are generated by the media and then maintained in the face of all evidence is the heart of the book. As Glassner explains, there are four required ingredients for an epidemic of panic: Reporters are able to an take isolated incident, say a child is arrested after bringing a knife to school because of a movie he/she watched, and turn it into an epidemic on a nationwide scale: “Knife-toting child shows the sad state of American youth.” The scare plays into already-formed preconceptions about the world at large and targets a group without the ability or the resources to correct the mis-belief (in this case, the preconception is that the next generation is severely degraded, a view expressed perennially since Roman times). Moreover, the story often obscures mundane but critical issues that deserve coverage such as the under-funding of public education. News organizations pick up on the story and seek out other incidents to cover in order to demonstrate the trend. This is a direct contradiction of the scientific method. Rather than observing the evidence and drawing a conclusion based on facts, reporters form a story and then pick out the evidence to support their conclusion, ignoring all the vast amounts of contradictory statistics. One point repeatedly stressed throughout the book is that “anecdote trumps facts.” In other words, an emotional story will always move viewers more than hard statistics even when the numbers show the disproportionate amount of fear we attribute to relatively harmless, even safe, activities. One example that bears mentioning is that in the entire history of commercial aviation, about half as many people have been killed in plane crashes as are killed every year in the United States in motor vehicles. The common refrain that the most dangerous part of any international trip is the drive to the airport is perfectly accurate. Nonetheless, the media is able to stoke people’s relatively minor qualms about flying (claustrophobia or a fear of heights) into massive panic conflagrations with an incessant amount of coverage when a plane so much as lands heavily. On the exceedingly rare occasion when a plane accident does end in fatalities, the media focuses not on the larger trends that show how anomalous the crash is, but on families and individuals that have been affected by the accident. Glassner does not deny that any death is indeed a tragedy for those affected, but he points out that there are no similarly touching stories for the common events that actually kill a majority of Americans, such as heart disease or problems that evolve on a much longer time-scale, such as a widening gap in income levels. Furthermore, positive news stories that would show how much forward progress humanity has made never elicit even a mention. “Another plane lands safely: that’s 25 million in a row” does not make for an exciting headline even though it was true during 2007 and 2008 when not a single passenger was killed by an airplane in the US. This period was noticeably bereft of coverage of planes precisely because they were too reliable. However, the frenzy started back up again in 2009 at the first crash of an airplane, demonstrating the classic cycle of fear: people are evolutionary inclined to be cautious about the world and place greater weight on negative news, the media is all too glad to provide numerous stories of suffering to feed these worries, and the public becomes even more afraid. The fearful public must then watch the nightly news for the next thing to be scared of. We suffer from an overload of information resulting in an increased exposure to a decreasing number of bad things that happen in the world. As Glassner and others have pointed out, it is not the rate of violent crime and suffering that have increased, it is only our perception of these events. Why Should We Care? The Culture of Fear may seem like a glum book, and it is for large parts. However, Glassner makes sure to point out times when journalism did its job by highlighting causes that would have gone unnoticed and ensuring that they were addressed. During the 1980s and 1990s, drunk driving received extensive media coverage, which combined with the efforts of groups such as Mothers Against Drunk Driving, led to a 31% decline in the number of alcohol related highway fatalities. Research showed that the rate fell faster in years with more coverage, indicating the public is willing to change their behavior with enough coverage and the right message. Journalism and media coverage should unite us in our common interests — namely health, prosperity, and sustainability — rather than alienate us against outsiders and one another. Moreover, news organizations need to serve as a barrier to the abuse of power by highlighting unsavory practices (see Bernstein and Woodward and their investigation of some suspicious events at the Watergate Hotel) and fully covering the impact of legislation. Lately however, it has seemed that rather than being a check on politicians, the media has become a tool used by political parties to manipulate a public only too willing to be led to supposed “safety.” Like it or not, the media has an exceedingly influential hold on our nation, especially in the political arena. In 2004, during the presidential campaign, the Bush administration pressured the head of the Department of Homeland Security, Tom Ridge, to raise the nation’s threat level because they had noticed that it correlated with the sitting President’s approval rating. Increasingly, we are being fed only the news we want to see and hearing only one side of a discussion, political or otherwise. Facebook shows us stories and posts based on what we have liked in the past, and even a simple Google search is influenced by your click history. We unquestioningly accept these stories partly because its much easier to think something is true if we agree with it (a phenomenon known as confirmation bias). This division of every issue is an us vs. them, right vs. wrong dichotomy is detrimental because it does not even allow one to take the point of view of the other side. As Glassner mentions in the last chapter of the book, added 10 years after the original publishing, the fears of the 1990s were almost exclusively directed at other groups within our society. In the years post 9/11, our fears shifted to a distrust of any outsiders which at least temporarily slowed down our increasing distrust of our fellow citizens. Yet, now it appears that we are beginning to turn our fears inward again, except that this time the target is not small subsets of the population, but the entire “other side.” There is still hope to reverse this trend though. There are many large problems to be addressed in the 21st century, and perhaps we can unite behind these endeavors. The first step is to turn off the news and concentrate on the larger issues and real trends that will shape our society on a long-term timescale. At the very least, consume any media with a more skeptical attitude, particularly when real-world statistics do not align with the story. As a public, we must demand that journalism return to its original purpose of holding power in check and covering stories that need to be addressed.In the end, we must remember that the news is not a bastion of truth, but a manufactured product that is designed to take advantage of human psychology and keep us watching without questioning. It is only by recognizing that the content we see is designed by an organization with an agenda, that we can take a step back, examine the situation, and make decisions based not on fear, but on rationality. Recommendation When evaluating non-fiction I tend not to give a book a rating out of ten or a number of stars (or whatever shape happens to be in vogue). Rather, I like to ask the question: did I get anything more out of reading the whole book than I could have gotten from a ten-minute summary? My opinion is that some books are important for the content itself, but some books are only useful for the big idea. In the case of A Culture of Fear, even though the main idea was easy to grasp, I found that finishing the book was still beneficial and I would recommend reading through it all. Although most of the examples are somewhat dated, each offered a different perspective and lessons on the architecture of an artificial panic. Moreover, the final additional chapter, added in 2009, was more relevant, with a focus on terrorism and other fears in the early years of the 21st century. The additional chapter also presented some notes of optimism as it discusses the election of Barack Obama, a decidedly anti-fear candidate who ran on the message of hope, the antidote to fear. Although recent events have again highlighted the power of fear in motivating voters, I do not think it is evidence of our society sliding backwards into an “age of unreason.” Rather, I think that rational decision making can reassert itself but it will take a concerted effort by the public to demand a higher level of journalist integrity from our news organizations. The book concludes with a powerful message that shows where we should place our aspirations thanks to a quote from Michelle Obama when asked why she let Barack run for President: And as more people talked to us about it, the question came up again and again, what people were most concerned about. It was fear. Fear again, raising its ugly head in one of the most important decisions that we would make. Fear of everything. Fear that we might lose. Fear that he might get hurt. Fear that this might get ugly. Fear that it would hurt our family. Fear. You know the reason why I said ‘Yes’? Because I am tired of being afraid. I am tired of living in a country where every decision that have made over the last ten years wasn’t for something, but it was because people told us that we had to fear something. We had to fear people who looked different from us, fear people who believed in things that were different from us, fear one another right here in our own backyards. I am so tired of fear, and I don’t want my girls to live in a country, in a world, based on fear. We can escape from the fear-driven cycle with an application of conscious logic when making our decisions. The next time you go to the ballot box or decide to drive rather than fly, ask yourself, am I making this decision out of fear, or because it is the best choice for myself and the country? Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Motivated attorneys Energetic advocacy groups Heartbreaking anecdotes Sympathetic victims",Home of the Scared,4,published,53,2533,0.0,0,0,0,0,0,0
123,0,691.285764424213,0,https://medium.com/p/the-triumph-of-peace-f48500983749,0,None,2017-07-05 08:51:00,8.33,14,5,2017-07-03 20:18:00,"['Books', 'Psychology', 'History', 'Humanism']","The Triumph of Peace A review of The Better Angels of Our Nature: Why Violence Has Declined One-Sentence Summary: Human violence at all levels and timescales has substantially declined due to a set of forces which have subdued our destructive impulses while promoting our peaceful tendencies. For such a positive idea, the central premise of Steven Pinker’s The Better Angles of Our Nature: Why Violence Has Declined is remarkably controversial. Whenever I dare to mention that the 21st century is the most peaceful in history, I am greeted not by shouts of joy but by a chorus of vehement opposition. Is it not only that people do not believe this concept, but that they seem to not want to believe that humanity is really getting better all the time. The idea runs counter to all popular narratives and is so shocking that it requires altering one’s entire mindset about the world in order to acknowledge it. Initially, I found it difficult to accept as well, but after enough exposure to the concept, I have been convinced that humans are indeed treating each other better now than ever before. As a result, I have gradually become a rational optimist, and while I still have concerns about our society, I sleep a little better at night knowing that when viewed through the lens of history, we are on the right track. Pinker’s 832-page masterpiece exhaustively covering this topic has been by far and away the most influential work in the shifting of my outlook. Before Pinker, a cognitive scientist at Harvard University, can explain why violence has declined, he needs to convince us that conflict levels really have fallen, a difficult task in the negativity-dominated media culture of today. Unlike a news broadcast though, Pinker does not need to rely on anecdotes because he can draw on statistics from hundreds of studies conducted all over the world. Amidst all the statistics is one dominant theme: violence on all scales has declined throughout the course of human history. Pinker breaks up this long gradual decrease into six distinct movements. At one end, on the level of entire human civilizations and millennia, is the Pacification Process, characterized by the transition from hunting/gathering to permanent agricultural societies beginning roughly 12000 years ago. This era saw at least a five-fold reduction in rates of violent death as feuding tribes coalesced into the first cities and states with central governments which provided a check on the chronic raiding and wars that had been a daily part of life. At the other end of the spectrum, on the scale on individual humans, is the movement Pinker calls the Right Revolutions, beginning in the decades since World War Two and continuing through today. The past 60 years have seen a decline in personal violence such as abuse against spouses, children, and animals, and expanding rights and freedoms for previously denigrated minority groups. To cite a single figure, in this period, rape declined by 80% and went from a topic that barely even acknowledged as an issue to a cause that is seen as the responsibility of our society to address. In between the large civilization-wide and the small personal scale are four intermediate trends that illustrate how genocide, interstate conflicts, civil wars, homicides, domestic abuse, child abuse, bullying, and torture have all decreased to historical lows. It is not only that strife itself has declined, but also that our attitudes towards violence have undergone a substantial shift. What were once considered everyday or even amusing acts are now so horrible as to be beyond comprehension. A 15th century Englishman could entertain himself by watching a public execution, terrorizing a prisoner in the stocks, cheering as a bear chained to a stake was torn to pieces by hunting dogs, or observe the trial of a witch which always ended in a bonfire (and not the kind you would want to roast marshmallows on). Prisoners, political dissenters, and religious heretics were tortured in an endless variety of manners (it is no coincidence that the Christian Church has adopted an instrument of torture as its symbol). Life was, in the words of Thomas Hobbes, “nasty, brutish, and short.” Less value was placed on each individual life, which made it much easier to justify carrying out terrible acts or taking a life. In contrast, today, even the animals that we eat have their own lobbying groups, and a child making a gun signal at school is cause for uproar. We may argue about political correctness run amok, or the needless sheltering of our children, but this is a necessary price to pay for the peaceful society we enjoy. The mere fact that we value our children enough to prevent them from bullying each other shows how much progress we have made from times when infanticide was common and children were more or less disposable. The heightened aversion to violence in modern society can occasionally seem over the top, but Pinker argues it is a sign of how precious life has now become. Our recognition of and sensitivity to violence may have increased in recent years, but that is an indicator of the emphasis we now place on preserving and bettering all human lives. Pinker is clearly a proponent of lists. Besides the six trends of declining violence, he outlines five inner demons that drive humans to conflict, four “better angels of our nature” which lead us to commit fewer violent acts, and five historical forces that have favored our peaceable motives. (The title of the book comes from Lincoln’s first inaugural address which ends with “The mystic chords of memory… will yet swell the chorus of the Union, when again touched, as surely they will be, by the better angels of our nature.” ) Lists can oversimplify complex topics and leave little room for nuance, but Pinker does a fantastic job of using lists to condense the most importance information into easy-to-remember sections. The most intriguing part of the book is the five historical forces that promote our peaceful nature and it is worth briefly outlining them in order to answer the question: “Well, why has violence decreased?” Together, these five historical forces have subdued our five inner demons — predation, dominance, revenge, sadism, and ideology — and promoted our four better angels — empathy, self-control, a sense of morality, and reason. Pinker believes it is necessary to identify external (exogenous) causes of the decline rather than say there is more peace today because humans have become more peaceful because that would be circular reasoning. Moreover, the premise of the book is not that we have evolved to become more peaceful, as evolution through the mechanism of natural selection is too slow to observe effects even on the timescale of hundreds of years. Instead, we have shaped our civilization in a manner that makes it easier to exercise some traits that evolution has provided us with and harder to exercise others. Furthermore, these historical forces are alive and well in the world today, and though violence is not guaranteed to decline, there is no reason to doubt that current trends will reverse themselves. Why Should We Care? The idea that the world is becoming less violent is one of the most important but least appreciated concepts of the 21st century. As Pinker points out, we have a disturbing tendency to denigrate modernity. The belief that peaceful, nature-loving people existed in harmony before the ruthlessly violent modern world came into existence is prevalent and can be seen in depictions such as Rousseau’s “noble savage.” Yet native peoples took as much advantage of their environment (often leading to their extinction) as we do in the modern world and treated each other far worse than we would even believe possible. This false belief in a more peaceful past leads to a nostalgia for a simpler age, or in the case of the United States, to return the country to the way it was 60 years ago. The past may have been simpler (in terms of the amount of information the average person had access to) but it was by no means better for the average citizen. Anyone who answers the question: “If you could live as an everyday individual at any time in history when would it be?” with an answer other than the present day is deluded. We are living in the Golden Age of Civilization, and there is no evidence to suggest that the state of the world will regress on a significant timescale. (Fortunately, the view that we are progressing is gradually becoming more common, at least among academics, with recent titles such as The Rational Optimist and The Moral Arc: How Science Makes Us Better People.) In addition to showing us why we should praise modernity (while reminding us that we should work to constantly improve the human condition), The Better Angels of Our Nature outlines the five most important trends driving the conflict decline which can inform policy makers and us voters as to the most critical issues to consider if we want to make society safer and healthier. Pinker has done the difficult work of identifying the positive trajectories, and now it is up to the rest of us to ensure that they continue, that for example, we increase access to birth control, or that we emphasize rational thinking in education. Once the fact that violence has declined is accepted, we need to realize that this is not by accident but rather through a series of forces that we can continue to support in our policies and individual choices. As Pinker puts it, we have come “to reframe violence as a problem to be solved rather than a contest to be won” and The Better Angels of Our Nature provides us with the framework to continue in our progress on this issue. Recommendation In order for me to recommend that someone devote the time necessary to finish a work of this size (the audiobook lasts for some 36 hours, or in my time measurement system, 288 miles of running) it has to be life-changing. While I believe the central idea of The Better Angels of Our Nature to be perspective altering, I don’t think the entire book is necessary to drive the point across. I found the work compelling from start to finish, but I realize that not everyone has the focus (or the time) to pay attention to lists of statistics and research results. It is enough to know that violence has declined and that there are hundreds of studies out there to demonstrate it. If you really want to absorb the critical part of the book, read Chapter 10 which outlines the five historical forces. Then, go and read the Wikipedia page, which has a thorough rundown of the six trends of declining violence, five inner demons, and four better angels. Together with Bill Gate’s review of the book, this will cover about 90% of the concepts and you can then feel confident that you can talk more knowledgeably on this subject than 99% of your fellow humans. And, if you are so inclined, 36 hours is a relatively short time to learn the full details of the most important trend in the existence of humanity. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Gentle Commerce: The second historical force comes down to cold, sobering math: our neighbors are worth more to us alive than dead. War is a zero-sum game and the gains of the victor are exactly cancelled out by the suffering of the defeated, resulting in no net gain for humanity. Trade, in contrast, is a positive-sum game with both sides coming out of an exchange better off than before. Our modern interconnected world is built upon this very idea. Why would we be so foolish as to kill our neighbors if we depend on them for food? (We are not going to start a war with China because we cannot celebrate July 4 without flags and fireworks made in China). The extent of trade has increased in lockstep with the increase in the speed of transportation, and countries, particularly democracies, are so interlinked that a disruption in one part of the supply chain will have global repercussions. When it is cheaper to buy a resource rather than plunder a nation for it, economics will prevail. Empowerment of Women: It seems too simplistic to say that as women have gained leadership roles at the national level (although not nearly at acceptable rates), they have pacified nations because of their feminine nature. However, Pinker does not shy away from admitting there are physical and psychological differences between men and women that leads women to be less violent and less likely to act on their impulses. When was the last great or even minor war started by two women? Throughout history, women have been a civilizing force. In fact, Pinker makes the claim that the wild frontier of the American West was finally civilized once women were able to make their way west (in the words of Pinker: “Nature abhors a lopsided sex ratio.”) It is not just the presence of women in power roles that falls under the category of empowerment of women, but also increased access to birth control. As Bill Gates and others have noted, no country has significantly reduced its poverty rate without expanding access to contraceptives. Birth control, especially when the decision to use it rests solely with women, allows mothers to have fewer children, leading to a greatest investment in time and resources in each child. Furthermore, birth control allows women to plan when to have children allowing them to be an integral part of the workforce. The empowerment of women has been a theme I have run across in many works lately, and I now view it as one of the most effective methods for improving standards of living around the world. Pinker would add reducing violence to the list of benefits that arise from the inclusion of women in power structures and giving women control of the reproductive cycle. The Expanding Circle of Empathy: Up until the very recent past, most humans never made it more than a few miles from their birth place and rarely, if ever, interacted with someone outside their tribe on a peaceful basis. Contrast that with today when cheap air travel has made it possible to travel across the country on a day’s wages or fly around the world in 24 hours. This ability to experience a larger slice of the world has drastically increased the diversity of cultures and humanity that we are exposed to on a daily basis. Rather than highlight our differences, this exposure has led us to see the common humanity that we all share. We observe that like us, people from all over the world have hopes, fears, emotions, and everyone is motivated by the same basic desire to see the world become a better place. This exposure to others cultures and viewpoints has come about not only because of physical travel, but also through all types of media and the telecommunications revolution. It might seem like the fantasy of a high-school literature teacher, but the claim that fiction can allow us to take the views of others and increase our empathy has solid psychological evidence behind it. Studies have shown that people put themselves in the position of someone they hear described in a story and will even sympathize more with the characters in a well-told fictional story than other people in real life. The ideals of the Enlightenment in the 18th century, particularly humanism, with its beliefs that all people have inherent worth and share common values, was spread through literature that was available to a larger percentage of the population than ever before. Modern television, social media, and communications have made it simple to immerse one’s self in a different culture or temporarily adopt a different viewpoint. As the number of people and ways of life that we can experience increases, our circle of empathy expands to include ever more individuals and our xenophobic tendencies are tamped down. Instead of relying on what our national leaders tell us about the people of a nation we are about to invade, we can perform a simple search and within seconds read an article which shows that these people are not savages, but humans just like us. If you ever need another reason to watch a movie or read a blog, simply say “I’m expanding my circle of empathy and making the world a less violent place” and no one will say you are wasting your time. The Escalator of Reason: The final historical force that has led to a decrease in our violent tendencies has been the triumph of logic over superstition and belief. Rationality is what allows us to perform the calculations that show we are better off trading with our neighbor rather than killing him. In addition, through measured thinking, we can play out multiple future scenarios to observe that the one ending in conflict makes all parties suffer. Although current political discourse would seem to suggest otherwise, humans are becoming more rational. Researchers have noted that IQ scores have consistently risen nearly 3 points per decade or 10 points per generation, a phenomenon known as the Flynn effect. Taken at face value, this means that an average teenager in 2017 would have an IQ of 130 in 1917, placing them in the top 2% of the population. A closer analysis shows that where the greatest progress has been made is not general knowledge, but in abstract reasoning, or the ability to process complex ideas that do not have a physical grounding. Abstract reasoning is what we employ when we consider hypothetical scenarios or think through cause and effects. Through rational thinking we can take a step back and look at the larger picture which shows the destructiveness of cycles of violence and the futility of ideologies. Moreover, reason can overpower our baser human emotions such as vengeance or tribalism. Rationality is often portrayed as cold and unfeeling, but it is only through applying logical thought processes that we can engineer more peaceful civilizations and keep our natural human responses in check. The Leviathan: Here Pinker is referring to a strong, legitimate central government. Throughout history, as disparate bands of people were gathered together into societies united by a single government, violence between groups has fallen markedly. A central authority figure has a monopoly on the legitimate use of force, thereby taking the role of justice out of the hands of citizens. Contrary to popular belief, less than 15% of murders are carried out for economic gain. Rather, the majority are motivated by personal revenge or the perpetrator’s sense of justice. When there is a system in place to punish those who carry out acts of violence, the cycle of revenge can be broken (consider the Hatfields and McCoys who took it upon themselves to mete out their own justice in a feud lasting three decades). Furthermore, people respond to incentives, and when carrying out a crime has a greater cost in terms of punishment than potential gain, there is little reason to take the risk. The role of government is therefore twofold: provide the correct incentives to encourage peaceful interactions between citizens, and if that fails, institute a system of justice which prevents people from needing to carry it out themselves. Pinker demonstrates the dual roles of the Leviathian through the lens of the “great crime decline” of the 1990s, in which rates of violent crime declined by more than 50%. There have been a number of theories put forth to explain this trend, but Pinker appeals to the simplest, that crime declined because policing and jail time for crimes increased. (This in itself is a controversial idea, but the other competing theory, first put forth in Freakonomics is even more so. The authors of that book argue that the decline in crime displayed the effects of the Roe vs. Wade decision of 1972. The children that would have been born had abortion remained illegal would have come of age in the 1990s and these children on average would have been more likely to grow up impoverished without a decent support system, two factors that have been linked to an increased crime rate. Unfortunately for this theory, Pinker shows that the numbers do not quite line up.) New York City serves as the main example, where mayor Rudy Giuliani instituted a tough-on-crime stance known as “broken windows policing,” which aimed to clean up neighborhoods by punishing even minor crimes harshly in order to prevent an increasing spiral of illegal activity. Therefore, the city provided both the incentive (in this case disincentive) to reduce crime by increasing the likelihood that a crime would end in jail time, and the strong police presence needed to enforce the law. The idea that a strong police force and more individuals in jail means lower crime rates is intuitive and can be extrapolated to the level of nations with international bodies such as the United Nations, which enforces economic sanctions to disincentivize conflicts. Governments, when led by despots, have been among the worst practitioners of violence, but the order imposed by a well-balanced authority can drive down rates of violence both within and between nations.",The Triumph of Peace,4,published,60,3892,0.0,1,0,0,0,0,0
117,0,676.7728080794792,0,https://medium.com/p/nasa-internship-report-dd8a23aaf58a,0,None,2017-07-19 21:09:00,19.62,47,62,2017-07-19 19:07:00,"['Space', 'NASA', 'Intern', 'Engineering', 'Aerospace']","NASA Internship Report Near Earth Asteroid Scout Attitude Control System Validation and Verification Author’s Note: The following is a report of my work I completed while an intern at the NASA Marshall Space Flight Center (MSFC) in Huntsville, Alabama. This project was conducted in Winter/Spring 2017 with the Near Earth Asteroid Scout Control, Navigation, and Mission Analysis Team. [Numbers in brackets are citations that refer to references listed at the end] Abstract Near Earth Asteroid (NEA) Scout is a 6-unit Cubesat which will sail to an asteroid within one Astronomical Unit (AU) of Earth. Upon rendezvous with asteroid 1991 VG, NEA Scout will perform optical scientific observations in preparation for an eventual crewed expedition to an asteroid. NEA Scout is a solar sailing satellite, a unique class of spacecraft propelled primarily by the reflection of photons from an eighty-six square meter composite sail. In addition to optical data gathering and characterization of an asteroid, NEA Scout will demonstrate the feasibility of solar sailing technology for interplanetary missions. The Attitude Control System (ACS) for NEA Scout is composed of three primary actuators: Reaction Wheels (RW), the Reaction Control System (RCS), and an Active Mass Translator (AMT). The Guidance and Control (G&C) software which manages the ACS is developed in MATLAB and modeled using Simulink Block Diagrams and will be autocoded into C before being uploaded to the spacecraft. Before the code development can be completed, it must undergo extensive verification, to ensure that it satisfies all expected requirements, and validation, to assess if it fulfills the intended purpose. Validation and Verification (V&V) of the NEA Scout ACS consists of numerous smaller tasks that form part of the larger ongoing development effort. For each V&V step, scripts must be written, data must be gathered and analyzed, and changes need to be implemented based upon the results. Hardware sensors, including sun sensors, must also be tested both for accuracy and to ensure smooth integration with the complete ACS software. Validation and Verification is crucial to the development of a robust ACS in advance of final preparation of NEA Scout for launch on the Space Launch System (SLS) Exploration Mission 1 (EM-1) planned for 2018. I. Introduction The concept of sailing through the solar system propelled only by the momentum of the sun’s photons is not a new one. As early as the 1920s, the father of astronautics, Konstantin Tsiolkovsky, described using “tremendous mirrors of very thin sheets” to attain “cosmic velocities.” [1] The idea of propellantless space travel using solar sails has been an attractive method to overcome the limitations of a finite reaction mass and enable a wide range of missions throughout the solar system. As shown by James Clerk Maxwell in 1873, photons, though massless, have momentum and therefore exert pressure on a surface. Solar sails take advantage of the miniscule pressure of solar radiation by using massive, extremely thin and light sails that maximize the number of photons reflected. An ideal solar sail would be flat and completely specularly reflective as a photon that is reflected straight back from a surface will impart twice its initial momentum to the surface. This momentum is transferred to the spacecraft, and, over a large enough sail, and a long enough time period, substantial accelerations can be attained. Moreover, by controlling the angle of the sail normal relative to the sun (the Sun Incidence Angle or SIA), the sail attitude or direction is altered and the sail will gain or lose orbital momentum. This allows the solar sail to tack, sailing towards the sun, or turn outwards towards the outer reaches of the solar system[1]. Although solar sailing missions were investigated extensively by the Jet Propulsion Laboratory (JPL) in the 1970s as an opportunity to rendezvous with Halley’s Comet, it has not been until the past decade that solar sails have seen flight missions. The Interplanetary Kite-Craft Accelerated by Radiation of the Sun (IKAROS) launched by the Japanese Aerospace Exploration Agency (JAXA) in 2010[2] was the first solar sailing craft to make it into space followed by NanoSail-D launched by NASA in late 2010 [3]. In 2015, the Planetary Society launched Light-Sail 1 [4], and Light-Sail 2 is planned for liftoff on a Falcon Heavy in 2017. Near Earth Asteroid Scout (NEAS) is the latest iteration from NASA in this unique category of spacecraft. NEA Scout’s main objective is to sail to an asteroid within 1 Astronomical Unit (AU) from Earth and perform a close (<10 km) flyby [5]. NEA Scout is a 6-unit Cubesat and is approximately 30x20x10 cm in size. It weighs slightly over 11 kg and is designed based on the common Cubesat philosophy of utilizing Commercial Off-the-Shelf (COTS) parts. When the solar sail is completely unfurled (see left side of Fig. 1), it will be 86 m² although it is only 2.5 micrometers (µm) thick. The sail itself is composed of CP1 polymer with a 10 nanometer (nm) coating of aluminum on the front side (facing towards the sun) and an uncoated back side [6]. NEA Scout is viewed by NASA as a precursor to a potential crewed mission to an asteroid as well as a demonstration of the feasibility of using solar sails for interplanetary missions. The current target for NEA Scout is 1991 VG, although that is subject to change depending on the launch schedule. NEA Scout is expected to reach the asteroid after a 2–2.5 year journey and will fly by at a relative speed of 10–20 m/s. NEA Scout will fly on the first launch of NASA’s Space Launch System (SLS) Exploration Mission One (EM-1), and is one of thirteen secondary cubesat missions that will be launched from the Multi-Purpose Crew Vehicle Stage Adapter (MCA) [5]. The Attitude Control System (ACS) on NEA Scout is responsible for maintaining the correct heading (direction) of the spacecraft and is shown on the right side of Fig. 1. It is composed of three actuating systems: an Active Mass Translator (AMT), a Reaction Control System (RCS), and Reaction Wheels (RW). Each system has its own set of requirements and responsibilities. The RCS utilizes 6 cold gas thrusters and is tasked with managing the roll (z-axis) momentum of the RW, performing the detumble upon deployment from the SLS MSA, accomplishing the trajectory correction maneuver to put the spacecraft on the right initial path, and recovering from any unforeseen situations. The AMT manages the pitch and yaw (x-axis and y-axis) momentum of the RW by shifting the center of mass of the spacecraft relative to the center of pressure which alters the magnitude and direction of the moment produced by the solar force. The RW allow fine directional pointing for science, communication, and trajectory guidance. They change the attitude of the spacecraft by exerting a torque as they speed up. However, the RW have a maximum rate of 5000 rotations per minute (RPM)5000 which means that in certain situations they will need to be slowed down, or desaturated, by the RCS. The ACS also has a suite of sensors used to determine the state of the spacecraft. Three coarse sun sensors are used on detumble for sun pointing before the solar panels can be deployed to charge the batteries. After the initial phase of the mission, the start tracker will be used for accurate attitude data at low body rates. The star tracker is the primary sensor in terms of providing attitude and body rates for the mission after the sail has been deployed. Finally, an inertial measurement unit (IMU) provides body rate information during detumble and when relatively high body rates are expected. The IMU demonstrates more accurate performance than the star tracker at high body rates, but the rates are much noisier and require extensive filtering. Therefore, at lower body rates, the star tracker is the preferred sensor for determining the state of the spacecraft. The unique characteristics of solar sails, including high solar disturbance torques, low frequency flexible body effects, and a large moment of inertia with the sail deployed, make attitude control a challenge. [6] Therefore, extensive Validation and Verification (V&V) of the ACS is required in order to ensure that the system meets all requirements, can fulfill its intended mission purpose, and is robust against unforeseen states. Initial development and testing of the ACS has demonstrated that it can carry out the entire mission concept of operations under nominal conditions. The Guidance and Control (G&C) software manages the ACS and is implemented as a series of MATLAB scripts and modeled using Simulink Block Diagrams as shown in Fig. 2. As MATLAB is a high-level interpreted language, it is too slow and unreliable to be uploaded directly to the spacecraft. Therefore, prior to being loaded onto NEA Scout, the MATLAB G&C software will be auto-coded into C. However, before development can be completed, the software must be validated to ensure that it can meet all requirements of the mission. The definition of software verification from the IEEE-STD-610 states: “the process of evaluating software during or at the end of the development process to determine whether it satisfies specified requirements.” [7] For NEA Scout, these requirements are derived from the mission objectives which determine the parameters needed by the ACS. V&V is intended to catch any errors in the code-base and test off-nominal scenarios. Rather than a single focused project, validation and verification is composed of a number of smaller steps that test a variety of situations and compare results to expected outcomes. V&V encompasses writing MATLAB scripts to test different phases of the mission, analyzing the resulting data to determine if requirements were satisfied, reporting the data to the G&C team, and implementing any changes based upon the simulation. This report details several studies which were undertaken in order to assess various aspects of both the G&C software and the ACS functionality. The main objectives were to test different phases of the mission and situations that could be encountered by the spacecraft, to ensure that the system was robust to off-nominal conditions, and to check that there were no unnoticed errors in the software that could result in a mission failure. In addition to working with the software simulation, this report also details testing of the sun sensor engineering development unit. This testing is also a crucial aspect of validation and verification because it characterizes the sensors for calibration purposes and tests that the sensors correctly interface with the software. The objectives of the sun sensor testing is ensuring that the data from the sensors is accurate and that it is correctly interpreted by the model to inform ACS actions. Each test documented in this report is independent, but also forms part of the larger process of G&C development for NEA Scout. The work documented here is but a small part of the entire process that goes into making a spacecraft a success. NEA Scout may be a small spacecraft, but it is a complex project with many people and independent entities working to ensure that its sailing mission will be a successful one. II. Verification and Validation Analyses 1. Reaction Control System Manufacturing Variability Study The Reaction Control System (RCS) on NEA Scout has five primary responsibilities corresponding to different phases of the mission. These duties are composed of initial spacecraft detumble, initial sun-pointing and attitude hold, the trajectory correction maneuver (TCM), reaction wheel z-axis momentum desaturation, and safe mode operation. [6] The RCS uses six cold gas thrusters for actuation with Refrigerant-236fa as the propellant. Two of the thrusters are situated axially and are only used during the TCM where they are fired continuously to provide the necessary delta-v to set NEA Scout on its course. The four non-axial thrusters will be used for attitude control and reaction wheel momentum desaturation during the remainder of the mission. The layout of the RCS is shown in Fig. 3. The critical responsibility of the RCS is the initial detumble of the spacecraft upon ejection from the Space Launch System Stage Adapter. All of the secondary payloads on SLS EM-1 will be launched, resulting in uncertain deployment conditions. It is expected that NEA Scout will be tumbling upon ejection with a maximum body rate of 10 degrees per second on each axis. The RCS must be able to null these angular rates to stabilize the spacecraft and perform initial sun pointing in order for the solar panels to be deployed to charge the batteries. The spacecraft carries only 1.25kg of propellant for the entire mission, and subsequently, the amount of propellant consumed during the detumble is a parameter of significance. Vacco, the manufacturer of the RCS cold-gas thrusters, can meet the design thrust magnitude requirements for each thruster within ±10%. This equates to 25 mN ± 2.5 mN (milliNewtons) of thrust for each thruster. Likewise, the angle (direction) of each nozzle is accurate to within ± 0.5˚[6]. The thrusters are oriented with both a clock (azimuth) angle of 45˚ relative to the x-axis, and an elevation angle relative to the xy-plane. Both of these angles on each thruster are subject to the same variability. The purpose of this study was to determine the effects of this manufacturing variability on the fuel consumed and the time to detumble. The baseline fuel use for the detumble maneuver with the specified exact thrust magnitude and direction and starting with a 10 degrees/second rate on all three body axes was 1.389 grams. Similarly, the baseline time to detumble was 41.6 seconds. The time to detumble was calculated based on the time taken to null the body rates to less than 0.1 degrees/ second on each axis. The manufacturing variations in magnitude and direction were studied separately, with the objective of characterizing the worst case in each category. For the magnitude deviation, there were four anomalous cases identified which are summarized in Table 1. The performance characteristics of the case are summarized in the left half of the table while the right side shows the exact configuration of variability in each of the four non-axial jets. The fourteenth run had a fuel usage more than twice that of the baseline and was the most extreme case in terms of fuel usage. Although this is a large amount relative to the baseline nominal case, it was determined to be of no significant concern for the overall mission as the amount of fuel consumed was less than 0.25% of the total fuel available. Interestingly, several of the cases turned out have better performance than the baseline both in terms of fuel use and time to detumble. Fig. 4 and 5 show the plots of time to detumble and the fuel usage for the four anomalous cases identified from the thrust magnitude variation study and summarized in Table 1. The next step was to perform the same analysis but instead of varying the magnitude of the thrust, alter the direction of the four off-axial jets to model the variation specified by the manufacturer. Each of the jets had an uncertainty of 0.5˚ on both the elevation and azimuth angle. As before, the four most extreme cases were identified for further inspection. These cases are summarized in Table 2. The worst case in the direction variation configuration used almost three times the fuel use of the baseline case, but again, the total fuel used was judged to be inconsequential for the overall mission especially as the detumble is a one-time event. The conclusion from the thrust magnitude and direction manufacturing variation study is that the manufacturing tolerances will allow for a satisfactory detumble even under the worst case scenario deployment of 10 degrees/second on each body axis. Although the fuel usage could be up to three times greater than the baseline depending on the exact configuration of the variation, this was determined by the G&C team to be within acceptable levels for the one-time detumble maneuver. The total fuel usage for even the worst case was under 0.5% of the fuel budget for the mission. Even with a worst case manufacturing variation, the effects for the entire mission would be minimal and all objectives could still be accomplished. Further work in the area of hardware variability could include accounting for both the angle variation and the thrust variation of the thrusters in the same run. Based on the preliminary work, this was deemed unnecessary because none of the initial cases were severe enough to impact the mission. However, the hardware variation for the RCS and other systems could potentially impact other areas of the mission concept of operations and should be investigated in order to implement any corrections that could mitigate or overcome decreased performance. 2. Modeling Effects of Specular Reflectivity on Thrust Light incident upon a surface, can be either reflected, absorbed, or transmitted. The light that is reflected can be further categorized as being either specularly reflected or diffusely reflected. Specular reflection refers to when the reflected light ray has the same angle relative to the surface normal as the incident light. Diffuse (Lambertian) reflection on the other hand, is when the reflected light rays is scattered and leaves the surface at an angle different from that of the incident ray [8]. These two definitions are summarized in Fig. 6. The symbol s is called the specular reflectivity coefficient and represents the fraction of reflected light that is specular. It is a crucial material property that greatly influences both the magnitude and direction of thrust that a solar sail is able to extract from solar radiation pressure. As the specular reflectivity coefficient decreases, the diffuse reflection increases and the solar sail will experience a greater force in the plane of the sail, resulting in a larger torque [1]. The torque resulting from the force of the sun’s photons is called a solar torque. This solar torque must be managed by the attitude control system in order for the spacecraft to remain oriented correctly. The solar torque generated on the x-axis and y-axis is handled by the Active Mass Translator (AMT). This actuator shifts the spacecraft center of mass relative to the center of pressure and is therefore able to trim the torques from the order of micro-Newton-meters (mNm) to tens of nano-Nm. The torque about the z-axis is known as the roll or “windmill” torque and must be controlled by the RCS using a minimal, but non-zero amount of fuel. Consequently, it is of great interest to the guidance and control group to characterize the solar torque as a function of the specular reflectivity coefficient. Although this value is a material property, for a given material it can change depending on the extent of deformation of the material. For a solar sail, this deformation occurs in the form of wrinkles. There are three categories of wrinkles that occur in a solar sail the size of NEA Scout; macro-, meso-, and microscale. Wrinkles on the order of a meter to centimeters (1 –0.01 meters) are labeled macro scale and can be effectively modeled in structural finite element models (FEM) such as that created for the NEA Scout G&C software model. Wrinkles on the microscale, or those smaller than 10s of micrometers (10–6 meters), can be considered a material roughness and thus the effects on the sail optical characteristics are captured by the Bidirectional Reflectance (BDRF) test. However, wrinkles in the middle size category, considered meso-scale (millimeters to tens of micrometers, 10–3–10–5 meters), cannot be accurately modeled using either material characteristics or an FEM. As shown by Heaton et al. [8] these wrinkles must be taken into account through the alteration of the specular reflectivity coefficient as the wrinkles effectively increase the diffuse reflection of the solar sail material. The first characterization of the specular reflectivity coefficient was completed at the Jet Propulsion Laboratory as part of the preparation for the proposed Halley’s Comet mission. These tests were conducted on pristine samples and later research performed in 2015 lowered the specular coefficient from 0.94 ± 0.04 to 0.89 ± 0.045. [8] However, more recent testing on larger 10 cm² samples of the sail material used for NEA Scout has shown that the specular reflectivity coefficient for a full-scale solar sail upon deployment will be even lower. [8] Furthermore, the coefficient can vary widely across the sail because the meso-scale wrinkle distribution is highly non-uniform even when unfurling the sail in a controlled test environment. Testing performed at MSFC observed values for the specular reflectivity coefficient that ranged from 0.804 to 0.300 across samples from the sail. This is a significant cause for concern as this value determines the magnitude and direction of the solar force experienced by the sail which in turn affects spacecraft performance, from the characteristic acceleration to the solar torques. The default value for the specular reflectivity coefficient in use in the development of the MATLAB and Simulink model was 0.89. The purpose of this study was to characterize and describe the effects of changing the value to a more realistic value provided by recent full-scale sail tests. The ideal optical model of the force on a solar sail comes from the Jet Propulsion Laboratory and was published in McInnes [1] and Wright [9]. The equations governing the tangential and normal force are described as follows: where the terms in Eq. (1) and (2) are defined in the nomenclature. Eq. 1 describes the force tangential to the plane of the sail and Eq. 2 describes the force normal to the plane. The specular reflectivity coefficient is given as s and is a crucial variable in both equations. As s decreases, the sail experiences a greater force tangential to the plane of the sail and therefore receives greater solar torques. After some deliberation, it was decided that the majority of cases for the investigation should be run with a coefficient of 0.563 and 0.444. The cases would be run with a single value of s across the entire sail and these were judged to be the lowest realistic effective average values for the entire sail of NEA Scout. The modeling study on the ramifications of altering the specular reflectivity coefficient had two phases: first, an optimized standalone thrust model developed [8] for the G&C simulations was used to characterize the levels of force and torque experienced by the sail for a selection of specular reflectivity coefficients across a range of sun incidence angles (SIA). Once the effects had been described in an ideal model, the specular reflectivity coefficient changes would be implemented across the entire MATLAB model in order to see if the attitude control system would be able to handle the resulting solar forces and torques (torque is used interchangeably with moment). The results of running the sail model in the stand-alone thrust model are shown in Fig. 7 and 8. Changes in the specular coefficient do not significantly impact the magnitude of the force but the magnitude of the solar torque is substantially altered. Here, the magnitude of the force is defined as the norm of the tangential and normal forces and the magnitude of the moment is defined as the norm of the x, y, and z moments. The worst case from the first phase of the study corresponds to the lowest specular coefficient as expected and is characterized by the greatest solar torques across the range of SIA. After completing the preliminary studies in the stand-alone thrust model, the variation in specular reflectivity was implemented in the full simulation. The script used to test the effects of the variation in thrust and moment was a simple attitude hold script that attempted to maintain the same attitude of the sail over the entire length of the simulation. In the full simulation, the roll angle (clock angle) of the sail was set at 0˚ and the sun incidence angle was increased until the attitude control system could no longer manage the solar torque. Fig. 9 shows the main scope from a simulation run with a specular reflectivity coefficient of 0.563 and a sun incidence angle of 55˚. As can be seen in the lower right plot, the Active Mass Translator (AMT) does not run into its limits which are at 3.5 cm on the y-axis and 8 cm on the x-axis. The x-axis and y-axis momentum of the RW can be successfully managed under these conditions by the AMT and will not require the RCS jets to fire to desaturate the wheels. In order to quantify the effects of changing the value, the maximum SIA for which the AMT could manage the x-axis and y-axis momentum was determined for both the case with s = 0.563 and s = 0.444. With a coefficient of 0.444, the maximum SIA was 45˚ while at a coefficient of 0.563, the maximum SIA that could be tolerated was 55˚. The maximum nominal SIA for the mission is 50˚, and therefore, even with the worst feasible specular coefficient, the AMT should is able to handle the resulting solar torques across nearly the entire range of SIA. When the AMT cannot handle the momentum of the wheels, the RCS is forced to fire to desaturate the wheels and manage the x-axis and y-axis momentum in addition to its nominal duty of managing the z-axis momentum. The conclusion that was drawn was that even accounting for a significant decrease in the specular reflectivity coefficient, the RW x-axis and y-axis momentum should be adequately managed by the AMT for nearly all expected operating angles of the spacecraft. Further work in this direction should include an attempt to characterize the optical properties of the entire sail. Due to the size and the delicacy of the sail, testing the entire sail on Earth has not been feasible. Currently, testing [8] has been carried out on 10 cm² pieces of the sail, which is a significant size difference from the full sail at 86 m². Even with this small sample, the optical properties varied widely across the sample because of the distribution of wrinkles. It is clear from this current study that the optical properties of the sail can have a large effect on the performance of the mission. Although this study did not identify any significant reasons for concern for the NEA Scout mission, this is an area that has been difficult to accurately characterize and more research is needed in preparation for future solar sailing missions. Ideally, optical data can be gathered from NEA Scout when it is deployed on orbit that will help advance the state of specular reflectivity knowledge. The greatest limitation currently confronted by solar sails is the lack of flight experience, and any flight data gathered by NEA Scout will prove invaluable to the nascent province of solar sailing. 3. Passive Roll Control Although NEA Scout utilizes the propellantless method of solar sailing for primary propulsion, like all other spacecraft, it is still operationally limited by the amount of propellant it can carry because of mission critical functions that still must be carried out by the RCS. Therefore, any potential method for reduction of fuel use is attractive to the mission. One such area that holds promise for reducing or even eliminating fuel consumption is that of passive rol control. The current concept of operations specifies that the x-axis and y-axis momentum of the reaction wheels is managed through the use of the AMT and the z-axis (roll) momentum is managed by the RCS. The RCS will also be commanded to fire if the speed of the wheels crosses a pre-determined threshold as may occur when the AMT alone cannot manage the solar torque. A worse case estimate [6] of fuel consumption used by the RCS to manage the roll momentum for the entire mission is 240 grams (out of 1.25 kg total for the mission). Even though this fuel use is accounted for in the nominal mission plan and will not affect the primary objectives, any reduction in this value will add margin for unplanned circumstances and could open up the possibility of extending the mission to visit another celestial body. Consequently, one area of research for NEA Scout is propellantless z-axis (roll) momentum management. The roll torque experienced by the spacecraft is a function of both the roll and SIA. At SIA larger than 20˚, there is a zero crossing in the roll torque at certain roll angles as illustrated in Fig. 10. For a given SIA above 20˚, there is at least one roll angle at which the solar z-torque is zero. If the solar sail can achieve this angle at a low enough roll rate, the sail should “catch” at the angle, that is, the sail would oscillate about that roll angle rather than continue to roll around the z-axis and build up momentum. These equilibrium points occur where the slope of the torque vs roll angle plot is negative and where the roll torque crosses zero. This occurs at clock angles of near 0˚ and near 180˚ as demonstrated in Fig. 10. At these stable points, the sail will oscillate about equilibrium and over a long enough time frame, the magnitude of the oscillations would decay due to structural damping. This process could also be sped up with an additional energy sink. [6] The objective of this study was to determine the initial roll angles (used interchangeably with clock angles) that allow the sail to reach a stable oscillation point for a given SIA. The designated approach was to first explore the problem in a stand-alone passive roll control model, and then move on to modeling the situation in the full simulation. Table 3 shows a summary of results for a subset of the different angle combinations which were run. The preliminary work demonstrated that depending on the SIA and the starting roll angle, the sail could reach a stable point without the use of the RCS. Fig. 11 demonstrates the attainment of a stable oscillation angle. The top plot in Fig. 11 shows that the sail oscillates about a roll (clock) angle of 20˚ indefinitely. In the unstable cases by contrast, the roll angle continues to grow over time without bound and the spacecraft spins about the z-axis until the RCS must fire to control the RW momentum. After categorizing the cases into stable and unstable, several trials were then run in the entire simulation, in order to verify the results and look for unexpected results that may not have manifested in a simplified model. A further investigation was conducted to determine if the unstable cases could be stabilized using the RCS. The problem in the unstable cases is that the sail begins at an initial roll angle where it is subjected to significant z-axis torques. Therefore, when the sail reach the zero crossing in the roll torque graph, the sail already has too much momentum to be “caught” at a stable point and continues spinning and gaining momentum. To resolve this issue, the hypothesis is that the RCS could fire for a minimal amount of time and coerce the solar sail into a specified roll angle about which the sail would be stable. To test this hypothesis, an unstable case was run with the RCS activating above a body rate threshold of 3 x 10–3 deg/sec on any of the body axes. The initial conditions were SIA = 30˚ and initial clock angle = 100˚ which had been verified as an unstable case in the full-scale simulation. The results of this examination are illustrated by the main summary in Fig. 12. Of primary interest in Fig. 12 is the plot in the upper right corner showing body rate vs time (the y-axis scale is in 10^-3 Nm and the x-axis scale is in 10⁵ seconds). The sail spins about all three axis with an initially increasing body rate on the z-axis as indicated by the red line. After the body rate crosses the threshold for RCS activation, the RCS fires and nulls the body rate on this axis at around 0.7 x 10⁵ seconds. After this point, the sail is caught around a stable point and the body rates on all three axes remain below the threshold. This indicates that rather than continuing to spin about the z-axis, the sail has is oscillating about a stable point. Moreover, for the rest of the simulation, the RW speeds remain well below the 5000 rpm saturation limit and the AMT does not come close to reaching its limits. Overall, the test demonstrated that a stable point could be reached from an initially unstable configuration using the RCS. While the RCS would consume propellant to initially induce the sail into this stable point, the overall fuel usage for this situation would be less than having the RCS continually fire to desaturate the z-momentum of the RW. In conclusion, this study proved that at least in the full-scale model of NEA Scout, passive roll control is a feasible option for managing the z-axis momentum. If a stable point can be reached either with or without the assistance of the RCS, the sail will oscillate about one clock angle and the RCS will not be needed to desaturate the reaction wheels resulting in lower fuel usage and increasing the mission possibilities. 4. Reflectivity Control Device (RCD) A promising proposed technology for propellantless attitude control is that of reflective control devices (RCD). The amount of force imparted on a surface by solar radiation pressure is proportional to the reflectivity of the surface as shown in Eq. 1 and 2. The concept of an RCD is that by altering the reflectivity of small areas of a solar sail, the force exerted on that region would be changed and a torque could be generated that could be used to orient the spacecraft as required. A region of the sail that is more reflective will generate a larger force that an equal sized area that is less reflective all other factors being equal. [1] The method examined in this study calls for each RCD to be composed of two regions, which are placed on opposite ends of one of the sail booms (support beams for the sail). The proposed design further specifies two RCDs on the sail which results in a total of four areas, one on each of the tips of the booms. By altering the reflectivity of one area in each pair, there will be a difference in forces exerted on the two areas making up the RCD. The torque is then this difference in force times the moment arm of the areas to the x-axis or y-axis. A schematic is presented in Fig. 13 although this design specifies eight RCDs, with four situated vertically and four situated horizontally. [10] The method of altering reflectivity for attitude control was successfully demonstrated by the Japanese Aerospace Exploration Agency (JAXA) IKAROS solar sailing spacecraft launched in 2010 [2]. This was the world’s first validation of interplanetary solar sailing, and featured several novel techniques which had yet to be implemented including RCDs. On ICAROS this took the form of flexible multi-layer sheets placed around the sail that transitioned from diffuse-dominant reflectance to specular-dominant reflectance. The sheets are composed of liquid-crystals, and when a voltage was applied, the sheet became more reflective and hence has a higher specular coefficient and generates more force from the solar radiation pressure. When the voltage is removed, the sheet is rendered opaque and the reflection is dominated by diffusion lowering the force. By manipulating the cycling of these patches, the entire spacecraft attitude could be controlled [2]. A picture of IKAROS with the RCDs labeled is shown in Fig. 14 The concept under consideration for this study is the same as that of IKAROS although the implementation varies slightly with the two RCDs being placed at the four corners of the sail rather than along the edges as for the Japanese spacecraft. A further difference is that for the present study, when the voltage is off, the panels are opaque and when the devices are turned on, the RCD panels become transparent. Thus, the panels transition from diffuse reflection when off to transmission when on and generate a torque from the difference in force experienced by each panel [10]. The center of the sail remains the same CP1 polymer coated with aluminum for greatest reflection of the photons and the maximum thrust. Although it is too late in the timeline for NEA Scout to be equipped with RCDs, the current study was performed on a NEA Scout-like solar sail to gauge practicality for future missions. MSFC is currently working with researchers at the University of Maryland for the development of the films used in the RCDs. The researchers at Maryland [10] use polymer dispersed liquid crystal (PDLC) film as the altered material. These films can be easily switched from opaque to transparent and have been implement in flat panel displays and smart windows including the Boeing 787 Dreamliner windows which can be electronically dimmed. [11] Research conducted at the University of Maryland has focused on characterizing the optical properties of these PDLC films for use in RCDs. The primary figure of merit for the RCDs is the change in the fraction of incoming photon momentum absorbed when the device is switched off versus when it is switched on. The maximum fraction of momentum that can be transferred to a sail from a photon is 2.0 if the photon is specularly reflected. On ICAROS, the fraction of momentum transferred when the device was switched on was 1.51 and when the device was switched off it dropped to 1.40 as the reflection became more diffuse. The PDLC investigated at the University of Maryland has a fraction of momentum transfer of 0.43 when on and 0.93 when off for a difference of 0.50 [10]. As the difference in the fraction of momentum transferred increases, the force difference between the regions increases and therefore the torque available for controlling the sail increases in step. Work completed thus far at the University of Maryland has characterized the PDLC films based on the thickness of the film and yielded data for the energy usage, weight density, and difference in fraction of momentum transferred for several different thicknesses. In order to determine the feasibility of PDLC RCDs for a NEA Scout-size solar sail, the raw numbers provided by the researchers for the PDLC film needed to be translated into a control torque produced by a reaction control device. This could be accomplished by taking the fraction of momentum transferred provided by the researchers and calculating the control torque that would be generated as a result. The torque produced by a single RCD can be calculated using Eq. 3: where T is the torque in Nm, P is the solar radiation pressure at 1 AU (4.52E-06 N/m²), p_diff is the difference in the fraction of momentum transferred when the device is off versus on, r is the length of the sail boom in m, A is the total area of the RCD in m², and α is the sun incidence angle of the sail in radians. This is a simplified version of the thrust equations presented in McInnes [1] and Wright [9] and is valid for a flat plate. For a preliminary estimate of the feasibility of the technology, this calculation was deemed acceptable. The results of the study are presented in Table 4. The required area was derived based upon the area needed to generate a control torque of 5 x 10^-6 Nm, considered the largest solar torque that will be encountered during the NEA Scout mission. The difference in momentum transfer declines as the thickness of the PDLC decreases which results in a larger area requirement to achieve the torque. However, the thinner PDLC films have a reduced power and weight density meaning even though they may need to be larger in size to meet the requirements, the mass and power consumption is reduced compared to thicker films. The Area Required and Mass of the RCD vs PDLC thickness is shown in Fig. 15. From the initial calculations, the power, weight, and area requirements were deemed to be on a feasible order for a NEA Scout size solar sail. If the RCD is placed flat against the plane of the sail, it will only be able to generate a change in the force that is normal to the surface of the sail which would limit the device to creating control torques about the x-axis and the y-axis. In order to generate a z-axis torque, a force must be created by the RCD tangential to the sail or in other words, in the plane of the sail. The solution derived is to place the RCD at an angle to the plane of the sail. This angle, defined as the actuator angle, is positive for a counterclockwise rotation and is the angle between the plane of the sail and the plane of the PDLC film. The next step of the study was therefore to calculate the ideal angle for the actuator that will allow for the maximum control torque about the z-axis across the widest range of SIA. Calculating the control torque from the introduction of an actuator angle is done by modifying the cosine squared term in Eq. 3 as shown in Eq. 4: where β is the actuator angle in radians. This equation yields the control torque about the z-axis, or the roll control torque. This equation can then be plotted for several actuator angles across a range of sun incidence angles as shown in Fig. 16. The majority of actuator angles generate a control roll torque greater than the solar disturbance torque across nearly the entire range of sun incidence angles. The maximum sun incidence angle expected under nominal mission conditions is 50˚, and thus, the RCD would be an effective means for controlling the roll torque. The best candidate was selected as the 40˚ actuator angle because this resulted in a control roll torque greater than the solar disturbance torque across the range of anticipated sun incidence angles. Moreover, as the actuator angle is increased, the force normal to the plane generated by the RCD will decrease. An RCD with an actuator angle of 90˚ would generate no normal force and could not control the x-axis and y-axis torque, and 40˚ was a compromise between normal and tangential forces or between in-plane (x-axis and y-axis) and out-of-place (z-axis) control torque. The final step in the study was to calculate the control torque generated by a pair of RCDs. The preliminary work was carried out accounting for a single RCD with the two regions placed on opposite corners of the sail. The design specifies that an actual implementation would most likely have two pairs of RCDs placed on the four sail corners. The actuator angle used in these calculations was 40˚ and the RCDs magnitude of control torque was calculated for across the range of SIA and for several difference roll angles. The resulting magnitude of the control torque (the norm of the torque on all three axes) is shown in Fig. 17 along with the solar disturbance torque. The x-axis is a range of sun incidence angles while the different shaded lines represent the clock angle of the sail. Overall, the graph and associated numbers proved that the RCD set-up as specified for the current study works across the majority of sun incidence and clock angles. The magnitude of the control torque is greater than the disturbance torque across nearly all the expected sun incidence angles and at most roll angles. The conclusions of this work was that RCDs composed of PDLC films would be both practical and effective for a solar sail of NEA Scout’s size. Using the thickest PDLC film, each RCD would need to be 0.411 m² and would consume 1.86 Watts of power with a weight penalty of 0.0164 kg. The total power consumption would thus be slightly over 3.6 watts and the total weight would be slightly more than 30 grams accounting for only the PDLC films. To give a sense of scale, the total battery capacity on NEA Scout is 48 watt-hours. With peripherals, including wiring and the electrodes needed to apply the voltage, the total weight of the devices would be expected to be less than 100 grams compared to a total weight for NEA Scout of over 11 kg [6]. Based on this work, RCD devices would be feasible on a NEA Scout platform. The total penalty to weight and power would need to be considered, but the ability to control the attitude of the sail without expending propellant makes the reflective control device a worthwhile investment that could expand the scope of possible missions for a solar sail. Further work must be done to develop the entire RCD architecture, and testing would need to be carried out on larger samples with a potential flight demonstration as well. The Reflective Control Device technology has already been successfully proven by ICAROS, and an implementation on a future NASA solar sail mission is probable given the state of the research and the maturity of the technology. 5. Sail Recovery Modeling Among the range of off-nominal situations that NEA Scout may encounter, the most critical is that in which the solar sail flips around with the back of the sail facing directly towards the sun. There are several possible causes for this circumstance but the most likely source would be a failure of software resulting in a complete loss of the ACS. Even with the AMT in a trimmed position, the sail is not passively stable and experiences a small solar disturbance torque. Over a long enough time period, without any active control from the ACS, solar radiation pressure alone will eventually cause the sail to flip around with the back exposed to the sun. Although a complete turn-around is a severe situation, it does not necessarily have to result in a failed mission if the computer systems are able to reboot and an appropriate safe mode can be implemented. As shown in the diagram of NEA Scout in Fig. 1, the booms (support beams) for the sail are shaded from the sun by the solar sail itself. During the planning stages of NEA Scout, [5] this design choice was implemented when it was discovered that the booms would suffer severe thermal deformation if they were to be exposed to the sun. Even though the 7 meter-long booms are made of steel, they are extremely thin and can distort up to 1 m at the tips under direct sunlight in space. If the sail flips around and the backside faces towards the sun, there will be nothing to shield the booms from the thermal radiation and they will severely deform within an estimated time frame of 15–30 seconds. Exact predictions of the extent of boom deformation are nearly impossible to make because of the uncertain thermal stresses that the booms will be subject to and the difficulty in modeling thermal deformation. The best estimates provided by the Jet Propulsion Laboratory specify a nominal boom deformation of 0.6 m and a maximum deformation of 1.2 m [12]. Even in the nominal case, this deformation significantly affects the performance of the sail. As the sail curvature increases from the deformation, the diffuse reflection of sunlight by the sail increases and thus the solar torque increases. The present investigation was undertaken in order to determine how long it would take the sail to flip under different situations, what conditions the sail will experience if it does flip, what would be an appropriate safe mode for recovery of the sail, and how much would the flip and recovery cost in terms of fuel and overall impact to the mission. The first step towards simulating a sail reversal was to create representations of the backside of the sail. In total, five different models were generating covering the range of possible booms deformations. The first model, labeled as the “undeformed” backside sail model, was intended to serve as a baseline with no deformation. This model, shown at the far left of Fig. 18, was made from the front side model of the sail but with the sail normals reversed in the tensor model. The other four models were simplified pyramids which attempted to account for the uncertain deformation. Four “deformed” backside sail models were needed because both the magnitude and the direction of the deformation of the booms is unknown [12]. Two of the deformed models have the booms deforming towards the sun, and the other two have the booms deforming away from the sun with one model in each pair having a magnitude of deformation of 0.6 m and the other having the maximum 1.2 m estimated magnitude of deformation. Two of these models are shown in the center and right of Fig 18. After the sail models had been completed, the next phase of the study was to develop a script that modeled a loss of attitude control of the sail. An existing script was modified to create a “free drift” simulation [13] in which the sail is allowed to freely rotate solely under the influence of solar radiation pressure. This script models a complete shutdown of computer systems which could have any number of causes for a spacecraft and actually occurred on LightSail 1 as a result of a file size overload [4][. (LightSail 1 systems recovered eight days later with a radiation-induced spontaneous computer reboot and the mission was able to continue successfully). The first round of trials using the free-drift script were used only to determine the behavior of the sail under no attitude control and did not include any attempts at recovery. Each model of the sail was tested starting at a SIA of 45˚ (a sail incidence angle greater than 90˚ indicates that the sail has reversed) with three different initial body rates. The results of the first phase of three runs for the non-deformed backside sail model are summarized in Table 5: The time for the sail to flip is heavily dependent on the initial starting body rate of the sail. The rate of 0.04 deg/sec on the y-axis was selected because that is the maximum body rate that is allowed during a communications slew of the spacecraft. This case modeled what would occur were the systems to shut down during a slew at the maximum rate which was considered to be the situation of most concern. Fig. 18 shows several parameters from the simulation of the non-deformed model run with an initial body rate of 0.04 deg/sec on the y axis. The bottom plot shows sun incidence angle (in degrees) versus time (in 104 seconds) and the moment when the sail reverses is identified as when the SIA goes over 90˚. Furthermore, an oscillatory motion of the sail can be observed in this plot. The sail clearly flips (SIA greater than 90˚) and then returns to the front side of the sail (SIA less than 90˚) repeatedly. This behavior mimics that observed on NanoSail-D2, when it was flown in 2011 [3]. NanoSail-D2 could not be controlled, and began to continuously rotate from back of the sail to the front reaching body rates on the order of tens of degrees/second. The behavior demonstrated in this case is much the same although not as severe. The top plot shows the body rate (in degrees/second) for all three axes versus time. The body rate on the y-axis builds over time, and after 6 hours, has reached an alarming 0.2 deg/sec. However, this is only after around 6 hours of a complete loss of control and it is estimated that if the computer systems were to completely shut down, they could be rebooted from the ground within 15 minutes. Therefore, it is not expected that the sail would have enough time to build up substantial body rates on any axis. Based on the results from running the “free drift” script [13], four of the worst cases were selected to be run in a recovery script that attempted to model a recovery from the loss of attitude control. The primary objective of this phase of the study was to develop and test a safe mode that could be implemented by the G&C software with a minimal impact to the mission concept of operations. The safe mode was defined by the G&C team as using the RCS jets to point the spacecraft at the sun in as short a time frame as could be achieved. The reasoning behind this decision is that it would allow the spacecraft to regain knowledge of its attitude and charge the solar panels to ensure that all systems will be adequately powered. When the spacecraft initially reboots, it will have no information about its attitude and therefore will have to rely on the sun sensors to point at the sun under the power of the RCS thrusters. Two different recovery initiation criterion were defined. The first began recovery from the maximum SIA in order to mimic a worst case scenario while the second condition began recovery after a time period of 15 minutes to model the estimated reboot time for the computer systems. The main parameters to calculate for the study were the time for the spacecraft to reach a sun-pointing attitude, and the fuel used by the RCS to achieve this condition. Table 6 summarizes the results of the recovery simulations run with the 1.2 m deformed sail model which had demonstrated the most severe behavior in the initial phase of the research. Even under the most extreme initial conditions and starting recovery from the maximum sun incidence angle, the time to recover was only 0.733 hours or 44 minutes and the fuel use was less than 12 grams. The conclusion from the second phase of the study was that the spacecraft would be able to successfully recover from a complete loss of computer systems. Moreover, the fuel and time penalty from recovery was not significant for a one-time event. This analysis further showed that a safe mode defined as sun-pointing using the RCS is practical and will be implemented into the G&C software. Another significant finding was that using IMU body rates to inform the G&C software during recovery was preferable to using star tracker rates. The star tracker is designated as the main provider of body rates and attitude knowledge during the parts of the mission where it is not expected that the body rates will be higher than 0.4 deg/sec. The IMU body rates are nominally supposed to be used only upon detumble when the star tracker will be rendered ineffective by the sun. However, in the recovery cases, using the IMU body rate information significantly reduced fuel usage because the overall body rate caused by the RCS maneuver is relatively high. At these higher rates, the IMU information can be more effectively filtered and was found to be the better choice to inform the software of the state of the spacecraft. More work on any recovery aspect of the mission should include developing a more accurate model of the backside of the sail that accounts for the boom deformation. Once that has been completed, the sail model can be tested in the free drift and recovery scripts to better characterize the behavior of the spacecraft upon flipping and the effects on the mission objectives due to this situation. 6. Sun Sensor Testing The G&C software cannot correctly manage the ACS without a proper knowledge of the spacecraft’s state including position, orientation, and rates. The NEA Scout ACS contains several different sensors which are applicable in different stages of the mission. Three coarse sun sensors are to be used primarily on deployment from SLS in order to point the spacecraft at the sun and allow the solar panels to begin charging. The sun sensors are able to provide the information necessary for the ACS to calculate the sun vector in the body frame of the spacecraft which orients the spacecraft relative to the sun [14] The sun sensors can also be used for rough rate estimates by determining the change in spacecraft orientation over time. Furthermore, the sun sensors will be used at times when the star tracker is “blinded” or has a reduced field of view whether that be from the Earth, Moon, or Sun. Thus, the sun sensors are a crucial part of the mission and must be tested both for accuracy and to ensure that the data collected by the sensors is correctly interpreted by the G&C software. NEA Scout uses three sun sensors attached to three different-facing surfaces of the spacecraft to ensure that one sensor can sense the sun at all times. One individual sun sensor is shown in Fig. 20. Each sun sensor is composed of four Silicon photodiodes that are designed to be most sensitive in the visible spectrum with wavelengths of light from 480 nm to 660 nm [14]. These four photodiodes produce an electrical signal with a strength proportional to the intensity of the light incident on the photodiode. From these four diode counts and the location of the sun sensor on the spacecraft, a sun-vector in the body frame of the spacecraft will be calculated by the G&C software. The initial task was to determine if the sun sensor could be correctly figured. The sun sensors for NEA Scout are provided by Blue Canyon Technologies (BCT) and the particular sun sensor available for testing [15] was an engineering development unit (EDU) that included the necessary communications interface to allow the sun sensor inputs to be sent to a personal computer. Configuration tests were done in the robust product demonstration test-bed in building 4600 at MSFC. The sun sensor was attached flat to a mounting bracket and subjected to electromagnetic radiation from a high intensity incandescent lamp. Several orientations of the sun sensor were tested. After each test, the resulting raw data was fed into a G&C processing script which calculates the sun vector of the spacecraft. The vectors that were output could be manually checked because the angle of the sun sensor relative to the lamp (effectively the sun incidence angle) was known and the sun sensor was modeled as being on the +z face of the spacecraft. For a sun incidence angle of 0˚, the resulting unit sun vector should be [0, 0, 1] which was found to be the result. An additional task for initial testing was taking the sun sensor outside and verifying that the photodiodes were generating the correct outputs when exposed to the sun at 1 AU. From the BCT technical information, [15] the photodiodes have a minimum electrical count threshold they should register when exposed to the sun at 1 AU. After taking into account the 30% attenuation of the sun’s intensity by the Earth’s atmosphere and the seasonal decrease in solar intensity due to the 23.5˚ obliquity of the Earth, the photodiode counts were in the specified range. These initial configuration tests demonstrated that the engineering development unit was performing as expected and that the G&C software could convert the raw data from the sun sensors into usable information. The next step was to perform a series of tests with the sun sensor attached to a rotating telescope that could provide truth data in order to determine the accuracy of the sensor. Unfortunately, time constraints did not allow for this round of testing to be completed before the drafting of this report. However, the associated equipment for the testing was manufactured and the test description that follows will executed before the full avionics testbed. Testing is to be carried out at the Automated Lunar and Meteor Observatory (ALaMO) located on the Redstone Arsenal. This facility has a 14 inch telescope atop a 15 meter tall tower with a roll-off roof. This telescope is ideal as it features automated sun tracking and can be operated across the range of testing rates and right ascension and declination angles. Five different tests were developed to test the accuracy and capabilities of the sun sensor as summarized in Table 7. For each case, the raw diode measurements will be collected as well as the inertial right ascension and declination angles from the telescope that will serve as truth data. All measurements are to be taken at 5 samples/second and the sun sensor information is read and written to .csv files by a Python script specifically developed for the tests. With set-up time and potential delays between trials, the entire procedure duration is estimated to be four hours and will require a clear sky for the duration of the experiment. The Blue Canyon Sun Sensor Engineering Development Unit [16] electronics set is shown in Fig. 21: The sun sensor itself is mounted on a 3-D printed plate specifically designed for this test and modeled using Computer Aided Design. The grey box on the right of Fig. 21 is the XACT attitude control system box. It interfaces with the red box designated the XTC communications tool that is used to communicate between the sensor and a computer [16]. The red box is connected to a power supply and via a USB cable to a computer. The information from the sun sensor is sent to the XACT ACS using the RS-422 communication protocol. The XTC adapts this signal into a USB signal that can be used to record and save data for later analysis. The results from the initial sun sensor testing demonstrate that the sun sensor is correctly configured for the NEA Scout mission, and that the G&C software can correctly read and interpret the inputs to generate the sun vector used to orient the spacecraft. Although the final phase of sun sensor testing has yet to be completed, it will provide an accuracy check for the sun sensor and is scheduled to be completed before the full avionics testbed. Based on this study, the G&C software will be able to rely on the information provided by the sun sensor in order to issue commands to the Attitude Control System. III. Conclusions Validation and Verification of any complex system is composed of many subtasks rather than one main assignment. Each individual survey has its own takeaways and implementations that combine together to result in a robust spacecraft that is prepared to meet all mission objectives. The conclusions from each investigation detailed in this report are summarized as follows: 1. The RCS has a quantified level of manufacturing variability in both the angle and magnitude of the thrust from each jet. The variation is of concern for the initial detumble of NEA Scout upon deployment from SLS when body rates could be as high as 10 deg/sec on each axis. A number of cases were run to simulate the detumble maneuver with different combinations of deviations in the magnitude and direction of the thrust. In the worst documented case, the fuel usage of the RCS was four times greater than the nominal case. However, considering the total amount of fuel used, any excess fuel consumption during detumble resulting from manufacturing variations would not severely impact the mission. Time to detumble was not significantly altered by the thrust variation in the RCS. 2. A decrease in the specular reflectivity coefficient caused by meso-scale sail wrinkling will result in a larger solar torque on the spacecraft. Modeling of the decreased specular reflection in the full spacecraft model showed that the increased solar torque will not significantly impact the mission across the range of expected operating solar incidence angles because the solar torques can be adequately managed by the AMT and RCS. Even the lowest possible specular reflection value would not result in a failure to complete the mission objectives. However, more research needs to be done to characterize the optical properties of the entire sail. 3. Passive roll control by means of oscillation about a stable equilibrium angle is possible for NEA Scout. Above a 20˚ sun incidence angle, there are specific clock angles around which the spacecraft oscillates rather than continuing to roll and build up z-axis (roll) momentum. If a stable clock angle is attained, the RCS will not be required to activate to desaturate the roll momentum of the RW. By utilizing passive roll control, the sail can be stabilized in roll without the need for the RCS to desaturate the reaction wheels. Moreover, if the spacecraft is initially at an unstable clock angle, it can be placed into a stable attitude with a relatively minor firing of the RCS jets. Overall, this option would reduce fuel usage when compared to the current concept of operations which specifies frequent firings of the RCS to desaturate the RW roll momentum. 4. Reflective control devices (RCD) are a feasible option for controlling the sail without fuel consumption. These RCDs would be composed of polymer dispersed liquid crystal (PDLC) films with changeable optical properties. By positioning one RCD pair on each boom, an in-plane (x-axis and y-axis) control torque can be generated to counteract the solar torque. Furthermore, by tilting the RCDs at an angle to the sail, a roll control torque can be generated allowing for all control of the torque on all three axes of the spacecraft. The total weight and power cost of the RCDs for a sail the size of NEA Scout is well within practical consideration. 5. In the event of a complete loss of computer systems, the sail will reverse with the back side facing towards the sun because the spacecraft is not statically stable. This will result in severe deformation of the booms and will set the sail oscillating from the back and the front at a body rate up to 0.2 deg/sec. In order to recover from this situation, a safe mode was defined as using the RCS to point the spacecraft at the sun relying on the IMU body rates to inform the G&C software. The fuel consumption and time to implement this maneuver should not affect the mission for a single occurrence. 6. The Blue Canyon Technology sun sensors supplied to the NEA Scout mission need to be tested for accuracy and compatibility with the software. Initial testing has shown the sensors are correctly configured and the data inputs can be interpreted by the control software. Further testing to characterize the accuracy of the sun sensors has been defined. Although these investigations covered a wide range concerns, they are but a small subset of the tasks that comprise the validation and verification of a system as complex as a spacecraft. The results of the documented studies have proved that the G&C software which manages the ACS is robust and can successfully carry out all mission objectives. The development of the NEA Scout software is nearing completion and these results have provided the team with confidence that the control model will be able to execute its responsibilities. The ground assessment and examination phase while shortly be finished and the ultimate test of the engineering will come with the launch of Near Earth Asteroid Scout on SLS Exploration Mission One. IV. Acknowledgements The author would like to thank the entire Near Earth Asteroid Scout Guidance and Control team in EV-42 at NASA Marshall Space Flight Center for their guidance and instruction throughout the duration of the work documented. In addition, the author would like to thank all the engineers, scientists, technicians, and laborers upon which this work, as in all engineering work, builds. This report would not be possible without the foundations of space exploration laid down by countless engineers over the decades that have put humans on the brink of becoming a multiplanetary species. V. References [1] McInnes, C., Solar Sailing: Technology, Dynamics, and Mission Applications, Chichester, UK: Springer-Praxis, 1999. [2] Tsuda, Y., Mori, O., Funase, R., Sawada, H., Yamamoto, T., Saiki, T., Endo, T., Yonekura, K., Hoshino, H., and Kawaguchi, J., “Achievement of IKAROS — Japanese deep space solar sail demonstration mission”, Acta Astronautica, Vol. 82, №2, 2013, pp. 183–188. [3] Johnson, L., Whorton, M., Heaton, A., Pinson, R., Laue, G., and Adams, C., “NanoSail-D: A solar sail demonstration mission”, Acta Astronautica, Vol. 68, №5–6, 2011, pp. 571–575. [4] Betts, B., Nye, B., Vaughn, J., Greeson, E., Chute, R., Spencer, D., Ridenoure, R., Munakata, R., Wong, S., Diaz, A., Stetson, D., Foley, J., Bellardo, J., and Plante, B., LightSail 1 Mission Results and Public Outreach Strategies, Pasadena, California: Planetary Society, 2016. [5] McNutt, L., Johnson, L., Clardy, D., Castillo-Rogez, J., Frick, A., and Jones, L., Near-Earth Asteroid Scout, Huntsville, AL: NASA Marshall Space Flight Center, 2014. [6] Orphee, J., Diedrich, B., Stiltner, B., Becker, C., and Heaton, A., “Solar Sail Attitude Control System for the NASA Near Earth Asteroid Scout Mission”, International Symposium on Solar Sailing, Huntsville, AL: NASA Marshall Space Flight Center, 2017. [7] Department of Defense Documentation of Verification, Validation, and Accreditation (VV&A) for Models and Simulations.” United States Department of Defense, DOD Instruction 5000.61, December 9, 2009. [8] Heaton, A, Ahmad, N., and Miller, K., “Near Earth Asteroid Scout Thrust and Torque Model”, International Symposium on Solar Sailing, Huntsville, AL: NASA Marshall Space Flight Center, 2017. [9] Wright, J., Space sailing, Yverdon [u.a.]: Gordon and Breach, 1993. [10] Ma, D., Murray, J., and Munday, J., “Controllable Propulsion by Light: Steering a Solar Sail via Tunable Radiation Pressure”, Advanced Optical Materials, Vol. 5, №4, 2016 [11] Wang, U., “Making Smart Windows that Are Also Cheap”, MIT Technology Review Available: https://www.technologyreview.com/s/420221/making-smart-windows-that-are-also-cheap/. [12] Carlisle, G., Stuart, J., and Lantoine, G., “NEA Scout Peer Review”, 2016. [13] Orphee, J., and Stiltner, B., “Near Earth Asteroid AMT-RCS Calibration”, 2017. [14] Bullock, D., Edberg, D., Heaton, A., Stiltner, B., Becker, C., Diedrich, B., and Orphee, J., Performance Characterization of Sun Sensor and Inertial Measuring Unit for the Near Earth Asteroid Scout Mission, Huntsville, AL: NASA Marshall Space Flight Center, 2017. [15] Blue Canyon Technologies, BCT Sun Sensor Technical Drawing Rev. C, Doc. Num: 3ICD1065 Boulder, Colorado: 2016. [16] Blue Canyon Technologies, XACT GUI User Guide Revision B, Drawing Number: 3PR0750, Boulder, Colorado: Blue Canyon Technologies, 2014. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",NASA Internship Report,3,published,316,13048,0.0,0,0,0,0,0,0
126,77,670.9083520225232,20,https://medium.com/p/deep-neural-network-classifier-32c12ff46b6c,1,None,2017-07-25 17:54:00,17.65,14,1728,2017-07-24 11:31:00,"['Machine Learning', 'Neural Networks', 'TensorFlow', 'Scikit Learn', 'Python']","Deep Neural Network Classifier A Scikit-learn compatible Deep Neural Network built with TensorFlow TensorFlow is a open-source deep learning library with tools for building almost any type of neural network (NN) architecture. Originally developed by the Google Brain team, TensorFlow has democratized deep learning by making it possible for anyone with a personal computer to build their own deep NN, convolutional NN, recurrent NN, that can be applied in a diverse array of fields. Scikit-learn is an open-source machine learning library with implementations for developing and perfecting numerous types of machine learning models. Both libraries feature high-level functionality that make designing models in Python relatively simple, but both libraries also have their limitations: Scikit-learn has no native implementation for neural networks, while Tensorflow has no built-in functionality to efficiently evaluate a wide range of network hyperparameters. We can overcome both these problems by developing a Scikit-learn compatible deep neural network class using TensorFlow. We can then take advantage of Scikit-learn built-in model hyperparameter tuning tools such as GridSearchCV or RandomizedSearchCV to optimize our deep neural network. All the code is written in Python and available on GitHub on my machine learning projects repository. The main files are dnn_classifier.py, the Python file containing the classifier, and Deep Neural Network Classifier.ipynb, a Jupyter Notebook with the implementations of the neural network. I welcome any criticism/comments and the code will change as I improve it over time. This project was inspired and aided by Hands-On Machine Learning with Scikit-Learn and TensorFlow by Aurelien Geron. Code Here is the code for the deep neural network class in its entirety. Using the Model with the MNIST dataset Now that we have developed the model, it’s time to put it to use on some traditional machine learning datasets. To test it out, we can use the MNIST hand-written digit dataset, a collection of numbers written out by hand and labeled. First we can import the classifier object and load the MNIST data. Then we can instantiate an instance of the class using the default hyperparameters (we’ll pass in a log directory so we can track training on TensorBoard and a random state to ensure consistent output across runs). Then we can train the classifier and pass in a validation set to see progress. The classifier prints the statistics every show_progress epochs (which was left at the default 10). Because we used a validation set, the classifier will implement early stopping when training. This means that if the validation loss does not improve for a given number of epochs, the classifier will stop training because further iterations will likely not improve performance. Each iteration that the validation loss does decrease, the model weights are saved so that the best model can be restored at the end of training. This is one way to combat overfitting on the training set and creating a classifier with a lower variance that can better generalize to novel data. We can also launch Tensorboard from Windows PowerShell to observe the training curves: We can see that the validation accuracy peaked at epoch 10. This is valuable information because it shows that we might only need to train for 1/10th of the original amount of training epochs. The validation accuracy also begins to increase towards the end (before it is stopped early) suggesting that we might benefit from a few more training epochs. It looks like the minimum validation loss occurred near the first iteration and then rose after that before heading back down. Perhaps it we increase the maximum number of epochs without a decrease in the validation loss, we can achieve better accuracy. The final accuracy on the test set can be assessed: Our default neural network achieves near 95% accuracy on identifying hand-written digits. That’s not great, but we have not optimized any of the network hyperparameters. Let’s take a look at a couple of predictions by plotting the handwritten images themselves and the probability (confidence) that the classifier has for each digit: And the results As can be seen, not too bad. This isn’t a very difficult task, but it can be easy to see how results such as this translated to more practical data, such as self-driving cars identifying objects in their environment (although that is likely a task better suited to convolutional NN or a CNN in combination with a recurrent NN). Scikit-learn Hyperparameter Tuning We can significantly improve the accuracy by tuning the hyperparameters of the model using Randomized Search CV and/or Grid Search CV. This is the reason why we made the model compatible with Scikit-learn in the first place. We will create a sensible set of parameter distributions and let Scikit-learn evaluate a range of them for us. This process may take several hours to days depending on your set-up. I would recommend running this on your GPU if possible or investing in setting up a Google Cloud compute instance (they will give you $300 in credit when you create a new account). Let’s see what the best hyperparameter configuration is from the random search: And the performance on the test set: Not too bad after a relatively small search through the hyperparameter space. We only used 15 different combinations, so it is likely that a better tuned network exists. After we use a randomized search, we can further improve our model by doing a grid search focused around the best settings returned by the search. Still not perfect, but improving. The best model likely exists out there but we haven’t found it yet. For now, we’ll take 98.2% accuracy which is probably better than I would be able to do (I know that when I write numbers, people can usually identify them 80% of the time). As a final step, we want to save our best model weights. This will save all of the model weights so if we create an identical TensorFlow graph, we can put all of these weights into the graph and there will be no need for the time-consuming training step (assuming we still want to classify hand-written digits. If we change the task, we will have to re-train the model). Titanic Dataset Let’s take a brief look at the performance on another standard machine learning dataset, the Titanic passenger list. The goal with this dataset is to predict which passengers survived and which did not based on age, passenger class, gender, fare paid, cabin, destination, etc. This is a historically accurate dataset and quite interesting to investigate. We can first load in the dataset into a Pandas dataframe and look at the info: The labels will be whether or not the passenger survived (1 for yes and 0 for no). The features will be all the columns except for survived (that would make things a little too easy), name, body identification number, and boat. Let’s get the data into training and testing sets: It looks like we have 981 passengers in our training data and 328 in our testing data. After one-hot encoding, there are 2831 features to use to determine whether or no a passenger survived. We can now train on the data: Well, we are clearly overfitting the training data! We can reduce that by implementing dropout, reducing the number of hidden layers, reducing the number of neurons per layer, or implementing early stopping. For now let’s check out the accuracy on the test set and then implement dropout: Even though we are badly overfitting the training data, our classifier can still pick with 76% accuracy whether or not a passenger on the Titanic would survive the journey. Let’s try dropout with 0.75 of the input to each layer retained: Well, dropout clearly reduced the extent of overfitting as evidenced by the lower scores on the test set. However, it did not substantially improve overall accuracy on the test set (dropout of 0.5 achieved 75% accuracy). We could use randomized search to narrow down the range of hyperparameters and then grid search to optimize the network but I’ll leave that for now. Just for fun, let’s put myself and my dad on the Titanic and see what our survival chances are. I will put in some true information (I’ll be generous with my dad’s age) and then randomly choose other feature values. Well, we can’t claim to know how the model chooses, but it’s clear that I should avoid boats for a while. Probably not the most useful information, but I trust the algorithm and its judgement that my dad is the type of person who would survive the sinking of a ship (definitely a compliment). Next Steps There are a number of issues with this classifier, with the most obvious being that all of the hidden layers have identical hyperparameters. They have the same number of neurons, activation function, learning rate and other hyperparameters even though that is typically not the case in a real-world implementation. As an example of the limitations, the original paper on dropout: “Dropout: A Simple Way to Prevent Neural Networks from Overfitting” used a dropout rate of 0.2 on the input layer and around 0.5 on the hidden layer. The neural network developed here uses the same dropout rate across layers which may be why the performance did not significantly improve with dropout. Now that you have an idea of the capabilities of this neural network, have at it! Feel free to copy, disseminate, and adapt this code. Let’s see what you can do with it and how it can be improved! Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",Deep Neural Network Classifier,4,published,9791,1778,0.04330708661417323,1,0,0,0,1,1
128,252,668.76774370022,56,https://medium.com/p/object-recognition-with-googles-convolutional-neural-networks-2fe65657ff90,2,None,2017-07-27 21:17:00,23.69,12,5738,2017-07-26 20:49:00,"['Machine Learning', 'Neural Networks', 'Python', 'Object Recognition', 'Image Classification']","Object Recognition with Google’s Convolutional Neural Networks Classifying Images Using Google’s Pre-Trained Inception CNN Models Convolutional neural networks are the state of the art technique for image recognition-that is, identifying objects such as people or cars in pictures. While object recognition comes naturally to humans, it has been difficult to implement using machine algorithms and until the advent of convolutional neural networks (beginning in earnest with the development of LeNet-5 in 1998) the best computer was no match for the average child at this deceptively challenging task. Recent advances in CNN design, notably deeper models with more layers enabled by the availability of cheap computing power and enhanced techniques such as inception modules and skip connections, have created models that rival human accuracy in object identification. Moreover, CNNs are poised to make real-world impacts in areas from self-driving vehicles to medical imaging evaluation (a field where computers are already outperforming humans). However, training convolutional neural networks, in particular implementing the backpropagation method used to update the model parameters, is computationally expensive. The greater the amount of training data (labeled images) and the deeper the net, the longer the training time. Reducing network depth or the amount of training data is not advisable as the performance of any machine learning system is directly related to the number of quality training examples, and deeper networks (up to a point) perform better. Additional performance-enhancing techniques, such as dropout or batch normalization, increase computation time as well. Properly training a useful image recognition network on ten of thousands of labeled images could take months or longer on a personal computer. Moreover, developing the correct architecture and selecting the optimal hyperparameters requires training the network hundreds or thousands of times which means we had better be prepared to spend several decades at this project if we limit ourselves to laptops. Fortunately, Google has not only developed several iterations of an ideal architecture for image classification (in 2014 GoogLeNet won the Imagenet Large Scale Visual Recognition Challenge where models must identify 1000 different classes of objects) but they have also released models fully trained on 1.2 million images across 1000 categories. This means that instead of building our own network and enduring training epoch after training epoch, we can used the Google pre-trained model to perform high-accuracy object recognition. All of the Python code for this project was written in a Jupyter Notebook. The complete notebook and project is available on my machine learning projects Github repository. This project was adapted from the Google Tensorflow slim walkthrough Jupyter Notebook and was aided by the book Hands-On Machine Learning with Scikit-Learn and Tensorflow by Aurelien Geron. Inception Neural Network Google has a number of neural network models that they have made available for use in TensorFlow. The two models we will use here are the Inception-v3 and Inception-v4. They both make use of inception modules which take several convolutional kernels of different sizes and stack their outputs along the depth dimension in order to capture features at different scales. Both networks also borrow the concept of a residual network with skip connections where the input is added to the output so that the model is forced to predict the residual rather than the target itself. Using this architecture, Inception-v4 was able to achieve 80.2% top-1 accuracy and 95.2% top-5 accuracy on the Imagenet dataset, or in other words, the network correctly determined the object in an image 4/5 of the time and 19/20 times, the correct prediction appeared in the top five probabilities output by the model. Other models developed by Google (notably Inception-ResNet-v2) have achieved slighter better results, but the Inception-v3 and -v4 networks are still at the top of the field. Retrieving the Pre-Trained Models To obtain the appropriate Python libraries, go to the tensorflow/models GitHub repository and download or Git clone the repo. All the work we will be doing should be run from within the slim library, so navigate to that folder and create a new Python script or Jupyter Notebook there. The next step is to download the most recent checkpoint of the Inception networks. The list of models can be found on the tensorflow/models Git Hub. To download a different model, simply replace the “inception_v3_2016_08_28.tar.gz” with the architecture of your choice (other code may also need to be modified). Processing Images for Use Now that the models have been downloaded, we need a way to ensure the images are the right configuration for the network. The Imagenet images are all 299 pixels by 299 pixels (height x width) x 3 color channels (Red-Green-Blue). Therefore, any images we send through the network will have to be in the same format. The Inception networks also expect images scaled to be between 0 and 1, which means that the pixels values needed to be divided by 255 (the maximum intensity value for a color). While this is relatively straightforward, the slim library we are working in already has a built-in picture processing function in the inception_preprocessing.py script in the preprocessing directory. This function takes in an image as a three-dimensional array of pixels values and returns the correctly formatted array for evaluation by the Inception network. It also has a number of other capabilities for use with training such as shifting or altering the image which make the network invariant to aspects of the image (such as orientation) that do not affect the object in the image. This technique can also be use to augment a small dataset by including copies of each image that have been shifted, scaled, or rotated. We will pass in is_training = False so the image will be processed for evaluation and will only be resized. The images that we want to classify should be placed in a new images directory located with the slim folder (or change the root_dir in the preceding code). For now, we will stick to jpg and png images although other formats could also be processed. To generate the correct images, we create a TensorFlow session to run the TensorFlow operations. We return the raw image so we can plot it, as well as the processed image shaped into [batch_size, height, width, color_channels]. Display Example Images We can download any image we want and place it in the images directory. However, for the network to have any chance of being correct, we will need images that are included in the ImageNet dataset. The complete list of 1000 classes can be found as text here. (When the network is used for evaluation with is_training=False, it will have 1001 classes because it adds an extra “background” category.) Choose a couple of images and download them to the images directory. It is best if the images are close-up and feature the object in the center. We can first write a small function to plot the image using matplotlib.pyplot and with the %matplotlib inline magic function to display plots in the Jupyter Notebook. We can also check the size of both the raw image and the processed image: In the case of this image, because the original size was too small, the preprocessing function adds extra pixels by interpolating between existing pixel values. This results in a blurry image which should not significantly affect the performance of the CNN. One more image for fun: Image Recognition The heart of this project is the prediction of classes for the pictures. Now that we have several images (feel free to gather as many as you like. It might be interesting to see what the CNN guesses for classes of images it never saw during training.) We will write a function that takes in the name of an image and the version of the CNN to use (currently limited to the Inception architecture, either “V3” or “V4”), plots the raw image, and shows the top-10 predictions below the plot. The code is relatively straightforward. We apply the correct argument scope and function depending on the model. (Note that we need to set the number of classes to 1001 and is_training=False.) After constructing the TensorFlow computational graph using the inception_v3 function (or inception_v4), we create a TensorFlow session to feed the image through the network. We use a saver to restore the model weights that we downloaded earlier. The graph returns the logits, which are the unscaled outputs from the network, and the probabilities, which are the result of passing the logits through the softmax activation function. After we get the predictions, we create a list of tuples with the index of the prediction and the associated probability. We then sort these tuples by probability and print out the top-10 class names by probability according to the network. Prediction Results Here a few typical results. Pretty good predictions from Inception_v3! Let’s see what v4 predicts: It’s good to see that the models are in agreement. I’ll give the model a few more easy pictures. It’s interesting to note not only that the CNN made the right prediction, but also the other potential candidates. The second option here at least makes sense but some of the others seem way off (the probabilities are very small and using two decimal places round to zero). These results are pretty impressive. Granted, all of the images I choose were relatively easy to identify and prominently featured the object of interest, both conditions which are not likely to be replicated in the real world. In reality, things do not stay still, and a scene is likely to have hundreds or thousands of different objects that may need to be identified (and to think we do this constantly without ever breaking a sweat). Nevertheless, this network is a decent start to the problem and even with a busier image, we can obtain accurate results: This network is designed and trained to identify one image class in each image, a task to which it is well-suited. However, if we introduce more elements into a single picture, the predictions begin to break down. There are some good predictions in the mix, but overall, the model becomes overwhelmed by the noise. Neural networks can also be applied for multilabel classifications, such as the example above, which has objects in many different classes. the model is also limited in that it has only been trained on 1000 classes. If the image is not in those classes, then we are out of luck. It will give a best guess but we will need to train it ourselves to expand the capabilities. As far as the network is concerned, it has never seen a giraffe before, so there must be no such thing. Nonetheless, were we to show the network many examples of labeled giraffes, it would soon become adept at identifying them as well. Next Steps Currently, we are limited to the 1000 classes learned by the network. If we want to expand the range of pictures, then we will need more data. In particular, we need hundreds of images labeled with the one class if we actually want the network to learn how to identify that object. We can take that on in another post, so for now start gathering or hand labeling images (or have a graduate student do it for you). CNNs are the state of the art, but in the end , they rely on millions of images that have been hand-labeled through thousands of human-hours of work. Luckily, this training data only has to be prepared once, and then it can be re-used. Maybe we will soon reach the point where we can have weaker CNNs train on images that have been labeled by stronger networks without any humans involved (although then we may have to worry about Superintelligent AI as discussed by Nick Bostrom). To train the model on our own data, we will unfreeze at least one layer before the outputs (in order to adjust the model weights to our data) and add a new output layer with the correct number of classes. There are a number of extra steps we can take with this example, such as drawing labeled boxes on the images, or visualizing some of the models using TensorBoard which could provide some insight into the model. This demonstration is not ground-breaking by any means, but it demonstrates the fundamental programming principal of DRY: Don’t Repeat Yourself. When you want to build your own object recognition system, your first words shouldn’t be “ how do I start?” but “who has developed a model that I can improve upon?” In that line of thinking, feel free to use, disseminate, share, and most importantly, improve this code! Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",Object Recognition with Google’s Convolutional Neural Networks,8,published,24225,2345,0.10746268656716418,1,0,0,0,1,1
118,22,665.911323617558,2,https://medium.com/p/make-an-effort-not-an-excuse-27e20a568029,0,None,2017-07-30 17:50:00,33.09,7,46,2017-07-30 09:06:00,"['Motivation', 'Productivity', 'Inspiration', 'Thinking']","Make an Effort, Not an Excuse Overcoming the “I would, but…” response There must be a universal law prohibiting the discussion of New Year’s Resolutions after the first three weeks of January. By that point, even the most determined of us have lost our resolve and our collective shame renders the subject taboo. Naturally, this means that I decided the best time to discuss my goals for the year was in the middle of summer. For such an extremely data-driven person — there isn’t a product on my desk that didn’t get a 4.5 customer rating or above — it may be surprising that I forgo specific quantifiable resolutions. That is, I don’t outline a certain number of tasks that I need to finish by a certain date. Instead, I like to think of yearly themes. If this sounds a little abstract, and too much like some self-help magazine headline (“The year of holistic harmony”), stick with me. The theory behind a yearly principle is straightforward; it is not the infrequent, large decisions that dictate the course of our lives, but the countless everyday choices we make without a second thought. Where we go to college might matter in the long run, but our options for college were dictated by thousands of smaller decisions leading up to the decisive moment, such as whether or not we decided to study for that extra hour, or if we took the few minutes to fill out a scholarship form. Keeping a yearly theme in mind means we can make these daily decisions not by mere habit, but in the context of a guiding precept. To give this concept some firm backing, consider some past yearly themes: efficiency: don’t work more, work more effectively; relax: this test, project, game, etc. will not dictate the rest of your life; and failure: accept mistakes as a chance to learn. If the yearly theme is learning from failures rather than trying to avoid them, then we can implement this by being more willing to try new things or take on difficult challenges that carry a chance we won’t succeed. Likewise, a yearly theme of efficiency means I might lock my computer down from every program except Word (or in reality Sublime Text 3 or Microsoft Visual Studio) for 4 hours a day and get all my work done in that period instead of spending 10 hours a day on my computer where half of that time is spent on various distraction-delivery platforms. We don’t need to repeat the mantra over and over, or use it for every single decision — trying to pick out a breakfast cereal that agrees with the theme of relaxation could be a real conundrum — but by keeping an overarching principle in mind, we can subtly influence the everyday decisions that over time determine the course of our lives. My guiding principle in 2017 is make an effort not an excuse. Late last year, I noticed I had an issue: my default response to being assigned a task was to immediately brainstorm reasons why I couldn’t do it or why someone else would be better suited for it. This occurred for a range of small or large projects both at work and school, and it seriously impaired the quality of my work when I would inevitably suck it up and start working. A prominent manifestation of this attitude occurs in what I have come to call the “I would, but…” response. Like other personal flaws I have discovered, I first noticed this in other people. (This is partly because every teenager is sure that he/she has no failings and is always right. Interestingly, this tendency is most often exhibited in those under 20 and those over 80, who have much more in common than you might think. For example, both groups share a belief that they are great drivers when clearly they should not be allowed on the road). Although I generally don’t like to talk about running — it’s something I do, not the defining facet of my life and I abhor the sense of moral superiority that some people believe exercise gives them— but when I inevitably am asked about hobbies (generally when there can be no more possible talk about the weather) I have to admit that I spend a not-insignificant portion of my free time wearing down my joints. Well over half the time, the response I get begins with “I would be a runner, but…” followed by any number of creative explanations. It was during one of these conversations, when I had stopped paying attention as someone described how they didn’t run because they didn’t like the smell of pine trees along the running trail, I realized I do the exact same thing. Every time when I would be given a task, my mindset immediately went to “I would do that, but…” Furthermore, the reasons I cleverly devised to justify not doing a project were nothing more than excuses, my personal version of the elaborate stories people told me about why they didn’t run. Much as there is nothing to be gained from someone explaining to me the circumstances why they are not a runner (it doesn’t matter why you don’t run, the only important thing is that you don’t), it wasn’t important why I thought I shouldn’t have to do a project. In the end, I was either going to do it or I wasn’t and starting out with the assumption that a project is not worth my time is one way to ensure that the end result will be less than optimal quality. 80% of the effort I expend on a project seems to be getting started, and I could reduce that significantly if I did not have to convince myself that a project is worthwhile in the first place. Consequently, I decided that 2017 was the perfect time to implement a new attitude. Since “The Year of Avoid the Phrase ‘I Would, But’” won’t fit nicely on a t-shirt (should I ever decide to monetize this idea) I went with the simpler: “make an effort not an excuse”. The implementation of this yearly theme has primarily come in situations in which I find myself starting to utter the dreaded “I would, but...” This principle applies not only to school and work projects, but to almost all of my daily decisions. Several of the “I would, but…” areas I need to work on are general life goals such as “I would volunteer more, but I would rather spend my time getting paid”; or “I would write more, but no one cares what I have to say.” In the first case, I selfishly only consider the value of my time and not those who I could help, and in the second, I shouldn’t be writing for the reaction of others in the first place, I need to write as a way to sort out my own thoughts. Both of these realizations were harsh truths that were there all along, but I had been avoiding acknowledging them by sticking with my default excuse-making reaction. As the year has progressed, I found a number of situations where I was justifying my own failure before I had even begun a task. Moreover, I have noticed other related objectives that complement the central tenet of fewer excuses. The most useful of these has been “don’t complain,” a great policy which not only makes you more productive because you do not have to waste cognitive faculties finding reasons to gripe, but also means you are a nicer person to be around. The recognition that I had become an “I would, but…” type of person was, as most critical examinations of ourselves tend to be, painful but illuminating. I won’t claim that a yearly theme (it doesn’t even have to be yearly, a weekly or monthly theme is a great start) will “10x your productivity” or transform your life, but the repeated application of a constructive guiding principle can have noticeable effects in work, school, and personal relationships. There will always be successes and failures; this post is evidence that I have convinced myself to write more, while I still wish I volunteered more often. The numbers in the win/loss column are important to quantify the effectiveness of a lifestyle change, but just as critical is that there is always an aspect of our daily routine we are constantly striving to improve. The lack of a finish line with a yearly theme cuts both ways. On the minus side, it means that you can justify any amount of progress, no matter how small, as satisfying the theme, but, on the other hand, it means that you are never fully done and always have chances to do even better. Moreover, if you have a stated goal such as read one book every month, and January passes without you turning a single page, you will probably completely give up and write reading off your to-do list. Conversely, if you have a theme, such as “year of using my free time productively” and you trip up for one or two months, there will still be numerous opportunities every day to get back on track. A yearly theme is more forgiving than a specific goal, which acknowledges the fact that humans fail often but mistakes almost always can be overcome. Eliminating the phrase “I would, but…” has yielded me positive results, not the least of which is I now slow down and think about my response to a new task rather than supplying my stock answer. If I right away begin to consider how I might solve a problem rather than how to avoid the problem, it becomes much easier to find the motivation to just get started. Granted, there are situations in which you do need to justify why you shouldn’t do a task for which you are not suited, and an ability to formulate a rational reason in this circumstance can be invaluable. However, when I look back on my life, the greatest experiences have never begun with the words “I would, but..” Instead, they have started with “Sure, I would love to do that…” Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator","Make an Effort, Not an Excuse",7,published,139,1895,0.011609498680738786,0,0,0,0,0,0
124,0,664.056211127014,1,https://medium.com/p/the-ascent-of-humanity-54ce077aeada,0,None,2017-08-01 14:21:00,21.74,17,15,2017-07-15 20:04:00,"['History', 'Books', 'Review', 'Humans']","The Ascent of Humanity A Review of Sapiens by Yuval Harari One-sentence summary: The history of humanity is best viewed as three revolutions: the cognitive revolution beginning 70,000 years ago characterized by the development of language; the agricultural revolution that began 12,000 years ago and led to the first permanent settlements enabled by large-scale cooperation; and the scientific revolution which commenced around 1600 when the modern ideals of humanism, liberalism and democracy were first adopted and technological progress began its exponential path. It’s perfectly acceptable if you graduated from high school with no desire to ever pick up a history book again. The endless listing of names and dates typical of the American history curriculum is incredibly effective at driving any enthusiasm for studying the past out of students. My knowledge of history post-high school consisted of a jumbled mix of (entirely American) names and events (Betsy Ross wrote the Constitution right?). The few times I went so far as to begin a history book since then, I have felt my mind shut down at the first mention of a name-date-event combination. I enjoy books with bold ideas, those that examine trends and try to explain movements, rather than those that get mired in the endless details of who exactly did what when. Some history books seem like they are on the verge of stepping back and looking at the big picture only to zoom right back in and lose the forest in a thicket of trees. I like the sound of studying the past to learn from our mistakes, but when the past is presented in list form, it can be pretty hard to take away anything relevant. Therefore, I was excited if somewhat skeptical when I heard about Sapiens: A Brief History of Humankind, a book with an idea no less grand than the entire story of humanity, from our first upright steps on the African Savannah 2 million years ago to this very day (and even slightly into the future). The fact that the book seeks to explain central driving themes of human progress gave me hope that this would be a history book that eschewed traditional formats. Another fault of history as it is conventionally taught is that is gives the appearance that the past is linear, with one foregone conclusion leading to another, often driven by a single character (the “Great Man” fallacy). Sapiens, written by Israeli history professor Yuval Harari, demonstrates instead that the past was full of innumerable diverging paths with never any guarantee of forward progress in one direction. It is constructive to view history through the overall trends that enable these different possibilities and to try and understand how we have changed biologically, socially, and morally during each of these periods. The choice of any dividing era of history will always be arbitrary, but Harari chooses a handy (and easily remembered) breakdown into three ages: the cognitive revolution, agricultural revolution, and scientific revolution. Harari has taken on a task with an ambitious scope, but by replacing names and dates with bold ideas, has succeeded in creating a highly readable account of how we got to this point (and where we might be heading). The Cognitive Revolution Our tale begins with an interesting question that few of us have likely considered: why are we, Homo sapiens, (Latin meaning “wise man”) the only still-living species in the genus Homo? (Brief note: a species is the most specific form of taxonomic classification for an organism, while genus is one step up the ladder. Organisms within the same species can mate and produce fertile offspring while species in the same genus are similar but not exact genetic matches). The genus Canis (“dog”) for example contains numerous species as diverse as domestic dogs, wolves, coyotes, and dingoes. We know for a fact that many other species of humans walked the Earth at one time, from the first Homo habilis (“handy man”) that appeared around 2.1 million years ago to Homo neanderthalensis which went extinct a mere 40,000 years ago (I’ll use the convention established by Harari of referring to members of the genus Homo as humans and the species sapiens as sapiens). The two plausible options for our current single species situation are interbreeding and extermination. Either sapiens thoroughly mixed with other human species to the point where they were assimilated into a single species, or our ancestors wiped the other members of the Homo genus off the Earth. As is often the case, the truth is likely somewhere between the two extremes. Recent studies have shown that modern humans in Europe and the Middle East share 1–4% of their unique human DNA with Neanderthals. It may not seem like a large amount, but it is enough to irrevocably prove that sapiens and neanderthalensis interbred to some extent. While this is intriguing in and of itself for the complex dynamics that it suggests — what would a meeting between two species of ancient humans look like? Were these inter-species relationships forbidden or encouraged? — the more interesting conundrum is that archaeological evidence suggests that sapiens was neither the smartest nor the most physically capable of the Homo genus. How then did sapiens manage to survive when all other of our brethren did not? The best theory to explain the endurance of sapiens is that individually our species was not better adapted to survive, but collectively, sapiens was far more capable than any other Homo species. This early sapien collaboration was enabled by language, in particular, language that could explain abstract concepts not grounded in concrete reality. Abstract concepts provide the difference between saying “there is a lion over there, we should watch out right now” to “there is a lion over there, perhaps we should build a fence to protect ourselves in the future.” The fence and the future do not yet exist, but it is the ability to imagine that they do so and to take actions reflecting the possible future reality that set sapiens apart. Human’s advantage over other animals was enabled by an ability to master our environment through the creation of tools and by using resources beyond their primary value (eating a piece of fruit from a bush is extracting the primary value of the fruit; shaping a rock into an arrowhead is a secondary use of the rock) while sapiens’ advantage over other humans arose because of developments in the speech region of the brain. Improved communication capabilities allowed for planning and the spreading of beneficial innovations between individuals. Archaeological finds suggest that while some members of Homo used the same basic ax for over a million years — can you imagine a smartphone model that does not improve for millennia? — while sapiens constantly developed new iterations of tools, such spears that made it possible to attack an enemy from a distance. The cognitive enhancements were not dedicated solely to the warfare. Early sapiens also formed the first cultures and began making objects that served an aesthetic in addition to functional purpose. While none of these concepts presented in the book are groundbreaking, they are presented in a broader context that lets readers see how the details of a particular evolution feature affected the success of our species. Furthermore, it is possible to extrapolate these findings to the present day; these trends suggest that today we should promote exactly those features that made our early ancestors so capable as a collective. Laws that prohibit free communication and policies that prevent the spread of information inhibit the very advantages that allowed us to attain our privileged position as the sole representatives of Homo on the planet. The Agricultural Revolution and Shared Myths The tale of humanity begins to take off with the first permanent settlements near modern-day Israel and Jordan about 12,000 years ago. Contrary to popular perception of the agricultural revolution as a great step forward in average living standards, Harari paints a much darker picture calling agriculture “History’s biggest fraud.” What could possibly be negative about settling down for a life of guaranteed sustenance in a village surrounded by other humans? Well, for starters, work hours increased after the development of farming. Raising crops is unsurprisingly labor-intensive, and early farmers had 12 or more hours of back-breaking work to look forward to each day, as opposed to hunter-gatherers who could expect to spend about 4–6 hours on an average day foraging for nutrition. Even modern humans living in societies with 40-hour work weeks spend more hours supporting themselves than did hunter-gatherers. Moreover, the type of work involved with agriculture was repetitive and mentally tedious compared with the problem-solving and varied environments encountered in a hunting lifestyle. Studies of pre-historic anatomies suggests that humans living in the first permanent settlements were physically much smaller than their hunter-gatherer relatives, indicating that the quality of nutrition actually decreased after we began raising our own crops (Harari points out that it is more accurate to say that corn and wheat domesticated humans rather than the other way around because crops forced us to settle in one place). We traded a life of constant novelty (and danger) as nomadic hunter-gatherers for banality, long work hours, and poor nutrition as fledgling farmers. If the transition to agriculture was so detrimental on an individual level, than why did humanity nearly unanimously decide to adopt this system? The answer is simple: humans traded individual life satisfaction for the well-being of the entire species. Individuals in villages had less enjoyable lifestyles, but they were able to support and raise more children because of the guarantee of at least a survival amount of sustenance. Consequently, the population of agricultural societies boomed and farming tribes were able to dominate nomadic bands through sheer numbers. Agricultural, for better or worse, became the default option for humans and allowed for the beginning of the exponential growth in population that has not slowed to this day. The ability to trade individual happiness for the good of the species was enabled by the creation of what Harari has labeled inter-subjective realities. The two conventional forms of human experience are subjective: concepts that cannot be touched such as emotions and ideas, and objective: physical realities like the objects and environment around us. Inter-subjective realities occupy a middle-space: they are creations that exist only in the human mind but enable collective action because a vast number of people believe in them. When we look at our modern world, we see we are surrounded by inter-subjective realities: religion, money, government, nations, and corporations are all shared myths. There are no gods except in the minds of humans and once enough people stop believing in a particular god, she/he will cease to exist, as evidenced by the thousands of religions throughout human history that are no longer practiced. Money is perhaps the best example of an inter-subjective reality. We don’t generally stop to think about it, but the pieces of green paper we spend so much of our waking time trying to acquire have no inherent value. They by themselves cannot satisfy our hunger, increase our happiness, or cure illness. However, they can be traded to achieve those ends because everyone in our society believes that money should have value. If all humans were to collectively stop thinking that money had any value tomorrow, then that would be the new reality. Moreover, we can exchange money for goods and services without any pieces of paper changing hands. As an illustration, consider that $60 trillion worth of transactions are conducted around the world on a yearly basis yet only $6 trillion of actual money exists. The entire system depends on the large-scale belief that the changing of numbers on a screen can represent a change in ownership of a physical object. The idea of shared myths that enable collective action is the most important in the book. When viewing the world in this context, we can see how nearly all of our institutions are founded on one inter-subjective reality or another. Laws only have force because enough people believe in them to allow us to create police forces that enforce the rules. Government is not a fundamental constant of the universe, but rather a system where we willing subject ourselves to the control of a larger entity because we have convinced ourselves that is the proper thing to do. The critical aspect of inter-subjective realities is even though the concepts themselves have no concrete existence, they inspire action that results in the transformation of the world around us. Humans are incapable of maintaining a social circle larger than 125–150 individuals through direct communication (known as Dunbar’s number). Any group smaller than this can be compelled to action through personal relationships between members of the group. However, as human societies expanded past this number at the beginning of the agricultural revolution, inter-subjective realities came to take the place of relationships as a motivating factor for action. Two citizens in a village of 10,000 may be complete strangers, but they are willing to work together if they share the same god, just as two individuals in a large corporation today can collaborate even without a personal relationship because they believe in the purpose of the corporation. As living conditions for individual humans decreased with the adoption of farming, humanity as a whole successfully expanded and citizens were compelled to work together because they shared the same myths and stories. When we must work with a stranger, we find it easier if they share the same belief systems, such as a political alliance or nationality. Even though we may have no personal experiences together, we share a common set of myths and values that unite us and allow for productive collaboration. The Scientific Revolution and Our New Guiding Myths Beginning 450 years ago, the shared myths of humanity underwent a transformation. We slowly began to discard religious dogmas and an unquestioning acceptance of the order of the world in exchange for secular beliefs and inquiry. While there had been individuals who investigated the natural world and made some progress in explaining how things work in antiquity (the Greek Eratosthenes and the Persian Avicenna come to mind), the general consensus prior to the 1600s was that God — or another diety — had created the world in its present form and that there were no unanswered questions if only one looked for the answers in the word of God. It was only as individuals and gradually societies began to throw off the shackles of religious thinking and admitted that humans did in fact not know everything that progress began in fields such as biology, geology, math, and physics. The scientific revolution truly began once we started to admit our own ignorance. Much as the deeper you delve into a subject, the more you realize you don’t know about it (a phenomenon that manifests itself in the Dunning-Kruger effect), as we began to admit our ignorance in some areas, we quickly discovered that we were standing at the very edge of an unimaginably vast continent of knowledge. The embrace of our ignorance was the dawn of the remarkable era characterized by incredible gains in our knowledge of the world. However, as Harari reminds us throughout the book, science does not just happen for the sake of knowledge. There always must be something used to justify scientific inquiry, and for the majority of the past 400 years, those forces have been imperialism and capitalism. Today we remember the great exploration fleets for their discoveries of new lands (that in reality had been inhabited for thousands of years) but the real purpose was for empires such as France, Spain, and Britain to expand their reach. Even the voyage of the HMS Beagle, the famous expedition on which Darwin conducted his research that decades later would lead to the theory of evolution by natural selection, was a surveying expedition with the intention of mapping out new territories for military advantage and expansion. The point is not to discount the knowledge gained through these expeditions, but it is to admit there was always an ulterior motive behind the discoveries. Today, science is mostly conducted under the guiding hand of capitalism. In the end, someone must pay the bills for any research, and the decision for which projects to fund often comes down to an economic potential. Again, this in no way disparages science, but we must remember that science never occurs in a vacuum and there are always interested parties behind any finding that should encourage a healthy amount of skepticism. As in agricultural times, our society still stands on shared myths. Moreover, although in our secular society we like to think of them as much more advanced than traditional belief systems, in the end, these collective beliefs, namely humanism, liberalism, and democracy, are nothing more than modern-day instantiations of religions. Humanism, or the concept that every individual has inherent worth and dignity has become the dominant world view in developed countries. In contrast to conventional belief systems, humanism holds that individuals should derive their own ethics and values internally rather than depending on an external definition delivered by a higher being. Furthermore, humanism emphasizes the importance of rationality and evidence-based decision making. In order to live as humanists, we need to respect the differences of every individual and work to ensure the average standard of living of every human increases regardless of what tribe they are from. Humanism is best viewed as a universal religion with the only sacred object being the human or humanity as a collective. Classical Liberalism, which is distinct from the modern day political idea of liberalism in the United States, embraces the autonomy of all individuals and posits that the greatest right any human has is that of self-determination, or the ability to choose one’s path. Liberalism and humanism go hand in hand as both belief in the worth of individuals and the idea that equality should be held up as a goal for any human society. These two worldviews originated in the thinking of many 17th century philosophers such as Sir Thomas More and Francis Bacon, and, in combination with Democracy, or the idea that people should be able to determine their own government (democracy is from the Greek meaning “rule of the people”), they form the basis of our modern institutions. The takeaway from this section is not that humans today are vastly different and superior to our ancestors, but that we share the exact same tendencies which manifested themselves under different names. We need shared beliefs in order for our society to function, so we should not advocate moving away from them, but it is helpful to take a step back and realize that much of what we take for granted and think of as unchangeable rests on a foundation not of concrete but of mythical constructions of our imaginations. What Comes Next Harari is definitely more interested in the present/future of humanity than on the past. This can be observed in the rapidity with which he zooms over tens of thousands of years of humanity (although granted there are not that many records to draw upon from this era) in order to arrive at the present day and then delve boldly into the future. As a long-term optimist more concerned with the future than the past, I found the more modern coverage of the book fascinating. We are entering uncharted waters as the pace of social norms change not on the millennia-scale of biological evolution, but on the yearly or even monthly pace of technological advancement. Several possible outcomes for the future are presented here which are explored in depth in the sequel to Sapeins, Homo Deus: A Brief History of Tomorrow (review coming soon…). At the risk of putting words in his mouth, Harari seems to be cautiously optimism about the long-term prospects of humanity. As he points out, we have made remarkable progress in the latter half of the 21st century in reducing the rates of war, famine, and plague to historical lows, and we are now turning to even more ambitious projects such as the pursuit of machine superintelligence and vastly expanded human lifespans. Yet, we still have not answered some basic questions about ourselves. Even as we have gotten much wealthier, healthier, and live for substantially longer, the subjective happiness of humans has not increased. Although survey data does not go back much further than 100 years, people have reported about the same level of life satisfaction for over the decades. Moreover, people in wealthier countries are not generally any happier than those in developing parts of the world. How should we view the collective progress of humans over the past two million years if all the advances have not made us one iota happier? Harari suggests that we need to spend our research efforts in the coming decades not so much on physical well-being, but on understanding the much more complex world of emotions and mental states. The reason we are not content even with objectively less suffering in our lives is that happiness = reality-expectations. When reality exceeds our expectations, we experience joy, and when our expectations were much greater than our actual experience, we are disappointed. The issue is that every time we improve our material standing, we raise our expectations which means that reality must also get better in order for us to experience emotional gains. Likewise, we view ourselves in the context of our neighbors and people in our socioeconomic class as opposed to past humans or even those in other countries living in much different circumstances. The happiness equation implies that we even though we will always be driven to improve our well-being, any improvement will not really make us satisfied. We are on a quixotic quest for the objects that we think will make us happier as we increasingly push to the side those parts of our lives — personal relationships, time spent with family, quiet times sitting and staring at the scenery — that actually have been shown to bring us lasting joy. Harari ends the book by saying that the question we need to ask moving forward is not “What do we want?”, but “What do we want to want?” While cryptic, the message is that in the 21st century, we should concentrate on defining what makes us happy and how we can construct a society that leads to the greatest increase in happiness rather than continuing to increase wealth. Instead of judging the success of a country on its Gross National Product (the total value of all the goods and services produced by the citizens of a country) we should compare countries based on Average Citizen Happiness. I agree with Harari that the next frontier in medicine might not be physical, but mental and emotional. We have figured out the keys to a long life, but now we need to concentrate on the factors for a enjoyable life. Where I disagree with Harari is in his dire prediction for the future of humans as machine intelligence becomes superior to humans in any number of tasks. The main argument is that humans will be rendered economically useless by more capable machines in the near future. In prior technological revolutions, when we lost our physical advantage over machines, we switched to industries in which we maintained an intellectual advantage, but already algorithms that are more skilled than humans at numerous cognitive tasks are being implemented in many situations. While the concept of technological evolution leading to human irrelevance seems to be prevalent among some of the leading technology figures of our day, I am more optimistic about the ability of humans to adapt to changing situations. Sapiens demonstrates that it was human ingenuity, enabled by a unique set of biological and cognitive advances, that allowed us to shape the world to our will and this ability to solve complex problems through cooperation has us well prepared for the next set of challenges. Recommendation There are so many critical ideas in this book — inter-subjective realities, ignorance begets knowledge, communication as a radical innovation, how to define happiness — that reading any review or summary can never capture a majority of them. I recommend reading the entire book and then sitting quietly for a whole week to let it sink it. I finished this book brimming with subjects to talk about and was quite disappointed to realize that not everyone I encountered had also read the book. Sapiens is opinionated at times, and the last section is more aptly categorized as science fiction than history, but that is part of what differentiates it from traditional mundane history accounts. Sapiens is incredibly well-researched, but any history book, no matter how thorough, is bound to have minor errors that will only be revealed through subsequent findings. The key to avoiding being wrong when talking about the past is to focus on the larger trends, those that will be unlikely to change even as the underlying details shift. The book is readable precisely because it does not spend time examining every minute detail of historical situations. This work, along with several other history books with bold ideas (Guns, Germs, and Steel and The Better Angels of Our Nature) have finally managed to overcome the four dreadful years of history I experienced in high school and made me once again curious about the past. When I want to be put to sleep, I know that I have a stack of old history textbooks that I can rely upon, but when I want something interesting to discuss at work or the dinner table, I am confident that Sapiens will provide me with conversations for years to come. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",The Ascent of Humanity,4,published,69,4684,0.0,16,0,0,0,0,0
129,725,658.0542564945255,76,https://medium.com/p/facial-recognition-using-googles-convolutional-neural-network-5aa752b4240e,6,None,2017-08-07 14:24:00,14.04,38,4013,2017-08-20 15:14:00,"['Machine Learning', 'Python', 'Neural Networks', 'Facial Recognition', 'Deep Learning']","Facial Recognition Using Google’s Convolutional Neural Network Training the Inception-v3 Neural Network for a New Task In a previous post, we saw how we could use Google’s pre-trained Inception Convolutional Neural Network to perform image recognition without the need to build and train our own CNN. The Inception V3 model has achieved 78.0% top-1 and 93.9% top-5 accuracy on the ImageNet test dataset containing 1000 image classes. Inception V3 achieved such impressive results — rivaling or besting those of humans — by using a very deep architecture, incorporating inception modules, and training on 1.2 million images. However, this model is limited to identifying only the 1000 different images o which it was trained. If we want to classify different objects or perform slightly different image-related tasks (such as facial verification), then we will need to train the parameters — connection weights and biases — of at least one layer of the network. The theory behind this approach is that the lower layers of the convolutional neural network are already very good at identifying lower-level features that differentiate images in general (such as shapes, colors, or textures), and only the top layers distinguish the specific, higher-level features of each class (number of appendages, or eyes on a human face). Training the entire network on a reasonably sized new dataset is unfeasible on a personal laptop, but if we limit the size of the dataset and use a consumer-grade GPU or a Google Cloud GPU compute engine, we can train the last layer of the network in a reasonable amount of time. We probably will not achieve record results on our task, but we can at least see the principles involved in adapting an existing model to a new dataset. This is generally the approach used by industry (embodying the DRY: Don’t Repeat Yourself programming principle) and can achieve impressive results on a reduced time-frame than developing and training an entirely new CNN. All of the Python code for this project is in a Jupyter Notebook available on my machine learning projects GitHub repository. The Jupyter Notebook should be run from within the Slim folder in the TensorFlow Models GitHub repository that was downloaded in the previous post. This project was adapted from the TensorFlow Slim Walkthrough Jupyter Notebook and aided by the great book Hands-On Machine Learning with Scikit-Learn and Tensorflow by Aurelien Geron. I ran the TensorFlow sessions in the Jupyter Notebook on a 2GB Nvidia GeForce 940M dedicated graphics card on my laptop. The training times reflect the capabilities of my laptop and will vary considerably depending on your hardware. I highly recommend setting up a compute engine using Google Cloud to run the Jupyter Notebook (or other computationally taxing projects) on a cloud server. Google has GPUs available to rent by the compute-minute and provides free credit when you create an account. (I am also looking forward to the public availability of TPUs or Tensor Processing Units that promise to considerably speed up machine learning training). Labeled Faces in the Wild Selecting an appropriate dataset is the most important aspect of this project. The dataset needs to contain enough valid labeled images in each class to allow the neural network to learn every label. There is no magic threshold for number of image per class, but more images, so long as they are correctly labeled, will always improve the performance of the model. The ImageNet dataset contains an average of 1200 images per class, which would be prohibitive for training with the resources available to the typical consumer (although this will change as cloud computing resources become ever more ubiquitous and the price of computing power continues to decline). After a bit of research, I decided to forgo the traditional labeled flowers dataset (this is available as part of TensorFlow and forms the basis for the TensorFlow walk-through tutorial on training the Inception CNN for a novel dataset) in favor of the Labeled Faces in the Wild (LFW) dataset. This is a collection of 13000 images of 5749 individuals gathered from the Internet. Using all of the classes (individuals) would result in a useless model because many of the individuals have only a single image. There is no possibility that even the most powerful convolutional neural network (as of the time of this writing) could learn to identify an individual from a single image. In order to give the model a chance to learn all of the classes, I decided to limit the data to only the 10 individuals with the most images in the dataset. This results in 10 classes with at least 50 images in each class. There are several versions of the Labeled Faces in the Wild dataset, with different transformations applied to the images. I choose to use the images that had been processed through deep funneling, a technique of image alignment that seeks to reduce intra-class variability in order to allow the model to learn inter-class differences. Ideally, we want images belonging to the same individual to be similarly aligned so the network learns to differentiate between images based on the face in the image and not the particular orientation of the face, lighting in the image, or background behind the face. The original purpose of the LFW dataset is facial verification, or identifying whether or not two pictures are of the same individual (this could be useful in security applications such as unlocking a door based on one’s face or identifying suspected criminals). Deep funneling was shown to improve the performance of a neural network on image verification. Although we are dealing with image identification, that is, we give the network one image and want to predict the class, image alignment through deep funneling should improve the performance for our task based on the same principles that allow it to work for image verification. Throughout this project, keep in mind that the Inception network has been trained on 1.2 million images of objects and not a single human face. Our task is to train a single layer of the network (out of more than 100 total) to differentiate between 10 different human faces. We are adapting a CNN that has never previously seen a human (‘human’ was not one of the 1000 labels in the ImageNet dataset) to accomplish image identification. The idea that model good for one task are often adept at related problems is a powerful concept in machine learning and the techniques demonstrated in this project are of greater important than the final overall accuracy (although it is fun to be right). Download the Latest Checkpoint of Pre-Trained Inception Model We first need to make sure that we have the most up-to-date Inception V3 model with the parameters learned through training on the ImageNet dataset. The following code will download the latest version and extract the required checkpoint: Download the Labeled Faces in the Wild Dataset Once we have the correct model (feel free to adapt this project to a different pre-trained CNN provided by Google) we need to download the full LFW deep-funneled dataset. We will use a similar process to download and extract the dataset to a new directory: The complete deep-funneled dataset is 103 MB. The dataset is structured with a top-level folder called ‘faces’ that contains a number of folders each with the images of a single individual. We now need to work on finding the individuals with the most images. We can use the os Python module to find the number of images total and the number of images of each individual. We will sort the people in the dataset by the number of photos of them and then move them to a new directory. Each folder in the new directory will have all the images for one individual and will be renamed with the number of images and the name of the individual. This should allow us to easily create training, validation, and testing sets. It is a good idea to not use all the images! With 4069 individuals with only a single image, a neural network trained on this dataset would be thoroughly confused by all the different classes (in more technical terms, the bias of the neural network would be too great because it could not actually learn the underlying differences between each class). Now we can create the new directories: We can check to make sure that the new directory was correctly created by manual inspection and comparing the new directory to the original. We have the top-10 individuals by number of photos with the images in an easy-to-access location. We can now split the data into training, validation, and testing datasets with a few basic operations. Unfortunately, we can see right away that we have a limited number of pictures in some of the classes. 50 images may be enough for a human to correctly learn a new face (most of the time one image is sufficient thanks to our extraordinary capacity for facial identification) but it is a relatively small sample when compared to ideal machine learning training sets. We can also see that there is an uneven class distribution with George W. Bush and Colin Powell making up more than half of the images. This is a problem because the CNN might learn to identify these individuals with more photos at the expense of all others. In other words, it might be able to correctly identify every photo of George W. Bush, but it will also incorrectly label many people as Bush who are not because in cases where the network is unsure, the default option could be Bush simply because it has seen more images of Bush. If we were to get many false positives labeled as Bush, we would say that the model has a high recall with respect to Bush — it correctly identifies Bush when it is in him — but it has a low precision — many of the times it claims to see Bush, it is not him. There will always be a tradeoff between precision and recall, and it is up to the developer to adjust the training data and architecture to achieve the correct balance. Maybe we want to create a classifier with a high recall for Tony Blair, and we can tolerate a number of false positives. On the other hand, we might want a classifier that identifies suspected criminals with high precision which would not produce many false positives (false accusations in this case) at the expense of a few false negatives. For now, we will leave the training data as is and see what kind of results we can achieve with a non-optimal dataset. We need a training set to allow the network to learn the classes, a validation set to implement early stopping when training, and a testing set to evaluate the performance of each model. I will put 70% of each class in a training set, 5% in a validation set, and 25% in a testing set. We can also look at the distribution of images in each class: Clearly we have a biased dataset as discussed above. It will be interesting to see the performance of the network on the images of Bush and Powell when testing as compared to the others. The next step is to read the images and labels into arrays that can be split into the three sets. We can use the pyplot module from the matplotlib library to read in the .jpg images as numeric arrays and then convert to numpy arrays: Then we will create the set splits with the following code: Notice that the datatypes for the images are float pointing numbers and the labels are integers (both with 32 bit precision). Let’s take a look to make sure that the shape of all the arrays looks to be in order: That looks perfectly fine (if a little limited on the amount of training data). The data wrangling step is now complete. Often, this is the most time-intensive and costly part of the entire machine learning workflow. We were using a clean dataset to begin with, which cuts down on data preparation, but it still took a few lines of Python code to create the appropriate datasets. As a check for accuracy (and for a little bit of fun) we can visualize the images in each class. I have to say, that if this neural network can correctly identify more than 5 of these individuals, then it will outperform me on this task. We can plot 2 images from each class: (All 15 leaders can be seen in the Jupyter Notebook) Processing the Images The ImageNet CNN requires that the images be provided as arrays in the shape [batch_size, image_height, image_width, color_channels]. Batch size is the number of images in a training or testing batch, the image size is 299 x 299 for Inception V3, and the number of color channels is 3 for Red-Green-Blue. In a given color channel, each specific x,y location denotes a pixel with a value between 0 and 255 representing the intensity of the particular color. ImageNet requires that these pixel values are normalized between 0 and 1 which simply means dividing the entire array by 255. Inception has built-in functions for processing an image to the right size and format, but we can also use scipy and numpy to accomplish the same task. All of the images in the LFW data are 255 x 255 x 3 and these will be converted to 299 x 299 x 3 and normalized to pixel values between 0 and 1. The following code accomplishes this processing (which is applied during training): Define Layer of Inception CNN to Train In order to adapt the CNN to learn new classes, we must train at least one layer of the network and define a new output layer. We use the parameters that Inception V3 has learned from ImageNet for every layer except the last one before the predictions in the hope that whatever weights and biases are helpful in differentiating objects can also be applied to our facial recognition task. We therefore need to take a look at the structure of the network to determine the trainable layer. Inception has many layers and parameters (about 12 million), but we do not want to attempt to train them all. The inception_v3 function available in the TensorFlow slim library (in the nets folder) returns the unscaled outputs, known as logits, as well as the endpoints, a dictionary with each key containing the outputs from a different layer. If we look at the endpoints dictionary, we can find the final layer before the predictions: The ‘PreLogits’ layer is exactly what we are looking for. The size of this layer is [None, 1, 1, 2048] so there are 2048 filters each with size 1 x 1. This layer is created as a result of applying an average pooling operation with an 8 x 8 kernel to the Mixed layers and then applying dropout. We can change this to a fully connected layer by eliminating (squeezing) the two dimensions that are 1, leaving us with a fully connected layer with 2048 neurons. We then create a new output layer that takes the prelogits as input with the number of neurons corresponding to the number of classes. To train only a single layer, we specify the list of trainable variables in the training operation. What the network is learning during training is the connection weights between the 2048 neurons in the prelogits layer and the 10 output neurons as well as the bias for each of the output neurons. We also apply a softmax activation function to the logits returned by the network in order to calculate probabilities for each class: After isolating the single layer to train, the rest of the code is fairly straightforward and typical for a neural network used for classifcation. We use average cross entropy as a loss function and use the Adam Optimization function with a learning rate of 0.01. The accuracy function will measure top-1 accuracy. Finally, we use an initializer and a saver so that we will be able to save the model during training and restore it at a later time. Batch Image Processing During training, we will be passing batches of images to the network. To create these batches, we can write a function that will apply the processing function previously defined at runtime. This function will create the training and testing batches at runtime. However, for early stopping, we will be passing all the validation examples through the network at once, so we can go ahead and process the entire validation set: The validation set is now ready to use for early stopping and we can use this function to create training batches during training (we cannot pass all of the training or testing examples through at once because of the limited amount of memory on the GPU, but the 68 validation examples can fit in memory). TensorBoard Visualization Operations TensorBoard is an extremely vital tool for visualizing the structure of the network, the training curves, the images passed to the network, the weights and biases evolution over training, and a number of other features of the network. This information can guide training and optimization of the network. We will stick to relatively basic features of TensorBoard such as looking at the structure of the network, and recording the training accuracy, validation accuracy, and validation loss. We will use the date and time at the start of each training run to create a new TensorBoard log file: When we want to look at the statistics, we can run TensorBoard from Windows Powershell (or the command prompt) by navigating to the directory and typing: Before we start training, let’s take a look at the computational graph of the Inception V3 CNN. The above figure shows the entire model with each name scope enclosed in a different node. Any of the nodes can be expanded to view the details. For example, here is the expanded new output layer: Here is the expanded Inception V3 node: While these computational graphs can be difficult to parse, they contain useful information about the inputs and outputs flowing through the graph. Using namespaces allows us to create a more readable graph and if we wanted to develop our own CNN, the visualization tools available with TensorBoard would be invaluable (see TensorBoard embedding below). Later in this project, we will use TensorBoard to examine the training curves to compare different data augmentation methods. Training Finally, we are ready to train the neural network (or at least one layer) for our facial recognition task. We will train using early stopping, which is one method for reducing overfitting on the training set (having too high of a variance). Early stopping requires periodically testing the network on a validation set to assess the score on the cost function (in this case average cross entropy). If the loss does not decrease for a specified number of epochs, training is halted. In order to retain the optimal model, each time the loss improves, we save that model. Then, at the very end of training, we can restore the model that achieved the best loss on the validation set. Without early stopping, the model continues to learn the training set better with each epoch but at the cost of generalization to new instances. There are many implementations of early stopping, but we will use a single validation set and stop training if the loss does not improve for 20 epochs. Each time we run a different model, we will create a new TensorBoard file and save the model parameters to restore for evaluation: The training code produces the following output: We already can learn quite a bit from this information. The loss on the validation set was at a minimum on the 5th epoch, and the remaining epochs did nothing to improve the performance of the network. Therefore, we probably do not need to train for so long, but as we are using early stopping, at least the model saved will be the one with the highest performance. We can also see that training took nearly 40 minutes. Evaluate Performance Once we have trained the neural network, we need to see if it has any usefulness. To do this, we will evaluate against the test set, which the model has never before seen. As in training, we will go through the test set on batch at a time: The testing code produces the following output: Our first attempt at facial recognition correctly identified 62% of individuals. That is by no means impressive for any machine learning system (even if it might outperform most humans) but it at least shows that the network did in fact learn during training. Luckily, if we are not satisfied with this performance, there are a number of steps that we can take in order to improve the neural network that do not involve building our own CNN. Data Augmentation The next step is to aim for higher accuracy. The approach we will employ will not modify the neural network itself, but rather will work to expand the dataset. There are two main approaches to augment or increase the amount of training data. The first is simply to gather more labeled training images. That can be time intensive and quite tedious, but as long as the images are valid, it will improve the performance of a machine learning system. Fortunately, there is another approach known as data augmentation. For images, this means taking the existing pictures and applying various shifts that do not alter the identifiable features in the image but rather the presentation of the features. For example, the location of a face within an image should not matter, only the face itself is important for identification. Therefore, if we take one image and shift the face in the image to many locations, we can make the network robust to changes in face location. Likewise, we can rotate the face to make the network invariant to changes in orientation of features. In effect, we are creating a network that is invariant to manipulations of the presentation of the images and only learns to differentiate the faces themselves. Other transformations include altering the background of the images or the lighting or contrast of images to force the network to learn the features in the image and not superfluous differences. We take the existing images, apply a transformation, append the transformed images to the original images with the correct label, and then train the network on the expanded dataset. Shifting Images The first transformation to implement will shift the image in four directions (left, right, down, up). TensorFlow has plenty of operations for data augmentation of images, but because we will perform fairly simple transformations, we can use scipy and numpy. The following code applies the four shifts to the images: We can then visualize the transformation: As you can see, the facial content of the image has not been altered, but the location of the face in the image has been changed. In theory, this will make the network robust to changes in facial location within an image. Finally, we need to append the transformed images to the original training set: The end result of this process is a 5x larger training set. Now, we can use the same training and evaluation process to see if our data augmentation procedure had a positive effect. Training on Augmented Data We will first train with the augmented data on a clean version of the pre-trained model. The results are below (code to train the model is almost the same as the un-augmented training and can be found in the Jupyter Notebook): The training time did significantly increase because of the larger number of training examples. The following output shows the result of evaluation on the test set: Well, that made quite a difference! Our accuracy jumped from 62.5% to 72.7%, a relative improvement of 16%. However, we did pay for it with a greater than 400% increase in training time. In the real world, we would need to weigh the training time versus performance with a cost-benefit analysis to determine the acceptable balance between accuracy and development time. Another approach we can use is to train using the augmented dataset but start from the best checkpoint saved by the un-augmented model. This should speed up training and could result in a performance boost as well. The idea is that the parameters will need less training to converge to their optimal values than starting with a clean pre-trained model. To implement this, we will simply change the checkpoint path that we initialize the parameters with: The rest of the training code is the same. We can then again evaluate on the test set: Starting from the un-augmented checkpoint did not reduce training time (167 minutes versus 166 minutes for the clean start) but it did result in a 1.6% relative improvement in accuracy. Again, there are any number of data augmentation methods we could try, and we are only limited by time and computing power. I will try one more simple data augmentation. Flipping Images Another straightforward image transformation is to flip the content of the image from left to right. Imagine this as placing a mirror vertically dividing the image in two and mapping the left to the right side and the right side to the left side (perhaps that is a bad explanation. If so, scroll to the photos below). This preserves the facial content of the images but alters the orientation, again with the purpose of making the network invariant to the presentation of the face within the image. The following code flips the images from left to right: Visualizing the images provides a better context for this transformation than a written explanation: The content of the images has not been altered, only flipped left to right. We then append these flipped images to the original dataset and end up with a 2x larger training set: Training from a clean start results in a 66.2% accuracy and a 70 minute training time. Starting from the best previously saved model results in a 66 minute training time and a 60.5% accuracy. In this case, we achieved a 6% relative increase in accuracy over the un-augmented training set with less than a doubling of training time. In industry, this might be a better trade than the 16% performance increase at the cost of 400% increase in training time that we got from the shifted images augmentation. While there are any number of additional transformations to try, we are more interested in the process than achieving perfect accuracy. To that end, we can restore the best-performing model — the model trained on the shifted images dataset starting from the un-augmented checkpoint — and look at the results. Visualizing Performance of the CNN To look at the results, we create a simple function that prints the correct label, the image, and the ten probabilities corresponding to each class. The function will take in an index to use from the test set. We could use images from the Internet, but the images in the test set have already been deep-funneled and provide an appropriate standard for evaluation because our classifier was not trained on these images. The following code creates the function: Let’s make some predictions: That’s a pretty decent performances so far. Of course, to correctly represent the 73% testing accuracy, we need to observe that the network is not always correct: I would not be surprised if the majority of the wrong classifications were incorrectly labeled as Powell or Bush. The network has seen many more images of these two individuals than any other during training so they could become the default option when the CNN is unsure. If you were classifying faces, and a plurality of the training images belonged to George W Bush, then if you saw an image you were not sure about when you were being tested, the best bet would be to guess Bush because he constituted the largest percentage of the images. Distribution of Test Predictions To back up my prior assertion with some data (or to prove it false), we can take a look at all the predictions on the test set to see if Bush and Powell make up more than their fair share of predictions. This requires a slight modification of the testing code to save each set of predictions for later analysis: Then we can generate the labels using the argmax function (that returns the index of the maximum value) applied across the first axis (across each row): Then adjust the test set to match the size of the test predictions (because some of the test set is not used when batches are created): The test accuracy from manually evaluating is the same as in the TensorFlow session which is a good indication that we correctly stored the predictions. Now we can numerically evaluate the distribution of predictions against the actual distribution of images in the test set: We can also show the difference between the test set composition and the prediction composition: Individuals with a negative percentage had less predictions from the neural network than they had images in the test set, while individuals with a positive percentage generated more predictions than images in the test set. My analysis of the performance of the CNN was not entirely accurate. While George W Bush did have more predictions than he should have according to the composition of the test set, Colin Powell actually received fewer predictions than would be expected. From this information, we could generate a confusion matrix and determine the recall and precision for each individual, but this post is already getting long enough as it is! For now, we will settle with a couple graphical distributions to visualize the differences in composition of classes between the test set and predictions. We can start with frequency plots: The most revealing graph is the relative difference between the composition of the test set and the predictions: There is a large discrepancy between the number of predictions made for some individuals and their representation in the test set. We could say there is a systematic bias towards or against some individuals, but the more likely explanation is that with so few training and testing examples, the relative distributions are mostly chance. If we were to run the training again, we would likely get a significantly different distribution because the model is not able to identify each individual with absolute certainty which results variations across training and evaluation runs. TensorBoard Training Curves Finally, if we want to analyze our CNN performance in even greater depth, we can look at the training accuracy, validation loss, and validation accuracy as a function of the epoch for training on the various datasets. This will allow us to see the extent of overfitting (or maybe underfitting) on the training set as well as the effect of data augmentation on the training performance. We kept the statistics relatively simple and did not take advantage of the capabilities of TensorBoard, such as visualizing histograms that show the evolution of weights and biases over the epochs. The full features of TensorBoard (including embedded visualizations) are incredible and are a necessity to use when developing a full CNN or any neural network. The legend for all of the training runs is below. To summarize, we have five runs defined by the dataset and starting point (with training time and accuracy): The first graph from TensorBoard is the training accuracy: This is evaluated on a single training batch of 32 instances, so it is not a great indicator of overall performance, but we can see that it training accuracy does increase with number of epochs up to a maximum value with all of the models. Notice that the accuracy ends near 100% which can be a sign of overfitting especially when the validation accuracy is not nearly that high. This is one reason why we chose to implement early stopping so the model could generalize better to new images. Early stopping should reduce the variance of the model, or its sensitivity to training examples allowing it to make better predictions on instances it has never encountered. Early stopping can however increase the bias of a model which is a measure of how closely the algorithm learns the training data. High bias means the neural network essentially overlooks much of the training examples. Training any machine learning model is always an exercise between high variance, which leads to poor generalizations to new instances, and high bias, which prevents the network from learning the underlying relationships during training. The next curve is the validation loss: The curve shows that the validation loss does not continually decrease, another sign of overfitting. For all of the training runs, the loss is minimized within the first ten training epochs indicating that further training is unnecessary. The two models trained on the shifted augmented dataset have the lowest validation loss proving that adding shifted images to the training data improved the performance of the network. This is also demonstrated by the validation accuracy curves: These validation accuracy plots clearly show the benefit of augmenting the training data. The lowest performing model in this graph is the un-augmented dataset, followed by the left-to-right flipped dataset, with the shifted dataset, also the largest, on top in terms of accuracy. The validation set is relatively small at 68 images and so will be subject to some variation across runs, but these curves show that we are on the right track with the data augmentation approach and would likely benefit from collecting more labeled training images or performing additional transformations to the existing images. The graphs also show that all of the models achieved the best performance within a handful of epochs. This indicates that we might not need to train the model for as long and we can decrease the number of epochs without a decrease in the loss function required to implement early stopping. Even with simple statistics, the training curves can provide us valuable information about the best strategy for developing a neural network for any task we want to perform. Conclusion Rather than an attempt to set a record for facial identification accuracy, this project was more a proof-of-concept to demonstrate the process of training an existing network for a different objective. We were able to take a model pre-trained on a related dataset and adapt it to a task of our choosing. In the process, we were able to save ourselves weeks or even months of development and training time. We saw how we could retain all the parameters of each layer except for the last layer, create a new output layer, and train the single unfrozen layer to achieve adequate performance on facial recognition with a model originally trained to distinguish between 1000 objects. Moreover, we successfully explored the concept of data augmentation by image transformation and we were able to use TensorBoard to understand the computational graph (or at least see the graph) and to observe the training curves which validated our use of data augmentation. There are any number of additional steps to this project that would improve the accuracy. The ones most likely to have a significant impact are to gather more labeled training data or train more layers of the network. Both of these are easy to implement but time-intensive. For now, this project serves to validate the adaption approach for development of a CNN. This adaptation process is indicative of an industry application in which a company is more likely to use an existing high performance model than start from a clean slate because of the development and training resources required for such a task. Moreover, deep-learning is a collaborative field, where new discoveries necessarily build off previous findings and novel applications for existing neural networks arise out of a mindset of sharing. Great ideas die in isolation, and it is only by collaborating with diverse entities across fields that world-altering innovations can be developed. In that line of thinking, feel free to use, share, and adapt this code in any way you see fit! Addendum: Who Does the Inception CNN Think I Am? Originally published at medium.com on August 7, 2017. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Shifted Augmented data with clean start (166.7 min, 72.7%) Shifted Augmented data with restart from un-augmented checkpoint (167.6 min, 73.9%) Flipped Augmented data with clean start (69.2 min, 66.2%) Flipped Augmented data with restart from un-augmented checkpoint (65.8 min, 60.5%) Un-augmented data with clean start (38.7 min, 62.5%)",Facial Recognition Using Google’s Convolutional Neural Network,8,published,28580,6666,0.10876087608760876,-13,0,0,0,1,1
127,96,657.1143654684029,13,https://medium.com/p/exploratory-data-analysis-with-r-f9d3a4eb6b16,0,None,2017-08-08 12:58:00,12.14,43,512,2017-08-08 11:20:00,"['Data Science', 'R Programming', 'Data Analysis', 'Udacity']","Exploratory Data Analysis with R Examining the Doctor’s Appointment No-Show Dataset Author’s Note: The following exploratory data analysis project was completed as part of the Udacity Data Analyst Nanodegree that I finished in May 2017. All code for this project can be found on my GitHub repository for the class. I highly recommend the course to anyone interested in data analysis (that is anyone who wants to make sense of the mass amounts of data generated in our modern world) as well as to those who want to learn basic programming skills in an applied setting. Abstract Doctor’s appointment no-shows are a serious issue in the public health care field. Missed appointments are associated with poorer patient outcomes and cost the health care system in the US nearly $200 each. Therefore, it comes as no small surprise that reducing the rate of no-shows has become a priority in the United States and around the world. Numerous studies have been undertaken in order to determine the most effective means of reducing rates of absenteeism at with varying degrees of success. The first step to solving the problem of missed appointments is identifying why a patient skips a scheduled visit in the first place. What trends are there among patients with higher absence rates? Are there demographic indicators or perhaps time-variant relationships hiding in the data? Ultimately, it was these questions that drove my exploratory data analysis. I was curious as to the reasons behind missed appointments, and wanted to examine the data to identify any trends present. I choose this problem because I believe it is an excellent example of how data science and analysis can reveal relationships which can be implemented in the real-world to the benefit of society. Introduction to Dataset I wanted to choose a dataset that was both relatable and could be used to make smarter decisions. Therefore, I decided to work with medical appointment no shows data available on Kaggle. This dataset is drawn from 300,000 primary physician visits in Brazil across 2014 and 2015. The information about the appointment was automatically coded when the patient scheduled the appointment and then the patient was marked as having either attended or not. The information about the appointment included demographic data, time data, and conditions concerning the reason for the visit. There were a total of 14 variables I included from the original data. The variables and the description of the values are as follows Set up R Markdown Data Organization and Cleaning Let’s take a look at the structure of the dataframe to identify cleaning/organizing that may need to be performed. From the structure of the dataframe, I can see there is some data housekeeping that needs to be done. First, I want to change the Status variable into an integer 0 or 1. A value of 1 will indicate that the patient did not show up as I am concerned with the variables that are most strongly correlated with a missed appointment. Moreover, I need to convert the registration date and the appointment date to date objects. I decided not to maintain the time (hours:minutes:seconds) associated with the registration date for the appointment although it was available (time of day was not available for the appointment date). I converted both the appointment registration date and appointment date to date objects so I could investigate seasonal and time patterns in the data. Based on the updated structure of the dataframe, it looks like I am on the right track but I still have a couple more modifications to make to the dataframe. I will create month, year, and day fields for the appointment and also rename the columns to a more consistent and readable format. That should about do it for the structure of the main dataframe. I am also concerned with missing/corrupted data, so I will look at the summary of each field to see if any obvious errors or outliers are present in the data. The summary of the no_shows dataframe reveals plenty of intriguing information as well as several errors that need to be corrected. Right away, it is clear that an age of -2 is not correct, and 113 is stretching the boundaries. However, looking at the oldest people in the world, it is plausible that 113 could be a correct age. I will filter out any negative ages and create a histogram of ages to see if there are a significant amount of outliers at the upper end. Moreover, this data summary shows that 30.24% of appointments are missed based on the mean for the status field. The rates for the various patient conditions can also be seen, and the most commonly coded reason for an appointment is hypertension at nearly 22% of visits. Also, the weekends appear to be a very unpopular time for doctor’s appointments in Brazil, or perhaps the clinics from which this data was drawn are not open on the weekend. Either way, I will need to keep the small sample size of appointments on the weekend in mind when I perform an analysis by weekday. Frequency Distribution of Ages The age histogram does not aligh exactly with my intutions though it is close. I expected that it would be bi-modal, with peaks in patient count at the youngest ages and at the oldest ages. However, while the largest number of patients do appear to be the youngest, the number of patients remains fairly constant into the middle ages, with a second peak around age 60 and then a steep decline into the oldest ages. Based on the visualization, there do not appear to be a large number of outliers in the upper range of the ages. I was skeptical of the high proportion of visits from those aged 0 and decided I needed to do some research. Based on the description of the dataset, this data is for primary care physicians in the public sector and so I looked for a breakdown of ages of patients at these appointments. I could not find statistics from Brazil, but based on official statistics from the Centers for Disease Control in the United States, children under 1 year of age make up 2.6% of all physician visits. I can create a better plot showing the percentage each age is of all patients. I can also quickly check the percentage of visits comprised of patients aged 0 in the data. The calculation shows that 0-year olds make up 3.44% of visits. While the initial frequency distribution may looked skewed, the spread of the data along the years makes it look like 0-year olds make up considerably more of the visits than is actually the case. The corrected bar plot does a better job of representing the proportion of patients from each age. From the research by the CDC and the visualization, I conclude that the age distribution skew is not bad but rather legitimate data. To check for outliers in the waiting time field (or how long between the date the appointment was made and the actual date of the appointment), I will make another histogram. Based on the summary statistics of the dataframe, the longest waiting time was 398 days, with a median of 8 days, and a mean of 13.84 days. From the histogram and the statistics, it is clear that there are a small number of patients at the upper end of the distribution who schedule their appointment very far in advance. I wonder if the 398 days is accidentally a mistake that occured when someone choose the wrong year for their appointment! The graph is certainly long-tailed and positively skewed with a few extreme outliers at the upper limit. It appears there is only one patient with a waiting time over one year and just over 100 (out of 300000) with a waiting time of over six months. I will exclude the patient with the waiting time over one year, but I will treat the others as good but extreme data. Here is a version of the histogram that better represents the wait time for most patients. As can be seen in the chart, a plurality of patients wait less than 10 days between the scheduling and the date of their appointment. Based on these visualizations, and the summary statistics for the dataframe, I am confident that the data content of the main dataframe is valid. I can now start the exploration phase of exploratory data analysis! It is time to discover the relationships, or lack thereof, between the variables in the data. Exploring the Variables The first step I can take is to find the correlations between all of the columns to see if any stand out as particularly compelling. Of course, I expect that all the trends will not reveal themselves straight away. The rcorr function from the Hmisc library will calculate the Pearson’s correlation coeffient between every field of a dataframe. The Pearson correlation coefficient, is a measure of how linearly dependent two variables on each other. The coefficient value is between -1 and +1, and two variables that are perfectly linearly correlated will have a value of +1. From those correlation values, there are no standouts that are strong linear predictors of whether or not a patient will miss a visit (given by the status column). It appears that there are not even any strong relationships whatsoever though there is a moderate relationship between age and hypertension. However, I still think there are meaningful relationships to extract from the data. My approach will be to group the data by various fields and then determine if the average absence rate shows a relationship with the fields. The first group of variables I will look at will be those associated with the time-variability of the appointment date. Date of Appointment To start off the time-series analysis, I will want to look at absence rate by month of the year. I first needed to group the appointments by month of year and then find the average absence rate for each month. From the graph and the correlation coefficient, it appears there is a minor relationship between month of the year and absence rate. Missed visits do appear to rise as the year progresses, but with only 12 data points, the 95% confidence interval for the correlation coefficient is quite large (-0.05 to 0.85). I would expect that during the end of the year, people tend to be busier and may miss more appointments. It could also be possible that at the beginning of the year, individuals make resolutions to see the doctor and keep a higher percentage of appointments, but the trend could also be noise. Next, I will create a similar absence rate over time graph, but this time by day of the month. Looking at the plot we can see that there is a very slight positive correlation between day of the month and absence rate but again, the 95% confidence interval spans a wide range of values. To further explore any time variability in the data, I can create the same graph by day of the year and broken out by gender. From these charts, it appears looks as if there is no correlation betwen the time of year and the absence rate for appointments. Moreover, adding in the gender does not reveal any major descrepancies, although from the calculation, we can see that men on average have a 1% higher absence rate. There are several days for which the absence rate is 1, which bears some more investigation. My idea is that these points might corespond to public holidays. However, I also see that these days are not consistent across the two years, so maybe there are other major events that correspond to an increase in absences. I decided to overlay the holidays to see if that might reveal a relationship. I sourced the holidays based on public holidays in Brazil for each respective year. Again, it seems as if these holidays do not line up with higher rates of missed appointments. There is a sustained increase in the absence rate around the beginning of July, but it does not seem to correspond to any holidays. I researched festivals in Brazil in June-July 2015, and it appears that the Paraty International Literary Festival occured July 1–5, but I doubt that this explains the absence rate. There was not city data associated with the original dataset, so it is difficult to attempt to cross-reference specific dates with specific events. I will create a simliar graph for 2014. I remember that the 2014 soccer World Cup was held in Brazil, so perhaps the time around the world cup will have a higher absence rate. I will plot a few holidays and then the dates surrounding the World Cup (which lasted for a month). Perhaps there is a slight increase in absence rate during the World Cup? The graph does not clearly indicate either was but I can compare the average absence rate during the World Cup to the average absence rate over the entire length of 2014. There is a small boost, but not a significant rise in the absence rate during the World Cup. It looks like in terms of time variability, there is no significant correlation based on the day of the year or the day of the month. There is one more time variation I want to look at, and that is day of the week. To remind myself of the small sample sizes on the weekend, I also show the appointment days of the week count. Finally, it looks like there might be some time variability in this data when it is broken down by weekday. Excluding Saturday and Sunday because of their small sample sizes, Monday and Friday have the highest absence rate and Tuesday has the lowest. I will alter the plot to show the absence rate as a relative percentage change from the overall mean absence rate. Based on this chart, patients are 6% more likely to miss an appointment on a Monday compared to the overall average absence rate, and 4% less likely to miss an appointment on a Tuesday. This is an actionable discovery for both patients and doctors! If patients want to keep their appointments, and if doctors want to make sure their patients show up, they should schedule their appointments during the middle three days of the week. Patient Age I wanted to move on from the time variability and look at the demographic data. In particular, I am interested in whether or not age is correlated with absence rate. My initial guess would be that the youngest and the oldest patients would tend to have lower absence rates. Meanwhile, those patients in the middle would generally be healthier and thus would feel more inclined to skip an appointment. (Everyone is convinced they are invincible in their 20s. In fact, this was one of the issues associated with the initial rollout of Obamacare. Too many young, healthy individuals did not believe they needed insurance and therefore did not sign up for healthcare.) First I will group patients by age and then visualize the average absence rates. The chart reveals the possibility of a negative relationship between age and absence rate. First though, it is clear that the outliers in the upper end of the data introduce substantial variance. As there are only 719 patients over 90 out of 300000 entries, I think I can filter out any ages over 90 without impacting the validating of the data while reducing the noise. I will use that filter and then improve the aesthetics of the graph. We can observe that the youngest ages have a relatively low absence rate with an exception for 1-year-olds. The absence rate then climbs for teenagers, peaking around age 20, before beginning a long, gradual decline to around 70, at which point the absence rate rises again slightly. I would like to know precisely which ages have the highest and lowest absence rates. Moreover, what is the pearson correlation between age and absence rate? Here are more actionable conclusions. Public health officials need to work on getting teenagers to show up to their appointments! This is expecially crucial because studies have shown that habits form very early and are hard to change later in life. A trend of going to the doctor while younger will likely persist as a patient ages and lead to better lifelong health. We can further bin the data in five years increments to highlight the “problem years”, that is, the years with the highest missed appointment rate. I will plot the relative absence rate, or the average age group absence rate as a percentage relative to the average absence rate for all ages. Remember, in this plot, below the x-axis is better because it indicates that the age group has a lower rate of missed appointments than the average. Based on the chart, is it clear that the worst group in terms of absence rate is 15–20 year-olds and the best group for attedance is 70–75 year-olds. The visualization and the statistics are definitive when it comes to ages and missed appointments. The correlation between ages and absence rate is -0.86, which is strongly negative and indicates that as the age of the patient increases, statistically, that patient is less likely to miss a scheduled doctor’s appointment. Waiting Time After finding a strong relationship between age and absence rate, I will move on to examine other variables. I would like to search for a possible trend in absence rate by waiting time, or the time between when an appointment was scheduled, and when the appointment took place. My hypothesis is that patients with a longer waiting time will have a higher missed appointment rate because their condition has more time to change in the interim period. Moreover, I think that patients with a shorter waiting time are more likely to need urgent care or have a problem they deem to be pressing, and it would certainly be in their best interest to show up at the appointment. The graph and the correlation coefficient seem to demonstrate a lack of any relationship. However, thinking back on the histogram of waiting time from the initial exploration, the majority of the patients waited under 100 days. There are far fewer data points from people waiting more than 100 days, which is why the average absence rate for those times tends to be either 0% or 100%. If I limit the data to people with a waiting time less than 3 months (90 days), might there exist a relationship? Furthermore, I will group the data into 10 day segments. If the data is limited to those patients who scheduled fewer than 3 months in advance, there is a slight positive correlation between waiting time and absence rate. I tried to do some research on this subject in general, and found this from the Safety Net Dental Clinic: “Experience in many safety net dental programs also suggests that the incidence of broken appointments increases when appointments are scheduled more than three weeks in advance.” Based on my analysis, I concur. Patients who schedule less than 10 days out are 20% less likely to miss an appointment than those scheduling further out. The Safety Net Dental Clinic page mentions that often other life events get in the way when we schedule too far out, and although that may be a slightly different situation, the general pattern appears to hold in this case. Add a point to schedule as few days out as possible to the list of recommendations for patients looking to adhere to a scheduled appointment. SMS Reminders One of the most intriguing aspects of the dataset to me was the SMS (Short Message Service) text message reminder counts. Text message reminders have been implemented in many clinics because of their ease of use and low cost. Moreover, they have been shown to be effective in some situations. I want to examine this data to discover how or if SMS messages correlate with a failure to attend. I would think that patients who recieve SMS messages will be more likely to attend the appointment than those not receiving them. However, this could be complicated by the fact that younger patients are more likely to sign up for text message alerts, and as we have already seen, young individuals are more inclined to be record a no-show. Therefore, I will probably need to further look at who exactly is recieving the text messages in addition to the correlation between text messages and attendance at the appointment. The graph is somewhat surprising to me; one text message seems to decrease the absence rate while two increases the absence rate significantly. I would have expected more SMS reminders to be strongly correlated with a decrease in absence rate. Maybe the direction of this relationship is the opposite of what I thought. Instead of SMS messages persuading patients to attend a scheduled appointment, patients who are least likely to attend an appointment receive more text messages. One way to check for this would be to look at average SMS messages per appointment received by age. I expect there might be odd behavior at the ends of this graph (how young do children receive their first phone nowadays? Has the average 80-year old embraced text messages?). I expect that segmenting by age might reveal more nuances than the overall absence rate vs text messages. Sure enough, the average number of SMS reminders per appointment is greater among the younger ages. I looked through the data provider information, and I could not ascertain whether or not those SMS reminders were to the parent in the case of a very young child but that seems likely. From the statistics and the plot, there is a strong negative correlation between the number of SMS messages and the age of the patient. To further explore the absence rate vs text message, I will recreate the absence rate vs age graph, but this time, create separate curves for number of SMS reminders received. Looking at the graph, we can see that the absence rate versus age grouped by number of SMS reminders looks much the same for the 0 or 1 reminders. However, the graph for 2 reminders is much noisier. I suspect that is due to the lower sample size of people receiving two reminders (which was the maximum). In order to determine if there is an effect from text message reminders at a given age, I need to create a plot showing the difference in absence rates at each age between those receiving any reminder and those receiving no reminder. ```Indeed, it does appears that at a given age, the absence rate decreases with 1 or 2 text messages reminders. In fact, the average difference in absence rate holding age constant is -0.57%. That means that having receiving an SMS reminder at a given age decreases the chance that a patient will miss a scheduled appointment by 0.6%. This may seem small, but it could add up over millions of appointments. Moreover, there are certain ages when the effect of the SMS reminders is much greater. The reduction in missed appointments is as great as 2% at age ten and near 3% during other years. This reduction in the failure to attend rate is also what research into text message reminders has found. One study,conducted in Brazil, found: “The nonattendance reduction rates for appointments at the four outpatient clinics studied were 0.82% (p = .590), 3.55% (p = .009), 5.75% (p = .022), and 14.49% (p = <.001).” Furthermore, the conclusion of this research was: “The study results indicate that sending appointment reminders as text messages to patients’ cell phones is an effective strategy to reduce nonattendance rates. When patients attend their appointments, the facility providing care and the patients receiving uninterrupted care benefit.” Based on my analysis of the data, I must concur that at a given age, SMS reminders decrease the absence rate. Another valuable piece of information that could be quite easily implemented in the real world to improve outcomes! Patient Markers The final step in this exploratory analysis is to look at the absence rate by the condition for which the patient visited the doctor. I will group by the markers in the original data and then look at each average absence rate. There are considerable differences illustrated between those with different markers. Tuberculosis has the highest rate of absence, followed by alcoholism. However, these conditions also make up a small sample of the overall visits. Diabetes and hypertension have the lowest absence rate. Interestingly, patients marked as being part of the Bolsa Familia welfare program have a high rate of absences. This program rewards patients for attending appointments, which should induce patients to visit their doctor more often. Perhaps there are other factors at play here though. It could be that families on the welfare program cannot take the time to visit the doctor because of the opportunity cost of not working. To put the conditions in perspective, here is the percentage of all visits each condition makes up. The final plot I can make is of the conditions and the relative percentage difference in absence rates compared to the mean absence rate. Relative difference plots are helpful to me because it is possible to quickly identify if one variable is above or below the average. Again, for this particular plot, values below the x-axis are great because it means the absence rate is lower than the overall average. Create the relative absence rate compared to the age versus condition plot. The differences between conditions here are stark. Again, the smaller sample sizes associated with alcoholism and tuberculosis must be taken into account, but it is clear that patients with diabetes and hypertension are more likely to attend an appointment. In fact, they are more than 15% less likely to miss an appointment then the average patient. This is an interesting result and I wonder if it is because of the constant need for care when treating both of these conditions. However, it would seem like all of the conditions require care on an ongoing basis. I would need more detailed condition information before I drew any conclusions about possible causes of the absence rates broken out by condition. Summary of Analysis I started this project off with a single simple question: what factors are most likely to determine whether or not a patient shows up to their scheduled doctor’s appointment? As the exploratory data analysis went on, I found this one question had branched into dozens. Does the number of SMS reminders correlate with absence rates? How about if we compare SMS reminders for a given age? Are there major variabilities that we can observe associated with holidays? I found my curiousity and interest in the dataset only grew as I delved further and further. Although at first it appeared there were few meaningful relationships, by grouping and segmenting the data, clear trends emerged. Keeping in mind that this dataset may not be representative of all countries and health care systems, the following are the most notable discoveries from the patient no-show data: 2. There is a slight positive correlation between absence rate and the day of the month and as the month progresses, the percentage of missed appointments rises moderately. 3. There is no correlation in the absence rate over the days of the year. 4. Excluding the weekends, the day of the week with the highest absence rate is Monday followed by Friday. Tuesday had the lowest rate of absences with appointments on Tuesday 4% more likely to be attended than those on the other days of the week. 5. The age of patients was demonstrated a strong negative linear correlation with absence rates with a correlation coefficient of -0.86. 6. There was a slight positive correlation between the rate of missed appointments and how many days in advance the patient scheduled the appointment. 7. When looking at the dataset as a whole, patients who recieved two SMS reminders were more likely to miss an appointment than those who recieved no reminders. However, the correlation between age and average text messages recieved was -0.80 meaning that younger patients, who were more likely to miss appointments overall, received more text messages. Subsequently, the effect of text messages reminders can only be revealed by looking at messages received at a given age. 8. Patients whose appointments were marked for tuberculosis and alcoholism were the most likely to miss the visit, while those coded for hypertension and diabetes were the least likely to miss their appointment. These are but a few briefs observations that can be gleaned from this dataset. Keeping in mind that correlations do not imply causations (more serious link for those interested), and that some of the groupings results in small sample sizes, there are actionable items that with appropriate further study, could be implemented in the health care system. Revisiting Three Most Revealing Plots The final aspect of the project was to revisit and refine three of my earlier plots. Primarily I am concerned with whether or not the visualizations correctly convey the information within the dataset. My secondary objective is of course aesthetics because what good is even the most informative chart if it is not presentable? Absence Rate vs. Day of the Year This first chart was notable to me because of the lack of relationships it revealed. It is the graph of absence rate versus day of the year in 2014 with holidays (and the World Cup) included. I would have initially believed that absence rates spiked around holidays and near the end of the year. However, no clear trend emerged from this plot. In order to improve the information content and the aesthetics, I altered some of the colors, changed the scale on the axis, added in the mean absence rate for reference, and made sure all labels were accurate. The conclusion to draw from this visualization is that there is no trend in rate of missed appointments over the year by day. Even on major public holidays (of which the World Cup may be the greatest!) there is no noticeable change in absence rate around or on the holiday. This was backed up by the correlation coefficient which showed no linear relationship between day of year and the failure to attend statistic. The overall mean, plotted as the horizontal black line, shows the absence rate for all patients was 30.24%. Women on average had a slightly lower rate of 29.87% and men had a slightly higher rate at 31.00%. The gender discrepancy can barely be picked out on the graph, but it is present. Absence Rate vs. Age by SMS Reminders The second crucial visualization is the absence rate versus age broken down by number of SMS reminders. When I initially looked at the data showing that overall, people who received 2 text message reminders had higher absences rates, I was a little skeptical. However, after some thought, I realized that people who received text messages were likely to be younger, and I had seen that the younger the patient, the higher the absence rate. Therefore, I decided to look at the effect of text messages reminders at each given age. Based on the resulting chart, I saw that indeed, SMS reminders do reduce the absence rate at a specific age. To make the point clearer in the plot below, I plotted the average absence rate vs age for those who received no text reminders and for those who received either 1 or 2 reminders. The original graph was very noisy, so I applied a moving average over 5 years. The 5 year window was a selection based on the bias-variance tradeoff because while I wanted a smoother plot, I did not want to introduce a high amount of bias into the averages. After improving the graph, it is possible to observe the trend of fewer absences at older ages and the effectiveness of SMS reminders in reducing the failure to attend rate. The main two takeaways from this visualization are that as age increases, the absence rate decreases, and, at a given age, patients who receive at least one SMS reminder miss fewer appointments. The age distribution of missed appointments was in line with what I had expected, although I would have initially guessed that children under 10 would have the low absence rates comparable to what was observed in those aged 40 and over. It was not surprising to me that patients in their teens and early 20s had the highest failure to attend rate and it is clear that more effort needs to be expended in ensuring that this age group shows up for their appointments. This is one application where text message reminders could be most useful. Looking at the chart, it is also possible to observe that the rate of no-shows for those receiving an SMS reminder is lower than that for those recieving no reminder at almost every age. Overall, for a given age, text message reminders resulted in absence rates 0.5% lower which could make a substantial impact on the scale on a national health care system. Absence Rate vs. Waiting Time The third vital graph from this analysis was the relative absence rate vs waiting time graph. This graph displays the absence rate relative the overall mean absence rate for patient groups based on how far in advance they scheduled their appointment. I initially grouped the waiting time, or the elasped days between when the appointment was made and when the appointment occurred, by groups of five days. I then plotted the relative absence rate compared to the overall average for each group. I was surprised to discover how much lower the absence rate was for those sceduling less than 10 days in advance. To improve the plot, I increased the resolution of the graph by narrowing the bin widths to groups of 3 days. I also decided to add in a model created with the LOESS, Local Regression, method of regression. I created a simple model and then used the prediction it generated for each age grouping to draw a curve. The graph reveals the crucial information that appointments scheduled shorter out tend to have higher attendance rates. The actionable takeaway for patients from this graph is to schedule appointments on a shorter time scale, preferentially less than 10 days in advance. The absence rate for those whose appointments were scheduled only 3 days or less in advance was nearly 30% lower relative to the overall absence rate. Moreover, the model shows that the trend in increasing absence rate with longer waiting times is nearly linear for the first month. After that point, the noise in the visualization increases, but the majority of groups with wait times over 12 days had a higher absence rate than the mean. Patients would be wise to schedule their appointments as soon as possible in order to follow through on visiting the doctor. Taken together, these three visualizations illustrate several pieces of advice for patients and doctors who have a mutal interest in driving the absence rate as low as possible: 1. Day of the year does not affect the absence rate even near the holidays.2. Young adults are the most likely to miss their appointments and therefore will need extra prodding to attend an appointment. This prodding can come in the form of SMS reminders, which reduce the absence rate for a given age.3. Patients should schedule appointments as few days in advance as possible. Ideally appointments would occur within 9 days of being scheduled to increase the chance of attendance. Reflections The primary reason I wanted to learn the tools of data analysis was in order to could extract meaningful information from the mounds of data generated in our modern world. I want to be able to take hundreds of thousands or even millions of data points and extract insights which can be implemented in the real world to improve human institutions, such as the medical system. Exploring the patient no-show dataset has been a small step towards developing that ability. Although the sheer amount of data appeared overwhelming at first, and there were no correlations that immediately stood out, by selectively grouping the data and adjusting the visualization parameters, I was able to discover several key relationships.The main difficulty I had was beginning the analysis. With so many variables, it was a struggle to decide where I should first concentrate. However, once I started grouping the data, I found more and more directions I could explore until I felt that I was satisfied with the extent to which I had unraveled the data. The trends and patterns in the datadictate the questions that we should ask of it. R is a tricky language to pick up, but once I understood the patterns and syntax, I enjoyed the level of control it gave me over the visualizations. I was frustrated at times trying to perfect parameters of a graph, but in the end, I think I am a better data analyst because I had to work through all the intricacies of the R language. The next step forward, now that the exploratory data analysis of the no-shows dataset is complete, is to perform confirmatory data analysis. Based on my initial observations, I could form several hypotheses and then use rigorous statistical methods to test those hypothesis. Exploratory data analysis can discover potential relationships, but it takes statistical testing to determine whether these correlations are statistically meaningful. Moreover, this dataset is an ideal candidate for using machine learning to create classifiers that would identify patients likely to be a no-show at an appointment. The objective would be to make a model that would take in as features patient demographics and conditions, and would return the likelihood that a patient would fail to attend a scheduled doctor’s appointment. If the model was accurate enough, it could then be implemented in the real-world by ensuring that patients most likely to miss a doctor’s visit receive additional persuasion. Moreover, with a more complete dataset, including city information or detailed demographics, more relationships could be discovered such as absence rate correlations with the weather or with access to public transportation. The initial exploratory data analysis of the doctor’s appointment no-show data has revealed numerous potential relationships. The dataset holds actionable information and this project demonstrates the benefits of not just collecting large amounts of data, but thoroughly analyzing it to find the correlations that could be used to improve patient outcomes and public health. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Gender: M or F AppointmentReservationDate: date and time appointment was made AppointmentDate: date of appointment without time DayOfTheWeek: day of the week of appointment Status: Show-up or No-Show Diabetes: 0 or 1 for condition (1 means patient was scheduled to treat condition) Alcoholism: 0 or 1 for condition Hypertension: 0 or 1 for condition Smoker: 0 or 1 for smoker / non-smoker Scholarship: 0 or 1 indicating whether the family of the patient takes part in the Bolsa Familia Program, an initiative that provides families with small cash transfers in exchange for keeping children in school and completing health care visits Tuberculosis: 1 or 0 for condition SMSReminder: 0 ,1 ,2 for number of text message reminders sent to patient about appointment WaitingTime: integer number of days between when the appointment was made and when the appointment took place. December has the highest absence rate and January has the lowest but there is no trend in between. During the month-long World Cup in 2014 in Brazil, the absence rate increased by only 0.6%. There were some exceptions to this however. The youngest patients had a relatively low rate of missed appointments, then the absence rate rose and peaked in the teens, before gradually declining until age 70 where it increased by a small amount into the upper end of the range. 18-year-olds had the highest rate of absences at 40.2%. 72-year-olds had the lowest rate of absences at 20.2%. Based on the data analysis and concurring research, text message reminders decreased the absence rate. Patients with hypertension or diabetes were 15% more likely to attend a scheduled appointment than the average average patient. Age: integer age of patient There is no significant change in the absence rate over the course of the year broken down by month. Major Brazilian holidays did not correspond to any significant changes in the absence rate. As the age of the patient increased, the likelihood that they would not show up to their appointment decreased. Patients who made an appointment less than 10 days in advance were 20% more likely to attend the appointment than those who made the appointment further ahead of time. At a specified age, patients who recieved 1 or 2 text messages were 0.5% less likely to miss an appointment than those receiving no text messages. This effect was even more pronounced among certain age categories. Patients with families in the Bolsa Familia program were more likely to miss an appointment than the average patient.",Exploratory Data Analysis with R,5,published,4219,7559,0.01270009260484191,0,0,1,0,0,0
120,5,657.1143596646644,5,https://medium.com/p/exploratory-data-analysis-with-r-no-code-4c70de64c855,0,None,2017-08-08 12:58:00,18.62,31,27,2017-08-08 13:02:00,"['Data Science', 'R Programming', 'Udacity', 'Data Analysis']","Exploratory Data Analysis with R (No Code) Examining the Doctor’s Appointment No-Show Dataset Author’s Note: The following exploratory data analysis project was completed as part of the Udacity Data Analyst Nanodegree that I finished in May 2017. All code for this project can be found on my GitHub repository for the class. I highly recommend the course to anyone interested in data analysis (that is anyone who wants to make sense of the mass amounts of data generated in our modern world) as well as to those who want to learn basic programming skills in an applied setting. This version of the Exploratory Data Analysis project has all the code removed for readability. The version with all the R code included is also on Medium. Doctor’s appointment no-shows are a serious issue in the public health care field. Missed appointments are associated with poorer patient outcomes and cost the health care system in the US nearly $200 each. Therefore, it comes as no small surprise that reducing the rate of no-shows has become a priority in the United States and around the world. Numerous studies have been undertaken in order to determine the most effective means of reducing rates of absenteeism at with varying degrees of success. The first step to solving the problem of missed appointments is identifying why a patient skips a scheduled visit in the first place. What trends are there among patients with higher absence rates? Are there demographic indicators or perhaps time-variant relationships hiding in the data? Ultimately, it was these questions that drove my exploratory data analysis. I was curious as to the reasons behind missed appointments, and wanted to examine the data to identify any trends present. I choose this problem because I believe it is an excellent example of how data science and analysis can reveal relationships which can be implemented in the real-world to the benefit of society. I wanted to choose a dataset that was both relatable and could be used to make smarter decisions. Therefore, I decided to work with medical appointment no shows data available on Kaggle. This dataset is drawn from 300,000 primary physician visits in Brazil across 2014 and 2015. The information about the appointment was automatically coded when the patient scheduled the appointment and then the patient was marked as having either attended or not. The information about the appointment included demographic data, time data, and conditions concerning the reason for the visit. There were a total of 14 variables I included from the original data. The variables and the description of the values are as follows Let’s take a look at the structure of the dataframe to identify cleaning/organizing that may need to be performed. From the structure of the dataframe, I can see there is some data housekeeping that needs to be done. First, I want to change the Status variable into an integer 0 or 1. A value of 1 will indicate that the patient did not show up as I am concerned with the variables that are most strongly correlated with a missed appointment. Moreover, I need to convert the registration date and the appointment date to date objects. I decided not to maintain the time (hours:minutes:seconds) associated with the registration date for the appointment although it was available (time of day was not available for the appointment date). I converted both the appointment registration date and appointment date to date objects so I could investigate seasonal and time patterns in the data. Based on the updated structure of the dataframe, it looks like I am on the right track but I still have a couple more modifications to make to the dataframe. I will create month, year, and day fields for the appointment and also rename the columns to a more consistent and readable format. That should about do it for the structure of the main dataframe. I am also concerned with missing/corrupted data, so I will look at the summary of each field to see if any obvious errors or outliers are present in the data. The summary of the no_shows dataframe reveals plenty of intriguing information as well as several errors that need to be corrected. Right away, it is clear that an age of -2 is not correct, and 113 is stretching the boundaries. However, looking at the oldest people in the world, it is plausible that 113 could be a correct age. I will filter out any negative ages and create a histogram of ages to see if there are a significant amount of outliers at the upper end. Moreover, this data summary shows that 30.24% of appointments are missed based on the mean for the status field. The rates for the various patient conditions can also be seen, and the most commonly coded reason for an appointment is hypertension at nearly 22% of visits. Also, the weekends appear to be a very unpopular time for doctor’s appointments in Brazil, or perhaps the clinics from which this data was drawn are not open on the weekend. Either way, I will need to keep the small sample size of appointments on the weekend in mind when I perform an analysis by weekday. Frequency Distribution of Ages The age histogram does not aligh exactly with my intutions though it is close. I expected that it would be bi-modal, with peaks in patient count at the youngest ages and at the oldest ages. However, while the largest number of patients do appear to be the youngest, the number of patients remains fairly constant into the middle ages, with a second peak around age 60 and then a steep decline into the oldest ages. Based on the visualization, there do not appear to be a large number of outliers in the upper range of the ages. I was skeptical of the high proportion of visits from those aged 0 and decided I needed to do some research. Based on the description of the dataset, this data is for primary care physicians in the public sector and so I looked for a breakdown of ages of patients at these appointments. I could not find statistics from Brazil, but based on official statistics from the Centers for Disease Control in the United States, children under 1 year of age make up 2.6% of all physician visits. I can create a better plot showing the percentage each age is of all patients. I can also quickly check the percentage of visits comprised of patients aged 0 in the data. Percentage of patients with an age of 0:3.442069% The calculation shows that 0-year olds make up 3.44% of visits. While the initial frequency distribution may looked skewed, the spread of the data along the years makes it look like 0-year olds make up considerably more of the visits than is actually the case. The corrected bar plot does a better job of representing the proportion of patients from each age. From the research by the CDC and the visualization, I conclude that the age distribution skew is not bad but rather legitimate data. To check for outliers in the waiting time field (or how long between the date the appointment was made and the actual date of the appointment), I will make another histogram. Based on the summary statistics of the dataframe, the longest waiting time was 398 days, with a median of 8 days, and a mean of 13.84 days. From the histogram and the statistics, it is clear that there are a small number of patients at the upper end of the distribution who schedule their appointment very far in advance. I wonder if the 398 days is accidentally a mistake that occured when someone choose the wrong year for their appointment! The graph is certainly long-tailed and positively skewed with a few extreme outliers at the upper limit. Number of patients with wait time longer than one year:1Number of patients with wait time longer than six months:136 It appears there is only one patient with a waiting time over one year and just over 100 (out of 300000) with a waiting time of over six months. I will exclude the patient with the waiting time over one year, but I will treat the others as good but extreme data. Here is a version of the histogram that better represents the wait time for most patients. As can be seen in the chart, a plurality of patients wait less than 10 days between the scheduling and the date of their appointment. Based on these visualizations, and the summary statistics for the dataframe, I am confident that the data content of the main dataframe is valid. I can now start the exploration phase of exploratory data analysis! It is time to discover the relationships, or lack thereof, between the variables in the data. The first step I can take is to find the correlations between all of the columns to see if any stand out as particularly compelling. Of course, I expect that all the trends will not reveal themselves straight away. The rcorr function from the Hmisc library will calculate the Pearson’s correlation coeffient between every field of a dataframe. The Pearson correlation coefficient, is a measure of how linearly dependent two variables on each other. The coefficient value is between -1 and +1, and two variables that are perfectly linearly correlated will have a value of +1. From those correlation values, there are no standouts that are strong linear predictors of whether or not a patient will miss a visit (given by the status column). It appears that there are not even any strong relationships whatsoever though there is a moderate relationship between age and hypertension. However, I still think there are meaningful relationships to extract from the data. My approach will be to group the data by various fields and then determine if the average absence rate shows a relationship with the fields. The first group of variables I will look at will be those associated with the time-variability of the appointment date. Date of Appointment To start off the time-series analysis, I will want to look at absence rate by month of the year. I first needed to group the appointments by month of year and then find the average absence rate for each month. From the graph and the correlation coefficient, it appears there is a minor relationship between month of the year and absence rate. Missed visits do appear to rise as the year progresses, but with only 12 data points, the 95% confidence interval for the correlation coefficient is quite large (-0.05 to 0.85). I would expect that during the end of the year, people tend to be busier and may miss more appointments. It could also be possible that at the beginning of the year, individuals make resolutions to see the doctor and keep a higher percentage of appointments, but the trend could also be noise. Next, I will create a similar absence rate over time graph, but this time by day of the month. Looking at the plot we can see that there is a very slight positive correlation between day of the month and absence rate but again, the 95% confidence interval spans a wide range of values. To further explore any time variability in the data, I can create the same graph by day of the year and broken out by gender. Mean absence rate for males:30.99653%Mean absence rate for females29.86962% From these charts, it appears looks as if there is no correlation betwen the time of year and the absence rate for appointments. Moreover, adding in the gender does not reveal any major descrepancies, although from the calculation, we can see that men on average have a 1% higher absence rate. There are several days for which the absence rate is 1, which bears some more investigation. My idea is that these points might corespond to public holidays. However, I also see that these days are not consistent across the two years, so maybe there are other major events that correspond to an increase in absences. I decided to overlay the holidays to see if that might reveal a relationship. I sourced the holidays based on public holidays in Brazil for each respective year. Again, it seems as if these holidays do not line up with higher rates of missed appointments. There is a sustained increase in the absence rate around the beginning of July, but it does not seem to correspond to any holidays. I researched festivals in Brazil in June-July 2015, and it appears that the Paraty International Literary Festival occured July 1–5, but I doubt that this explains the absence rate. There was not city data associated with the original dataset, so it is difficult to attempt to cross-reference specific dates with specific events. I will create a simliar graph for 2014. I remember that the 2014 soccer World Cup was held in Brazil, so perhaps the time around the world cup will have a higher absence rate. I will plot a few holidays and then the dates surrounding the World Cup (which lasted for a month). Perhaps there is a slight increase in absence rate during the World Cup? The graph does not clearly indicate either was but I can compare the average absence rate during the World Cup to the average absence rate over the entire length of 2014.Mean absence rate during all of 2014\ [1] 30.08656Mean absence rate during 2014 World Cup[1] 30.79473 There is a small but not significant increase in the absence rate during the World Cup. It looks like in terms of time variability, there is no significant correlation based on the day of the year or the day of the month. There is one more time variation I want to look at, and that is day of the week. To remind myself of the small sample sizes on the weekend, I also show the appointment days of the week count. Finally, it looks like there might be some time variability in this data when it is broken down by weekday. Excluding Saturday and Sunday because of their small sample sizes, Monday and Friday have the highest absence rate and Tuesday has the lowest. I will alter the plot to show the absence rate as a relative percentage change from the overall mean absence rate. Based on this chart, patients are 6% more likely to miss an appointment on a Monday compared to the overall average absence rate, and 4% less likely to miss an appointment on a Tuesday. This is an actionable discovery for both patients and doctors! If patients want to keep their appointments, and if doctors want to make sure their patients show up, they should schedule their appointments during the middle three days of the week. Patient Age I wanted to move on from the time variability and look at the demographic data. In particular, I am interested in whether or not age is correlated with absence rate. My initial guess would be that the youngest and the oldest patients would tend to have lower absence rates. Meanwhile, those patients in the middle would generally be healthier and thus would feel more inclined to skip an appointment. (Everyone is convinced they are invincible in their 20s. In fact, this was one of the issues associated with the initial rollout of Obamacare. Too many young, healthy individuals did not believe they needed insurance and therefore did not sign up for healthcare.) First I will group patients by age and then visualize the average absence rates. The chart reveals the possibility of a negative relationship between age and absence rate. First though, it is clear that the outliers in the upper end of the data introduce substantial variance. As there are only 719 patients over 90 out of 300000 entries, I think I can filter out any ages over 90 without impacting the validating of the data while reducing the noise. I will use that filter and then improve the aesthetics of the graph. We can observe that the youngest ages have a relatively low absence rate with an exception for 1-year-olds. The absence rate then climbs for teenagers, peaking around age 20, before beginning a long, gradual decline to around 70, at which point the absence rate rises again slightly. I would like to know precisely which ages have the highest and lowest absence rates. Moreover, what is the pearson correlation between age and absence rate? Maximum absence rate of 40.21% occurs at age 18Minimum absence rate of 20.25% occurs at age 72 Here are more actionable conclusions. Public health officials need to work on getting teenagers to show up to their appointments! This is expecially crucial because studies have shown that habits form very early and are hard to change later in life. A trend of going to the doctor while younger will likely persist as a patient ages and lead to better lifelong health. We can further bin the data in five years increments to highlight the “problem years”, that is, the years with the highest missed appointment rate. I will plot the relative absence rate, or the average age group absence rate as a percentage relative to the average absence rate for all ages. Remember, in this plot, below the x-axis is better because it indicates that the age group has a lower rate of missed appointments than the average. Based on the chart, is it clear that the worst group in terms of absence rate is 15–20 year-olds and the best group for attedance is 70–75 year-olds. The visualization and the statistics are definitive when it comes to ages and missed appointments. The correlation between ages and absence rate is -0.86, which is strongly negative and indicates that as the age of the patient increases, statistically, that patient is less likely to miss a scheduled doctor’s appointment. Waiting Time After finding a strong relationship between age and absence rate, I will move on to examine other variables. I would like to search for a possible trend in absence rate by waiting time, or the time between when an appointment was scheduled, and when the appointment took place. My hypothesis is that patients with a longer waiting time will have a higher missed appointment rate because their condition has more time to change in the interim period. Moreover, I think that patients with a shorter waiting time are more likely to need urgent care or have a problem they deem to be pressing, and it would certainly be in their best interest to show up at the appointment. The graph and the correlation coefficient seem to demonstrate a lack of any relationship. However, thinking back on the histogram of waiting time from the initial exploration, the majority of the patients waited under 100 days. There are far fewer data points from people waiting more than 100 days, which is why the average absence rate for those times tends to be either 0% or 100%. If I limit the data to people with a waiting time less than 3 months (90 days), might there exist a relationship? Furthermore, I will group the data into 10 day segments. Patients with a waiting time over 90 days1646 If the data is limited to those patients who scheduled fewer than 3 months in advance, there is a slight positive correlation between waiting time and absence rate. I tried to do some research on this subject in general, and found this from the Safety Net Dental Clinic: “Experience in many safety net dental programs also suggests that the incidence of broken appointments increases when appointments are scheduled more than three weeks in advance.” Based on my analysis, I concur. Patients who schedule less than 10 days out are 20% less likely to miss an appointment than those scheduling further out. The Safety Net Dental Clinic page mentions that often other life events get in the way when we schedule too far out, and although that may be a slightly different situation, the general pattern appears to hold in this case. Add a point to schedule as few days out as possible to the list of recommendations for patients looking to adhere to a scheduled appointment. SMS Reminders One of the most intriguing aspects of the dataset to me was the SMS (Short Message Service) text message reminder counts. Text message reminders have been implemented in many clinics because of their ease of use and low cost. Moreover, they have been shown to be effective in some situations. I want to examine this data to discover how or if SMS messages correlate with a failure to attend. I would think that patients who recieve SMS messages will be more likely to attend the appointment than those not receiving them. However, this could be complicated by the fact that younger patients are more likely to sign up for text message alerts, and as we have already seen, young individuals are more inclined to be record a no-show. Therefore, I will probably need to further look at who exactly is recieving the text messages in addition to the correlation between text messages and attendance at the appointment. The graph is somewhat surprising to me; one text message seems to decrease the absence rate while two increases the absence rate significantly. I would have expected more SMS reminders to be strongly correlated with a decrease in absence rate. Maybe the direction of this relationship is the opposite of what I thought. Instead of SMS messages persuading patients to attend a scheduled appointment, patients who are least likely to attend an appointment receive more text messages. One way to check for this would be to look at average SMS messages per appointment received by age. I expect there might be odd behavior at the ends of this graph (how young do children receive their first phone nowadays? Has the average 80-year old embraced text messages?). I expect that segmenting by age might reveal more nuances than the overall absence rate vs text messages. Sure enough, the average number of SMS reminders per appointment is greater among the younger ages. I looked through the data provider information, and I could not ascertain whether or not those SMS reminders were to the parent in the case of a very young child but that seems likely. From the statistics and the plot, there is a strong negative correlation between the number of SMS messages and the age of the patient. To further explore the absence rate vs text message, I will recreate the absence rate vs age graph, but this time, create separate curves for number of SMS reminders received. Looking at the graph, we can see that the absence rate versus age grouped by number of SMS reminders looks much the same for the 0 or 1 reminders. However, the graph for 2 reminders is much noisier. I suspect that is due to the lower sample size of people receiving two reminders (which was the maximum). In order to determine if there is an effect from text message reminders at a given age, I need to create a plot showing the difference in absence rates at each age between those receiving any reminder and those receiving no reminder. Indeed, it does appears that at a given age, the absence rate decreases with 1 or 2 text messages reminders. In fact, the average difference in absence rate holding age constant is -0.57%. That means that having receiving an SMS reminder at a given age decreases the chance that a patient will miss a scheduled appointment by 0.6%. This may seem small, but it could add up over millions of appointments. Moreover, there are certain ages when the effect of the SMS reminders is much greater. The reduction in missed appointments is as great as 2% at age ten and near 3% during other years. This reduction in the failure to attend rate is also what research into text message reminders has found. One study,conducted in Brazil, found: “The nonattendance reduction rates for appointments at the four outpatient clinics studied were 0.82% (p = .590), 3.55% (p = .009), 5.75% (p = .022), and 14.49% (p = <.001).” Furthermore, the conclusion of this research was: “The study results indicate that sending appointment reminders as text messages to patients’ cell phones is an effective strategy to reduce nonattendance rates. When patients attend their appointments, the facility providing care and the patients receiving uninterrupted care benefit.” Based on my analysis of the data, I must concur that at a given age, SMS reminders decrease the absence rate. Another valuable piece of information that could be quite easily implemented in the real world to improve outcomes! Patient Markers The final step in this exploratory analysis is to look at the absence rate by the condition for which the patient visited the doctor. I will group by the markers in the original data and then look at each average absence rate. There are considerable differences illustrated between those with different markers. Tuberculosis has the highest rate of absence, followed by alcoholism. However, these conditions also make up a small sample of the overall visits. Diabetes and hypertension have the lowest absence rate. Interestingly, patients marked as being part of the Bolsa Familia welfare program have a high rate of absences. This program rewards patients for attending appointments, which should induce patients to visit their doctor more often. Perhaps there are other factors at play here though. It could be that families on the welfare program cannot take the time to visit the doctor because of the opportunity cost of not working. To put the conditions in perspective, here is the percentage of all visits each condition makes up. The final plot I can make is of the conditions and the relative percentage difference in absence rates compared to the mean absence rate. Relative difference plots are helpful to me because it is possible to quickly identify if one variable is above or below the average. Again, for this particular plot, values below the x-axis are great because it means the absence rate is lower than the overall average. The differences between conditions here are stark. Again, the smaller sample sizes associated with alcoholism and tuberculosis must be taken into account, but it is clear that patients with diabetes and hypertension are more likely to attend an appointment. In fact, they are more than 15% less likely to miss an appointment then the average patient. This is an interesting result and I wonder if it is because of the constant need for care when treating both of these conditions. However, it would seem like all of the conditions require care on an ongoing basis. I would need more detailed condition information before I drew any conclusions about possible causes of the absence rates broken out by condition. I started this project off with a single simple question: what factors are most likely to determine whether or not a patient shows up to their scheduled doctor’s appointment? As the exploratory data analysis went on, I found this one question had branched into dozens. Does the number of SMS reminders correlate with absence rates? How about if we compare SMS reminders for a given age? Are there major variabilities that we can observe associated with holidays? I found my curiousity and interest in the dataset only grew as I delved further and further. Although at first it appeared there were few meaningful relationships, by grouping and segmenting the data, clear trends emerged. Keeping in mind that this dataset may not be representative of all countries and health care systems, the following are the most notable discoveries from the patient no-show data: 2. There is a slight positive correlation between absence rate and the day of the month and as the month progresses, the percentage of missed appointments rises moderately. 3. There is no correlation in the absence rate over the days of the year. 4. Excluding the weekends, the day of the week with the highest absence rate is Monday followed by Friday. Tuesday had the lowest rate of absences with appointments on Tuesday 4% more likely to be attended than those on the other days of the week. 5. The age of patients was demonstrated a strong negative linear correlation with absence rates with a correlation coefficient of -0.86. 6. There was a slight positive correlation between the rate of missed appointments and how many days in advance the patient scheduled the appointment. 7. When looking at the dataset as a whole, patients who recieved two SMS reminders were more likely to miss an appointment than those who recieved no reminders. However, the correlation between age and average text messages recieved was -0.80 meaning that younger patients, who were more likely to miss appointments overall, received more text messages. Subsequently, the effect of text messages reminders can only be revealed by looking at messages received at a given age. 8. Patients whose appointments were marked for tuberculosis and alcoholism were the most likely to miss the visit, while those coded for hypertension and diabetes were the least likely to miss their appointment. These are but a few briefs observations that can be gleaned from this dataset. Keeping in mind that correlations do not imply causations (more serious link for those interested), and that some of the groupings results in small sample sizes, there are actionable items that with appropriate further study, could be implemented in the health care system. The final aspect of the project was to revisit and refine three of my earlier plots. Primarily I am concerned with whether or not the visualizations correctly convey the information within the dataset. My secondary objective is of course aesthetics because what good is even the most informative chart if it is not presentable? Absence Rate vs. Day of the Year This first chart was notable to me because of the lack of relationships it revealed. It is the graph of absence rate versus day of the year in 2014 with holidays (and the World Cup) included. I would have initially believed that absence rates spiked around holidays and near the end of the year. However, no clear trend emerged from this plot. In order to improve the information content and the aesthetics, I altered some of the colors, changed the scale on the axis, added in the mean absence rate for reference, and made sure all labels were accurate. The conclusion to draw from this visualization is that there is no trend in rate of missed appointments over the year by day. Even on major public holidays (of which the World Cup may be the greatest!) there is no noticeable change in absence rate around or on the holiday. This was backed up by the correlation coefficient which showed no linear relationship between day of year and the failure to attend statistic. The overall mean, plotted as the horizontal black line, shows the absence rate for all patients was 30.24%. Women on average had a slightly lower rate of 29.87% and men had a slightly higher rate at 31.00%. The gender discrepancy can barely be picked out on the graph, but it is present. Absence Rate vs. Age by SMS Reminders The second crucial visualization is the absence rate versus age broken down by number of SMS reminders. When I initially looked at the data showing that overall, people who received 2 text message reminders had higher absences rates, I was a little skeptical. However, after some thought, I realized that people who received text messages were likely to be younger, and I had seen that the younger the patient, the higher the absence rate. Therefore, I decided to look at the effect of text messages reminders at each given age. Based on the resulting chart, I saw that indeed, SMS reminders do reduce the absence rate at a specific age. To make the point clearer in the plot below, I plotted the average absence rate vs age for those who received no text reminders and for those who received either 1 or 2 reminders. The original graph was very noisy, so I applied a moving average over 5 years. The 5 year window was a selection based on the bias-variance tradeoff because while I wanted a smoother plot, I did not want to introduce a high amount of bias into the averages. After improving the graph, it is possible to observe the trend of fewer absences at older ages and the effectiveness of SMS reminders in reducing the failure to attend rate. The main two takeaways from this visualization are that as age increases, the absence rate decreases, and, at a given age, patients who receive at least one SMS reminder miss fewer appointments. The age distribution of missed appointments was in line with what I had expected, although I would have initially guessed that children under 10 would have the low absence rates comparable to what was observed in those aged 40 and over. It was not surprising to me that patients in their teens and early 20s had the highest failure to attend rate and it is clear that more effort needs to be expended in ensuring that this age group shows up for their appointments. This is one application where text message reminders could be most useful. Looking at the chart, it is also possible to observe that the rate of no-shows for those receiving an SMS reminder is lower than that for those recieving no reminder at almost every age. Overall, for a given age, text message reminders resulted in absence rates 0.5% lower which could make a substantial impact on the scale on a national health care system. Absence Rate vs. Waiting Time The third vital graph from this analysis was the relative absence rate vs waiting time graph. This graph displays the absence rate relative the overall mean absence rate for patient groups based on how far in advance they scheduled their appointment. I initially grouped the waiting time, or the elasped days between when the appointment was made and when the appointment occurred, by groups of five days. I then plotted the relative absence rate compared to the overall average for each group. I was surprised to discover how much lower the absence rate was for those sceduling less than 10 days in advance. To improve the plot, I increased the resolution of the graph by narrowing the bin widths to groups of 3 days. I also decided to add in a model created with the LOESS, Local Regression, method of regression. I created a simple model and then used the prediction it generated for each age grouping to draw a curve. The graph reveals the crucial information that appointments scheduled shorter out tend to have higher attendance rates. The actionable takeaway for patients from this graph is to schedule appointments on a shorter time scale, preferentially less than 10 days in advance. The absence rate for those whose appointments were scheduled only 3 days or less in advance was nearly 30% lower relative to the overall absence rate. Moreover, the model shows that the trend in increasing absence rate with longer waiting times is nearly linear for the first month. After that point, the noise in the visualization increases, but the majority of groups with wait times over 12 days had a higher absence rate than the mean. Patients would be wise to schedule their appointments as soon as possible in order to follow through on visiting the doctor. Taken together, these three visualizations illustrate several pieces of advice for patients and doctors who have a mutal interest in driving the absence rate as low as possible: 1. Day of the year does not affect the absence rate even near the holidays.2. Young adults are the most likely to miss their appointments and therefore will need extra prodding to attend an appointment. This prodding can come in the form of SMS reminders, which reduce the absence rate for a given age.3. Patients should schedule appointments as few days in advance as possible. Ideally appointments would occur within 9 days of being scheduled to increase the chance of attendance. The primary reason I wanted to learn the tools of data analysis was in order to could extract meaningful information from the mounds of data generated in our modern world. I want to be able to take hundreds of thousands or even millions of data points and extract insights which can be implemented in the real world to improve human institutions, such as the medical system. Exploring the patient no-show dataset has been a small step towards developing that ability. Although the sheer amount of data appeared overwhelming at first, and there were no correlations that immediately stood out, by selectively grouping the data and adjusting the visualization parameters, I was able to discover several key relationships.The main difficulty I had was beginning the analysis. With so many variables, it was a struggle to decide where I should first concentrate. However, once I started grouping the data, I found more and more directions I could explore until I felt that I was satisfied with the extent to which I had unraveled the data. The trends and patterns in the datadictate the questions that we should ask of it. R is a tricky language to pick up, but once I understood the patterns and syntax, I enjoyed the level of control it gave me over the visualizations. I was frustrated at times trying to perfect parameters of a graph, but in the end, I think I am a better data analyst because I had to work through all the intricacies of the R language. The next step forward, now that the exploratory data analysis of the no-shows dataset is complete, is to perform confirmatory data analysis. Based on my initial observations, I could form several hypotheses and then use rigorous statistical methods to test those hypothesis. Exploratory data analysis can discover potential relationships, but it takes statistical testing to determine whether these correlations are statistically meaningful. Moreover, this dataset is an ideal candidate for using machine learning to create classifiers that would identify patients likely to be a no-show at an appointment. The objective would be to make a model that would take in as features patient demographics and conditions, and would return the likelihood that a patient would fail to attend a scheduled doctor’s appointment. If the model was accurate enough, it could then be implemented in the real-world by ensuring that patients most likely to miss a doctor’s visit receive additional persuasion. Moreover, with a more complete dataset, including city information or detailed demographics, more relationships could be discovered such as absence rate correlations with the weather or with access to public transportation. The initial exploratory data analysis of the doctor’s appointment no-show data has revealed numerous potential relationships. The dataset holds actionable information and this project demonstrates the benefits of not just collecting large amounts of data, but thoroughly analyzing it to find the correlations that could be used to improve patient outcomes and public health. Originally published at medium.com on August 8, 2017. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Gender: M or F AppointmentReservationDate: date and time appointment was made AppointmentDate: date of appointment without time DayOfTheWeek: day of the week of appointment Status: Show-up or No-Show Diabetes: 0 or 1 for condition (1 means patient was scheduled to treat condition) Alcoholism: 0 or 1 for condition Hypertension: 0 or 1 for condition Smoker: 0 or 1 for smoker / non-smoker Scholarship: 0 or 1 indicating whether the family of the patient takes part in the Bolsa Familia Program, an initiative that provides families with small cash transfers in exchange for keeping children in school and completing health care visits Tuberculosis: 1 or 0 for condition SMSReminder: 0 ,1 ,2 for number of text message reminders sent to patient about appointment WaitingTime: integer number of days between when the appointment was made and when the appointment took place. December has the highest absence rate and January has the lowest but there is no trend in between. During the month-long World Cup in 2014 in Brazil, the absence rate increased by only 0.6%. There were some exceptions to this however. The youngest patients had a relatively low rate of missed appointments, then the absence rate rose and peaked in the teens, before gradually declining until age 70 where it increased by a small amount into the upper end of the range. 18-year-olds had the highest rate of absences at 40.2%. 72-year-olds had the lowest rate of absences at 20.2%. Based on the data analysis and concurring research, text message reminders decreased the absence rate. Patients with hypertension or diabetes were 15% more likely to attend a scheduled appointment than the average average patient. Age: integer age of patient There is no significant change in the absence rate over the course of the year broken down by month. Major Brazilian holidays did not correspond to any significant changes in the absence rate. As the age of the patient increased, the likelihood that they would not show up to their appointment decreased. Patients who made an appointment less than 10 days in advance were 20% more likely to attend the appointment than those who made the appointment further ahead of time. At a specified age, patients who recieved 1 or 2 text messages were 0.5% less likely to miss an appointment than those receiving no text messages. This effect was even more pronounced among certain age categories. Patients with families in the Bolsa Familia program were more likely to miss an appointment than the average patient.",Exploratory Data Analysis with R (No Code),7,published,145,7659,0.0006528267397832616,0,0,1,0,0,0
122,7,654.8220386687847,2,https://medium.com/p/the-technology-frontier-db4dd061ea97,0,None,2017-08-10 19:58:00,29.31,12,17,2017-08-09 06:58:00,"['Technology', 'Books', 'Review', 'Future', 'Predictions']","The Technology Frontier A review of Radical Technologies by Adam Greenfield One Sentence Summary: Current and near-future technologies offer great potential for enhancing our lives, but we need to consider the inherent trade-offs in adopting products and services that dictate an increasing portion of our everyday experiences. “We live in a society exquisitely dependent on science and technology, in which hardly anyone knows anything about science and technology”- Carl Sagan In the nearly three decades since Carl Sagan voiced his concern, we have adopted an amazing array of technologies into our daily lives — smartphones, laptops, the World Wide Web, digital maps, Siri, Alexa, Facebook, the Internet of Things — which have increased convenience and expanded our connectivity with other individuals around the world. Yet, we still do not grasp the fundamentals behind these technologies or realize what we sacrifice when we adopt them without a moment’s hesitation. We imagine technology as a beneficial force with only positive effects — greater ease and access to information — while we overlook the trade-offs — decreased privacy and autonomy — implicit in upgrading to the latest model. Advances promised to us in the coming decades — augmented reality, 3D printing, machine learning, cryptocurrency — are designed to satisfy our needs and bring us novel forms of entertainment. Before we blindly accept these products and services, it is critical that we understand the implications of relinquishing ever more control of our day-to-day experiences to the companies that provide them. This argument forms the premise for Adam Greenfield’s Radical Technologies: The Design of Everyday Life, which explains not only how modern and near-future technologies work, but the concessions we make when these developments are woven into the fabric of modern life. Ceding Control The modern smartphone has become our constant companion — you probably spend more time with your phone than with any individual — and the tools it offers us are indispensable to many of our daily routines. What we see in our smartphones is a sleek exterior housing and a 4.7 inch screen radiating crystal clear images of anything that we desire. What we experience with our devices is an ability to communicate with anyone in the world at a moment’s notice, or to navigate in an unfamiliar city and receive recommendations tailored to our interests. What we fail to observe is the $250 worth of precious metals and raw materials making up our phones that are extracted at an enormous environmental cost, or the $4 in labor required to assemble the phone in Shenzhen, a city of 18 million in China where 90% of the world’s electronics will pass through at some point in the manufacturing process. Also escaping our purview is the immense amount of data we generate in the process of every action involving our phones. When we receive directions from Google Maps, we don’t stop to consider how incredible it is that Google can provide us with real time maps of the entire world at no cost. In reality, Google has become one of the most valuable and influential companies in history through the accumulation of data, and every service they offer is provided not for free, but in exchange for access to the information our daily activities generate. When we stop at a store or drive faster than the speed limit while using Maps, Google records that information and can then sell it to advertisers or auto insurance companies that might be interested in your driving habits. Greenfield refers to this as the monetization of everyday experience. After companies realized they could extract valuable information from the activities we do on a regular basis, they developed seemingly free platforms to entice us to share more information. Facebook does not exist because it believes in the ultimate good of allowing humans to connect with one another, it exists because there are billions of dollars to be made from capturing our social interactions, our likes, and our interests. We are happy to exchange vast quantities of data about our personal preferences for a curated news feed with content that Facebook knows will keep us coming back day after day. Facebook, Google (now under Alphabet), Amazon, and Apple are becoming more than companies that offer us services and devices. By creating ecosystems that control ever more parts of our lives, they want to become decision-makers for the majority of our daily choices while gathering vast amounts of information in the process. Greenfield refers to these ecosystems as “Stacks,” or institutions that want to make themselves into the sole providers of services/products in a domain. The most efficient way to accomplish these aims is through the aggregation of user data which allows the stacks to provide you with better personal recommendations and sell that information to other companies. When we log into our social networks, we see precisely the kinds of things that Facebook has identified interest us based on our past tendencies. From too many personal experiences, I know the incredible effectiveness of Amazon product offerings, and if you are still unwise enough to use the web without an ad-blocker, you might have noticed ads for a product you researched on Amazon minutes before. In effect, both of these companies have become information filters. We give them access to our history, and they sort through the mounds of products and news stories and show us only what they have learned we want to see. To some degree this is a necessity; the amount of information in the modern age is overwhelming, and without a means to sift through it, we can waste time on irrelevant content. Yet, what are we missing in the news stories we never see or the search results that Google does not show us? The extreme polarization in modern politics is at least partly because the pre-filtering of content has made it possible for us to read only the information that supports our views. If you rely on Facebook as your sole source of news, you can go the rest of your life without seeing a contrary opinion. It might be nice to wake up on a Sunday morning and read stories that confirm your pre-existing beliefs, but this results in an inaccurate conception of human life that misses all of our nuances and conflicting opinions. Furthermore, society progresses not when individuals gather in echo chambers, but when all sides of an argument are allowed to intermingle and to discuss the issue. The only place these debates might occur for most of us is online, but they have no chance to occur when we artificially limit the diversity of information to which we are exposed. In effect, we have created is a system in which we are censored by our own likes and search history, which has real-world consequences for the political functioning of our country. Not only do the stacks determine the news we read and the search results we see, but over the past few years we have gradually off-loaded more and more decisions to their algorithms (an algorithm is a system that takes in some information and executes a set of instructions based on that information to produce an output). These algorithms have become astonishingly accurate thanks to the maturation of machine learning. The capability of machine learning algorithms is directly proportional to the amount of data they have access to because these models are “trained” on past data. For example, the Amazon product recommendation software is a machine learning algorithm that has learned from the shopping history of millions of users for almost two decades. Anytime you are logged in to your Amazon account, the algorithm compares your shopping and search history to all other users and recommends the products you are most likely to be interested in based on what other users with similar profiles have purchased. As you provide more information to the system, it gets better at predicting what you want. According to a recent study, with just 150 likes, Facebook knows your preferences better than close family members, and, with 300 likes, Facebook can predict your choices better than your spouse. This incredible ability to forecast our future behavior means we no longer need to make conscious consumer decisions. The music we listen to, the products we buy, the movies we watch, the route we take to work, the restaurants where we eat, and even the people we date are now routinely selected not by sentient humans, but by machine learning systems that might know us better than we know ourselves. What we gain in convenience we lose in autonomy, or the ability to make our own choices. Moreover, for now, these algorithms are still developed by humans and they reflect the biases of their creators. In 2017, those developers were overwhelmingly white, male, 20-somethings, living in a particular region of California. With each passing year, we pass more and more decisions to this small group of individuals employed by the Stacks. Engineering Our Downfall In addition to recommending us the perfect restaurant to try or the optimal route to take home, machine learning algorithms are rapidly developing the ability to perform tasks previously accomplished only by humans. Nearly every job held by humans today is based on patterns and algorithms: we encounter a situation, respond to it via a set of learned rules, and produce useful output as a result. Even the job of a doctor is an algorithm: they ask a patient a series of questions, check the patient’s medical history and symptoms, and then generate a treatment plan based on a set of established guidelines. There are nuances between the interpretation of a patient’s condition, but the overall process is always the same. Creative endeavors such as writing and music composition come down to recognizing and recreating the patterns humans find most enjoyable. Machine learning algorithms excel at pattern recognition and hence have conquered many human domains. Machine learning systems can now diagnose MRI scans more accurately than doctors, navigate city streets better than human drivers, and compose music that is indistinguishable from that of the best biological composer. Past threats of machines taking human jobs went unfulfilled because humans found a way to stay a step ahead. When agriculture was mechanized, we taught ourselves to operate machinery in factories. When robots that never needed to rest or lost concentration pushed us out of industry, we went to college and contributed to the service economy. However, the new machine intelligence leaves no room for humans to outsmart or outwork mechanical systems. A recent study concludes that 47% of jobs in America are vulnerable to automation within the next ten years. Humans could be at real risk for becoming economically irrelevant and current government systems are too slow to deal with this potential future. The only institutions with the ability to rapidly adapt are the stacks themselves, and they are the ones who most stand to gain from the adoption of machine intelligence. Even though we may literally be engineering our own irrelevance, it is inconceivable that research into machine intelligence will cease or even slow. Humans have an insatiable desire for progress, even when that progress comes with a demonstrable risk. Researchers are working to develop artificial intelligence, which can be thought of as conscious machine learning algorithms with their own motivations and values. An artificial intelligence that could create its successor would likely be the last invention humanity would ever need to make. Even without any malicious intent, AI could cause the end of humanity. In our quest to transform the world to our liking, we have caused the extinction of numerous species not out of harmful intent, but because we think that our objectives are more important than the survival of a lesser species. Who is to say that an AI would not feel the same way towards us? Although achieving AI could result in the eclipse of humanity, researchers will never give up the dream because of the difficulty of the challenge and the human nature to continue forging ahead into the unknown. This drive has resulted in all of the wonders we enjoy today and the current golden age of civilization, but it also poses an existential risk. While this may seem like science fiction and AI has been “two decades away” for 50 years, it is important that we discuss the implications of our increasing trust in algorithms and recognize that every decision we give up brings us a step closer to a world with no need for humans. Political discourse is rightly dominated by everyday concerns, but it also needs to encompass long-term planning that accounts for current scientific research. Rather than a reactive government, we need a rule-making body that can anticipate future needs and draft legislation in anticipation for the world of decades from now. Technologies of the Future Radical Technologies is not a pessimistic work. As an advocate for human-centered technology design, Adam Greenfield is best described as a techno-skeptic, and his book is a balanced assessment of new advances compared to the unbridled optimism emanating from Silicon Valley. Therefore, when Greenfield places his approval behind a new technology, his words carry more weight than an author who praises every new phone model. Foremost among the technologies that Greenfield believes will have a positive effect on society in the coming decades are 3D printing and cryptocurrency. News coverage of both of these subjects tends towards extreme praise (“3D printing will change the world by allowing everyone to be their own manufacturer”) or gloom (“cryptocurrency will enable criminals to conduct transactions with no traces”) which underscores the mundane but steady progress in both fields. 3D-printing, also known as additive manufacturing and embodied by companies such as MakerBot, could distribute ownership of the means of production, but it has so far failed because of high system costs and the availability of cheap products produced overseas. Why buy your own additive manufacturing system and filament to make a chair when you can simply go on Amazon and have any one of thousands delivered to your house in 2 days for under $100? The real people who could benefit from 3D-printing are exactly those who currently lack access to the technology, such as people living in developing countries where the global supply chain extracts from rather than benefits the poor. Additive manufacturing could create a sustainable circular economy, where once a product has reached the end of its useful life, it is simply melted down into material used to create another product. However, as long as the technology remains solely in the hands of mostly upper-class western individuals, it will not profoundly change the world. 3D-printing will be most useful not where it is an expensive hobby for tinkerers, but where it can create necessities of daily life by harvesting waste streams and open-source designs. Cryptocurrency, the most notable example of which is Bitcoin, is another development with great unrealized promise. Greenfield points out that both 3D-printing and cryptocurrency are brilliant concepts that have failed at the interface between the technology and the real world. Digital currencies are designed to enable greater freedom of transactions because they do not have to go through a central clearinghouse. Bitcoin records information on the Blockchain, a global record of all transactions conducted in the currency that is available for download by anyone with an Internet connection. The verification and recording of a transaction is not conducted by a single entity, but by every computer on the network. This means that theoretically, Bitcoin is both less susceptible to price fluctuations than traditional currencies because it is not pegged to a physical entity, and it is not as vulnerable to fraud because faking a transaction would essentially require fooling every single computer verifying the transaction. In the real world, Bitcoin has failed because it does not fulfill the basic function of a currency: it cannot be easily spent. Anyone willing to purchase and spend Bitcoin must first find a broker who exchanges American dollars for Bitcoin, obtain a wallet that can store the currency, and then find a vendor willing to accept the currency in exchange for a product. In an age with one-click checkout, this will never be a feasible option for the majority of consumers who value convenience over privacy. Bitcoin will ultimately be unsuccessful because of difficulty of use (and technical details such as the energy cost ) but the idea of a Blockchain will live on in other currencies and distributed autonomous organizations. When trust in government is eroding (whether or not that is justified), the idea of distributing responsibility to all users of a system could be invaluable to operations such as organizing protests or creating a smarter energy grid. Recommendation The future is shaped not by those who merely know a technology works, but by committed groups of individuals who understand the societal implications of new advances. Knowing how technology functions is important, but knowing how it is used is more critical. Radical Technologies examines both the fundamental building blocks of current and near-future developments and the potential ways in which they may impact society. This book is a worthwhile read in its entirety because it forces us to consider the concessions we make when we allow ever more devices and products to control our daily lives. Once we are aware of the extent to which our existence is dictated by algorithms, it is not difficult to find ways to gain back a little control. You can start by disabling notifications on your phone or choosing only select times to receive alerts to reclaim your time and attention. Next, stop self-censoring yourself and read a different news source or follow entities you might not agree with in order to at least understand their argument. Finally, be skeptical when making any consumer decision. Ask yourself: “Is this really what I want, or is this only what Google/Facebook/Amazon/Apple want me to believe that I want?” Often, the benefits of a new technology more than outweigh the trade-offs, and I would claim that the modern connected world is a much better and enriching place to live because of the computer revolution. Nevertheless, the question that we must ask with regards to any advance is how can it be incorporated into our lives without concentrating power in the hands of those select few who understand the technology? Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",The Technology Frontier,3,published,58,3341,0.0020951810835079317,1,0,0,0,0,0
125,89,653.0176748366321,17,https://medium.com/p/data-analysis-with-python-19434f5d6324,3,None,2017-08-12 15:17:00,11.83,29,666,2017-08-12 12:37:00,"['Python', 'Data Analysis', 'Data Visualization', 'Baseball', 'Udacity']","Data Analysis with Python A Brief Exploration of Baseball Statistics Author’s Note: The following exploratory data analysis project was completed as part of the Udacity Data Analyst Nanodegree that I finished in May 2017. All of the code can be found on my GitHub repository for the class. I highly recommend the course to anyone interested in data analysis (that is anyone who wants to make sense of the mass amounts of data generated in our modern world) as well as to those who want to learn basic programming skills in an application-based format. Introduction Since the publication of Michael Lewis’s Moneyball in 2003, there has been an explosion in interest in the field of sabermetrics, the application of empirical methods to baseball statistics. Teams looking for an edge have increasingly turned to analysis of all manner of player statistics, from the easy to understand home runs, to the exceedingly complex, such as weighted runs created and fielding independent pitching. The main goal of these efforts have been to identify players with high performance potential who may have flown under the radar and thus will not command as astronomical of a salary as more well-known names. For this analysis, I performed my own introductory sabermetrical excursion into the world of baseball statistics, although I stuck to more familiar baseball metrics for hitting and pitching such as Runs Batted In (RBI) and Earned Run Average (ERA). In particular, I was interested in the relationship or lack there of between various performance metrics for batting and pitching and player salaries. I wanted to determine which batting and pitching stats were most strongly correlated with salaries and why that might be so. However, I wanted to go further and examine a player’s change in performance metrics over seasons and how that may be related to the salary he earned. Therefore, the approach I took was to examine a single season of salaries, from 2008, and look at the hitting and pitching data from not only that year, but from the preceding two seasons (2006, 2007) and the following two seasons (2009, 2010). I had several questions about the data that I would seek to answer: 1. Which batting statistic, hits, home runs, or runs batted in, had the highest correlation with player salary?2. Which pitching statistic, earned run average, wins, or strikeouts, had the highest correlation with pitcher salary?3. Are these correlations higher in the two seasons preceding the salary year, or in the two seasons following the salary year?4. What can these correlations tell us about relative player value? I formed a hypothesis about each question based on my rudimentary baseball knowledge and I was well-prepared to accept conclusions that went against my pre-conceived notions.My four hypotheses were as follows: 1. Home runs will be the most highly correlated statistic with batter salary. I thought that since home runs were a “flashier” statistic, the public at large, and more importantly, the owners of the teams that pay the salaries, would reward batters who displayed a greater tendency to go long. 2. Similar to my reasoning for 1, I thought that wins would most highly correlate with pitcher salary as they are easily understood and it seems “fair” to reward pitchers who produce more wins for their team regardless of how representative a measure of a pitcher’s effectiveness wins may be. 3. The correlations between performance metrics and salaries will be higher in the preceding two seasons than in the following two seasons. Players who had two good years would be rewarded with a high salary in 2008, but then they would regress to the mean in the following two seasons. A player with two outstanding seasons may seem destined to have a streak of stellar years, but like many other aspects of human performance, baseball is inherently random, which means that outliers will tend to drift back towards the center over time. 4. Based on 3, teams should not look to sign players who are coming off multiple good years, but should instead try to discover players on the cusp of a breakout season. On the other hand, players who commanded a large salary in previous years should not be worth as much in subsequent seasons and teams would be wise to avoid them. The data I used for my analysis is from http://www.seanlahman.com/baseball-archive/statistics/ and the description of the various stats contained are at http://seanlahman.com/files/database/readme58.txt. I used the 2016 version of the batting, pitching, and salary datasets downloaded as comma separated value (csv) files. Any sources I consulted during the creation of this report will be linked to in the text. Data Wrangling I first needed to get a feel for the data I was going to be analyzing. I took a look through the raw data and made a few preliminary observations, such as the lack of data for the earliest years in the sets. This informed my decision to concentrate on salaries from 2008, and statistics from the period 2006–2010. Out of curiousity, I decided to graph the average salary of all Major League Baseball (MLB) players as a percentage change over time (from 1985) and then compared that to the median household income percentage change in the United States using data from Quandl. While both the median United States household income and MLB player salary have increased over time, MLB salaries have far outpaced the growth of all US salaries. MLB salaries have grown by about 800% in only 40 years! That is an annual inflation rate of 5.3% compared to only 0.84% for the general public. Clearly, MLB players must be working wonders to deserve such lucrative contracts. As I was concerned with 2008 salaries, I thought it would be helpful to get a sense of the scale of what players were paid in that season. A histogram was the best way to demonstrate that the majority of players are clustered around the same pay, with a positively skewed distribution demonstrating several significant outliers. Make sure to note that the x-axis is in millions of US dollars. I see that the indexes of the DataFrames are the row numbers from the csv file. That does not really bother me at this point because I am not using the index values for any analysis In my initial examination of the raw data, I observed that some player IDs had multiple entires in the same year. Looking at the source data more closely, I saw that was because these players had multiple stints recorded in the same season. That is, they started the season with one team, were traded to another team, and had played a full season across two or more different teams. I wanted to include these players in my analysis, so I grouped the data by player and year, and summed up the statistics for players who had multiple entries in the same year to compress their statistics into a single figure for the season. This did not affect my analysis as I was not concerned with the teams players were on when they compiled their figures. It looks like I have a decent start, but there are still several issues with the data. One is that I only want to analyze players who were able to play a full season in each of the years under consideration. This was in order to exclude players who may have been injured but still received a salary and would thus affect the average of the performance metrics (a player with 0 hits but a $10 million salary would have quite the effect on the data!). After doing some research on http://www.fangraphs.com/, I decided that a decent metric to quantify a full season would be 100 games per season for the batters and 120 innings pitched per season for the pitchers. I decided to limit the analysis to starting pitchers and not relievers or closers and this bar for pitchers would take care of that issue as well. Another primary issue with the data was that at this point it contained players who had even a single record for any of the five years [2006–2010]. I only wanted players who had records in every year in the range in order to track the same players across multiple seasons. I decided to write a function that could take in the batting and pitching DataFrames, and return DataFrames with only players who played in all five years. At this point I want to be sure that my DataFrames contain exactly what I want. The next few cells are checks to examine the DataFrames and make sure that the code has modified the data as intended. Given that the mean number of appearances of each player ID in the pitching and batting DataFrames was 5.0, I am confident my DataFrames correctly represent the data. They include only players with records in all years as well as only the players who meet the games played or innings pitched cut-offs for batters and pitchers respectively. I now want to include the salaries in the DataFrame. This will put in the same 2008 salary for all five years, which is fine at this point as the averaging will simply return the 2008 salary. I now want to create three separate DataFrames in order to make comparisons between performance and salary over time: The next cell provides me with the number of batters and pitchers I am dealing with who meet the criteria I specified. As a final step, I can drop the yearID from all the DataFrames. I want to keep the playerID because I will use it to compare player’s statistics across seasons. I want to look at a couple of my DataFrames at this point to make sure they are correct. I finally have the data wrangled in the format I want! It’s time to start the analysis. The driving motivation behind the analysis will be the four questions I posed in the introduction. As a quick reminder, I will be looking at which statistics correlatate most highly with salary for both batters and pitchers, and then I will look at how these correlations change over time. I will look at how the performance metrics and salary correlations differ between the entire five-year range, the two seasons preceding the salary year, and the two seasons following the salary year. Performance Metric and Salary Analysis The batters will lead-off the analysis (pun totally intended) and I will look at which stats from the entire five-year average are most highly correlated with the salary in 2008. Batting Analysis The correlation number returned is the Pearson correlation coefficient, a measure of how linearly dependent two variables are to one another. The coefficient value is between -1 and +1, and two variables that are perfectly correlated with have a value of +1. It is premature to draw absolute conclusions using only the correlation coefficient as what constitutes a strong vs weak correlation depends heavily on the field of study, the instruments used to record the measurements, and the variables themselves. However, I am using the correlation coefficient in relative terms in this analysis to determine whether one correlation is stronger than another. Based on this measure, for the entire five-year interval, runs batted in (RBI) has the highest correlation with salary followed by home runs and then hits. That is counter to my initial hypothesis that home runs would have the strongest correlation with salary. All three of the performance metrics were positively correlated with salary, indicating that players who perform better over the long term do in fact tend to be more highly compensated. The next step for this analysis was to determine whether these correlations would be stronger for the preceding seasons or for the following seasons. The correlation between the average performance metrics from the previous two years and salary is greater than that between the average performance metrics over the entire five years and salary. This suggests that players are indeed rewarded for above average performances in preceding seasons. That is, players who perform well for several years will then be more likely to command a high salary in the subsequent year. Next, we can look at the two seasons following the salary to see if the correlations remain. My guess is the correlation between the performance metrics and the salary will not be as strong in the following years because the players will not be able to maintain their high level of play that earned them the larger salary in the first place. As predicted, the correlations have all decreased. A player’s salary is not as linearly depedent on performance metrics in the years following the salary as it was on the performance metrics from the years preceding the salary. In summary, here is a chart showing how the correlations between performance metrics and salary change depending on the time span analyzed: As mentioned in the introduction, I think what is being demonstrated here is primarily an example of regression to the mean. Players perform well for two seasons, are awarded a larger contract, and then fail to live up to their earlier numbers in the following seasons. This is intriguing because it means that players that are awarded larger contracts based on their past performance should not be worth as much in the following seasons. Players earn a large contract because of several above-average years of performance, but then their performace will tend to revert to a more average level of play just as any outlier in a random process will gradually drift back towards the average over a long enough observation span. I’ll perform a similar analysis of the pitching metrics before I return to the batting stats to conduct a statistical test to test my hypothesis that players with larger salaries will see a decrease in performance from before to after the salary season. Pitching Analysis Right away, we can see that the correlation between ERA and salary is slightly negative as expected because a lower Earned Runs Average is better for a pitcher. We can also see that the strongest correlation is between wins and salary. This is what I had predicted because wins are easy for everyone (especially those paying the players) to understand and it just feels “right” to award a pitcher a higher salary if they generate more wins regardless of how good an indicator of a pitchers ability wins actually are. I must offer a word of caution regarding this dataset though. The sample size of 32 is relatively small compared to the 83 samples in the batting dataset. I will go through the same process as I did with the batters and see if the salary correlations are greater in the two seasons preceding the salary year or the following two seasons. Regression to the mean strikes again (pun once more totally intended)! As was demonstrated in the batting analysis, the two years preceding the salary had a stronger correlation with the salary than the two years following. In fact, wins, which were the most highly correlated metric over the entire five year average, had a negative correlation with 2008 salary in the following two seasons. That means that the more a pitcher had been paid in 2008, the fewer wins he had over the next two seasons! Granted, this dataset is small, and pitching statistics can be heavily influenced by fielding and the performance of the rest of the team (hence the reason for statistics such as fielding independent pitching used in advanced sabermetrics).Nonetheless, that is still an intriguing result. We see the need for identifying players before they hit their prime seasons so teams can extract more performance from players at a lower price (this is capitalism after all!). Once a player has reached their peak seasons, they will command a higher salary, but then their performance will tend to slip back towards the average and they should not be as highly “valued.” Here is a summary of the correlations between pitching performance metrics and salary: t-Test A more rigorous statistical analysis would help to prove whether regression to the mean is at play when examining the relationship between performance metrics and salary over time. In particular, I wanted to see if there existed a statistically significant difference in changes in performance between players with 2008 salaries above the mean, and players with 2008 salaries below the mean. I decided to focus on the batting performance metric of Runs Batted In because that had had the strongest correlation with salary. I knew that I would need to perform an independent samples t-Test as I wanted to compare the means of two sets of unrelated data. The mean that I wanted to compare between the two groups was the change in RBIs from the seasons preceding the salary to the years following the salary. I defined the change in RBIs as the following two seasons average RBIs minus the average RBIs from the previous two seasons. ΔRBI=Average_RBI_Following−Average_RBI_preceding Therefore, if this was a positive value, that would indicate the player had performed better in the seasons following the salary year, and a negative value would indicate the player had performed worse. If regression to the mean was at work, then the players with salaries above the mean in 2008 would see a smaller ΔRBI. If there was no regression to the mean, then there would be no significant difference in ΔRBI between the players with above average salaries and players with below average salaries. This means that the test would be a one-tailed t-Test as I was testing to see if the means were different in a single direction. The null hypothesis can be stated: players with above average salaries in 2008 will have an average ΔRBI greater than or equal to the average ΔRBI demonstrated by the players with below average salaries in 2008. In mathematical terms this is Ho:μa−μb>=0 where μa is the mean ΔRBI for players with above average salaries and μb is the mean ΔRBI for players with below average salaries. The alternative hypothesis is: players with above average salaries in 2008 will have an average ΔRBI less than the average ΔRBI of players with below average 2008 salaries. In mathematical terms this is Ho:nμa−μb<0 The steps to perform the t-Test were as follows: I first needed to create a DataFrame of batters that contained playerIDs, ΔRBI, and standardized salaries. Standardizing the salaries would allow me to see if a player had a salary above or below the mean. The DataFrame looks like what I want. I will now separate it into two DataFrames, one containing players with 2008 salaries above the mean, and one containing 2008 salaries below the mean. Visualize the data with a few plots. As can be seen in the histograms, both charts approximate a normal distribution with the ΔRBI for the players with 2008 salaries above the mean tending to be skewed more negative. Again, a negative ΔRBI indicated the player performed worse in the two seasons following the salary year as compared to the two seasons preceding the salary year. The one-tailed t-critical statistic for α=0.05 with 81 degrees of freedom is -1.664 from the t-score table. The value is negative as the null hypothesis is stated with respect to change in means in the negative direction. At this point, it can be seen that both samples of players demonstrated a decrease in RBIs between the preceding and following seasons. However, no conclusion about the statistical significance can yet be drawn. The t-statistic of -3.222 from the t-test is less than the t-critical of -1.664 for α=0.05. Therefore, I can reject the null hypothesis and conclude that the players with above average salaries in 2008 experienced a statistically significant decrease in performance relative to players with below average salaries in 2008 from the years preceding the salary to the years following the salary. This shows that regression to the mean was demonstrated as players with greater performance in the two preceding seasons were awarded higher salaries in 2008, but then saw their performance decline in the following two seasons more than the players who earned salaries below the average in 2008. In more formal terms, the t-test can be summarized as follows: An independent samples t-Test was conducted to compare the performance changes across seasons of Major League Baseball (MLB) players with above average salaries to those with below average salaries in 2008. There was a statistically significant difference in the change in Runs Batted In (RBIs) for the players with above average salaries (M=-14.597, SD=13.793) as compared to the players with below average salaries (M=-3.885, SD=15.990); t=-3.222, p<.05. These results suggest that players with higher salaries will see a larger decrease in their performance from before the salary year to after the salary year than players with lower salaries. Regression to the mean appears to be at work in MLB, and outliers such as exceptional batting performance as measured in RBIs will tend to return to the mean value given enough time. Machine Learning As a final part to this analysis, I wanted to perform some basic machine learning on the dataset to see if I could create a model that would be able to predict whether a player would have an above average salary in a year based on the previous two seasons of performance metrics. I decided to stick with salaries from only 2008, but in order to expand my dataset, I would include all players who had recorded complete seasons (defined as playing more than 100 games) in 2006 and 2007 and who had salary date in 2008. I know that machine learning improves as the amount of data fed into the training algorithm improves, so I wanted to get more samples. I would only be using batters, and I would use the three performance metrics of RBIs, home runs, and hits. I would use the sci-kit learn library built-in algorithms for my classifier. At this point, I have a DataFrame that contains the stats of players who recorded complete seasons in 2006 and 2007 and their salary for 2008. In order to perform machine learning, I needed features or inputs, and a desired label, or output. My three features I selected are RBIs, hits, and home runs averaged across the 2006–2007 seasons. The label will be whether or not the player with the statistics was paid an above average salary in 2008. I once again need to standardized the salaries and then convert them into a 1 or a 0. 1 will be the label given to above average salaries and 0 will be assigned to below average salaries. Great! It looks like my DataFrame is ready. I have 166 entries, which while not many by machine learning standards, is double the number I had for my full batting analysis. Next, I need to convert to NumPy arrays that can be fed into a classifier. I choose the Gaussian Naive Bayers algorithm for my classifier based on a flowchart from sci-kit learn. Based on this, I can see that the classifier is able to correctly predict with a better than chance accuracy whether a player will have an above average salary in 2008 based on the number of RBIs, home runs, and hits from the previous two seasons! I will train and test the classifier 100 times to get a better average prediction rate. This prediction rate is certainly not stellar, but it is better than chance. The classifier would benefit from more data, and maybe more features as well. As we have already seen, none of the three performance metrics analyzed in this report were very strongly correlated with salary, and there may be others that are a better predictor of salary. Further work could be done to determine the features (performance characteristics) that could indicate with a greater accuracy whether or not a player will command a higher than average salary. This was a minor foray into machine learning, but it demonstrated that even with a limited amount of data, a machine learning approach can make predictions with an accuracy higher than chance guessing. Conclusion There are several conclusions that can be drawn from this analysis but there are also numerous caveats that must be mentioned that prevent these conclusions from being accepted as fact. I also must state the ever-present rule that correlation does not imply causation. In other words, players statistics may be correlated with salary, but that does not mean the statistics caused a higher or a lower salary. The main conclusions are as follows: Besides the ever-important rule that correlation does not equal causation, there were several other limitations to the data analysis. The main caveats I was able to identify are as follows: In summary, this project demonstrated the entire process of investigating a dataset including: posing questions about a dataset, wrangling the data into a usable format, analyzing the data, and extracting applicable conclusions from the data. I was also able to apply a statistical test to my dataset as well as take a short excursion into the realm of machine learning to see if an algorithm was able to make predictions that model the real world. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator An average of statistics over the two seasons before the salary year [2006–2007] An average of statistics over the two seasons after the salary year [2009–2010] Determine t-critical for a one-tailed t-Test with degrees of freedom=group 1 samples+group 2 samples−2 Determine the sample mean and standard deviation of ΔRBI for players with 2008 salaries above the mean. Determine the sample mean and standard deviation of ΔRBI for players with 2008 salaries below the mean. Calculate the difference in means between the two samples. Calculate the standard error using both sample standard deviations normalized by their respective sample sizes. Calculate the t-statistic : t−statistic=Mean difference / Standard error Compare the t-statistic to the t-critical value and draw a conclusion regarding the null hypothesis. Report the results. When looking at pitchers from the range 2006–2010, the number of wins was the performance metric most highly correlated with salary in 2008. These correlations were stronger in the two seasons preceding 2008 than in the two seasons following 2008. A t-Test conducted on batters found that batters with an above average salary in 2008 exhibited larger declines in performance (as measured by number of RBIs) from the preceding seasons to the following seasons than players with a below average salary in 2008. This strongly suggests that players with an above average salary experience a regression to the mean in terms of performance. Based on the above point, teams should make an effort to discover players before they have a breakout season. That is basically what the Oakland Athletics management team, the subject of the book Moneyball, did in the early 2000s, and they were able to compete and beat teams with much greater player salaries. By finding players who were undervalued, they were able to achieve sustained success despite their meager payroll. Once a player has reached the level where they command a higher salary, they will tend to not perform as well as before their salary increased because their prior performance was an outlier and will gradually decline to a more average level. This phenomenon is not limited to baseball, but can be observed in all aspects of daily life as shown by numerous researchers. The age of players. As a player gets older and accumulates more years spent in the league, their salary tends to increase even if their performance does not. I did not control for the age of the players in the analysis. There is inherent randomness in baseball stats from season to season. Some years are better pitching years and some years are better for batters. Statistics can also vary widely between ballparks due to the geometry of the park or even the density of the air as is observed at the home of the Colorado Rockies. I did not control for the players home ballpark, team, or league in which the player accumulated their statistics. Post-season performances may factor more heavily into a player’s salary than during the regular season. A memorable performance in the World Series may do more to boost a player’s salary than several years of above average play during the regular season. A more thorough analysis would look at the relative impacts on salary of post-season performance compared to the regular season. Moreover, as mentioned before, I was looking at relatively basic statistics that have proven to not be the most effective measures of player performance. For example, a pitcher’s number of wins will depend heavily on factors such as the fielding of the team and the general ability of the other players on the team. This is why sabermetricians typically deal with advanced statistics derived from the more basic metrics. These measures attempt to account for all the different factors that might play into a single number such as hits. One of the more effective measures of perforamance is known as Wins Above Replacement (WAR) which assigns a number to a player representing how many wins they contributed to their team versus what an average player would have generated. WAR for batters takes into account at least 8 separate statistics! An average of statistics over the entire five seasons [2006–2010] Determine an appropriate αα level for the test. I choose α=0.05 When looking at batters from the range 2006–2010, the number of RBIs was the performance metric most highly correlated with salary in 2008. The size of the data. This is probably the most important to mention. After applying my various requirements to the datasets, I was left with 83 batters to analyze and only 32 pitchers. In order to account for this, I could analyze more salary years, or lower the requirements for a player to be in the dataset which might then have other additional effects on the analysis.",Data Analysis with Python,4,published,5630,5508,0.01615831517792302,0,0,0,0,0,1
131,24,652.9441028957523,17,https://medium.com/p/data-wrangling-with-python-and-sqlite-900d21bc5a53,1,None,2017-08-12 17:03:00,11.42,27,551,2017-08-12 15:47:00,"['Big Data', 'Data Wrangling', 'Python', 'Udacity', 'Sqlite']","Data Wrangling with Python and SQLite Cleaning the Cleveland Open Street Map Author’s Note: The following exploratory data analysis project was completed as part of the Udacity Data Analyst Nanodegree that I finished in May 2017. All of the code can be found on my GitHub repository for the class. I highly recommend the course to anyone interested in data analysis (that is anyone who wants to make sense of the mass amounts of data generated in our modern world) as well as to those who want to learn basic programming skills in an project-centered format. Introduction OpenStreetMap (OSM) is an open-source project attempting to created a free map of the entire world from volunteer-entered data. It is maintained by the OpenStreetMap foundation and is a colloborative effort with over 2 million contributors. OpenStreetMap data is freely available to download in many formats and presents an ideal opportunity to practice the art of data wrangling for several reasons: I decided to work with the metro area of Cleveland because it is where I currently attend university (at Case Western Reserve University), and I thought it would be intriguing to explore the city through the dataset after many hours spent experiencing the city on the ground. The data extract of Cleveland used for this project was downloaded from Mapzen Metro Extracts. Data Retrieval / Preliminary Exploration The first step was to download the map as an XML file. The original file clocked in at nearly 5.5 million lines, which was somewhat unwieldy for testing the first versions of auditing and cleaning scripts. Consequently, I decided to create smaller sample XML files for the first runs of the auditing scripts and for identifying the most common problems that would need to be cleaned. Using a provisional sampling script, I created a small file that contained one out of every 100 top-level element, and an intermediate size script that kept one out of 10 top-level elements. There are three core, top-level elements in OSM: Sample of OSM data as XML: The breakdown of top-level elements for the three file sizes is summarized in the following table: Data Auditing and Cleaning Approach Being in control of a large dataset can be both exciting and intimidating! There are numerous possiblities for exploration, but the sheer amount of information can be overwhelming. Before I officially began any data wrangling, I wanted to have a step-by-step plan in order to efficiently go through the process of auditing and cleaning the data. I decided on the following approach, adapted from the Udacity course on Data Wrangling: 3. Implement the data cleaning plan: run cleaning scripts and transfer the cleaned data to .csv files 4. Manually correct as necessary: import the data from .csv files to a SQL database and perform SQL queries on the data to identify any further inconsistencies that would necessitate returning to step 1. Data wrangling is an iterative procedure, and as such, I expected that I might need to work through these steps several times. Moreover, I knew that having a clear outline of the procedure to follow would save me untold hours of work and confusion. Data Quality There are five main aspects of data quality to consider when auditing a dataset: Auditing and Cleaning Elevation Looking at the official documentation for elevation data in OpenStreetMap, elevation in tags should be recorded in meters. Furthermore, from the United States Geological Survey list of elevation of major cities, I can see that the elevation of Cleveland ranges from 571' at the lowest point to 1050' at the highest point. Therefore, after converting to meters, all elevation data encoded in node tags should be between 174 m and 320 m. This would be a test for both uniformity, because all elevation data should use the same units, and accuracy as I was checking the elevations against a trusted source. The following code was used to find elevation points outside of the acceptable range and invalid elevations: The audit identified any elevations above or below those measures, as well as any elevations that could not be cast to an integer. I could have used regular expressions to identify problematic characters, but since I expected all the elevations to be integers, the simpler approach was to attempt to cast all the elevation data to integers. This initial effort yielded 2516 suspect elevations and 5 problem elevations in the entire dataset. I decided that assuming all the elevations above or below the accepted limits were false was a mistake. The USGS data was for the city of Cleveland itself, and my map included surrounding areas. Therefore, I took a closer look at the suspect elevations. The four highest values were [1188, 423, 419, 414] and the four lowest were [3, 3, 42, 321]. I decided that 423 m was a reasonable elevation for the surrounding area of Cleveland. However, clearly the 1188 was meant to be in feet, and the three lowest points were not realistic. I made the choice to eliminate the three lowest elevation tags and convert the largest value to meters. The elevations with bad characters were [‘318;313’, ‘304.5’, ‘298.4’, ‘679""’, ‘348;352;358’]. I decided to take the first elevation listed if there were multiple entries and convert the entry in feet to meters. Total, this meant I only had to correct 9 problem elevations out of 6500. That makes me confident that the accuracy and uniformity of the dataset was initially very high. The actual cleaning of the elevation data would take place when I converted the XML into CSV files. I created a mapping for correcting the elevations and implemented it in the following code: The result of running the code on the problem elevations is: Amenities The amenity tags associated with way elements was one facet of the data I could audit for validity, or conformation to a schema. The OpenStreetMap wiki has a page on the amenity key. This webpage shows all of the accepted amenities in OpenStreetMap and gives the purpose of the amenity tag: “To cover an assortment of community facilities including toilets, telephones, banks, pharmacies and schools.” I wanted to check if all the amenities tags in the Cleveland dataset were among the officially validated tags from OpenStreetMap. To do so, I downloaded the source of the webpage as XML and extracted all of the amentities from the table using the python XML module. I then compared the amentities in the Cleveland OpenStreetMap XML file to this set using the following code: I did not necessarily want to delete the amentities that could not be validated, but rather, I would reclassify them into the amenity categories already part of OpenStreetMap. The non-verified amentities and the number of times they occured in the dataset are presented below: As can be seen, none of the non-verified amenities occured more than once. Furthermore, several of the amenities were not in agreement with the purpose of the tag from the documentation and would be better placed under tags such as “leisure.” Therefore, I concluded that it would be best to reclassify the amenities that could not be verified according to the official dichotomy rather than modify the official list. I created a mapping based on the official description of the amenities and my judgement and applied it when converting the XML data to CSV files. Street Types Another variable I could could audit for validity and consistency was the street names associated with the node and way tag. From my initial exploratory examination of the data, I had noticed a wide variety in street name endings and abbreviations. Using a provisional street name auditing script, I compared the street endings to a standardized list and counted the number of times each non-standard type appeared using the following function: That produced the following dictionary listing the non-standard street names and the number of occurences: In order to standardize the street names, I created a mapping and applied it to the data during the conversion from XML to CSV files: The street mapping produces the following output: Postal Codes An additional audit I could perform for accuracy was on the postal codes (also known as ZIP codes in the United States for Zone Improvement Plan). My approach with the postal codes was to find a complete and trusted list of all the postal codes in the area I had downloaded, and then compare those to the postal codes in the OpenStreetMap file. The first step was to find all the postal codes. I initially searched for Cleveland zip codes and found the United States Postal Service data for postal codes in Cleveland. However, after finding that an alarmingly large percentage of the post codes in the XML file were not in this list, I realized that the area I had downloaded was a square and did not conform to the administrative boundaries of the city of Cleveland. My next approach was to examine the XML I had downloaded and find the latitude and longitude boudaries of the square. Then, using the FreeMapTools post code finding tool, I was able to draw the exact box on a map and extract all the zip codes within the specified box. This yielded me 276 zip codes. I then compared the zip codes within the XML data to this set and extracted those that did not match as well as any problem zip codes such as {‘44256:44321’} or ‘OH 44312’ which could not be converted to an integer. For the problem zip codes, if there were multiple codes separated by a colon, I took the first code as long as it was in the official list. For the zip codes outside of the geographic area, I decided to remove the tag entirely. The following code was used to perform the audit: After performing the audit, I had 41 postcodes outside of the boundary box and 242 zip codes that included multiple codes and/or a leading state abbreviation. The following code was used to perform the actual cleaning of the zip codes: Consistent Dates The last crucial issue I observed in my audit of the data was inconsistent date formatting. All the timestamps automatically made when a user makes an edit of the OpenStreetMap are recorded as: timestamp=”2009–05–21T03:59:40Z”. However, some of the dates under the secondary elements were in other formats such as “06/05/2006”. Dates in this form can be intrepreted ambiguously, and I wanted all dates to be recorded in a consistent format so they could be more procssed and clearly understood in any data analysis performed on the set. In the entire Cleveland XML source data, there were 16320 dates in the non-standard (except for in the United States) MM/DD/YEAR format. Below is the code for the audit: Looking through the documentation for OpenStreetMap concerning dates, I found the following statement: “There is no standing recommendation as to the date format to be used. However, the international standard ISO 8601 appears to be followed by 9 of the top 10 values for this tag. The ISO 8601 basic date format is YYYY-MM-DD.” Based on this information, I decided to adopt the ISO 8601 format for dates. The cleaning function to convert dates from MM/DD/YEAR to YEAR-MM-DD is below: SQL Database The first round of the cleaning procedure was executed when the Cleveland OSM data was converted from the original source XML file to eight different CSV files. Moreover, in the conversion process, the data was validated against a predefined schema.py file to ensure that both the structure of the csv files and the types of the data entered were as expected. This procedure was susccessfully completed after several tries and some reworking of the cleaning and converting scripts. However even though the data had been transferred, I expected that in the process of creating and exploring the SQL database, I would find more “dirty” data that I would need to correct. I was well aware that data wrangling is an iterative procedure, and consequently was ready to work through the steps of the process again as needed. My next step was to initialize a SQL (Standard Query Langauge) database and to create the tables within based on a pre-defined schema. I also created additional tables for the relation data using the following schema. The schema was designed to adhere to the components of database normalization and to allow for cross-referencing and joining of the data within the tables: I then imported the data from the eight CSV files into the eight corresponding SQL tables. I encountered a few issues along the way, mainly with the type of data in the CSV files not matching the type I had assigned to the columns of the table including user names that were completely integers. I manually resolved most of these issues and made a few alterations to the CSV files to ensure that all the data types matched what SQL expected. After successfully importing all the data into the tables, it was time to start the most rewarding phase of the project, investigating the data! I formulated a few questions to guide my exploration of the dataset: Answering these questions inenvitably led me down many different paths. I found the dataset very interesting, and was constantly making queries to answer new questions that arose. I used SQLite3 from the command line to create the tables and execute queries and I also ran some queries within the Jupyter Notebook. Exploring the SQL Database First, a few simple queries to find out how much data about Cleveland I had. The results are summarized below: Contributors to the Cleveland OpenStreetMap The breakdown of users who contributed to the map was the most intriguing topic to me. Again, I expected a samll handful of users (maybe bots) to dominate the edits. Sure enough, the very first entry has bot in the name! Looking through the users on OpenStreetMap, it does appear that “woodpeck-fixbot” is an automated account created by a longtime contributor to OSM. Examining some of the other top editors, it appears that at least some of them are automated as well. In order to delve further into the data, I wanted to look at the percentages of total edits contributed by the top few accounts. Looking at the statistics for contributing users shows a massive descrepancy among number of edits. The mean number of edits is 1529, a figure that is clearly skewed by the few super-users at the top. Meanwhile, the median number of edits is only 16. Next, I found the total number of unique users. There are a total of 1988723 edits across the dataset. This means that the top user, “woodpeck-fixbot,” alone contributed 29.62% of the map! Furthermore, the top 10 users contributed 62.56% of all edits. The remaining 1290 users made less than 40% of the edits. Let’s put that in graphical form. First write the query to find the top 10 contributors by number of edits: Graph the edits by top 10 contributors: Graph the top 10 users contributions as fraction of total contributions: To put the discrepancy into even starker terms, the number of users who made only a single edit can be found. Clearly, the Cleveland OpenStreetMap is dominated by a few users while the vast majority of editors make only a few contributions. Nonetheless, on the positive side, 1300 people have contributed to the map, which demonstrates the wide interest in contributing to open-source community driven projects. Leisure and Amentities What sorts of leisure activites might one find in the great city of Cleveland? Well, using the SQL database, that question could be quickly answered. I have to admit I was somewhat confused by the pitch identifier. However, a quick glance at the OpenStreetMap wiki for pitch reveals that the keyword pitch is for “an area designed for playing a particular sport, normally designated with appropriate markings. Examples include: tennis court, basketball court, ball park, riding arena.” That makes much more sense. Now, I can see what types of sports those pitches are used for: I would have expected basketball and American football to come out on top, but if each separate field or court is counted, then these numbers would be accurate. Next, I wanted to see the available amenities in Cleveland: Having lived in Cleveland for several years, I can personally tell you that even with all of that parking, it can be hard to find a parking place downtown! Just for fun, I wanted to find out the most popular types of cuisine in Cleveland: Finally, because Cleveland is a city known for its river (not a day goes by when someone does not mention to me that the Cuyahoga River is famous for catching on fire, although I am always quick to add not since the 1970s) I investigated the waterways. Those numbers seem a little high. I think what is occuring is that the same waterway is being tagged multiple times but in different places. I decided to group the waterways by name and then find how many times each waterway name appeared in the data: That looks a little more realistic. The famous Cuyahoga River does not even make it into the top 5 waterway mentions on the map. GNIS (Geographic Names Information System) and TIGER (Topologically Integrated Geographic Encoding and Referencing) Data Finally, I was interested to see where or even if users were sourcing their data from when were entering it onto the map. Looking through the CSV files, I had noticed that many tags were either GNIS or TIGER types. A quick search revealed that GNIS stands for the Geographic Names Information System, created by the USGS. It is a database that contains over 2 million cultural and physical features in the United States. Many of the node tags and way tags appeared to reference from this database. The TIGER tag refers to the Topologically Integrated Geographic Encoding and Referencing database from the US Census Bureau that includes features such as roads and rivers. These tags were applied to primarily the relations and also some of the ways. I wanted to see what percentage of each of the tags for nodes, ways, and relations were tied to either the GNIS or TIGER databases. These statistics appear to be reasonable. The nodes for the most part are single points, and as such, do not have to be tied to a particular physical feature. The ways on the other hand, are composed of numerous nodes and often describe a route such as a highway or a trail that would be more likely to be found in a database. The relations usually describe a building or maybe a certain type of restriction such as no-left turn. Those would be less likely to be documented in a geographic database. Of course, there could be other databases besides GNIS and TIGER that I did not investigate, but this shows that a substantial portion of the ways in the Cleveland OSM data are based on verified features. Additional Data Exploration What is Cleveland city planners’ favorite type of restriction? Average number of nodes in a way: Average number of members in a relation: Max number of tags assigned to one node: Additional Ideas for Analysis After observing the percentage of nodes, ways, and relations that have at least partially reference from a verified database, one potential idea to implement for the data would be to try and validate as many of the elements as is feasible. It should be possible to create a script that would read the XML file, and for each element, find the name tag of the feature and attempt to corroborate the information with an established database, whether that be GNIS, TIGER, or another option. For example, searching for “Doan Brook” in the GNIS database returns the following entry: This information could then be compared with the tags associated with “Doan Brook” in the OpenStreetMap and any incorrect user-entered information could be replaced with the validated data. This would be quite an undertaking for all of the elements in the original data, but it would vastly improve the quality of the data and reference it to a trusted source. The most intriguing aspect of the dataset to me was the skew in the percentage of edits made by users. I think it would be interesting to compare the quality of the data entered by bots compared to the quality of the data entered by users. Although bots should theoretically not make as many mistakes, they are programmed by humans, and an incorrectly designed bot could end up generating a large amount of “dirty” data with the creator never even realizing it. Humans users on the other hand, are slower but could potentially be more accurate and descriptive. Humans also have the advantage of being better able to judge the category a given feature should fall into and what tags should be associated with an element. Perhaps the best approach would be for bots to label the more straightforward features, such as nodes, and then have humans add in the details such as tags and descriptions that accurately model how the feature is used or experienced in the real world. These maps would not be possible to create in such detail without automated bots, but they occasionally need audits to ensure that the automated programs are not executing unintended actions. Moreover, humans are able to add nuanced data that even the most carefully programmed bot would never be able to figure out without having physically experienced the land being mapped. Conclusions Overall, I was surprised by the quality of the original dataset. There was far less invalid, inaccurate, or problematic data than I was expecting. To take a single example, I only had to correct 9 problem elevations out of nearly 6500 elevation tags in the data. OpenStreetMap seems to be an accurate model of the actual world, as shown by at least one study. The quality of any open-source project is directly dependent on the users, and it is clear that the OpenStreetMap users care deeply about ensuring that the map they have collectively created is complete and accurate. Similar to how a community of users were able to create an online encyclopedia, Wikipedia, that is nearly as accurate as professionally produced encyclopedias, the OpenStreetMap community has generated a remarkably accurate map of the world. Judging the completeness of the data is difficult without going through the city feature by feature and ensuring that it is accounted for in the OSM. However, I think that as a metro area, Cleveland would tend to be more complete than rural areas. I would expect that larger cities have more complete and accurate datasets because of the higher concentration of users to enter data. I would imagine that the real challenge for the OpenStreetMap community is creating accurate maps of rural communities that have decreased access to data and a wide user base willing to update a map. Perhaps this is why bots could be of greatest use. They could use GPS data or satellite pictures, perhaps coupled with machine learning, to identify different geographical and architectural features that could then be validated by a human user. This project was a great exercise in data auditing, cleaning, and analysis as well as the basics of creating and interacting with a SQL database. The next steps would be to actually implement the data cleaning by running the scripts on entire sections of the OSM data and then updating the map itself. OpenStreetMap is very consciously aware of checking and improving the quality of the mapand using automated tools, it should be possible to further improve on the relatively high quality of the Cleveland OpenStreetMap. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator The dataset is for any area is free to download in many formats including XML (eXtensible Markup Language) The data are relatable and human-understandable because they represent real places and features Ways are made up of ordered lists of nodes that describe a linear feature such as a trail, or an area such as a park. They contain a list of the nodes that comprise the way as well as tags for detailed information. Relations are comprised of an ordered list of members that can be either nodes or ways. They are used to represent logically or geographic relationships between features and contain a list of members as well as tags describing the element. Create a data cleaning plan based on the audit Identify the causes of any inconsistent/incorrect data Develop a set of corrective cleaning actions and test on a small sample of the XML data Accuracy: Does the data agree with reality or with a trusted external source? Completeness: Are all records present? Consistency: Is data in a field (down a column) or for an instance (across a row) in logical agreement? Uniformity: Are the same units used across a given field? What was the breakdown of edits by users? This was related, and I expected to see a small handful of users (again, most likely bots) that made the vast majority of additions to the data. (In the field of citizen science, it has been determined that 80% of the work is done by about 10% of the contributors. What are the most popular leisure attractions in Cleveland? What are the most popular amenities in Cleveland? From this data, I could further divide that into categories such as type of eatery or specific waterways. How many of the data points were sourced from a database such as TIGER or GNIS? The entire dataset is user-generated meaning there will be a significant quantity of “dirty” data Nodes represent a single point and have an id, latitude, and longitude. They may also contain descriptive tags that elaborate on the node if it is on an item of interest Audit the data: identify errors/missing or generally “dirty” data in the original XML file Validity: Does the data conform to a standard format? Who contributed the most edits to the dataset? Based on previous experience with user-created community projects, I believed that there would be several automated bots that tended to dominate the number of edits.",Data Wrangling with Python and SQLite,6,published,4826,4772,0.005029337803855826,0,0,0,0,0,1
114,5,650.069288716551,5,https://medium.com/p/controlling-your-location-in-google-chrome-6c0b216d1ba1,2,None,2017-08-15 14:02:00,28.57,6,2799,2017-08-14 20:43:00,"['Privacy', 'Security', 'Google', 'Internet']","Controlling your Location in Google Chrome A Simple Step to Regain (some) of Your Digital Autonomy For most of us, the story of our relationship with Google is one in which we have willingly ceded to Google ever more control of our digital lives. While this has resulted in great product recommendations and personalized search results, we have to wonder when it becomes too much. Fortunately, there are a number of steps you can take to stem the loss of digital autonomy. One of the simplest actions is learning how to take control of your location in Google Chrome. Manually Set Location Using Developer Tools Using Google Chrome’s developer tools, you can easily set your location to any latitude and longitude coordinates. To access the developer tools in the console, press Control+Shift+I (Command+Option+J on Mac) or right-click on any web page and select inspect. Next, click the three vertical dots in the upper right of the developer tools panel (when you hover over the dots you should see “Customize and Control DevTools”). Find the “More Tools” entry, and click to expand the options. Select the “Sensors” option as shown. This will display the sensors tab in the bottom half of the developer tools panel. The first category under sensors is geolocation which will be set to “No override.” Expand the options and choose either a custom latitude and longitude set of coordinates or a preset location to change the physical location reported by your browser. An easy way to find latitude and longitude coordinates is to open Google Maps and click on any location. The latitude and longitude of the center of your screen will appear in the address bar. Testing it Out Let’s see how this well this works. Say we are looking for housing options in a new city. If I do a simple Google search for housing, Google will filter my search results based on my location. Google gets your location from your IP address, location history, or recent search history, which can be great when you want location-relevant results, but frustrating if you want to see results for other locations. Luckily, now we can set our location to anywhere and compare the results. I’ll choose Shanghai and see what comes up. After setting my location to Shanghai, Google has decided to show me a map of Shanghai with potential locations of interest, but the websites listed are all for housing in New York. Perhaps this is because Google has figured out that people in Shanghai often look for housing in New York? I can’t claim to know how the search algorithm works, but it is clearly dependent on where Chrome thinks you are. This modification can be used on any website that asks for your location, such as weather sites. Using the developer tools to manually set location is a very limited method. You must set your location on any new tab you open, and this change will not persist after you close the developer tools. However, there are a number of better solutions. Disable Website Locations If you do not want any websites to be able to access or even ask for your location, open the Google Chrome settings by clicking the three vertical dots at the upper right of Chrome ( when you hover over them you should see “Customize and control Google Chrome” ). Click on settings which will open the Chrome settings in another tab. Scroll down to advanced and click to expand the advanced settings. Under Privacy and security click on “Content settings” to control the information websites can access. Find and click on the option called location. By default, Google Chrome will set this to “Ask before accessing” which is why you will often receive a pop-up box when arriving at a website asking if you want to allow or block the site from using your location. In order to not see this box again, click on location and then click on the blue slider which should change it to grey as shown below. If you see this screen, then congratulations, you have blocked all websites from accessing your location. However, Google can and will still know your location and can filter search results accordingly. From this location page, you can also manually designate the sites that have access to your location and those that are blocked. The websites that are allowed and blocked are those that you have either allowed or blocked respectively when asked by your browser in the past. If there is a website that you clicked “allow” but now no longer want to access your location, you can remove it from the allow list. Likewise you can do the same to websites you may have accidentally blocked. Virtual Private Networks For a more persistent method of altering your location, consider using a Virtual Private Network (VPN). These services route your computer’s traffic through servers that can be located in any country. Therefore, your IP address will be reported as that of the server, and you can typically choose the location of the server (if you pay for a decent VPN). VPNs are a necessity if you are doing any secure work on public networks and are a great way to access content that may be blocked in your particular geographic region (if for example you happen to live under an oppressive regime). As long as you are connected to the VPN, your traffic will appear to be coming from the server’s IP address and neither Google nor any website will be able to determine your true location. I recommend doing some research and paying a small yearly fee (about $5) for a legitimate VPN. They provide an extra layer of security and anonymity especially in public locations where most users typically do not take any precautions. Conclusion You now have a simple way to temporarily make Google or any website think you are somewhere else, or to obscure your location permanently if you choose to use a VPN. This is only a tiny step, but it is a step in the right direction of maintaining both your anonymity and security. Digital literacy is as important in the 21st century as print literacy was in the 20th, and this one action can be the start of regaining control of your digital experiences. If you would like to know more ways to keep yourself safe and/or anonymous online, I recommend Kevin Mitnick’s book The Art of Invisibility. Mitnick is a former criminal hacker who now works for the good guys and his book outlines practical advice for digital safety and privacy. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",Controlling your Location in Google Chrome,6,published,9797,1214,0.004118616144975288,0,0,0,0,0,0
115,171,644.941875752014,32,https://medium.com/p/machine-learning-with-python-on-the-enron-dataset-8d71015be26d,2,None,2017-08-20 17:06:00,15.28,31,2056,2017-08-20 15:18:00,"['Machine Learning', 'Python', 'Udacity', 'Data Analysis']","Machine Learning with Python on the Enron Dataset Investigating Fraud using Scikit-learn Author’s Note: The following machine learning project was completed as part of the Udacity Data Analyst Nanodegree that I finished in May 2017. All of the code can be found on my GitHub repository for the class. I highly recommend the course to anyone interested in data analysis (that is anyone who wants to make sense of the mass amounts of data generated in our modern world) as well as to those who want to learn basic programming skills in an project-focused format. Introduction Dataset Background The Enron email + financial dataset is a trove of information regarding the Enron Corporation, an energy, commodities, and services company that infamously went bankrupt in December 2001 as a result of fraudulent business practices. In the aftermath of the company’s collapse, the Federal Energy Regulatory Commission released more 1.6 million emails sent and received by Enron executives in the years from 2000–2002 (History of Enron). After numerous complaints regarding the sensitive nature of the emails, the FERC redacted a large portion of the emails, but about 0.5 million remain available to the public. The email + financial data contains the emails themselves, metadata about the emails such as number received by and sent from each individual, and financial information including salary and stock options. The Enron dataset has become a valuable training and testing ground for machine learning practicioners to try and develop models that can identify the persons of interests (POIs) from the features within the data. The persons of interest are the individuals who were eventually tried for fraud or criminal activity in the Enron investigation and include several top level executives. The objective of this project was to create a machine learning model that could separate out the POIs. I choose not to use the text contained within the emails as input for my classifier, but rather the metadata about the emails and the financial information. The ultimate objective of investigating the Enron dataset is to be able to predict cases of fraud or unsafe business practices far in advance, so those responsible can be punished, and those who are innocent are not harmed. Machine learning holds the promise of a world with no more Enrons, so let’s get started! The Enron email + financial dataset, along with several provisional functions used in this report, is available on Udacity’s Machine Learning Engineer GitHub. Outlier Investigation and Data Cleaning The first step is to load in the all the data and scrutinize it for any errors that need to be corrected and outliers that should be removed. The data is provided in the form of a Python dictionary with each individual as a key and the information about the individual as values, and I will convert it to a pandas dataframe for easier data manipulation. I can then view the information about the dataset to see if anything stands out right away. From the info about the dataset, I can see that all the fields are floating point numbers except for the poi identification which is True/False. There are 146 rows in the dataframe which most likely mean there are 146 individuals’ information. Some of the maximum values seem unreasonable such the total payments which is $309 million or the to messages at 15,000! Those could be valid values, but they look suspicious on a first pass through the information. Another observation is that there are numerous NaNs in both the email and financial fields. According to the official pdf documentation for the financial (payment and stock) data, values of NaN represent 0 and not unknown quantities. However, for the email data, NaNs are unknown information. Therefore, I will replace any financial data that is NaN with a 0 but will fill in the NaNs for the email data with the mean of the column grouped by person of interest. In other words, if a person has a NaN value for ‘to_messages’, and they are a person of interest, I will fill in that value with the mean value of ‘to_messages’ for a person of interest. If I chose to drop all NaNs, that would reduce the size of what is already a small dataset. As the quality of a machine learning model is proportional to the amount of data fed into it, I am hesitant to remove any information that could possibly be of use. One simple way to check for outliers/incorrect data is to add up all of the payment related columns for each person and check if that is equal to the total payment recorded for the individual. I can also do the same for stock payments. If the data was entered by hand, I would expect that there may be a few errors that I can correct by comparing to the official PDF. Sure enough, there are two individuals for which the sum of their payments does not add up to the recorded total payment. The errors appear to be caused by a misalignment of the columns; for Robert Belfer, the financial data has been shifted one column to the right, and for Sanjay Bhatnagar, the data has been shifted one column to the left. I can shift the columns to their correct positions and then check again to see if the individual payment and stock values add up to the respective totals. Correcting the shifted financial data eliminated two errors and there are not any more that are apparent from examination of the dataset. However, looking through the official financial PDF, I can see that I need to remove ‘TOTAL’ as it is currently the last row of the dataframe and it will throw off any predictions. (The total row is what was displaying the suspect maximum numbers in the summary of the dataframe.) Likewise, there is a row for ‘THE TRAVEL AGENCY IN THE PARK’, which according to the documentation, was a company co-owned by Enron’s former Chairman’s sister and is clearly not an individual that should be included in the dataset. I can now look for outlying data points recorded in the different fields. I will try to be conservative in terms of removing the outliers because the dataset is rather small for machine learning in the first place. Moreover, the outliers might actually be important as they could represent patterns in the data that would aid in the identification of persons of interest. The official definition of a mild outlier is either below the (first quartile minus 1.5 times the Interquartile Range (IQR)) or above the (third quartile plus 1.5 times the IQR): low outlier<first quartile−1.5 x IQR high outlier>third quartile+1.5 * IQR My approach will be to count the number of outlying features for each individual. I will then investigate the persons with the highest number of outliers to determine if they need to be removed. As this point, I need to do some research before blinding deleting outliers. Based on the small number of persons of interest initially in the dataset, I decided not to remove any individuals who are persons are interest regardless of their number of outliers. An outlier for a person of interest could be a sign of fradulent activity, such as evidence that someone is laundering illicit funds through the company payroll or is paying an accomplice to remain silent about illegal activity. I will manually examine several of the top outlying inviduals to see if I can glean any insights and to determine whom to remove. A few interesting observations about the outliers: Total, I decided to remove four people from the dataset. I believe these removals are justified primarily because none of these individuals were persons of interest and they all were upper-level executives with pay levels far above the average employee. I do not think these top executives who did not commit fraud are indicative of the majority of employees at Enron who also did nothing illegal (i.e. they were not persons of interest). There are a total of 2800 observations of financial and email data in the set now that the data cleaning has been finished. Of these, 1150 or 41% are 0 for financial (payment and stock) values. There are 18 persons of interest, comprising 12.9% of the individuals. Initial Algorithm Training and Performance Metrics The first training and testing I will do will be on all of the initial features in the dataset. This is in order to gauge the importance of the features and to serve a baseline to observe the performance before any feature selection or parameter tuning. The four algorithms I have selected for initial testing are Gaussian Naive Bayes (GaussianNB), DecisionTreeClassifier, Support Vector Classifier (SVC), and KMeans Clustering. I will run the algorithms with the default parameters except I will alter the kernel used in the Support Vector Machine to be linear and I will select number of clusters = 2 for KMeans as I know in advance that the targets are only two categories that should be classified. Although accuracy would appear to be the obvious choice for evaluating the quality of a classifier, accuracy can be a crude measure at times, and is not suited for some datasets including the Enron one. For example, if a classifier were to guess that all of the samples in the cleaned dataset were not persons of interest, it would have an accuracy of 87.1%. However, this clearly would not satisfy the objective of this investigation which is to create a classifier that can identify persons of interest. Therefore, different metrics are needed to evaluate the classifiers to to gauge performance. The two selected for use in this project are Precision and Recall. precision=true positives / (true positives+false positives) recall=true positives / (true positives+false negatives) Precision is also known as positive predictive value while recall is called the sensitivity of the classifier. A combined measured of precision and recall is the F1 score. Is it the harmonic mean of precision and recall. Mathematically, the F1 score is defined as: F1 Score=2 (precision x recall) / (precision+recall) For this project, the objective was a precision and a recall both greater than 0.3. However, I believe it is possible to do much better than that with the right feature selection and algorithm tuning. For the majority of my tuning and optimization using GridSearchCV, I will use the F1 score because it takes into account both the precision and recall. Scaling Initial Algorithm Training and Performance Metrics The first training and testing I will do will be on all of the initial features in the dataset. This is in order to gauge the importance of the features and to serve a baseline to observe the performance before any feature selection or parameter tuning. The four algorithms I have selected for initial testing are Gaussian Naive Bayes (GaussianNB), DecisionTreeClassifier, Support Vector Classifier (SVC), and KMeans Clustering. I will run the algorithms with the default parameters except I will alter the kernel used in the Support Vector Machine to be linear and I will select number of clusters = 2 for KMeans as I know in advance that the targets are only two categories that should be classified. Although accuracy would appear to be the obvious choice for evaluating the quality of a classifier, accuracy can be a crude measure at times, and is not suited for some datasets including the Enron one. For example, if a classifier were to guess that all of the samples in the cleaned dataset were not persons of interest, it would have an accuracy of 87.1%. However, this clearly would not satisfy the objective of this investigation which is to create a classifier that can identify persons of interest. Therefore, different metrics are needed to evaluate the classifiers to to gauge performance. The two selected for use in this project are Precision and Recall. precision=true positivestrue positives+false positivesprecision=true positivestrue positives+false positives recall=true positivestrue positives+false negativesrecall=true positivestrue positives+false negatives Precision is also known as positive predictive value while recall is called the sensitivity of the classifier. A combined measured of precision and recall is the F1 score. Is it the harmonic mean of precision and recall. Mathematically, the F1 score is defined as: F1 Score=2 (precision x recall)precision+recallF1 Score=2 (precision x recall)precision+recall For this project, the objective was a precision and a recall both greater than 0.3. However, I believe it is possible to do much better than that with the right feature selection and algorithm tuning. For the majority of my tuning and optimization using GridSearchCV, I will use the F1 score because it takes into account both the precision and recall. The only data preparation I will do for initial testing of the algorithms is to scale the data such that it has a zero mean and a unit variance. This process is called normalization and is accomplished using the scale function from the sklearn preprocessing module. Scaling of some form (whether that is MinMax scaling or normalization) is usually necessary because there are different units for the features in the dataset. Scaling creates non-dimensional features so that those features with larger units do not have an undue influence on the classifier as would be the case if the classifier uses some sort of distance measurement (such as Euclidean distance) as a similarity metric. Here is a good dicussion of feature scaling and normalization. The results from running the four classifiers on the entire original featureset with no parameter tuning are summarized in the table below From the first run through the four algorithms, I can see that the decision tree performed best, followed by the gaussian naive bayes, support vector machine, and Kmeans clustering. In fact, the decision tree and naive Bayes classifiers both perform well enough to meet the standards for the project. Nonetheless, there is much work that can be done to improve these metrics. A Quick Note on Validation The validation strategy used here is a form of cross-validation that is implemented in the provided tester.py script. Cross-validation performs multiple splits on the dataset and in each split, forms a different training and testing set. Each iteration, the classifier is fit on a training set and then tested on a testing set. The next iteration the classifier is again trained and tested, but on different sets and this process continues for the number of splits made of the dataset. Cross-validation prevents one from making the classic mistake of training an algoithm on the same data used to test the algorithm. If this happens, the test results may show that the classifier is accurate, but that is only because the algorithm has seen the testing data before. When the classifier is deployed on novel samples (implemented in the real world), the performance may be poor because it was trained and tuned for a very specific set of instances. The classifier will not be able to generalize to new cases becuase it is only fit and tuned to the specific samples it is tested on. Cross-validation solves this issue by training and testing on multiple different subsets of the features and labels and is ideal for use on small datasets to avoid overfitting. Throughout my analysis, I used cross-validation to assess the performance of my algorithms. The tester.py script uses the StratifiedShuffleSplit cross-validation method, and GridSearchCV, which is used to find the optimal number of features and the best parameters, employs cross validation with the StratifiedKFolds cross-validator. In both cases, the dataset is split 10 times into training and testing sets. Feature Engineering The next step is to create new features from the existing information that could possibly improve performance. I will also need to carry out feature selection to remove those features that are not useful for predicting a person of interest. After thinking about the background of the Enron case and the information to work with contained in the dataset, I decided on three new features to create from the email metadata. The first will be the ratio of emails to an individual from a person of interest to all emails addressed to that person, the second is the same but for messages to persons of interest, and the third will be the ratio of email receipts shared with a person of interest to all emails addressed to that individual. The rational behind these choices is that the absolute number of emails from or to a person of interest might not matter so much as the relative number considering the total emails an individual sends or receives. My instinct says that individuals who interact more with a person of interest (as indicated by emails) are themselves more likely to be a person of interest because the fraud was not perpertrated alone and required a net of persons. However, there are also some innocent persons who may have sent or received many emails from persons of interest simply in the course of their daily and perfectly above-the-table work. At this point I will also create new features using the financial data. I have a few theories that I formed from my initial data exploration and reading about the Enron case. I think that people recieving large bonuses may be more likely to be persons of interest becuase the bonuses could be a result of fraudulent activity. It would be easier to pass off illegal funds as a bonus rather than a salary raise which usually involves a contract and input from shareholders. The two new features will be the bonus in relation to the salary, and the bonus in relation to total payments. There are now a total of 25 features, some of which are most likely redudant or not of any value. I will perform feature reduction/selection to optimize the number of features so I am not worried about the initial large number of features. Moreover, the algorithms I am using are able to train relatively quickly even with the large number of features because the total number of data samples is small. After adding in the features, the results for all of the algorithms have improved and are summarized below: The F1 score for the decision tree is stil the highest followed by the Gaussian Naive Bayes Classifier. At this point, neither the SVC with the linear kernel nor the KMeans clustering pass the standards of 0.3 for precision and recall. I will drop the latter two algorithms and I will also drop the GaussianNB in favor of the AdaBoost Classifier because I want to experiment with tunable parameters and GaussianNB does not have any. AdaBoost takes a weak classifier, and trains it multiple times on a dataset, each run adjusting the weights of incorrectly classified instances to concentrate on the most difficult to classify samples. AdaBoost therefore works to iteratively improve an existing classifier and can be used in conjuction with a DecisionTree or GaussianNB. Feature Visualization In order to understand the features I have, I want to visualize at least some of the data. Visualizing the data can help with feature selection by revealing trends in the data. The following is a simple scatterplot of the email ratio features I created and the bonus ratios I created. For the email ratios, my intuition tells me that persons of interest would tend to have points higher in both ratios and therefore should tend to be located in the upper right of the plot. For the bonus ratios, I would expect similar behavior. In both plots, the non persons of interest are clustered to the bottom left, but there is not a clear trend among the persons of interest. I also noticed suspiciously that several of the bonus to total ratios are greater than one. I thought this might be an error in the dataset, but after looking at the official financial data document, I saw some individuals did indeed have larger bonuses than their total payments because they had negative values in other payment categories. There are no firm conclusions to draw from these graphs, but it does appear that the new features might be of some use in identifiying persons of interest as the POIs exhibit noticeable differences from the non POIs in both graphs. The plot below is a scatter matrix showing all the relationships between four selected features: ‘bonus’, ‘total_payments’, ‘to_poi_ratio’, and ‘from_poi_ratio’. The diagonals are histograms because a variable correlated with itself is simply one. The persons of interest are plotted in yellow and the non persons of interest are the purple points. Overall, there do seem to be a couple of trends. Looking at the bonus vs from_poi_ratio, the persons of interest tend to be further to the right and higher than non persons of interest. This indicates that persons of interest tend to recieve larger bonuses and they send more emails to other persons of interest. The scale on the total payments graph makes those graphs somewhat difficult to read but it can be seen that persons of interest tend to also have higher total payments. This trend can be seen in the to_poi_ratio vs total_payments scatter plot. Perhaps this demonstrates that persons of interest recieve more total payments due to fraudulent activity or because they tend to be employed at an upper level in the company and therefore command higher salaries. Overall, the visualizations do not offer many clear trends. However, it is still important to get a feel for the scale of the data and to check and see if there are any patterns evident that could inform the creation of new features or the selection of existing features. A great visualization of the Enron email and financial dataset is available as a navigable dashboard. Feature Selection There are several methods available for performing feature selection in machine learning. One is simply to look at the feature importances for a classifier and modify the list of features to exclude those with an importance below a chosen threshold. Another is to use SelectKBest and have the k-best features, defined by the amount of variance explained, automatically selected for use in the classifier. I will look at the feature importances for both the DecisionTree and the AdaBoost Classifier, but I would prefer to use SelectKBest to actually choose the features to keep. Additionally, I can use GridSearchCV in combination with SelectKBest to find the optimal number of features to use. This will run through a number of k values and choose the one that yields the highest value according to a designated performance metric. First, I will manually look at the feature importances for both classifiers to get a sense of which features are most important. One of the neat aspects about machine learning is that it can help humans to think smarter. By looking at what the algorithm chooses as the most important features to identify persons of interest, it can inform humans what they should be looking for in similar cases. (One example of this in the real world is when Google’s AlphaGo defeated Lee Sedol in Go, it advanced the entire state of Go by providing new insights and strategies into a game humans have played for millenia.) Find the feature importances for the AdaBoost Classifier It is interesting to compare the feature importances for the DecisionTree and the AdaBoost classifiers. The top 10 features are not in close agreement even though both classifiers achieve a respectable F1 Score greater than 0.5. However, rather than manually selecting the features to keep, I will use GridSearchCV with SelectKBest to find the optimal number of features for the classifiers. GridSearchCV runs through a parameter grid and tests all the different configurations provided to it. It returns the parameters that yield the maximum score. I will use a scoring parameter of F1 because that is what I would like to maximize, and a cross-validation with 10 splits to ensure that I am not overfitting the algorithm to the training data. According to the grid search performed with SelectKBest with the number of features ranging from 1 to 24 (the number of features minus one), the optimal number of features for the decision tree classifier is 19. I can look at the scores assigned to the top performing features using the scores attribute of SelectKBest. SelectKBest defaults to scoring parameters using the ANOVA F-value which is a measure of variation between sample means. It describes how much of the variance between labels is explained by a particular feature. A higher value therefore means that there is more variation in that feature between person of interests and non persons of interest. The following table summarizes the Decision Tree features and the ANOVA F-Value returned by SelectKBest with k = 19. These are the features I used in my final DecisionTreeClassifier. Running the DecisionTreeClassifier with SelectKMeans and k = 19 yields an F1 score of 0.700. I very pleased with that result and have decided on the 19 features I will use with the DecisionTreeClassifier. Any further improvement from this classifier will come in the parameter tuning section of the investigation. A similar procedure with GridSearchCV and SelectKBest will be carried out to determine the optimal number of features to use with the AdaBoostClassifier. The ideal number of parameters for SelectKMeans using GridSearch for the AdaBoostClassifier was 23. This resulted in a slightly lower F1 score of 0.689 and I will use the 23 highest scoring features with the AdaBoostClassifier. The F-Scores are the same so I will not show them all again. At this point I could also perform Principal Component Analysis to further reduce the number of features, but I think that the performance I am seeing does not necessitate the use of PCA, and the algorithms do not take very long to train even on a large number of features. PCA creates new features by selecting dimensions of the data with the greatest variation even in these features do not necessarily represent actual quantifiable values in the dataset. I prefer the idea that I know exactly all the features I am putting into the model if that is possible. This is one way that I try to combat the black box problem in machine learning. If I at least know what is going in to a model, then I can try to understand why the model returned a certain classification and it can inform my thinking to enable me to create smarter machine learning classifiers in the future. Hyperparameter Optimization Parameter tuning is the process of optimizing the “settings” of a machine learning algorithm to achieve maximum performance on a given dataset. A machine learning algorithm is simply a sequence of rules that is applied to a set of features by a computer in order to arrive at a classification. Parameter tuning can be thought of as iteratively altering these rules to produce better classifications. Sci-kit learn implements default parameters for each algorithm designed to get a classifier up and running by generalizing to as many datasets as possible with decent performance. However, it is possible to achieve greater performance by changing these algorithm parameters (although they are often used interchangeably, technically ‘hyperparameters’ are aspects of the estimator that must be set beforehand by the user while the model ‘parameters’ are learned from the dataset; see here for a discussion). The proces of parameter tuning for a dataset can either by carried out manually, by selecting different configurations, performing cross-validation, and selecting the settings that return the highest performance, or it can be automated by another algorithm such as GridSearchCV. A parameter grid is passed to GridSearchCV that consists of a number of combinations of parameters to test with an algorithm, and the search returns the parameters that maximize performance. The parameters available for an algorithm in sci-kit learn can be found in the classifier documentation. My process for algorithm tuning will be to examine the sci-kit learn documentation, construct a parameter grid with a wide range of configurations, and use GridSearchCV to find the optimal settings. The decision tree will be up first. Looking at the sci-kit learn documentation for the DecisionTreeClassifier, there are numerous parameters that can be changed, each of which alter the manner in which the algorithm makes decisions as it parses the features and creates the ‘tree’. For example, criterion can either be set as ‘gini’ or ‘entropy’ which determines how the algorithm measures the quality of a split of a node, or in other words, which branch of the tree to take to arrive at the correct classification. (‘gini’ utilizies the gini impurity while ‘entropy’ maximizes the [information gain] at each branching(https://en.wikipedia.org/wiki/Information_gain_ratio). I will put both options in my parameter grid and let grid search determine which is the best. The other three parameters I will tune are min_samples_split, max_depth, and max_features. GridSearch will be directed by cross-validation with 10 splits of the data, and the scoring criteria is specified as F1 because that is the primary measure of classifier performance I used to account for both precision and recall. The best parameters identified are shown above. Using GridSearch saved me from the tedious taask of running through and manually evaluating all of the combinations of hyperparameters. I will now implement the best parameters and assess the Decision Tree Classifier using the cross validation available in the tester.py function. According to the cross validation in tester.py, my F1 score is around 0.800 with the optimal parameters. I am satisfied with the recall and precision score of the DecisionTreeClassifier but I will try the AdaBoostClassifier using the same approach because I am curious to see if I can beat the F1 score. It’s time to look at the Sci-kit learn documentation for the AdaBoost Classifier to see the parameters available to tune. The AdaBoostClassifier boosts another ‘base’ classifier, which by default is the Decision Tree. I can alter this using the base_estimator parameter to test out a random forest and the gaussian naive bayes classification. The other parameters I can change are n_estimators which is how many weak models to fit and learning_rate, a measure of the weight given to each classifier. AdaBoost is generally used on weak classifiers, or those that perform only slightly better than random. One example would be a decision stump or a decision tree with only a single layer. In theory, the AdaBoost Classifier should perform better than the decision tree because it runs the decision tree multiple times and iteratively adjusts the weights given to each featureto make more accurate predictions. However, in practice, a more complex algorithm does not always perform better than a simple model and, at the end of the day, more quality data will beat a highly tuned algorithm. Here is a brief summary of the results using GridSearch for feature selection and then for algorithm tuning. Results The results from running the final versions of the algorithms are shown below: Conclusion Although the script I used to test the classifier implemented cross-validation, I was skeptical of the relatively high precision, recall, and F1 score recorded. I was conscious that I had somehow overfit my model to the data even though the script implements cross-validation. Looking through the tester.py script, I saw that the random seed for the cross-validation split was set at 42 in order to generate reproducible results. I changed the random seed and sure enough, the performance of my model decreased. Therefore, I must have made the classic mistake of overfitting on my training set for the given cross-validation random seed, and I will need to look out for this problem in the future. Even taking precuations against overfitting, I had still optimized my model for a specific set of data. In order to get a better indicator of the perfomance of the Decision Tree model, I ran 10 tests with different random seeds and found the average performance metrics. The final results for my model are summarized below: The results are still relatively high given the nature of the persons of interest identification task, but not as questionably high as testing with a single random split of the data. Due to the small size of the available data, even a minor change such as altering the random seed when doing a train-test split can have significant effects on the performance of the algorithm which must be accounted for by testing over many subsets and calculating the average performance. A precision score of 0.782 means that of the individuals labeled by my model as persons of interest, 78.2% of them were indeed persons of interest. A recall score of 0.820 means that my model identified 82.0% of persons of interest present in the entire dataset. The main takeaway from this project was the important of quality data as compared to fine-tuning the algorithm. Feature engineering, through the creation of new features and the selection of those with the greatest explained variance, increased the F1 score of the classifier from ~ 0.40 to ~ 0.70. However, tuning the hyperparameters of the algorithm only increased the F1 score to 0.80. Subsequently, when developing future machine learning models, I will focus on collecting as much high-quality data as I can before I even think about training and tuning the algorithm. This point was elucidated in a 2009 paper from Alon Halevy and Peter Norvig titled “The Unreasonable Effectiveness of Data.” The main argument of the paper is that as the amount of valid data increases, the choice of algorithm matters less and less and even the simplest algorithm can match the performance of the most highly tuned algorithm. Moreover, this project showed me that human intuition about a situation will not always match the results returned by a machine learning model. For example, Imy hypothesis was that the most highly predictive indicators of fradulent activity would be emails to/from persons of interest. Using automatic feature selection showed that the order of importance was bonus, from_poi_ratio, salary, total_stock_value. My intuition was partly correct, but the algorithm also had other (and more accurate) ideas for what mattered when it came to identifying persons of interest. Nonetheless, this discrepancy between my thinking and the classifier provides a chance to learn from the machine and update my mindset. Ultimately, I see the importance of machine learning not in outsourcing all decisions to algorithms, but in using machines to process and provide insights from data that can inform smarter thinking and enable humans to create more efficient systems.\ Appendix tester.py Provisional Function provided by Udacity. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Mark Frevert served as chief executive of Enron Europe from 1986–2000 and was appointed as chairman of Enron in 2001. He was a major player in the firm, although not a person of interest. I believe that he is not representative of the average employee at Enron during this time because of his substantial compensation and will remove him from the dataset. Timothy Belden was the former head of trading for Enron who developed the strategy to illegally raise energy prices in California. He was a person of interest and will definitely remain in the dataset. Jeffrey Skilling replaced Kenneth Lay as CEO of Enron in 2001 and orchestrated much of the fraud that destroyed Enron. As a person of interest, he will remain in the dataset. John Baxter was a former vice Enron vice chairman and died of an apparent self-inflicted gunshot before he was able to testify against other Enron executives. I will remove him from the dataset as he is not a person of interest. John Lavorato was a top executive in the energy-trading branch of Enron and received large bonuses to keep him from leaving Enron. As he was not a person of interest, and the large bonus ended up skewing his total pay, I think it would be appropriate to remove him from the dataset. Lawrence Whalley served as the president of Enron and fired Andrew Fastow once it was apparent the severity of Enron’s situation. He was investigated thoroughly but not identified as a person of interest and therefore will be removed from the dataset. Kenneth Lay, the CEO of Enron from 1986–2001, presided over many of the illegal business activites and hence is one of the most vital persons of interest. Precision is the number of correct positive classifications divided by the total number of positive labels assigned. In other words, it is the fraction of persons of interest predicted by the algorithm that are truly persons of interest. Mathematically precision is defined as Recall is the number of correct positive classifications divided by the number of positive instances that should have been identified. In other words, it is the fraction of the total number of persons of interestin the data that the classifier identifies. Mathematically, recall is defined as Precision is the number of correct positive classifications divided by the total number of positive labels assigned. In other words, it is the fraction of persons of interest predicted by the algorithm that are truly persons of interest. Mathematically precision is defined as Recall is the number of correct positive classifications divided by the number of positive instances that should have been identified. In other words, it is the fraction of the total number of persons of interestin the data that the classifier identifies. Mathematically, recall is defined as",Machine Learning with Python on the Enron Dataset,8,published,13455,6800,0.02514705882352941,0,0,0,0,1,1
97,12,643.0227229919329,3,https://medium.com/p/the-worst-they-can-say-is-no-212a1c571aad,0,None,2017-08-22 15:09:00,53.68,6,73,2017-08-22 14:01:00,"['College', 'Opinion', 'Reflections', 'Anxiety']","The Worst They Can Say is No Overcoming Social Anxiety with an Attitude Change Going into college I had a serious case of social anxiety. From conversations with classmates, I know I wasn’t the only one suffering from this problem, although everyone’s fears took different forms. My anxiety manifested itself whenever I had to start a conversation either in person or online. A typical example occurred when I would need to email professors to apply for a research position or just to ask for help. Every time, I would spend a few agonizing hours composing the ideal paragraph only to receive a reply within minutes saying simply “Yes” with the tag “Sent from my iPhone.” Inevitably, I would brief a sigh of relief and tell myself next time I would remember the professor would only spend a few seconds on my email and it did not have to be perfect. I knew that whenever I would read an email, I did not analyze every word and others were not expecting immaculate messages. Nevertheless, the next time I had to write a dreaded email, I would forget my promise and waste precious time crafting a message. Debilitating social stumbling blocks such as this affect all of us to some degree because we have a natural aversion to situations where we might be subjected to social disapproval. This makes perfect sense from an evolutionary standpoint — we would not want to be alienated from a group when we depend on them for survival — but it does not serve us well in the modern world where we must expose ourselves to rejection in multiple interactions every day. Even though one of my goals after high school was to embrace failure rather than trying to avoid it, I knew that social anxiety led me to avoid trying new opportunities because I was afraid of any form of rejection. Although I was able to identify this condition before my freshman year, it was not until my junior year that I was able to overcome it by embracing the mindset of “the worst they can say is no.” This attitude is best illustrated with a personal example. My junior year, I had a thermodynamics class with a particularly stubborn professor. A former astronaut in training (he trained for 2 years as a back-up but never flew), he was known to spend an entire lecture discussing the intricacies of sailing because some topic, say surface friction, had reminded him of a decades-old experience. He would then end the lecture by assigning homework and telling us that since he was out of time, we would just have to refer to the book to learn any topics he was supposed to have covered. His first day message was: “I am not here to teach you, because you should learn everything on your own from the book while doing homework.” This is not unusual for a college course, but there was a slight catch in this particular class. The book that the professor was referring to had gone out of print 5 years beforehand and only a few used copies were offered on Amazon starting at $500. Moreover, there was no ebook that could be acquired, even through questionably legal methods. While several students shelled out the money for the book in the hope that they would be able to sell it at the end of the semester, I balked at the price. Even after the professor was told was this book was nearly impossible to obtain, he refused to change the textbook and said simply “share one book with five people. That’s $100 each. That’s the price of a pair of shoes except the book is more useful.” After looking through every resource available (friends, the library, the entire Internet), I decided to put the motto of “the worst they can say is no” into practice. I searched for the author of the book, found his school email, and sent him a message explaining the situation and asking if it would be possible for him to sell a digital copy of his book. My reasoning was that surely the book had been digitally designed and it would have to be relatively easy to make it available for purchase. A few hours after hitting send, I received the completely unexpected but welcome reply: “Send me your address and I will have a copy of the book in the mail tomorrow.” I offered to at least pay for the shipping, but I could not convince the author to accept any money. All he wanted was a promise I would not sell the book after the semester. I assured him that it would be passed on through several years of engineers (assuming that the professor would never change his ways). A few days later, the book showed up in the mail with a nice message from the author. After I put it through its paces, it is currently in the hands of another engineer about to take the same class, with the same professor, using the same book. This story serves not to brag about my ability to talk my way into a free book, but to demonstrate the effectiveness of “the worst they can say is no” mentality. Before putting this attitude into practice, I would never have done something as crazy as emailing an unknown professor asking for a copy of his book, even though it took only 5 minutes of my time and saved me hundreds of dollars. My anxiety would have been too great and prevented me from hitting send. I won’t guarantee that this particular application will work every time (although to be honest I’ve only tried it once) but the general principle that you might as well ask has been a game-changer in my college career. Since this exchange, I no longer agonize over emails to professors (I do still check for spelling though) and I have improved at applying the same concept in person. Anytime I am presented with an opportunity that holds a chance for rejection, I remind myself that the worst case scenario is simply “no.” Even receiving a no can be a chance to improve one’s self if it is followed up with the question “What can I do better next time?” Applying for a job, trying for a raise, or contacting a professor for help can seem intimidating, but there is no consequence to asking, and not asking could be a major source of regret. I look back on many chances I have missed , but there are very few experiences that I have agreed to that I would take back. Memorizing a saying like “the worst they can say is no” will never solve your problems, but it can serve as that extra confidence needed to get you to hit the send button or start a conversation. This outlook is particularly relevant to those students heading back to college because you will be exposed to more opportunities than at any other point in your life and you will need to constantly reach out for advice. Although from an evolutionary viewpoint, our inclination to hesitate in social situations could have served us well, that does not mean that we have to accept the default response (xenophobia may be natural, but we should still do everything in our power to combat it). Although some traits may have benefited us in the past, we need to realize that we can suppress harmful innate (or learned) responses and develop more sensible reactions for modern conditions. “The worst they can say is no” is an effective message that can be used to overcome natural social anxieties and anyone heading off to college or hesitating when applying for a new opportunity should embrace this attitude change. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",The Worst They Can Say is No,7,published,136,1412,0.0084985835694051,0,0,0,0,0,0
112,21,612.0264810665857,7,https://medium.com/p/artificial-intelligence-part-1-search-a1667a5991e5,0,None,2017-09-22 15:04:00,13.21,24,302,2017-09-22 13:33:00,"['Programming', 'Artificial Intelligence', 'Search', 'Computer Science']","Artificial Intelligence Part 1: Search Solving the Eight-Puzzle using A-Star and Local Beam Search Author’s Note: The following is a project I completed for Introduction to Artificial Intelligence at Case Western Reserve University. The complete Python code and documentation can be found on my machine learning GitHub page. One quick comment about my code: as an engineer, I pride myself on getting the job done. That means I often put functionality ahead of clean code. I would appreciate any constructive criticism for optimizing my Python code! Introduction The eight puzzle provides an ideal environment for exploring one of the fundamental problems in Artificial Intelligence: searching for the optimal solution path. Although the eight puzzle is a simplified environment, the general techniques of search employed to find the solution can translate into many domains, including laying out components on a computer chip or finding the best route between two cities. Two different search strategies were explored in this project: a-star and local beam search. A-star search was tested with 2 heuristics, while local beam was tested with several values for the beam width. The complete project was coded in Python. The eight_puzzle was implemented as a class with attributes to represent the state of board, a number of utility methods to aid in the solution of the puzzle and visualizing the solution path, and most importantly, the two methods for solving the puzzle. The solution of the eight puzzle is the sequence of blank tile moves that lead from the starting state to the goal state. As the solution is a path, my approach used Python dictionaries to keep track of all nodes considered in each search. Each node includes the current state (the position of all tiles on the board), the parent state, and the action required to get from the parent state to the child state. When the goal state is reached, the algorithm can then transverse backwards through the dictionary starting at the goal state and linking each child state to its parent while recording the series of moves of the blank tile. This solution path can then be displayed and the length of the solution path can be determined. The efficiency of each search can be quantified with an effective branching factor calculated using the length of the solution path and the number of nodes generated during the search. eight_puzzle.py Contains the eight puzzle class with all attributes and methods to represent and solve the puzzle. play_puzzle.py Contains code for interacting with the puzzle from the command line or as a series of commands contained in a text file. The following commands can be used to interact with the puzzle from the command line. These commands are implemented as methods of the eight puzzle class. Set the state of the puzzle to the specified argument. The format of the state is “b12 345 678” where each triple represents a row and “b” is the blank tile (the only tile that can be moved). Randomly set the state of the puzzle by taking a random series of n integer actions backward from the goal state. As only half of the initial states of the eight puzzle are solvable, taking a series of steps backward from the goal state ensures that the resulting state will have a solution. Displays the current state of the puzzle in the form “b12 345 678” Displays the current state in an aesthetic and understandable format. Displays the solution as a sequence of moves in an aesthetic and understandable format. Move the blank one place in the specified direction. The move must be one of the following: [“up”, “down”, “left”, “right”] Allowed moves will be determined by the current state of the puzzle. Solve the eight puzzle using the specified heuristic. The choices for the heuristic are ‘h1’ or ‘h2’. ‘h1’ counts the number of misplaced tiles from the goal state while h2 represents the sum of the Manhattan distances of each tile from its correct position in the goal state. After solving the puzzle, the solution path should be printed as a series of actions to move from the starting state to the goal state. The length of the solution path should also be displayed. Solve the eight puzzle using local beam search with the beam width equal to the integer k. In general, a larger value of k is more likely to find a solution becuase the search space is larger, but larger k values will require retaining more nodes in memory. After solving the puzzle, the solution path should be printed as a series of actions to move from the starting state to the goal state. The length of the solution path should also be displayed. The maximum integer n number of nodes to generate during a search. A node is generated when its parent state is expanded. When this limit is exceeded, the code will display an error message. If max nodes is not specified, a default maximum number of 10000 nodes is passed to the method. Read and execute a series of commands from a text file. The commands are specified in the same format as the commands for the command line except without the dashes. An example text file is shown below: setState ""3b2 615 748"" printState solveBeam 50 maxNodes 2000 Command Implementation To demonstrate the functionality of the commands, I explain the code that implements the command as well as examples of running the command from the command line. I also show examples of reading commands from a specified text file. -setState and -printState setState and printState are both self-explanatory as setState can be used to set the state of the puzzle to any legal configuration and print state can be used to display the state and confirm that the state is as expected. setState checks to make sure that the string is a legal representation of the eight puzzle and then sets the state. printState iterates through the puzzle board and diplays the state in the same format as that used to set the state. The following code block shows an example of setting the state to “325 678 b14” and displaying the result in both string format and prettyPrint format. This command can be run from the command line. The 0 in the pretty printed state is the blank tile. -randomizeState This method takes a random sequence of “n” allowed moves backwards from the goal state. The resulting state will be solvable. The implementation first sets the state of the puzzle to the goal state, then, for n iterations, the code finds the list of available moves and takes on at random. The state is then set to the result of the move for the next iteration. The following code block shows an example of randomizing the state by taking 50 random steps from the goal state. The code also displays the state after randomizing in both string format and prettyPrint format. -move This method moves the blank one spot in the specified direction. If the move is not allowed based on the current state of the puzzle, a message is displayed. The move is made as a series of else if statements and the new state is returned as a list. The following two examples first illustrate taking a valid move and then an invalid move. (The puzzle is first set to the goal state for demonstration purposes.) Valid move: moving “right” from the goal state: Invalid move: moving “up” from the goal state: -solveAStar The main part of the a-star search code is a loop that runs until either a solution is found or the maximum number of nodes is generated. On each iteration, the code finds the available moves from the current state of the puzzle, and then checks to make sure that these moves do not result in a state that has already been expanded or is already on the frontier. If the move satisfies these conditions, the total cost of the move is then evaluated by taking the path cost of the resulting state plus the heuristic of the resulting state (see background on A-Star below). The state that results from the move is then added to the priority queue and at the end of the iteration, the priority queue is sorted by total cost. At the end of each iteration, the best state is chosen from the front of the priority queue and expanded. The node that is expanded will then be added to the expanded_nodes dictionary in order to generate the solution path at the end of the loop. The states that are not expanded on an iteration will be added to the frontier in order to keep track of which nodes have been generated but not expanded. The goal check occurs when a node is expanded rather than when it is generated because it might be possible that the first time the goal state is encountered is not the lowest cost path to the goal. Once a solution is found, the expanded_nodes dictionary is passed to another function that generates and displays the solution path. The total number of nodes generated is the sum of the nodes in the frontier_nodes dictionary and the expanded_nodes dictionary. Background on A-star search A-star search is a powerful algorithm for finding an optimal solution path. A-star search chooses the next state based on the total cost of all nodes on the frontier (those that have been generated but not yet expanded). The total cost of a node is defined as f(state)=g(state)+h(state) f(state), the total cost of the state, is the sum of g(state), the path cost to get to the state, and h(state), the heuristic function evaluated at the state. In the eight puzzle, the path cost is simply the depth of the state, or the number of moves to get from the start state to the goal state because each move incurs the same cost. The heuristic is either h1, the number of tiles misplaced from their position in the goal state, or h2, the sum of the Manhattan distances of all tiles from their position in the goal state. The heuristic is essentially an estimate of the remaining path distance from the state to the goal. A-star implements a priority queue, where the frontier nodes in the queue are ordered by total cost with the lowest cost nodes at the front of the queue. Each iteration, the lowest cost node (first in the queue) is expanded and then removed from the queue. My algorithm implements a priority queue, as well as a dictionary to keep track of the expanded nodes and a dictionary to keep track of the nodes on the frontier. The solution path is the sequence of actions from the start state to the goal state, the solution length is the number of actions required, and the number of nodes considered is the number of nodes expanded plus the number of nodes on the frontier. A-star search is optimal (it finds the shortest solution path) and complete if the heuristic is both admissible and consistent. Admissibility means that the heuristic never overestimates the distance to the goal state. A heuristic is consistent if the estimate to the goal for any node is always less than or equal to the estimate to the goal for any neighboring vertex plus the path cost of reaching that vertex. (This is a general form of the triangle inequality). Both the h1 and h2 heuristics are admissible and consistent guaranteeing that they will find the optimal solution if it exists. However, the two heuristics are not equally efficient as will be demonstrated in the calculation of the effective branching factors. The following example shows solving the puzzle from the state “312 475 68b” with a-star using the h1 heuristic and the default number of max nodes (10000). The command can be run from the command line. This puzzle can be solved in four moves and the solution is printed out in a pretty format to see the sequence of moves. As can be seen, a-star using the h1 heuristic finds the optimal (shortest path cost) solution. The following example shows what happens if max nodes is specified and the puzzle is not able to find a solution within the maximum number of nodes to consider. This is an unsolvable puzzle, and as can be seen, no solution is found (try increasing the maximum number of nodes and give your computer some exercise!) The following example demonstrates solving a randomly generated puzzle taking 10 moves backwards from the goal state. The puzzle is solved with a-star using the h2 hueristic and a maximum of 500 nodes. The algorithm correctly solves the puzzle with both heuristics as demonstrated. The efficiency of the a-star algorithm and a comparison between the two heuristics will be discussed in the effective branching factor section of this report. -solveBeam The main part of the local beam search code is a loop that runs until either a solution is found or the maximum number of nodes is generated. Each iteration, all successor states of all current states are generated by finding all available moves. Each successor state is then checked to make sure that it has not already been considered. It if has not, then the algorithm will find the total cost of the state using the evaluation function (see Local Beam Search Background below). The successor states are then added to the priority queue and the queue is sorted by total cost. At the end of the iteration, the k best states (the width of the beam search) are retained from the list of successor states. The k best states then become the current states from which to generate all possible successor states on the next iteration. All nodes are added to a dictionary in order to find the solution path when the loop ends. The goal check occurs when a node is generated because the first time the goal state is encountered represents the end of local beam search. Once the solution is found, the dictionary of all nodes is passed to a function that finds and displays the solution path. Local Beam Search Background Local beam search implements a priority queue as in a-star search, but the evaluation function does not take into account the path cost to reach the state. The evaluation function can be defined in any manner as long as it is zero at the goal state. The evaluation function I developed for local beam search is f(n)=h1(state)+h2(state) Where f(state) represents the total cost of the state, h1(state) represents the number of misplaced tiles from the goal state of the state, and h2 represents the sum of the Manhattan distances of the tiles in the state from the goal state. This evaluation function was selected because the h1 and h2 heuristics are both consistent and admissible and work well for solving a-star. A-star considers the path cost to each state when ranking the states whereas local beam only considers the evaluation function of the successor states. In effect, local beam is “blind” to the past and to any future beyond the next step (a-star is also “blind” to the future beyond the next step.) At each iteration, local beam search will calculate the total cost of all successors of all current states and order the successor states by lowest to highest cost. The algorithm will then retain the “k” best states for the next iteration. This limits the amount of states that need to be kept in memory. Local beam search with k=1 is equivalent to hill climbing because the algorithm will just select the successor state with the lowest evaluation function. Local beam search with k=∞ is equal to breadth first search because the algorithm will expand every single state at the current depth before moving deeper in the search tree. The following example demonstrates solving the eight puzzle using local beam search with a beam width of 10 and the maximum number of default nodes (10000). This solution has a true length of 4. The following example shows randomizing the state with 20 random moves from the goal state, and finding the solution using local beam search with a beam width of 10 and a maximum of 5000 nodes. Local beam with a beam width of 10 is able to find the solution. However, local beam can get stuck in local optima if the beam width is too narrow. -readCommands An additional part of this project was to allow the program to read a sequence of commands from a text file. This was implemented in the play_puzzle.py function using file parsing. The following example illustrates using a text file to set the state of the puzzle, displaying the initial state of the puzzle, and then solving the puzzle using local beam with a beam width of 50 and 2000 nodes max. Contents of “set_state_solve_beam.txt” The following example illustrates using a text file to randomize the state of the puzzle with 16 random steps back from the goal, solving the puzzle using a-star with the h2 heuristic, and pretty printing the solution path in an understandable format. Contents of “randomize_solve_astar.txt”: randomizeState 16 solveAStar h2 prettyPrintSolution The above examples demonstate that the program meets all of the requirements as specified in the project description. The final step of the project is to investigate the efficiency of the two algorithms and the two heuristics. Algorithmic efficiency can best be quantified by the effective branching factor. Efficiency Calculations Effective Branching Factor The branching factor for a specific node is the number of successor states generated by that node. The effective branching factor is an estimate for the number of successor states generated by a typical node. The formula to find the effective branching factor is: N=b_eff+b_eff²+b_eff³+…+b_eff^d where N is the total number of nodes generated, b_eff is the effective branching factor, and d is the length of the solution path. To calculate the effective branching factor, I generated 100 starting states for each solution length from 2 moves to 24 moves (by 2). This was done by taking moves backwards from the goal state with the condition that no state could be repeated twice as shown in the following code. For each solution path length, there are 100 starting states. To evaluate both algorithms and the two heuristics, I iterated through all of these starting states and solved them using the a-star search with the h1 and h2 heuristics and local beam search using a beam width of 10 and 100. I recorded the number of nodes generated by each algorithm in finding the solution and then took the average across the 100 puzzles. The average number of nodes generated for each algorithm are shown in the table below: The effective branching factor can then be calculated by solving the equation described above for b_eff. This equation was solved by moving all terms of the equation to one side and finding the root of the equation graphically. A lower branching factor indicates a more efficient algorithm. As can be seen, the most efficient algorithm tends to be A* using the h2 heuristic with local beam with a beam width of 10 improving as the solution depth increases. If given more time and computing resources, a worthwhile study would be to see if these trends continue as the solution depth increases. The longest solution path to the eight puzzle is 31 moves and if the effective branching factor of local beam continues to decrease, it might be the most efficient algorithm for long solution paths. This makes sense because as the solution depth increases, the benefits of retaining fewer nodes on each iteration will increase. The potential downside of local beam search is that it can become stuck in local optimum and not find the global optimum, or the correct solution path. Local optimum can be avoided using the incredible algorithm known as simulated annealing The branching factors of the algorithm configurations are summarized in the table below: Visualization of Efficiency Using the R programming language, I quickly visualized the efficiency of the two algorithms. a_star_h1 refers to the a-star algorithm using the h1 heuristic, a_star_h2 refers to the a-star algorithm using the h2 heuristic, local_beam_10 refers to the local beam algorithm with a beam width of 10, and local_beam_100 refers to the local beam algorithm with a beam width of 100. Conclusions The eight puzzle is a wonderful problem for exploring the basics of search algorithms. This project demonstrated solving the eight puzzle using a-star search with two different heuristics, and using local beam with two beam widths. Further work on the eight puzzle could involve solving the puzzle using a-star search informed by a more sophisticated heuristic such as that developed by Hanson et al in 1992. Another intriguing route for exploration of the eight puzzle would be training a neural network to solve the puzzle using the sequence of actions as a label and the starting state as a feature. Graduate work done by Peterson in 2013 used neural networks to attempt to derive a better heuristic for the eight puzzle. I would like to acknowledge the help provided by the resources linked to in the report as well as the textbook Artificial Intelligence: A Modern Approach by Russell and Norvig. I thoroughly enjoyed developing an eight puzzle solver using search algorithms and am looking forward to greater challenges in artificial intelligence. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator -setState [state] -randomizeState [n] -printState -prettyPrintState -prettyPrintSolution -move [direction] -solveAStar [hueristic] -solveBeam [k] -maxNodes [n] -readCommands [file.txt]",Artificial Intelligence Part 1: Search,5,published,2286,3904,0.005379098360655738,0,0,0,0,0,0
102,13,527.2238404196065,3,https://medium.com/p/the-case-for-criticism-964c43f7e058,0,None,2017-12-16 10:20:00,63.16,8,24,2017-10-07 06:06:00,"['Education', 'Criticism', 'Self Improvement', 'Lessons Learned']","The Case for Criticism When I’m wrong, call me out! Here’s a radical idea: every time you refrain from correcting someone out of politeness, you are doing them a disservice by robbing them of a valuable opportunity for self-improvement. Lately, I have noticed a disconcerting trend: people in daily life are more reluctant to point out and correct other’s mistakes. This has been most pronounced than in the classroom where it seems to be impossible for a student to actually give a wrong answer. Instead, they are “on the right track” or “have the basic concept” no matter how far away from the correct response they actually are. Now, I understand professors may not want to discourage a student by flat out telling them they are wrong, but despite recent political developments in the US, we are not living in a post-truth world. There are still right and wrong answers, optimal ways to solve a problem, and opinions that belong in “the dustbin of history.” The aversion to correcting false statements extends beyond the classroom. In my time with NASA, I sat in on many meetings where a manager said something all the engineers knew was technically wrong, but no one would ever think to speak up and point this out. Afterwards, we talked about how the manager had no clue how things actually got done, but instead of explaining that to him, we let him continue in his false beliefs. As a final example, consider a typical scenario in the early rounds of the popular singing contest American Idol: a young contestant, full of enthusiasm and confidence walks onto the stage and belts out an absolutely unbearable rendition of a pop song. How on Earth did they come to believe they had any singing talent? Because no one had ever been told them how bad they really were. For years, they had been surrounded by family and friends who politely told them they were a great performer, a delusion that finally was shattered in front of a national audience. All it would have taken was a single honest person to tell the contestant that maybe they should pursue a different path to save them hours of wasted effort and the embarrassment of a failed audition. What is the driving force behind the criticism decline? The answer has two parts. First, the powerful inertia of the status quo prevents people from taking action to step up and make a correction. It takes considerable effort to not only point out a falsehood, but then to patiently explain to someone why they are incorrect. We are all so absorbed in our own lives and problems that dealing with others’ issues can be overwhelming. It’s much easier to just let the mistake slide and continue on with the conversation or meeting rather than be “that” person who has to make a fuss. The problem with this mindset is that progress only happens by going against the status quo. It’s the disruptive individuals and companies that end up having the greatest influence as opposed to those who sit silently and let dismal conditions persist. “That” person who speaks up may seem obnoxious, but I would argue that any company should seek out those individuals who are not afraid to challenge standard operating procedure. The unwillingness of people at large companies to go against the grain, has been given the name, “the culture of complacency,” and has been blamed for numerous disasters, including the BP Gulf of Mexico oil spill and the crash of Korean Air 801. Societal advances are made not through passivity, but by those people who look at the current way of doing something and think “that could be done more efficiently.” Mindlessly agreeing with a manager or co-workers is admittedly easier than correcting them, but it will never lead to meaningful change. The second aspect of the criticism decline is our own feelings of vulnerability. We believe that if we point out the mistakes of others, then others will be inclined to mention our own shortcomings. It is admittedly very difficult to open ourselves to criticism, even when we know how often we are wrong. I expect to be questioned when I give a presentation, but my initial internal reaction to any correction is still extremely negative. We all have built complex representations of the world in our heads, and when one part is challenged, it can feel as if the whole edifice may crumble. When we think about correcting someone else, we start to wonder if our own viewpoint could be wrong as well. What if our correction of the initial mistake is itself corrected by someone else? Or, even worse, what if someone questions one of the foundations of our worldview and forces us to think critically about our viewpoints? The issue with this line of thinking is that it implicitly suggests that being exposed to our own failures is a negative. I would argue this is actually one of the most valuable opportunities we are given. It is through making mistakes and learning from them that the most rapid self-improvement occurs. Yes, when we correct someone else, we open ourselves up to correction, but that is actually something to strive for rather than avoid. What Can be Done? We need to approach the problem from two directions: not only must we overcome our hesitation towards delivering constructive criticism, but we need to get better at responding when it is directed towards our ideas. The best way to shift our mindset with regards to the latter part of the problem is thinking about the following: we only make progress by learning from mistakes, however, making blunders can only be painful and time-consuming. Wouldn’t it be great if we could learn from failures without having to actually go through them ourselves? Well, that is exactly what happens when someone corrects us. If someone can see that we are on the path to messing something up because they themselves have done the exact same thing, then it is in our best interest to listen to them rather than carry through on our intended course of action. One individual can only make so many mistakes, but by learning from those of others, we can expand the effective number of experiences to which we are exposed. Think about Amazon reviews: rather than buy inferior products ourselves, we can rely on other’s experiences and purchase only the one item that has been tested and approved by hundreds of other people. Experience and knowledge should not be kept to each individual, but rather shared among a group. That ultimately is the benefit of working with others: rather than having to learn everything firsthand, we can use the collective knowledge of those around us to solve our problems. Then, when we make a mistake, we contribute what we learned back to the group and the entire knowledge base we can draw from grows. The single best way to get better at accepting criticism is to surround yourself with people who are smarter than you. If you ever think that you are the smartest person in the room, then you need to move. This semester, I have been fortunate to work as an undergrad in a research lab with a number of grad students with much more experience than me. At first, I tried to keep quiet out of fear of being exposed as naive, but eventually I gained the confidence to speak up after I saw even the post-docs were routinely corrected. Soon, I was benefiting enormously from the: make a mistake — receive constructive feedback — improve methods cycle. I have come to embrace the limits of my knowledge and I try to learn as much as possible from the previous experiences of others. In particular, I enjoy receiving feedback from one post-doc who is well, a little blunt in her criticism. At first, her style was off-putting, but once I realized she was only trying to help, I started to seek out her opinions. Now, I often send her my work with the explicit instructions to be as harsh as possible, an assignment in which she takes particular delight. This technique of intentionally opening yourself up to recurring criticism now even has a name: “rejection therapy,” which falls under the general psychological principle of classical conditioning. The idea is that by repeatedly exposing ourselves to failure on purpose, we can start to view failures as a normal course of life and not disastrous setbacks. As society is becoming more failure-averse, there is a growing recognition of the need for making constructive mistakes. To be better receivers of criticism we need to shift our view of mistakes from entirely negative, to frustrating in the short-term, but a long term positive investment. The other half of addressing the criticism decline is learning how to give constructive feedback. There are any number of ways to tell someone they are incorrect, from the straight-up “that’s wrong” to the round-about “well, maybe, perhaps you don’t quite have everything quite right.” The correct method of delivery depends on the audience and sometimes it’s necessary to know when to exercise restraint. A recent situation presented me with a conundrum that I am still not sure I correctly resolved. I was at a talk by a Google employee on the subject of impostor syndrome, where you finally get that dream position, but then feel like you really don’t belong and that at any time, someone might point it out. This is a serious subject that prevents many people who are underrepresented in a field from even trying to get a position because they feel as if they won’t be accepted. The presenter shared several methods for overcoming impostor syndrome which she said had worked for her. One of these was “power poses”, or the idea that you gain confidence by adopting certain stances such as standing with your hands on your hips at a meeting. The beneficial effects of power poses had been written up in a journal article and one of the authors had given a TED talk which has since been viewed almost 45 million times. However, I had recently read an article from another of the original authors who took back all the results of the study and said that she did not believe power poses actually work. Further studies had also failed to prove any benefits to “power poses.” The presenter mentioned the original journal article as proof these poses worked, and I was faced with the question of whether or not to interrupt her and point out the flaws in the study and the debate over the results. In the end, I decided not to because I thought it would distract from her main message. After the talk I went to the presenter and told her about the recent developments concerning the study. We ended up having a good discussion on the reproducibility crisis in the social sciences (about 40% of study results cannot be replicated), and I thought I handled the situation well. However, I still wonder if I should have pointed out the debate over the study so everyone in the audience could hear the entire story. I am not making an argument for seeking out as many opportunities as possible to tell people they are wrong. However, when you encounter an obvious false statement or inefficient solution to a problem, feel free to speak up. Don’t be afraid to be that disruptive person at a meeting, because you may prevent the next Gulf of Mexico oil spill, or find a way to make a process twice as efficient. In the end, remember both when receiving and giving criticism, it is always the idea and not the person that is incorrect. Disagreements lead to far more progress than mindless acceptance of the status quo, and people are often willing to change their methods if you will have a reasoned discussion with them. Learn to associate criticism not with negativity, but with an opportunity for improvement and start to enjoy the benefits of shared knowledge and experiences. In that spirit, if anything I have written in this post, or any other post, or anything I say in real-life is wrong, do me a favor and let me know! Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",The Case for Criticism,4,published,38,2264,0.005742049469964664,70,1,0,0,0,0
107,5900,516.1821060521644,892,https://towardsdatascience.com/random-forest-in-python-24d0893d51c0,34,Towards Data Science,2017-12-27 11:20:00,17.68,21,31076,2017-12-26 15:11:00,"['Machine Learning', 'Data Science', 'Random Forest', 'Python', 'Programming']","Random Forest in Python A Practical End-to-End Machine Learning Example There has never been a better time to get into machine learning. With the learning resources available online, free open-source tools with implementations of any algorithm imaginable, and the cheap availability of computing power through cloud services such as AWS, machine learning is truly a field that has been democratized by the internet. Anyone with access to a laptop and a willingness to learn can try out state-of-the-art algorithms in minutes. With a little more time, you can develop practical models to help in your daily life or at work (or even switch into the machine learning field and reap the economic benefits). This post will walk you through an end-to-end implementation of the powerful random forest machine learning model. It is meant to serve as a complement to my conceptual explanation of the random forest, but can be read entirely on its own as long as you have the basic idea of a decision tree and a random forest. A follow-up post details how we can improve upon the model built here. There will of course be Python code here, however, it is not meant to intimate anyone, but rather to show how accessible machine learning is with the resources available today! The complete project with data is available on GitHub, and the data file and Jupyter Notebook can also be downloaded from Google Drive. All you need is a laptop with Python installed and the ability to start a Jupyter Notebook and you can follow along. (For installing Python and running a Jupyter notebook check out this guide). There will be a few necessary machine learning topics touched on here, but I will try to make them clear and provide resources for learning more for those interested. Problem Introduction The problem we will tackle is predicting the max temperature for tomorrow in our city using one year of past weather data. I am using Seattle, WA but feel free to find data for your own city using the NOAA Climate Data Online tool. We are going to act as if we don’t have access to any weather forecasts (and besides, it’s more fun to make our own predictions rather than rely on others). What we do have access to is one year of historical max temperatures, the temperatures for the previous two days, and an estimate from a friend who is always claiming to know everything about the weather. This is a supervised, regression machine learning problem. It’s supervised because we have both the features (data for the city) and the targets (temperature) that we want to predict. During training, we give the random forest both the features and targets and it must learn how to map the data to a prediction. Moreover, this is a regression task because the target value is continuous (as opposed to discrete classes in classification). That’s pretty much all the background we need, so let’s start! Roadmap Before we jump right into programming, we should lay out a brief guide to keep us on track. The following steps form the basis for any machine learning workflow once we have a problem and model in mind: Step 1 is already checked off! We have our question: “can we predict the max temperature tomorrow for our city?” and we know we have access to historical max temperatures for the past year in Seattle, WA. Data Acquisition First, we need some data. To use a realistic example, I retrieved weather data for Seattle, WA from 2016 using the NOAA Climate Data Online tool. Generally, about 80% of the time spent in data analysis is cleaning and retrieving data, but this workload can be reduced by finding high-quality data sources. The NOAA tool is surprisingly easy to use and temperature data can be downloaded as clean csv files which can be parsed in languages such as Python or R. The complete data file is available for download for those wanting to follow along. The following Python code loads in the csv data and displays the structure of the data: The information is in the tidy data format with each row forming one observation, with the variable values in the columns. Following are explanations of the columns: year: 2016 for all data points month: number for month of the year day: number for day of the year week: day of the week as a character string temp_2: max temperature 2 days prior temp_1: max temperature 1 day prior average: historical average max temperature actual: max temperature measurement friend: your friend’s prediction, a random number between 20 below the average and 20 above the average Identify Anomalies/ Missing Data If we look at the dimensions of the data, we notice only there are only 348 rows, which doesn’t quite agree with the 366 days we know there were in 2016. Looking through the data from the NOAA, I noticed several missing days, which is a great reminder that data collected in the real-world will never be perfect. Missing data can impact an analysis as can incorrect data or outliers. In this case, the missing data will not have a large effect, and the data quality is good because of the source. We also can see there are nine columns which represent eight features and the one target (‘actual’). To identify anomalies, we can quickly compute summary statistics. There are not any data points that immediately appear as anomalous and no zeros in any of the measurement columns. Another method to verify the quality of the data is make basic plots. Often it is easier to spot anomalies in a graph than in numbers. I have left out the actual code here, because plotting is Python is non-intuitive but feel free to refer to the notebook for the complete implementation (like any good data scientist, I pretty much copy and pasted the plotting code from Stack Overflow). Examining the quantitative statistics and the graphs, we can feel confident in the high quality of our data. There are no clear outliers, and although there are a few missing points, they will not detract from the analysis. Data Preparation Unfortunately, we aren’t quite at the point where you can just feed raw data into a model and have it return an answer (although people are working on this)! We will need to do some minor modification to put our data into machine-understandable terms. We will use the Python library Pandas for our data manipulation relying, on the structure known as a dataframe, which is basically an excel spreadsheet with rows and columns. The exact steps for preparation of the data will depend on the model used and the data gathered, but some amount of data manipulation will be required for any machine learning application. One-Hot Encoding The first step for us is known as one-hot encoding of the data. This process takes categorical variables, such as days of the week and converts it to a numerical representation without an arbitrary ordering. Days of the week are intuitive to us because we use them all the time. You will (hopefully) never find anyone who doesn’t know that ‘Mon’ refers to the first day of the workweek, but machines do not have any intuitive knowledge. What computers know is numbers and for machine learning we must accommodate them. We could simply map days of the week to numbers 1–7, but this might lead to the algorithm placing more importance on Sunday because it has a higher numerical value. Instead, we change the single column of weekdays into seven columns of binary data. This is best illustrated pictorially. One hot encoding takes this: and turns it into So, if a data point is a Wednesday, it will have a 1 in the Wednesday column and a 0 in all other columns. This process can be done in pandas in a single line! Snapshot of data after one-hot encoding: The shape of our data is now 349 x 15 and all of the column are numbers, just how the algorithm likes it! Features and Targets and Convert Data to Arrays Now, we need to separate the data into the features and targets. The target, also known as the label, is the value we want to predict, in this case the actual max temperature and the features are all the columns the model uses to make a prediction. We will also convert the Pandas dataframes to Numpy arrays because that is the way the algorithm works. (I save the column headers, which are the names of the features, to a list to use for later visualization). Training and Testing Sets There is one final step of data preparation: splitting data into training and testing sets. During training, we let the model ‘see’ the answers, in this case the actual temperature, so it can learn how to predict the temperature from the features. We expect there to be some relationship between all the features and the target value, and the model’s job is to learn this relationship during training. Then, when it comes time to evaluate the model, we ask it to make predictions on a testing set where it only has access to the features (not the answers)! Because we do have the actual answers for the test set, we can compare these predictions to the true value to judge how accurate the model is. Generally, when training a model, we randomly split the data into training and testing sets to get a representation of all data points (if we trained on the first nine months of the year and then used the final three months for prediction, our algorithm would not perform well because it has not seen any data from those last three months.) I am setting the random state to 42 which means the results will be the same each time I run the split for reproducible results. The following code splits the data sets with another single line: We can look at the shape of all the data to make sure we did everything correctly. We expect the training features number of columns to match the testing feature number of columns and the number of rows to match for the respective training and testing features and the labels : It looks as if everything is in order! Just to recap, to get the data into a form acceptable for machine learning we: Depending on the initial data set, there may be extra work involved such as removing outliers, imputing missing values, or converting temporal variables into cyclical representations. These steps may seem arbitrary at first, but once you get the basic workflow, it will be generally the same for any machine learning problem. It’s all about taking human-readable data and putting it into a form that can be understood by a machine learning model. Establish Baseline Before we can make and evaluate predictions, we need to establish a baseline, a sensible measure that we hope to beat with our model. If our model cannot improve upon the baseline, then it will be a failure and we should try a different model or admit that machine learning is not right for our problem. The baseline prediction for our case can be the historical max temperature averages. In other words, our baseline is the error we would get if we simply predicted the average max temperature for all days. We now have our goal! If we can’t beat an average error of 5 degrees, then we need to rethink our approach. Train Model After all the work of data preparation, creating and training the model is pretty simple using Scikit-learn. We import the random forest regression model from skicit-learn, instantiate the model, and fit (scikit-learn’s name for training) the model on the training data. (Again setting the random state for reproducible results). This entire process is only 3 lines in scikit-learn! Make Predictions on the Test Set Our model has now been trained to learn the relationships between the features and the targets. The next step is figuring out how good the model is! To do this we make predictions on the test features (the model is never allowed to see the test answers). We then compare the predictions to the known answers. When performing regression, we need to make sure to use the absolute error because we expect some of our answers to be low and some to be high. We are interested in how far away our average prediction is from the actual value so we take the absolute value (as we also did when establishing the baseline). Making predictions with out model is another 1-line command in Skicit-learn. Our average estimate is off by 3.83 degrees. That is more than a 1 degree average improvement over the baseline. Although this might not seem significant, it is nearly 25% better than the baseline, which, depending on the field and the problem, could represent millions of dollars to a company. Determine Performance Metrics To put our predictions in perspective, we can calculate an accuracy using the mean average percentage error subtracted from 100 %. That looks pretty good! Our model has learned how to predict the maximum temperature for the next day in Seattle with 94% accuracy. Improve Model if Necessary In the usual machine learning workflow, this would be when start hyperparameter tuning. This is a complicated phrase that means “adjust the settings to improve performance” (The settings are known as hyperparameters to distinguish them from model parameters learned during training). The most common way to do this is simply make a bunch of models with different settings, evaluate them all on the same validation set, and see which one does best. Of course, this would be a tedious process to do by hand, and there are automated methods to do this process in Skicit-learn. Hyperparameter tuning is often more engineering than theory-based, and I would encourage anyone interested to check out the documentation and start playing around! An accuracy of 94% is satisfactory for this problem, but keep in mind that the first model built will almost never be the model that makes it to production. Interpret Model and Report Results At this point, we know our model is good, but it’s pretty much a black box. We feed in some Numpy arrays for training, ask it to make a prediction, evaluate the predictions, and see that they are reasonable. The question is: how does this model arrive at the values? There are two approaches to get under the hood of the random forest: first, we can look at a single tree in the forest, and second, we can look at the feature importances of our explanatory variables. One of the coolest parts of the Random Forest implementation in Skicit-learn is we can actually examine any of the trees in the forest. We will select one tree, and save the whole tree as an image. The following code takes one tree from the forest and saves it as an image. Let’s take a look: Wow! That looks like quite an expansive tree with 15 layers (in reality this is quite a small tree compared to some I’ve seen). You can download this image yourself and examine it in greater detail, but to make things easier, I will limit the depth of trees in the forest to produce an understandable image. Here is the reduced size tree annotated with labels Based solely on this tree, we can make a prediction for any new data point. Let’s take an example of making a prediction for Wednesday, December 27, 2017. The (actual) variables are: temp_2 = 39, temp_1 = 35, average = 44, and friend = 30. We start at the root node and the first answer is True because temp_1 ≤ 59.5. We move to the left and encounter the second question, which is also True as average ≤ 46.8. Move down to the left and on to the third and final question which is True as well because temp_1 ≤ 44.5. Therefore, we conclude that our estimate for the maximum temperature is 41.0 degrees as indicated by the value in the leaf node. An interesting observation is that in the root node, there are only 162 samples despite there being 261 training data points. This is because each tree in the forest is trained on a random subset of the data points with replacement (called bagging, short for bootstrap aggregating). (We can turn off the sampling with replacement and use all the data points by setting bootstrap = False when making the forest). Random sampling of data points, combined with random sampling of a subset of the features at each node of the tree, is why the model is called a ‘random’ forest. Furthermore, notice that in our tree, there are only 2 variables we actually used to make a prediction! According to this particular decision tree, the rest of the features are not important for making a prediction. Month of the year, day of the month, and our friend’s prediction are utterly useless for predicting the maximum temperature tomorrow! The only important information according to our simple tree is the temperature 1 day prior and the historical average. Visualizing the tree has increased our domain knowledge of the problem, and we now know what data to look for if we are asked to make a prediction! In order to quantify the usefulness of all the variables in the entire random forest, we can look at the relative importances of the variables. The importances returned in Skicit-learn represent how much including a particular variable improves the prediction. The actual calculation of the importance is beyond the scope of this post, but we can use the numbers to make relative comparisons between variables. The code here takes advantage of a number of tricks in the Python language, namely list comprehensive, zip, sorting, and argument unpacking. It’s not that important to understand these at the moment, but if you want to become skilled at Python, these are tools you should have in your arsenal! At the top of the list is temp_1, the max temperature of the day before. This tells us the best predictor of the max temperature for a day is the max temperature of the day before, a rather intuitive finding. The second most important factor is the historical average max temperature, also not that surprising. Your friend turns out to not be very helpful, along with the day of the week, the year, the month, and the temperature 2 days prior. These importances all make sense as we would not expect the day of the week to be a predictor of maximum temperature as it has nothing to do with weather. Moreover, the year is the same for all data points and hence provides us with no information for predicting the max temperature. In future implementations of the model, we can remove those variables that have no importance and the performance will not suffer. Additionally, if we are using a different model, say a support vector machine, we could use the random forest feature importances as a kind of feature selection method. Let’s quickly make a random forest with only the two most important variables, the max temperature 1 day prior and the historical average and see how the performance compares. This tells us that we actually do not need all the data we collected to make accurate predictions! If we were to continue using this model, we could only collect the two variables and achieve nearly the same performance. In a production setting, we would need to weigh the decrease in accuracy versus the extra time required to obtain more information. Knowing how to find the right balance between performance and cost is an essential skill for a machine learning engineer and will ultimately depend on the problem! At this point we have covered pretty much everything there is to know for a basic implementation of the random forest for a supervised regression problem. We can feel confident that our model can predict the maximum temperature tomorrow with 94% accuracy from one year of historical data. From here, feel free to play around with this example, or use the model on a data set of your choice. I will wrap up this post by making a few visualizations. My two favorite parts of data science are graphing and modeling, so naturally I have to make some charts! In addition to being enjoyable to look at, charts can help us diagnose our model because they compress a lot of numbers into an image that we can quickly examine. The first chart I’ll make is a simple bar plot of the feature importances to illustrate the disparities in the relative significance of the variables. Plotting in Python is kind of non-intuitive, and I end up looking up almost everything on Stack Overflow when I make graphs. Don’t worry if the code here doesn’t quite make sense, sometimes fully understanding the code isn’t necessary to get the end result you want! Next, we can plot the entire dataset with predictions highlighted. This requires a little data manipulation, but its not too difficult. We can use this plot to determine if there are any outliers in either the data or our predictions. A little bit of work for a nice looking graph! It doesn’t look as if we have any noticeable outliers that need to be corrected. To further diagnose the model, we can plot residuals (the errors) to see if our model has a tendency to over-predict or under-predict, and we can also see if the residuals are normally distributed. However, I will just make one final chart showing the actual values, the temperature one day previous, the historical average, and our friend’s prediction. This will allow us to see the difference between useful variables and those that aren’t so helpful. It is a little hard to make out all the lines, but we can see why the max temperature one day prior and the historical max temperature are useful for predicting max temperature while our friend is not (don’t give up on the friend yet, but maybe also don’t place so much weight on their estimate!). Graphs such as this are often helpful to make ahead of time so we can choose the variables to include, but they also can be used for diagnosis. Much as in the case of Anscombe’s quartet, graphs are often more revealing than quantitative numbers and should be a part of any machine learning workflow. Conclusions With those graphs, we have completed an entire end-to-end machine learning example! At this point, if we want to improve our model, we could try different hyperparameters (settings) try a different algorithm, or the best approach of all, gather more data! The performance of any model is directly proportional to the amount of valid data it can learn from, and we were using a very limited amount of information for training. I would encourage anyone to try and improve this model and share the results. From here you can dig more into the random forest theory and application using numerous online (free) resources. For those looking for a single book to cover both theory and Python implementations of machine learning models, I highly recommend Hands-On Machine Learning with Scikit-Learn and Tensorflow. Moreover, I hope everyone who made it through has seen how accessible machine learning has become and is ready to join the welcoming and helpful machine learning community. As always, I welcome feedback and constructive criticism! My email is wjk68@case.edu. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Acquire the data in an accessible format Identify and correct missing data points/anomalies as required Prepare the data for the machine learning model Establish a baseline model that you aim to exceed Train the model on the training data Make predictions on the test data Compare predictions to the known test set targets and calculate performance metrics If performance is not satisfactory, adjust the model, acquire more data, or try a different modeling technique Interpret model and report results visually and numerically Split data into features and labels Converted to arrays Split data into training and testing sets State the question and determine required data One-hot encoded categorical variables",Random Forest in Python,4,published,175724,4494,1.312861593235425,0,0,1,0,1,1
109,4100,516.1804968642014,475,https://medium.com/p/random-forest-simple-explanation-377895a60d2d,19,None,2017-12-27 11:22:00,42.07,11,37676,2017-12-20 19:28:00,"['Machine Learning', 'Learning', 'Data Science', 'Modeling']","Random Forest Simple Explanation Understanding the Random Forest with an intuitive example When learning a technical concept, I find it’s better to start with a high-level overview and work your way down into the details rather than starting at the bottom and getting immediately lost. Along those lines, this post will use an intuitive example to provide a conceptual framework of the random forest, a powerful machine learning algorithm. After getting a basic idea down, I move on to a simple implementation to see how the technique works and if it will be useful to me before finally working out the details by digging deep into the theory. With that in mind, after understanding the overview of the random forest here, feel free to check out part two of this post, an end-to-end example worked out in Python code. Taken together, these two articles will help you conquer the first two steps in the learning process and leave you well prepared to dive as far into the random forest and machine learning as you want! Decision Tree: The Building Block To understand the random forest model, we must first learn about the decision tree, the basic building block of a random forest. We all use decision trees in our daily life, and even if you don’t know it by that name, I’m sure you’ll recognize the process. To illustrate the concept, we’ll use an everyday example: predicting the tomorrow’s maximum temperature for our city. To keep things straight, I’ll use Seattle, Washington, but feel free to pick your own city. In order to answer the single max temperature question, we actually need to work through an entire series of queries. We start by forming an initial reasonable range given our domain knowledge, which for this problem might be 30–70 degrees (Fahrenheit) if we do not know the time of year before we begin. Gradually, through a set of questions and answers we reduce this range until we are confident enough to make a single prediction. What makes a good question to ask? Well, if we want to limit the range as much as is possible, it would be wise to think of queries that are relevant to the problem at hand. Since temperature is highly dependent on time of year, a decent place to start would be: what is the season? In this case, the season is winter, and so we can limit the prediction range to 30–50 degrees because we have an idea of what the general max temperatures are in the Pacific Northwest during the winter. This first question was a great choice because it has already cut our range in half. If we had asked something non-relevant, such as the day of the week, then we could not have reduced the extent of predictions at all and we would be back where we started. Nonetheless, this single question isn’t quite enough to narrow down our estimate so we need to find out more information. A good follow-up question is: what is the historical average max temperature on this day? For Seattle on December 27, the answer is 46 degrees. This allows us to further restrict our range of consideration to 40–50 degrees. Again, this was a high-value question because it greatly reduced the scope of our estimate. Two questions are still not quite enough to make a prediction because this year might be warmer or colder than average. Therefore, we also would want to look at the max temperature today to get an idea if the year has been unusually warm or cold. Our question is simple: what is the maximum temperature today? If the max temperature today was 43 degrees, it might be colder this year and our estimate for tomorrow should be a little lower than the historical average. At this point, we can feel pretty confident in making a prediction of 44 degrees for the max temperature tomorrow. If we wanted to be even more sure, we could consult additional sources such as AccuWeather or Weather Underground to get information such as the forecast for the max temperature that we could incorporate into our mental model. However, at some point there is a diminishing return to asking more questions, and we cannot keep gathering more data for ever. For now, let us settle with using those three questions to make a prediction. So, to arrive at an estimate, we used a series of questions, with each question narrowing our possible values until we were confident enough to make a single prediction. We repeat this decision process over and over again in our daily lives with only the questions and answers changing. At this point we are nearly ready to make the connection to the decision tree, but let’s take just a minute to look at a graphical representation of the intuitive steps we took to find our answer: We start with an initial guess based on our knowledge of the world and refine our estimate as we gain more information. Eventually, we stop gathering data and make a decision, which in this case is the max temperature prediction. Our natural approach to the problem is what we might call a question-and-answer flowchart. In fact, this flowchart is also a rudimentary model of a decision tree! However, we have not quite built a full decision tree because as humans we take some shortcuts that make sense for us but are not so intuitive to a machine. There are two main differences between our illustrated decision process and a real decision tree. First, we have neglected to list the alternative branches, that is the predictions we would have made if the answers to the questions had been different: for example, if the season had been summer instead of winter, our range of predictions would have been shifted higher. Moreover we phrased our questions such that they could take on any number of answers: when we asked ‘what is the maximum temperature today?’ the answer could be any real value. In contrast, a decision tree implemented in machine learning will list all possible alternatives to every question and will ask all questions in True/False form. This is a little tough to grasp because it is not how humans naturally think, and perhaps the best way to show this difference is to create a real decision tree from our prediction process: We notice right away that each question (the white blocks) has only two answers: True or False. Moreover, for each True and False answer there are separate branches. No matter the answers to the questions, we eventually reach a prediction (shown in the green blocks). This ‘computer-friendly’ version of the decision tree may look different than our intuitive model, but it works in the exact same way. Start at the node on the left, and progress through the tree answering the questions along the way. For our example, the season is winter so we take the True branch. We said the historical average was 46, so the second question is also True. Finally, the third answer is True as the max temperature today was 43. Therefore, the final prediction is 40 degrees for the max temperature tomorrow, close to our guess of 44. This model here encompasses all the basic qualities of a decision tree. I purposefully left out all technical details, such as how the ‘questions’ are formed and how the thresholds are set, but these aren’t really necessary to understand the model conceptually or even to implement it in Python code! One aspect of the decision tree I should mention is how it actually learns. We refined our estimated range based on how the answers to the questions fit into our framework of the world. If the season is winter, our estimate is lower than if it is summer. However, a computer model of a decision tree has no prior knowledge and would never be able to make the connection ‘winter = colder’ on its own. It must learn everything about the problem from the data we provide it. We know how to translate answers from the flow-chart into reasonable predictions because our of daily experiences. In contrast, the model must be taught each of these relationships such as that if the temperature today is warmer than the historical average, the max temperature tomorrow will likely be warmer as well. As a supervised machine learning model, a random forest learns to map data (temperature today, historical average, etc.) to outputs (max temperature tomorrow) in what is called the training (or fitting) phase of model building. During training, we give the model any historical data that is relevant to the problem domain (the temperature the day before, the season of the year, and the historical average) and the true value we want the model to learn to predict, in this case the max temperature tomorrow. The model learns any relationships between the data (known as features in machine learning) and the values we want to predict (called the target). The decision tree forms the structure shown above, calculating the best questions to ask in order to make the most accurate estimates possible. When we ask the decision tree to make a prediction for tomorrow, we must give it the same data it used during training (the features) and it gives us an estimate based on the structure it has learned. Much as humans learn from examples, the decision tree also learns through experience, except it does not have any previous knowledge it can incorporate into the problem. Before training, we are much ‘smarter’ than the tree in terms of our ability to make a reasonable estimate. However, after enough training with quality data, the decision tree will far surpass our prediction abilities. Keep in mind the decision tree does not have any conceptual understanding of the problem even after training. From the model’s ‘perspective’, it is simply receiving numbers as inputs and outputting different numbers that agree with those it saw during training. In other words, the tree has learned how to map a set of features to targets with no knowledge of anything about temperature. If we ask the decision tree another weather related question, it will have no clue how to respond because it has been trained for one specific task. That is basically the entire high-level concept of a decision tree: a flowchart of questions leading to a prediction. Now, we take the mighty leap from a single decision tree to a random forest! From Decision Tree to Random Forest My prediction for the maximum temperature is probably wrong. And I hate to break it to you, but so is yours. There are too many factors to take into account, and chances are, each individual guess will be high or low. Every person comes to the problem with different background knowledge and may interpret the exact same answer to a question entirely differently. In technical terms, the predictions have variance because they will be widely spread around the right answer. Now, what if we take predictions from hundreds or thousands of individuals, some of which are high and some of which are low, and decided to average them together? Well, congratulations, we have created a random forest! The fundamental idea behind a random forest is to combine many decision trees into a single model. Individually, predictions made by decision trees (or humans) may not be accurate, but combined together, the predictions will be closer to the mark on average. Why exactly is a random forest better than a single decision tree? We can think about it terms of having hundreds of humans make estimates for the max temperature problem: by pooling predictions, we can incorporate much more knowledge than from any one individual. Each individual brings their own background experience and information sources to the problem. Some people may swear by Accuweather, while others will only look at NOAA (National Oceanic and Atmospheric Administration) forecasts. Perhaps one person relies on a meteorologist friend for their predictions while another uses hundred of years of temperature data. If we only ask one individual, we would only take advantage of their limited scope of information, but by combining everyone’s predictions together, our net of information is much greater. Furthermore, the more diverse each person’s source of information, the more robust the random forest is because it will not be swayed by a single anomalous data source. If NOAA goes rogue and starts making predictions over 100 degrees and everyone relied on NOAA, then our entire model would be worthless. If instead, individuals in our ‘forest’ use a number of different weather sources, then our model will not be greatly affected by a single source and we can continue to make reasonable predictions. Why the name ‘random forest?’ Well, much as people might rely on different sources to make a prediction, each decision tree in the forest considers a random subset of features when forming questions and only has access to a random set of the training data points. This increases diversity in the forest leading to more robust overall predictions and the name ‘random forest.’ When it comes time to make a prediction, the random forest takes an average of all the individual decision tree estimates. (This is the case for a regression task, such as our problem where we are predicting a continuous value of temperature. The other class of problems is known as classification, where the targets are a discrete class label such as cloudy or sunny. In that case, the random forest will take a majority vote for the predicted class). With that in mind, we now have down all the conceptual parts of the random forest! Wrap-Up Machine learning may seem intimidating at first, but the entire field is just many simple ideas combined together to yield extremely accurate models that can ‘learn’ from past data. The random forest is no exception. There are two fundamental ideas behind a random forest, both of which are well known to us in our daily life: It is the combination of these basic ideas that lead to the power of the random forest model. Now that you hopefully have the conceptual framework of a random forest, I recommend you take a look at the second part of this post, where we walk through the entire process of implementing a random forest for the max temperature problem in the Python programming language. Even if you have never programmed before, you should be able to follow along! If this post has sparked your curiosity, there are any number of resources out there to learn more. Sometimes too many resources can be overwhelming when trying to learn a new concept, so I’ll limit my recommendations to a great website made by the creator of the random forest (Leo Breiman), the free Coursera series on random forests, and the excellent book Introduction to Statistical Learning (by James, Witten, Hastie, and Tibshirani) which is available free online. With programming in general, and machine learning in particular, there are so many resources freely available online that match or even exceed any formal education so feel as if you are behind if you never saw this in college! As always, I appreciate any feedback and constructive criticism. My email is wjk68@case.edu. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator The wisdom of the (random and diverse) crowd Constructing a flowchart of questions and answers leading to a decision",Random Forest Simple Explanation,4,published,89546,2850,1.4385964912280702,6,0,1,0,1,0
104,73,513.8688092830672,10,https://medium.com/p/complete-book-list-of-2017-85d1dc79b9d5,0,None,2017-12-29 18:51:00,36.69,9,62,2017-12-27 18:47:00,"['Books', 'Review', 'Reading', 'Book Summary']","Complete Books of 2017 One year, 50 books, innumerable ideas. Ranked in order of impact on my worldview. The world has never been more peaceful than it is now thanks to the civilizing force of strong governments, more trade between countries, increased respect for the role of women in society, the communication ability of mass media, and the ever-greater reliance on rational thought and critical thinking. 2. Sapiens: A Brief History of Humankind by Yuval Harari Humans distinguished themselves from all other animals on Earth by our communication abilities and our belief in “collective fictions”, constructs such as money, religion, and laws that only exist because many individuals believe in them together. 3. Homo Deus: A Brief History of Tomorrow by Yuval Harari Having conquered war, plague, and famine in the second half of the 20th century, humanity will spend the 21st century in a quest for superhuman abilities through genetic enhancements and human-machine integration. 4. Hands-On Machine Learning with Scikit-Learn and Tensorflow by Aurelion Geron Machine learning is now accessible to all with open-source, easy-to-use frameworks, and anyone can learn to build practical machine learning implementations with minimal resources. 5. Thinking, Fast and Slow by Daniel Kahnemann In our daily lives, we often make irrational decisions by employing shortcuts as a result of our automatic, quick system one instead of using our slow, logical system two. 6. Where Good Ideas Come From: The Natural History of Innovation by Steven Johnson Ideas emerge not from deep within a specific field, but at the intersection of different areas of study, which is why the internet and college campuses are such fertile grounds for innovation. 7. The Worst Journey in the World by Apsley Cherry-Garrard A first-hand account of Robert Scott’s 1912 expedition to be the first person to the South Pole which ended with Scott reaching the pole one month after Roald Amundsen and perishing on the return journey due to a series of mistakes and poor decisions. 8. Algorithms to Live By: The Computer Science of Human Decisions by Brian Christian and Tom Griffiths We can incorporate computer science solutions to problems such as constraint satisfaction and the explore-exploit into our daily decision-making processes to improve our lives. 9. Radical Technologies: The Design of Everyday Life by Adam Greenfield A well-thought-out review of the perils and potential benefits of emerging technologies such as 3-D printing and cryptocurrency, and a call for us to think about what we give up — mainly privacy and autonomy — when we uncritically adopt new technologies into our lives. 10. Better: A Surgeon’s Notes on Performance by Atul Gawande Medicine, as with any other field of human endeavor, is not perfect, but that should not prevent us striving to improve our methods and surpass our previous achievements. 11. Watership Down by Richard Adams An inspiring tale of survival and kinship focusing on a group of rabbits (yes really) that manages to encompass the entire range of human emotions in a page-turning narrative. 12. The Culture of Fear by Barry Glasner Despite unequivocal evidence the world is becoming a healthier, safer, richer, and more equitable place, people continue to believe everything is going downhill due to increased news coverage of negative events and no mention of the improvements made every single day. 13. The World Until Yesterday: What Can We Learn from Traditional Societies by Jared Diamond People in the modern, industrialized world have much to learn about child-rearing, elder care, and wellness from traditional tribes and chiefdoms. 14. Nudge: Improving Decisions about Health, Wealth, and Happiness by Richard Thaler and Cass Sunstein The findings of behavioral science can be used to improve both our personal lives and our societies if we accept and account for the fact that humans do not behave rationally and may need coaxing in order to make the optimal decisions for their well-being. 15. Deep Learning with Python by Francois Chollet Deep learning — mainly neural networks — is accessible through frameworks such as Keras in Python that abstract away much of the theory and let anyone with basic programming skills build state-of-the-art models. 16. Superintelligence: Paths, Dangers, Strategies by Nick Bostrom A shrill warning about the potential catastrophic effects of general artificial intelligence if it is not developed with the proper motivations and safeguards. 17. The Elephant Vanishes by Haruki Murikami A collection of short stories in the magical realism genre which describe worlds that are disconcertingly similar to ours but feature unexpected elements just beyond the realm of our physical reality. 18. How the Mind Works by Steven Pinker A thorough and enjoyable examination of various facets of human psychology such as emotions, vision, feminism, rationality, and consciousness through the lens of evolution. 19. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy by Cathy O’Neil Data science is supposed to enable equality through objective decisions, but models are ultimately built by humans with certain views and fed data that reflects the current state of the world, leading to a continuation of inequality because as the saying goes: “bias in, bias out.” 20. How Not to be Wrong: The Power of Mathematical Thinking by Jordan Ellenberg An accessible and entertaining look at how we can overcome many of the fallacies we make in areas such as probability and optimization using mathematical concepts. 21. The Borderlands of Science by Michael Shermer Science has undoubtedly been the greatest invention of humankind, but it has also been used to justify atrocities and sell worthless “cures” to the unaware public through “scientifical” language. 22. Complications: A Surgeon’s Notes on an Imperfect Science by Atul Gawande An honest journey with numerous illustrative stories through the uncertainty and mistakes that are inherent to medicine and which keep doctors constantly working to improve care. 23. Tribe: On Homecoming and Belonging by Sebastian Junger People in stressful situations, such as the bombing of London during WWII, emerge with a strong connection to those around them and a sense of community that is nearly absent from modern life. 24. Winter World: The Ingenuity of Animal Survival by Bernd Heinrich A surprisingly intriguing look at the varied ways animals survive harsh winters, from hibernating bears to the Kinglet bird species which spend the entire day eating food and the entire night shivering to stay alive. 25. Big Data: A Revolution That Will Transform How We Live, Work, and Think by Viktor Mayer-Schonberger and Kenneth Cukier Massive amounts of data will change our society in three ways: we will expect that all data, no matter how minor, should be collected; we will care less about the quality of individual data points because we can look at the aggregate; and we will place greater emphasis on finding correlations and less importance on establishing causation. 26. Once a Runner by John L. Parker Jr. A story about a collegiate cross country team that inspired me to start running seriously and complete several ultramarathons. 27. Packing for Mars: The Curious Science of Life in the Void by Mary Roach Humans are definitely not meant to live in space at all, and the efforts to put astronauts into space and keep them alive are equal parts incredible and hilarious in their ingenuity. 28. The Second Machine Age: Work Progress and Prosperity in a Time of Brilliant Technologies by Wrik Brynjolfsson and Andrew McAfee In the first machine age (the Industrial Revolution) machines were a complement to humans, but in the second, computer-driven machine age, humans cognitive abilities will be surpassed and we need to prepare for an economy in which people have no role. 29. Flash Boys: A Wall Street Revolt by Michael Lewis A detailed accounting of the rise of algorithmic trading, in which machines execute billions of trades a second, focused on the major players, both those who make millions, and those trying to reform the industry. 30. How We Decide by Jonah Lehrer Even though we know what choices we should make, we often choose the incorrect option because of cognitive shortcomings and unconscious decision-making, but we can take steps to make smarter choices. 31. Why People Believe Weird Things by Michael Shermer Stories of the paranormal, faith healing, or extrasensory perception are seductive even to highly-educated people because they want to think there is more to reality than we can perceive, and these false beliefs should be patiently addressed before they can affect public policy. 32. Letter to a Christian Nation by Sam Harris Religion, in particular Christianity, is a negative force in America and its influence in all aspects of society puts our country at a severe disadvantage compared to more secular countries where decisions are based on reason. 33. Kitchen Confidential by Anthony Bourdain An entertaining trip through Bourdain’s experiences as a chef, from his first jobs, to his role as a “celebrity” chef and what everyone should really know about the people who prepare their food. 34. Ultramarathon Man: Confessions of an All-Night Runner by Dean Karnazes An auto-biography of a normal guy who discovered he had an incredible talent for running extreme distances and has since completed some of the most grueling running competitions in the ultrarunning world. 35. Deep Survival: Who Lives, Who Dies, and Why by Laurence Gonzales The people who survive in extreme conditions and crises often do not seem to be the most physically capable, but rather are able to maintain control over their emotions, keep a positive attitude with regards to their situation, and make rapid decisions. 36. Mountains of the Mind: A History of a Fascination by Robert MacFarlane During the 19th century, mountains went from foreboding places inaccessible to humans to targets for full expeditions and casual hikers alike looking to conquer the heights and experience a sense of the sublime beyond ordinary daily toils. 37. Grunt: The Curious Science of Humans at War by Mary Roach In their insatiable quest to produce better weapons and soldiers, countries will go to extreme lengths and conduct ridiculous experiments to gain even a minor advantage. 38. Sherlock Holmes: The Published Apocrypha by Sir Arthur Conan Doyle and Jack Tracy Several stories by Conan Doyle and others about the famous detective, including several parodies, that do not appear in the standard complete collections of mysteries. 39. Why We Run: A Natural History by Bernd Heinrich A blend of a personal account of the author’s (a naturalist by trade) experience training for and winning the 1981 100-km ultramarathon in Chicago with an examination of other animals that run and/or complete astonishing journeys of their own. 40. High Crimes: The Fate of Everest in an Age of Greed by Michael Kodas An expose of the culture of corrupt guides and unsafe practices that are a product of an influx of rich climbers willing to pay whatever price to get to the top of the world’s tallest mountain. 41. One Summer: America, 1927 by Bill Bryson In his typical entertaining and informative style, Bryson examines a time in America when many monumental events were colliding including Lindbergh's flight across the Atlantic, the seeds of the Great Depression taking root, Prohibition was in full effect but openly flaunted, and mass culture was first starting to capture the nation’s attention. 42. The Time Machine by H.G. Wells A classic science fiction tail of a time-travelor who visits the far future to find a strange society in which humans have evolved into two classes with disconcerting consequences. 43. The Martian Chronicles by Ray Bradbury A collection of science fiction tales about the red planet and human attempts to colonize it despite encountering issues with the native inhabitants. 44. Bad Science by Ben Goldacre Science is often high-jacked by companies with products to sell who want to lend a veneer of credibility to their claims, and we need to be vigilant in identifying and correcting improper uses of science, as well as encouraging the proper application of the scientific method through practices such as randomized controlled studies and peer review. 45. The Art of Invisibility by Kevin Mitnick The former hacker turned security consultant Mitnick provides practical advice for maintaining anonymity and security both in-person and emphasizes that privacy is still possible but only for those willing to give up convenience and become extremely vigilant. 46. The Lost City of Z by David Grann In 1925, Percy Fawcett set off into the Amazon jungle to find the rich paradise of Z and was never seen again, prompting a series of searches throughout the decades including one conducted by the author in which he finds no additional information about the mystery. 47. The Hero of Ages by Brandon Sanderson Part three in the Mistborn stereotypical fantasy series in which the heroine must save the world from a dark force using special powers bestowed only a select few members of the population. 48. The Elegant Universe: Superstrings, Hidden Dimensions, and the Quest for the Ultimate Theory by Brian Greene An overview of the issues currently at the forefront of theoretical physics that touches on everything from higher dimensions to subatomic particles in a fairly accessible manner. 49. The Andromeda Strain by Michael Crichton A science fiction narrative in which a classified military satellite brings a space-virus to Earth that causes havoc and threatens to spread unless a select group of scientists can find a cure. 50. The Island of Doctor Moreau by H.G. Wells The shipwrecked protagonist finds himself stuck on an island with a crazy scientist whose specialty is creating animal-human hybrids that he can barely control. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator The Better Angels of Our Nature: Why Violence has Declined by Steven Pinker",Complete Books of 2017,4,published,169,2509,0.029095257074531686,2,0,0,0,0,0
116,126,511.08143621978013,11,https://medium.com/p/top-books-of-2017-c338630b99ad,0,None,2018-01-01 13:45:00,43.52,19,141,2017-12-27 18:48:00,"['Reading', 'Books', 'Review', 'Reflections']","Top Books of 2017 The 6 books that defined my year. 2017 was a struggle. It seemed as if every event in the news was designed to prove that that those of us bold enough to believe in the goodness of humanity are ludicrous. To cope with the turmoil of 2017, I abandoned the negativity-driven daily news cycle and shifted my source of information to books that look at the large picture spanning years or decades. Remarkably, what I found was not a downward spiral accelerating in the 21st century, but a story of gradually improving human conditions that shows no signs of stopping. Consider that newspapers could have run the headline “137,000 fewer people living in extreme poverty today than yesterday” every single day for the past 25 years and it would be entirely true. Yet they do not because the entire media story— on both sides of the aisle — is predicated on the concept that things are getting worse. It is only by extricating ourselves from this doom-and-gloom shouting and looking at the longer term that we can observe the upward trends. If nothing else, 2017 demonstrated that our view of the world is entirely dependent on what we choose to expose ourselves to. At the end of the year I can honestly call myself a rational optimist because of a worldview shaped by the following set of six influential books. Conditions are not guaranteed to improve on their own, but with continuous efforts by doctors, engineers, scientists, philanthropists, and others, we can continue on the road to a more prosperous, sustainable, and equitable world. The world has been steadily getting more peaceful for the past millennium and the 21st century is on track to be the least violent of any in human history. This must be the single most important but least understood fact of modern times. When I tell people this astonishingly positive information, I am almost always greeted not with cheerful acceptance, but hostile disbelief. People are entirely unwilling and moreover seem to not want to believe that the world is actually getting better! The simple reason is while the prevalence of violence has demonstrably been declining, the media coverage of conflict has soared in recent years. Crime and tragic stories drive headlines and human nature tends to place greater weight on negative information leading to a severe misrepresentation of the state of the world among the public. In the midst of this deception, Pinker’s work is a resounding refutation of the media’s worldview. The thesis of this masterpiece is violence has been declining on both a worldwide and individual scale for at least 1000 years and shows no signs of stopping. In case anyone doubts this hypothesis, the first third of the book is devoted to documenting six trends of declining violence with evidence for every trend presented in occasionally excruciating detail. Although Pinker gets caught up in a few too many lists and statistics, the overall message is clear: every kind of conflict, from state-sponsored wars and genocides, to personal violence such as rape and domestic abuse, has declined to historical rates. What could possibly be driving such an extraordinary movement? Pinker identifies five traits (inner demons) which we have brought under control, four human behaviors that reduce violence (the “better angels” of the title) and five historical factors. Of the three different categories, the five historical forces are the most convincing for explaining the rise of peace and are worth briefly mentioning here: I. Powerful state governments: known as the Leviathan theory, the idea that strong central ruling bodies reduce violence stretches back to at least 1651 and Thomas Hobbes’ book of the same name. The justice system is often (rightly) maligned but the evidence suggests it does work to reduce crime. The best example is the extraordinary decrease in violent crime in New York since 1990 with an increased police presence and more aggressive tactics. The nearly 90% reduction in crime likely has several factors, but research has demonstrated that adding more police officers (in a data-driven manner) will reduce crime rates. Police and the national military have “a monopoly on the legitimate use of force” which means individual citizens can rely on the state to enforce laws rather than meting out their own punishments. The revenge-driven cycle of violence is cut short by the state-backed authority of the justice system . Despite all their flaws, central governments and police forces create a more civil society because they create and enforce laws that tip the risk-reward balance away from carrying out a crime. II. Increased trade between countries: In simple economic terms, our neighbors are worth a lot more to us alive than dead. We can trade with any country around the world to acquire resources we may be lacking which is a much better option than launching a war. Moreover, while trade used to revolve around exchanging material goods, which means there was always an option to invade a country to extract its wealth, trade in the 21st century is based on exchanging knowledge, which requires citizens to be alive to to be of any value. Modern technology has significantly decreased the barriers to international trade and the products we use every single day depend on a massive worldwide commerce system that requires us to remain on friendly terms with all participants. III. Female empowerment: while it may seem superficial to claim that women are less violent than men by nature, Pinker is not shy about admitting there are fundamental biological differences between men and women. These differences are expressed as a decreased predilection for violence on the part of women. Female empowerment means more than just equal rights: it encompasses an entire range of progress, from more women in the workplace to leadership roles at the national level. As women have gained a louder voice in society (although it still is far too repressed), and the rights of females have expanded, they have been able to temper violent inclinations, and as Pinker notes, feminism has been a great benefit to all genders. IV. Rise and spread of mass culture: the spread of books, movies, television, and information via the internet allows us to experience the world from different cultural perspectives. It is more difficult to perpetrate a crime against someone with whom you identify, and mass media has shown us the common humans traits we all share. It may sound like your high school lit teacher’s fantasy, but reading fiction leads people to be more emphatic to those from different cultures. In decades past, citizens of a country relied on authority figures, such as the government or religious institutions, for information about the outside world. A nation intent on conquering another will portray the citizens of the target country as ruthless savages, a myth that can now be dispelled with a simple internet search that will let us live a day through anyone’s eyes. Mass culture is denigrated for supposedly corrupting our morals, but when it accurately portrays varied cultures, it demonstrates the basic fact that all humans share the same desires, needs, and emotions making it easy for us to see the absurdity in attacking anyone outside (or within) our own culture. V. Increased reliance on rational thinking: in what is called the “escalator of reason,” western society has shifted the basis for decision-making from superstition to logic. Religions have historical been among the worst perpetrators of conflict from the personal to the national level because are able to justify restrictions to their followers by portraying people of different beliefs as fundamentally inferior (as Voltaire said: “those who can make you belief absurdities can make you commit atrocities”). Through a rational decision-making process we can see this is simply not true and we can play out hypothetical situations where we swap places with those we would hurt. The conclusion from any logical examination of our situation is it makes more sense to cooperate than to fight. Conflict is a zero-sum game, in which one person only gains at the expense of another, but commerce is a positive-sum encounter in which both parties emerge better off. As governments become more secular and laws are passed based on reason rather than superstition, violence is no longer viewed as the primary tactic for conflict resolution. Overall, The Better Angels of Our Nature was an ideal counterpoint to the doom-and-gloom media stories that dominated the public consciousness in 2017. If we can step back and take the long view, there is still reason for optimism and although an individual year may be a step back, the large-scale trend is forward. However, as Pinker points out, we must keep in mind that this trend does not happen on its own, but is driven by the concerted efforts of individuals and teams who believe in the noble vision of a better world for all. We have a choice between sitting by and idly declaring the world is going to pieces, or we can take action to ensure humanity continues on its peaceful trajectory. 2. Sapiens: A Brief History of Humankind and Homo Deus: A Brief History of the Future by Yuval Harari I enjoy history books that focus not on endless lists of names, dates, and events (which is how history is almost always taught in schools), but on the larger ideas driving progress. These books are not recollections of past events so much as they are “theories of history.” Taken together, Sapiens and Homo Dues take on the fascinating tale of humanity, documenting where we came from and where we can expect to go in the next century. Sapiens records the fascinating tale of humanity, explaining how our species, Homo sapiens, was able to become the dominant creature on Earth. In Harari’s view, our main advantages over all other animals are our communication abilities and our belief in “collective fictions.” The first point is self-explanatory — communication allows us to coordinate efforts and greatly surpass the capabilities of a single individual — but the latter idea is more intriguing. Our largest institutions including nations, religion, money, laws, and corporations do not actually exist. They are entirely fabricated entities that are given power only because many people believe in them. Money is inherently worthless, yet we can exchange it for any goods and services because everyone agrees that it should have value. These unifying myths are able to induce individuals in societies to work together towards massive goals. When enough people buy into a collective fiction, it can affect reality because it is far easier to work together with someone who shares your political or religious beliefs. This is especially true when you are tasked with a project that you may never personally benefit from but that will leave the community as a whole better off. Harari describes how agriculture severely decreased individuals’ well-being but increased the viability of the entire species (Harari calls agriculture “humanity’s biggest fraud’). Unique communication capabilities allowed humans to work together and shared myths motivated us to take on grand projects leading to our conquering of species that individually were stronger and even smarter than Homo sapiens. Once we find out how we got here, we naturally will want to know where we are going next. Harari takes on this topic in Homo Deus, a fascinating blend of non-fiction concerning our current state and speculative fiction documenting possible futures. The initial main argument is in the second half of the 20th century, humanity solved the three big problems of war, plague, and famine that historically occupied our attention every waking moment. Harari summarizes this thesis: “For the first time in history, more people die today from eating too much than from eating too little; more people die from old age than from infectious diseases; and more people commit suicide than are killed by soldiers, terrorists and criminals combined.” Once we accept this idea, we realize the need for some other massive project to occupy us in the 21st century. Harari provides several compelling ideas all under the framework that we will work to surpass our biological human limitations through genetic engineering and human-machine integration. At first, we will use technology to merely correct our inherent deficiencies — think of diseases and birth defects — but it will soon move on to augmenting our abilities. Humans are never satisfied with the current state of the world, and we will not hesitate to bypass evolution in a bid for longer lives with superhuman abilities. Harari is rightly concerned about the implications of engineering our future selves, and also critically examines the implications of developing machine intelligence. The basic human experience of life can be summed up as follows: we take in information through our senses, process it using a set of rules learned through experience, and produce an output taking the form of action. This is what is known as an algorithm and computers are incredibly good at implementing. We already have computers that far exceed human abilities in domains such as reading medical images and driving vehicles that are simple cases of sensing, processing, acting. The next frontier for machine intelligence is creative pursuits such as writing and music composition which actually are fairly simple acts of pattern recognition and emulation. We have and will continue to improve our lives by harnessing machine intelligence, but Harari also cautions we could be rendered obsolete by computers that far surpass us in mental capabilities leaving masses of unemployed people with nothing to occupy their time. Personally, I think computers and artificial intelligence will not supplant human workers but will allow us to do more valuable work by automating the repetitive tasks we would not want to do anyway. However, it is beneficial to remain skeptical about embracing any new technology without question and Harari’s critical view is always in the back of my mind when I come to rely on Google for yet another daily activity. Many of the ideas touched on in Homo Deus are intriguing enough that I still find myself thinking about the book six months later, and I have had many great dinner conversations inspired by this book. To take a single example, Harari brings up the topic that we do not know the answer to the seemingly simple question of what makes us happy. He lays out the general equation: happiness = reality — expectations which could explain why as our lives become objectively better, our subjective level of satisfaction has not increased. Moreover, we have two selves: the narrative or remembering self, and the experiencing self. The experiencing self comprises our thoughts and emotions while we are engaged in an activity, while our narrating self weaves individual experiences into our life story. Each self is satisfied by different emotions and therefore to maximize our happiness, each side would tell us to take conflicting actions. Ultimately, the narrator of our lives has far more impact because we spend more time remembering events rather than taking part in them which means that we can gain long-term satisfaction by having experiences that fit into our internal narrative. Our lives are simply stories we tell ourselves, and any meaning or happiness we get derives from how our actions fit into our story. Some people choose to make their story a tragedy, while others like to live a triumph, and all events will be interpreted in the context of a narrative that we get to decide. Homo Deus is not afraid to take on these fundamental questions, and while the book occasionally veers into speculative science fiction, it provides a compelling examination of the human condition and gives a reasoned prediction for the future that seems just plausible enough to come to pass. We often don’t think more than a few days into the future, and it was refreshing to read a work that goes beyond the news cycles, making bold predictions about where we could be heading. Even if Harari turns out to be wrong (and he admits it is possible), the exercise of contemplating the future is critical because it allows us to re-evaluate and correct our actions if we do not like our current path. 3. Hands-On Machine Learning with Scikit-Learn and Tensorflow by Aurelien Geron Any textbook is important for the specific topics covered, however, this work makes the list not only for the details, but also for the central tenet that machine learning is both understandable and accessible enough for anyone to learn on their own. Now is an amazing time to get into the machine learning field because there are so many resources available (free of charge) that anyone with a decent laptop and an internet connection can become a skilled machine learning engineer without ever setting foot in a college classroom. The internet has not yet democratized education, but one area it has made accessible for everyone is machine learning and artificial intelligence. Geron emphatically demonstrates this by focusing on two popular, open-source libraries for the Python programming language, Skicit-Learn and TensorFlow. Both libraries (a name for a set of functions you can simply insert into your code so you don’t have to solve a problem that already has a solution) implement state-of-the-art algorithms, but are open for anyone to use and contribute to! I went into the book expecting to be challenged and thought I would give up before I reached the end because the topics would be beyond my abilities. Not only did I make it all the way through, I also completed nearly all of the end-of-chapter exercises and spent many pleasurable hours working through solutions (which Geron provides for free — and constantly updates — on the book’s GitHub page). This is not a reflection of any superior learning ability on my part, but from the straightforward manner in which topics are covered in the book and then implemented in code. Hands-On lives up to its subtitle: “Concepts, Tools, and Techniques to Build Intelligent Systems” as it thoroughly addresses the theory and also includes practical worked-out examples that the reader can then adapt for their own machine learning problems. Since finishing this book, I have used several of the concepts for both in-class projects and as part of my involvement with a data science research project at Case Western Reserve University. I have also taken graduate level data science and AI courses and found myself far ahead of my peers from this book alone. Again, this is not to emphasize my abilities but to highlight how you can get an education surpassing that of college courses by thoroughly working through resources available to anyone. Furthermore, while I have found the number of options for getting started in a field can be overwhelming and make choosing a place to start difficult, this book alone will get you up to speed in the ideas behind machine learning algorithms and how to implement them in the most popular language for data science. I’ll skip detailing the contents of the book, but it includes chapters on all the relevant ideas in both machine learning and in the increasingly valuable field of artificial intelligence (neural networks). Overall, this book profoundly altered my views on the accessibility of machine learning and has already had a measurable positive impact on my college work and research projects. Geron and other machine learning authors are doing a fantastic job of opening up a vital and exciting field to anyone willing to take the time to study on their own. Hands-On Machine Learning is the most enjoyable textbook I have ever opened, and the only one I have made it through completely on my own for the learning experience. 4. Thinking, Fast and Slow by Daniel Kahnemann The central tenet of behavioral economics — that humans do not make completely rational decisions — seems so obvious that it is a wonder it took until 1979 for the field to get started. It was in that year that Kahnemann and his research partner, Amos Tversky, released a paper titled Prospect Theory which described several ways in which humans act against their best interests. Since then, the field has had a profound influence in many areas, from government policy (the “Nudge Unit” in the UK) to professional sports leagues (Billy Beane, the central figure in Moneyball, was taking advantage of many mistakes other managers make). Thinking, Fast and Slow is Kahnemann’s comprehensive summary of the research that gave rise to behavioral economics. At the heart of this research is the idea that humans have two decision-making system, the quick, intuitive system one, and the slower, rational system two. System two is capable of making logical choices, but it is lazy leading to an over reliance on quick judgments from system one. Our fast responses rely on several mental shortcuts, or heuristics, that lead us to make choices against our best interests. The three main heuristics are intriguing enough to warrant mention here: I. Anchoring refers to our tendency to place too much weight on the first piece of information we hear when making a decision. The best example is when negotiating a price; the first number on the table has an inordinate impact on the final price. We tend to make insufficient adjustments from the initial offer, especially in areas where we have little experience. Kahnemann and Tversky found this effect repeated when answering many numerical questions even when the anchor number had nothing to do with the question. II. The representativeness bias means we judge an event based on how much it resembles another related event. This leads to errors such as base rate neglect, where we fail to take into account the background probability when evaluating the chance of unlikely events, and the conjunction fallacy where we judge the likelihood of two related events to be more likely than either event by itself, an impossibility. In our daily lives, this causes real harm when we do not consider sample sizes in medical trials, or in the classic gambler’s fallacy, where a gambler who has lost many bets in a row believe they are due for a win. III. Due to the availability bias, we associate the prevalence of an event by how readily we can recall occurrences of it. People vastly overestimate the dangers of flying because every single plane crash is exhaustively covered by the press whereas mundane dangers such as driving a car is commonplace therefore does not appear in the news. The more vivid the example, the more likely it will be recalled which is why people ignore vast amounts of data in the face of a single personal story. Behavioral economics may seem to be a collection of academic concepts, but the cognitive mistakes we make have real impacts on our lives such as when we fail to save enough for retirement because we discount the future or continue to poor resources into a negative investment because of the sunk-cost fallacy. However, we are not complete slaves to our error-prone system one, and we can learn to engage system two to make better choices especially in areas where we have little experience or the choice has significant consequences. Kahnemann is careful to not blame system one, as making rapid decisions came in great use at many points in our evolutionary history and continues to help us navigate our lives today. Nevertheless, the decisions we make now are more complex and require processing vast amounts of information, something our ancestors would never have encountered.The next time you need to make a critical decision, make sure to engage your system two, gather as much info as possible, and ideally consult with someone who has more experience in the domain. Academic research can often seem theoretical but Kahnemann makes it accessible, and with how far behavioral economics has crept into modern life, it is crucial that we are able to at least recognize basic fallacies that directly impact our everyday lives. 5. Where Good Ideas Come From by Steven Johnson The best innovations come not from deep within a narrow field, but from the overlapping areas between domains. Johnson calls this space “the adjacent possible,” where ideas are free to interact and combine. It is the ability to make connections between seemingly disparate research that leads to the greatest innovations and the inventors of a technology are often not the best ones to put it to use because they are focused on a specific use-case that might rather than the area where it can have the largest impact. Moreover, great ideas do not occur instantaneously, but are the product of decades of slow, steady progress. The lone genius in the lab myth is just that, a story told in hindsight that attempts to explain away invention as a magical moment rather than as a drawn out process. The gradual process of discovery is what allow connections between different ideas to be formed. An additional point made in the book is that collaboration is often a greater driver of innovation than competition which can restrict the flow of ideas and prevent concepts from mixing together. Finally, platforms such as the internet that encourage collaboration are fertile grounds for launching new products or technologies that can then build on top of each other. The larger and more diverse the network, the more opportunities for a breakthrough there will be. How can you have more good ideas? First, surround yourself with a diverse crowd of people who love to share their ideas. Second, work to make connections between adjacent fields rather than getting too fully immersed in a specific area. Finally, don’t be afraid to make mistakes! Many great innovations were the results of what at first appeared to be failures. Alexander Fleming discovered penicillin because a petri dish of bacteria he was working on became contaminated by mold that ended up killing the bacteria. Where are the best places for innovation? The internet and college campuses are ideal for combining new concepts because of the diversity of fields concentrated in one small area. However, make sure not to surround yourself with people who think alike because discussions are another driver of progress. The takeaway from Johnson’s book is that good ideas die in isolation. The more you share and interact, the more ideas will flow around you, and subsequently, you will have a greater chance to harness one of them and turn it into a successful innovation. Don’t be afraid if an idea does not pay off immediately, and keep a wide range of concepts in the back of your mind ready for the right opportunity, platform, or team to take it further. Creativity is certainly part of the idea-creation process, but there are also actionable steps you can take to increase the number of ideas to which you are exposed and hence the innovations you can make. At the very least, if you surround yourself with diverse individuals, you’ll have more interesting conversations! Without a doubt 2017 was a trying year. However, by switching my source of information from news to books and focusing on the long term rather than the day-to-day incessant shouting of the media, I was able to come out of the year more optimistic about the world. It may seem as if things are going downhill, but at the worst it is a temporary situation that is already showing signs of reversal with recent increases in activism and individuals boldly stepping forward to confront injustice. At the end of 2017 thanks to the books I read, I see the long term trend of humanity continuing towards a more equitable society, I see us using technology to augment our abilities rather than replace us, I see the internet making exciting new careers in machine learning and artificial intelligence accessible to anyone, I see the findings of behavioral economics implemented at the national and individual level to encourage people to make better decisions, and I look forward to taking advantage of the diverse atmosphere of ideas that only a college campus can provide. Our lives are fundamentally shaped by the stories we hear from others and the tales we tell each other, and if we choose to focus on people and teams working to improve the world, we can not only believe in a better future, but work to make it a reality. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",Top Books of 2017,4,published,324,5160,0.02441860465116279,4,0,0,0,0,0
106,1000,505.81033263825236,121,https://towardsdatascience.com/improving-random-forest-in-python-part-1-893916666cd,6,Towards Data Science,2018-01-06 20:15:00,22.78,17,7916,2018-01-03 21:38:00,"['Machine Learning', 'Python', 'Data Science', 'Learning']","Improving the Random Forest in Python Part 1 Gathering More Data and Feature Engineering In a previous post we went through an end-to-end implementation of a simple random forest in Python for a supervised regression problem. Although we covered every step of the machine learning process, we only briefly touched on one of the most critical parts: improving our initial machine learning model. The model we finished with achieved decent performance and beat the baseline, but we should be able to better the model with a couple different approaches. This article is the first of two that will explore how to improve our random forest machine learning model using Python and the Scikit-Learn library. I would recommend checking out the introductory post before continuing, but the concepts covered here can also stand on their own. How to Improve a Machine Learning Model There are three general approaches for improving an existing machine learning model: These are presented in the order in which I usually try them. Often, the immediate solution proposed to improve a poor model is to use a more complex model, often a deep neural network. However, I have found that approach inevitably leads to frustration. A complex model is built over many hours, which then also fails to deliver, leading to another model and so on. Instead, my first question is always: “Can we get more data relevant to the problem?”. As Geoff Hinton (the father of deep neural networks) has pointed out in an article titled ‘The Unreasonable Effectiveness of Data”, the amount of useful data is more important to the problem than the complexity of the model. Others have echoed the idea that a simple model and plenty of data will beat a complex model with limited data. If there is more information that can help with our problem that we are not using, the best payback in terms of time invested versus performance gained is to get that data. This post will cover the first method for improving ML models, and the second approach will appear in a subsequent article. I will also write end-to-end implementations of several algorithms which may or may not beat the random forest (If there are any requests for specific algorithms, let me know in the comments)! All of the code and data for this example can be found on the project GitHub page. I have included plenty of code in this post, not to discourage anyone unfamiliar with Python, but to show how accessible machine learning has become and to encourage anyone to start implementing these useful models! As a brief reminder, we are dealing with a temperature prediction problem: given historical data, we want to predict the maximum temperature tomorrow in our city. I am using Seattle, WA, but feel free to use the NOAA Climate Data Online Tool to get info for your city. This task is a supervised, regression machine learning problem because we have the labels (targets) we want to predict, and those labels are continuous values (in contrast to unsupervised learning where we do not have labels, or classification, where we are predicting discrete classes). Our original data used in the simple model was a single year of max temperature measurements from 2016 as well as the historical average max temperature. This was supplemented by the predictions of our “meteorologically-inclined” friend, calculated by randomly adding or subtracting 20 degrees from the historical average. Our final performance using the original data was an average error of 3.83 degrees compared to a baseline error of 5.03 degrees. This represented a final accuracy of 93.99%. Getting More Data In the first article, we used one year of historical data from 2016. Thanks to the NOAA (National Atmospheric and Oceanic Administration), we can get data going back to 1891. For now, let’s restrict ourselves to six years (2011–2016), but feel free to use additional data to see if it helps. In addition to simply getting more years of data, we can also include more features. This means we can use additional weather variables that we believe will be useful for predicting the max temperature. We can use our domain knowledge (or advice from the experts), along with correlations between the variable and our target to determine which features will be helpful. From the plethora of options offered by the NOAA (seriously, I have to applaud the work of this organization and their open data policy), I added average wind speed, precipitation, and snow depth on the ground to our list of variables. Remember, because we are predicting the maximum temperature for tomorrow, we can’t actually use the measurement on that day. We have to shift it one day into the past, meaning we are using today’s precipitation total to predict the max temperature tomorrow. This prevents us from ‘cheating’ by having information from the future today. The additional data was in relatively good shape straight from the source, but I did have to do some slight modifications before reading it into Python. I have left out the “data munging” details to focus on the Random Forest implementation, but I will release a separate post showing how to clean the data. I use the R statistical language for munging because I like how it makes data manipulation interactive, but that’s a discussion for another article. For now, we can load in the data and examine the first few rows. The new variables are: ws_1: average wind speed from the day before (mph) prcp_1: precipitation from the day before (in) snwd_1: snow depth on the ground from the day before (in) Before we had 348 days of data. Let’s look at the size now. There are now over 2000 days of historical temperature data (about 6 years). We should summarize the data to make sure there are no anomalies that jump out in the numbers. Nothing appears immediately anomalous from the descriptive statistics. We can quickly graph all of the variables to confirm this. I have left out the plotting code because while the matplotlib library is very useful, the code is non-intuitive and it can be easy to get lost in the details of the plots. (All the code is available for inspection and modification on GitHub). First up are the four temperature plots. Next we can look at the historical max average temperature and the three new variables. From the numerical and graphical inspection, there are no apparent outliers n our data. Moreover, we can examine the plots to see which features will likely be useful. I think the snow depth will be the least helpful because it is zero for the majority of days, likewise, the wind speed looks to be too noisy to be of much assistance. From prior experience, the historical average max temperature and prior max temperature will probably be most important, but we will just have to see! We can make one more exploratory plot, the pairplot, to visualize the relationships between variables. This plots all the variables against each other in scatterplots allowing us to inspect correlations between features. The code for this impressive-looking plot is rather simple compared to the above graphs! The diagonal plots show the distribution of each variable because graphing each variable against itself would just be a straight line! The colors represent the four seasons as shown in the legend on the right. What we want to concentrate on are the trends between the actual max temperature and the other variables. These plots are in the bottom row, and to see a specific relationship with the actual max, move to the row containing the variable. For example, the plot in the bottom left shows the relationship between the actual max temperature and the max temperature from the previous day (temp_1). This is a strong positive correlation, indicating that as the max temperature one day prior increases, the max temperature the next day also increases Data Preparation The data has been validation both numerically and graphically, and now we need to put it in a format understandable by the machine learning algorithm. We will perform exactly the same data formatting procedure as in the simple implementation: We can do all of those steps in a few lines of Python. We set a random seed (of course it has to be 42) to ensure consistent results across runs. Let’s quickly do a check of the sizes of each array to confirm everything is in order. Good to go! We have about 4.5 years of training data and 1.5 years of testing data. However, before we can get to the fun part of modeling, there is one additional step. In the previous post, we used the historical average maximum temperature as our target to beat. That is, we evaluated the accuracy of predicting the max temperature tomorrow as the historical average max temperature on that day. We already know even the model trained on a single year of data can beat that baseline so we need to raise our expectations. For a new baseline, we will use the model trained on the original data. To make a fair comparison, we need to test it against the new, expanded test set. However, the new test set has 17 features, whereas the original model was only trained on 14 features. We first have to remove the 3 new features from the test set and then evaluate the original model. The original random forest has already been trained on the original data and code below shows preparing the testing features and evaluating the performance (refer to the notebook for the model training). The random forest trained on the single year of data was able to achieve an average absolute error of 4.3 degrees representing an accuracy of 92.49% on the expanded test set. If our model trained with the expanded training set cannot beat these metrics, then we need to rethink our method. Training and Evaluating on Expanded Data The great part about Scikit-Learn is that many state-of-the-art models can be created and trained in a few lines of code. The random forest is one example: Now, we can make predictions and compare to the known test set targets to confirm or deny that our expanded training dataset was a good investment: Well, we didn’t waste our time getting more data! Training on six years worth of historical measurements and using three additional features has netted us a 16.41% improvement over the baseline model. The exact metrics will change depending on the random seed, but we can be confident that the new model outperforms the old model. Why does a model improve with more data? The best way to answer this is to think in terms of how humans learn. We increase our knowledge of the world through experiences, and the more times we practice a skill, the better we get. A machine learning model also “learns from experience” in the sense that each time it looks at another training data point, it learns a little more about the relationships between the features and labels. Assuming that there are relationships in the data giving the model more data will allow it to better understand how to map a set of features to a label. For our case, as the model sees more days of weather measurements, it better understands how to take those measurements and predict the maximum temperature on the next day. Practice improves human abilities and machine learning model performance alike. Feature Reduction In some situations, we can go too far and actually use too much data or add too many features. One applicable example is a machine learning prediction problem involving building energy which I am currently working on. The problem is to predict building energy consumption in 15-minute intervals from weather data. For each building, I have 1–3 years of historical weather and electricity use data. Surprisingly, I found as I included more data for some buildings, the prediction accuracy decreased. Asking around, I determined some buildings had undergone retrofits to improve energy efficiency in the middle of data collection, and therefore, recent electricity consumption differed significantly from before the retrofit. When predicting current consumption, using data from before the modification actually decreased the performance of my models. More recent data from after the change was more relevant than the older data, and for several buildings, I ended up decreasing the amount of historical data to improve performance! For our problem, the length of the data is not an issue because there have been no major changes affecting max temperatures in the six years of data (climate change is increasing temperatures but on a longer timescale). However, it may be possible we have too many features. We saw earlier that some of the features, especially our friend’s prediction, looked more like noise than an accurate predictor of the maximum temperature. Extra features can decrease performance because they may “confuse” the model by giving it irrelevant data that prevents it from learning the actual relationships. The random forest performs implicit feature selection because it splits nodes on the most important variables, but other machine learning models do not. One approach to improve other models is therefore to use the random forest feature importances to reduce the number of variables in the problem. In our case, we will use the feature importances to decrease the number of features for our random forest model, because, in addition to potentially increasing performance, reducing the number of features will shorten the run time of the model. This post does not touch on more complex dimensionality reductions such as PCA (principal components analysis) or ICA (independent component analysis). These do a good job of decreasing the number of features while not decreasing information, but they transform the features such that they no longer represent our measured variables. I like machine learning models to have a blend of interpretability and accuracy, and I generally therefore stick to methods that allow me to understand how the model is making predictions. Finding the feature importances of a random forest is simple in Scikit-Learn. The actual calculation of the importances is beyond this blog post, but this occurs in the background and we can use the relative percentages returned by the model to rank the features. The following Python code creates a list of tuples where each tuple is a pair, (feature name, importance). The code here takes advantage of some neat tricks in the Python language, namely list comprehensive, zip, sorting, and argument unpacking. Don’t worry if you do not understand these entirely, but if you want to become skilled at Python, these are tools you should have in your arsenal! These stats definitely prove that some variables are much more important to our problem than others! Given that there are so many variables with zero importance (or near-zero due to rounding), it seems like we should be able to get rid of some of them without impacting performance. First, let’s make a quick graph to represent the relative differences in feature importances. I left this plotting code in because it’s a little easier to understand. We can also make a cumulative importance graph that shows the contribution to the overall importance of each additional variable. The dashed line is drawn at 95% of total importance accounted for. We can now use this to remove unimportant features. 95% is an arbitrary threshold, but if it leads to noticeably poor performance we can adjust the value. First, we need to find the exact number of features to exceed 95% importance: We can then create a new training and testing set retaining only the 6 most important features. We decreased the number of features from 17 to 6 (although to be fair, 7 of those features were created from the one-hot encoding of day of the week so we really only had 11 pieces of unique information). Hopefully this will not significantly decrease model accuracy and will considerably decrease the training time. Now we go through the same train and test procedure as we did with all the features and evaluate the accuracy. The performance suffers a minor increase of 0.12 degrees average error using only 6 features. Often with feature reduction, there will be a minor decrease in performance that must be weighed against the decrease in run-time. Machine learning is a game of making trade-offs, and run-time versus performance is usually one of the critical decisions. I will quickly do some bench-marking to compare the relative run-times of the two models (see Jupyter Notebook for code). Overall, the reduced features model has a relative accuracy decrease of 0.131% with a relative run-time decrease of 35.1%. In our case, run-time is inconsequential because of the small size of the data set, but in a production setting, this trade-off likely would be worth it. Conclusions Instead of developing a more complex model to improve our random forest, we took the sensible step of collecting more data points and additional features. This approach was validated as we were able to decrease the error of compared to the model trained on limited data by 16.7%. Furthermore, by reducing the number of features from 17 to 6, we decreased our run-time by 35% while suffering only a minor decrease in accuracy. The best way to summarize these improvements is with another graph. The model trained on one year of training data is on the left, the model using six years of data and all features is in the middle, and the model on the right used six years of data but only a subset of the most important features. This example has demonstrated the effectiveness of increasing the amount of data. While most people make the mistake of immediately moving to a more powerful model, we have learned most problems can be improving by collecting more relevant data points. In further parts of this series, we will take a look at the other ways to improve our model, namely, hyperparameter tuning and using different algorithms. However, getting more data is likely to have the largest payoff in terms of time invested versus increase in performance in this situation. The next time you see someone rushing to implement a complex deep learning model after failing on the first model, nicely ask them if they have exhausted all sources of data. Chances are, if there is still data out there relevant to their problem, they can get better performance and save time in the process! As always, I appreciate any comments and constructive feedback. I can be reached at wjk68@case.edu Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Tune the hyperparameters of the algorithm Try different algorithms Separate data into features (independent varibles) and labels (targets) Convert dataframes to Numpy arrays Create random training and testing sets of features and labels Use more (high-quality) data and feature engineering One-hot encode categorical variables (day of the week)",Improving the Random Forest in Python Part 1,8,published,34743,3504,0.2853881278538813,2,0,1,0,1,1
111,189,504.7952190020371,45,https://towardsdatascience.com/data-science-a-practical-application-7056ec22d004,1,Towards Data Science,2018-01-07 20:37:00,28.18,15,797,2018-01-07 13:18:00,"['Data Science', 'Data', 'Weight Loss', 'Programming']","Data Science: A Personal Application Charting the Great Weight Challenge of 2017 One frustration I often hear from those learning data science is that it is difficult to make the leap from toy examples in books to real-world problems. Any learning process must necessarily start off with simple problems, but at some point we need to move beyond curated examples and into messy, human-generated data. This graphic pretty well sums up what I’ve gone through in my data science education, and although I’m not yet over the cursing mountain, I have climbed part of the way up through trying (and often failing) at numerous projects with real data: The best way to ascend this curve is build up your confidence over time, and there is no better place to start than a project directly related to your life. This post will demonstrate a straightforward application of data science to my health and that of my dad, a personal problem with clear benefits if there ever was one! The good news is that in order to apply data science for your personal benefit, you don’t need the data or resources of a massive tech firm, just a consistent set of measurements and free open-source analysis tools such as R and Python. If you stop to look, you will find data streams all around you waiting to be tracked. You might step onto a scale every morning and, depending on the outcome, congratulate or denigrate yourself, and then forget about it until the next day. However, taking a few seconds and recording a once-daily weight in a spreadsheet can yield a useful and clean dataset in a few months (and increases the chances that you will hit your goal). This data is perfect for allowing you to develop your data science skills on a real problem. At its core, data science is fundamentally about drawing intelligence from data and this post is an illustration of how data science can yield insights that improve real-world outcomes. Data science is a multi-disciplinary field — composed of computer science, statistics, and engineering — but the most vital aspect is also the most overlooked: communication. Your analysis could be great, but at the end of the day, managers, professors, and the general public care more about the final result than the exact methods. Being able to clearly communicate an answer to a data science question and the limitations of an analysis is a valuable asset in any data science toolbox. In this post, I have left out all of the code (done in R) used to create the graphs in order to focus on the results and what we can learn from them, but all code is available on the project GitHub page for anyone who wants to see how the magic happens. Likewise, the data is on GitHub and on Google Drive as a csv file for those wanting to follow along. I have also tried to provide resources on specific topics for those wanting to learn more. Now, it’s time to dive into the data science of the Great Weight Challenge of 2017! Disclaimer: First, all data presented in this project is real! My dad and I both believe in open data (up to a point) and we definitely don’t care about making ourselves look more successful than we are. Second, I will not try to sell you any weight loss products (although I did consider calling this post “How to Lose Weight with Data Science”). The Great Weight Challenge of 2017 Having good-naturedly teased each other for years about our respective struggles — mine to gain weight, and his to lose weight — my dad and I decided the best solution was a weight change competition. My dad’s performance would be measured by pounds lost, and mine by pounds gained. The only rules were: we had to weigh in once a day, the competition started on August 18 and ended January 1, 2018, and the loser had to pay the winner double the winner’s weight change in pounds. Because this is a real-world problem with actual people, neither the first nor the second rule was completely upheld! Nonetheless, over the course of the competition (which actually ended Jan 6), we gathered over 100 data points each, more than enough to yield many intriguing conclusions. We both decided to be as open about the challenge as possible and told family and friends about the competition to force us to follow through. After receiving plenty of well-intended advice, we drew up respective strategies. I decided to start eating lunching as I had developed an unhealthy habit of skipping the mid-day meal to focus on my work as an intern at NASA. My dad wanted to eat exactly the same diet but reduce portion sizes. This seemed like a wise decision because it meant he did not have to think about dieting, but made the same foods and served them on smaller plates. He also resolved to work on the exercise side by taking long walks, emphasizing the need not for a short-term weight-loss plan, but a healthier overall lifestyle. Results Might as well start with the entire results graph. So that’s it right? The whole competition summed up in one picture. Well, not entirely. It’s a good start, but there is plenty of insight to be drawn from working our way into the data. The lines drawn through the data are models made using the ‘loess’ regression method, while the points are the actual measurements. Right away we can see that both of us went in the right direction! However, this graph obscures a lot of information. We can’t even judge who won! For that we can turn to a plot showing each of our weight changes in pounds from the starting weight. We are using absolute values here so a higher number is better. We can clearly see that while the competition was initially close, my dad (Craig) pulled away at the end and won by a considerable margin. Congrats dad! Another takeaway is that weight measurements are quite noisy. We tried to take data at the same time every day, first thing in the morning, on the same scale, but there are so many factors that influence weight from day-to-day that looking at a single point is meaningless. It is only by examining a series of data points that a trend emerges. Additionally, each of our weight changes seems to resemble a square root relationship or a logarithm. That is, there is an initial rapid gain (or loss) that then levels off over time. This was expected because initially it is quite easy to make progress when you are motivated, but it can be difficult to keep up the momentum. Eventually we both settled into weight plateaus with the very final measurements showing slight signs of improvement that may or may not be trends. One slight issue with this result is it does not take body weight into account. If my dad loses 10 lbs, that is smaller relative to his body weight than if I gain 10 lbs. The next graph also shows changes, but this time in terms of percentage of body weight. Well, if you were rooting for me, things look much better in this graph. My percentage change was larger for most of the competition, and I was ahead until the last day when my dad just edged me in terms of percentage. It is interesting that both of us settled into a total change near 6% of body weight. This might suggest our bodies can fluctuate within a +- 6% range easily, but beyond that, further change is more difficult. Presented below are the final numerical results. Modeling Graphs can show us much information and qualitative trends quickly, but do not answer questions with a quantitative result. For instance, how much weight did each of us gain or lose on average per day? What is our predicted weight one year from now using all the data? These are questions that we must turn to modeling to answer. Simple Linear Modeling The best place to start with any modeling where we have a continuous variable such as weight is a simple linear regression approach. We will create a linear model with one response (y) variable and one explanatory (x) variable. We are interested in the relationship between weight and days since the start of the competition, therefore, the response is weight and the explanatory variable is days. From the graphs, we have seen this may not be the best representation of the data, but it is a good place to begin and allows us to quantity our respective weight changes. Craig’s model results are presented below. There is a lot of information here, but I’ll walk through it and point out what is important. The main parts to examine are the parameters, the numbers that define the model. In the case of a simple linear model these are the intercept and the slope as shown in the equation for a straight line: y = mx + b. For the weight challenge, this model becomes: weight = (weight change per day) * days + weight at zero days. The weight at zero days in the above summary is in the (intercept) row under the estimate column with a value of 227.78 lbs. The weight change per day is in the days row under the estimate column with a value of -0.024 lbs/day. This means, that under a linear model, my dad lost an average of 0.024 lbs per day on average. The other statistics presented above are slightly more detailed, but also informative. The R-squared represents the fraction of the in variation in the y variable (weight) which can be explained by the variation in the x variable (days). A higher R-Squared means the model better represents the data and we can see our model only accounts for 11.96% of the variation in weight. Additionally, we can look at the p-value to see if there is a real trend in our model or if our data is simply noise. A p-value is a common statistic that represents the chance of the observed data occurring at random under the model. For Craig’s model, the p-value is 0.0002642 which falls well below the generally accepted significance threshold of 0.05 (lower is better for a p-value because it means the data is less likely to have been generated by chance). Therefore, the chance that my dad’s weight loss is simply random noise is less than 3 in 10000. From this model, we can conclude that my dad’s weight loss over the course of the competition is a real trend! We can now turn to my simple linear regression model for a similar analysis. The model summary shows an intercept of 131.9 lbs, a weight change per day of 0.0095 lbs, an R-Squared of 0.04502, and a p-value of 0.01847. Our conclusions are: The p-value for my model is slightly higher than that for my dad, but it still falls below the significance threshold and the model shows a real trend. We can visualize how well linear models fit the data by slightly altering the full results code and changing the model trend line from ‘loess’ to linear. The takeaway from linear modeling is that my dad and I both demonstrated significant progress towards our weight change goals over the challenge. A generalized additive model goes beyond the linear relationship assumption of a simple linear model and represents a time-series (weight in this case) as a combination (addition) of an overall trend, and daily, weekly, or yearly patterns. This approach works very well for real-world data that often exhibits specific patterns. Our data was collected once a day for about 4 months, so there are only weekly patterns and an overall trend (to have daily patterns would require multiple observations per day). Nevertheless, we still are able to draw useful conclusions from an additive model. We can first plot the overall trend. This is similar to the smooth line we saw in the full results graph and shows us the overall trajectory of weight change. The next graph shows weekly trends in terms of weight lost or gained on each day of the week. This plot has actionable information because it shows which days are problems for our weight change objectives. Although my dad and I were trying to go in opposite directions, we had very similar weekly patterns. We both lost weight in the first two days of the work week, gained weight during the rest of the work week and trended downwards over the weekend. It is possible to read too much into this plots, but my interpretation for myself is that I tended to get much more exercise on the weekends (usually a couple multi-hour runs) which would reduce my weight heading into the work week. I would then catch back up on my weight when I was busy with classes, before losing momentum again over the weekend. My dad’s better performance on the weekends is likely also due to an increase in exercise when he was not in work. These results say I need to work on consuming more food on the weekends, and my dad needs to work on reducing his consumption during the week. A generalized additive model may seem complex, but we can use the results to determine simple actions to improve our health! Predictions A further benefit of modeling is that we can use the results to make predictions. We can make predictions with either the linear model or the generalized additive model, but because the additive model better represents the data, we will only make predictions with it. There are two estimates that are of primary interest: The first prediction will allow us to compare our performance in the second half of the competition from that predicted based on our first half data. The second prediction will give us an idea of where we expect to be one year from now. An essential aspect of predictions that is often overlooked are the uncertainty bounds. Managers usually only want a single number for a forecast, but that is an impossibility in an uncertain world. Even the most accurate model is not able to capture inherent randomness in the data or non-exact measuring devices. Therefore in order to be responsible data scientists, we will provide a range of uncertainty in addition to a single prediction number. The following graphs show the predictions for Craig and me for the planned end of the competition made from data up to November 1, 2017. The gray shaded region indicates that most crucial part of an forecast: the region of uncertainty. The red line shows us the most likely future results were we to continue exactly on the trend observed so far. The shaded region is the likely range of finishing weights based on the observations and taking into account the noise in the measurements. To put those ranges in perspective, we should look at the actual numbers: The actual weight for both my dad and I fell into the range predicted. The model did a relatively good job of predicting our actual weights two months in advance considering it was working on about 70 datapoints! As a final exploration of the models, we can look at where we can expect to be one year from now. Perhaps these prediction will not quite turn out , but they should at least give us a goal for the next year! Following are predictions for 2019 made using all the competition data: These graphs clearly show how uncertainty increases the further out we extrapolate from data. According to these plots, I could weigh more than my dad at the beginning of next year! This exercise is meant to be more for fun than as a serious prediction, but the forecast can improve if we continue to collect more data. Eventually, with enough information, we should be able to build a model that could theoretically make accurate predictions one year into the future. We’ll have to check back in next year for an update on the accuracy of these predictions! Conclusions The question to ask yourself at the end of any data analysis is: “What do I know now that I can use?” Sometimes the answer will be not much and that is perfectly fine! It simply indicates you need to collect more data or you need to rethink the modeling approach. However, in the analysis of the Great Weight Challenge of 2017, there are some insights to review and put into use! Finally, I will show two more plots which contrast the predictions for Jan 1, 2018 with the actual results on this date. I hope this post has shown how anyone can use data science in their daily life for personal or community benefit. My goal is to democratize data science and allow everyone to take advantage of this exciting field by demonstrating real uses of data science tools! As always, I welcome feedback and constructive criticism. I can be reached at wjk68@case.edu. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Dad (Craig): prime-aged male (I’ll let you guess what age that is), 5'11', starting weight 235.2 lbs, office worker, former competitive weight-lifter Will: Final Weight = 134 lbs, Absolute Change = 7.4 lbs, Percentage Change = 5.85% The model can explain only 4.5% of the variation in weight The observed results have a 1.85% chance of occurring due to pure chance Predictions for January 1, 2019 made with all measurements Will: Prediction = 135.6, Upper Bound = 143.4 lbs, Lower Bound = 127.3 lbs, Actual Weight on Jan 1 = 136.9 lbs Will: Prediction = 147.2 lbs, Upper Bound = 209.2 lbs, Lower Bound = 85.5 lbs, Goal Weight = 155.0 lbs Craig lost 0.024 lbs / day while I gained 0.0095 lbs /day. Both of us demonstrated significant trends in weight change. I need to work on gaining weight on the weekends, and my dad needs to work on losing it during the work week. Both of us performed better in the first half of the competition when weight change was consistently in the right direction, but we reached plateaus in the third month. Forecasts for the next year predict us both to continue on the right track, but with a considerably wide range of uncertainty. Craig: Final Weight = 219.8 lbs, Absolute Change = 15.4 lbs, Percentage Change = 6.55% I gained 0.0095 lbs/day over the course of the competition Predictions for January 1, 2018 made with our measurements from the first two months (through the end of October 2017) Craig: Prediction = 213.2 lbs, Upper Bound = 223.5 lbs, Lower Bound = 203.2 lbs, Actual Weight on Jan 1 = 220.8 lbs Craig: Prediction = 208.8 lbs, Upper Bound = 230.9 lbs, Lower Bound = 186.0 lbs, Goal Weight = 215.0 lbs Craig handily won the competition in terms of weight change although both of us were successful: Craig lost about 6% of his body weight, while I gained about 6%.",Data Science: A Personal Application,5,published,2828,3569,0.052956010086859066,0,0,1,0,0,0
113,11,504.24812525656256,3,https://medium.com/p/the-simple-science-of-global-warming-5a2070bcd03a,1,None,2018-01-08 09:45:00,34.52,5,1258,2018-01-07 20:44:00,"['Climate Change', 'Global Warming', 'Education', 'Environment']","The Simple Science of Global Warming A five-minute primer Recently, I found myself in a conversation with a global warming skeptic who said she was willing to change her mind if only someone could give her a five-minute explanation of how climate change worked. I opened my mouth to list all the stats and tell her 97% of climate scientists believe in human-caused global warming, but then I thought better of it. She had probably heard these arguments multiple times and wanted an explanation and not more talk about parts per million of carbon dioxide or potential sea level rise 100 years from now. Moreover, I realized that despite being a firm believer in human-caused global warming and an advocate for action, I didn’t actually know how the science worked. This was quite an embarrassing situation and I set out to form my own five-minute explanation. Instead of countering denier’s arguments with endless facts, I want to be able to walk them through the science and show that climate change is not an overly complex theory that only a handle of elite scientists can understand. In the process of forming my own explanation, I was reminded once again that you really don’t understand a topic until you have to explain it to others! Our atmosphere is composed of many gases that interact with light in different ways. All gases allow some energy from the sun (in the form of visible light) to pass through and reach the surface of the Earth. The Earth absorbs and then re-emits this energy as infrared radiation, which some gases, namely carbon dioxide (CO2), water vapor (H20), and methane (CH4), partially block from passing back into space. Instead, these “greenhouse” gases reflect the infrared radiation in all directions, including back towards the Earth increasing the temperature of the planet. This is what people are talking about when they say “greenhouse effect.” The greenhouse effect is a natural process, and, without our atmosphere, the temperature of the Earth would be about 0 F. The temperature our planet fluctuates as the concentration of gases in the atmosphere changes. Historical observations show that as carbon dioxide levels increase, temperature also rises because of an increased greenhouse effect. In fact, we know that temperatures have been higher (and much lower) in the past than they are now! Nature is very good at self-regulating and when the climate undergoes a sustained shift in one direction , there are natural mechanisms that will eventually push it the other way. However, about 150 years ago, humans entered the climate picture when we began extracting energy from oil, coal, and natural gas. Even as a proponent for climate activism, I admit these sources of fuels have been an incredible short-term benefit to humanity, powering the high-quality of life we enjoy today. Nevertheless, looking only at the immediate payoffs obscures the downsides of fossil fuels, namely the pollution, such as carbon dioxide, that results from burning them. Levels of carbon dioxide rise and fall naturally over time, changes that we can measure from ice cores drilled at the Poles. These are processes that would occur with or without humans on Earth and climate cycles are a historical fact. However, what distinguishes the past 150 years is the incredibly rapid increase in carbon dioxide levels. Based on past observations that temperatures rise with an increase in carbon dioxide and our knowledge of the greenhouse effect, we can conclude that increasing levels of carbon dioxide from using fossil fuels will lead to an increase in global temperatures. From the historical records, we also know that elevated temperatures cause sea levels to increase, storms to be more severe, and distributions of plant and animals species to change dramatically. I have to mention that the magnitude of these changes is very uncertain. Among climate scientists, the debate over human-caused global warming has been settled, but any reputable scientist will admit the possible effects cannot be exactly predicted. Some areas of the globe could even benefit, but overall, it is in our best interest to decrease the rate of change so we are able to adequately adapt. When explaining technical concepts, I like to emphasis that science is a human endeavor. Science is not a faceless institution that issues statements for its own benefits. Rather, it is a collection of diverse individuals all of whom genuinely try to use the scientific method to improve the world. Moreover, there are rigorous discussions about every scientific concept that are left out of the media summary of current findings. In the case of climate science, there was plenty of debate — Svante Arrhenius first proposed that burning fossil fuels would lead to temperature increases in 1896, and as late as the 1990s there were still mainstream doubts about the role of humans in global warming. However, there is now overwhelming agreement that humans are causing the climate to change. All the scientists I have spoken to are concerned not with enriching their own bank account, but in helping us to collectively address this problem. I think my generation (the ‘lazy’ millennials) need a crisis to bring us together, and there is no more monumental challenge than addressing climate change. The first step in solving any problem is admitting we have one, and taking the five minutes to explain how global warming works to someone who is not yet convinced will allow us to move beyond arguing and start taking action. A great free online resource for learning about climate change is Skeptical Science. For anyone wanting a readable introduction to climate change the best book to start with is Elizabeth Kolbert’s Field Notes from a Catastrophe. As always, I welcome feedback and constructive criticism. I can be reached at wjk68@case.edu Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",The Simple Science of Global Warming,6,published,3644,1073,0.010251630941286114,0,1,0,0,0,0
99,129,503.947251843206,44,https://towardsdatascience.com/a-theory-of-prediction-10cb335cc3f2,2,Towards Data Science,2018-01-08 16:58:00,31.01,11,773,2018-01-02 17:23:00,"['Statistics', 'Data Science', 'Book Review', 'Books']","A Theory of Prediction Review of The Signal and the Noise by Nate Silver The Signal and the Noise is probably the most informative non-technical book about the art of predicting ever written. It outlines what is best described as Nate Silver’s “Theory of Prediction”. Silver, creator of the data journalism site fivethirtyeight, takes readers through an information trip of diverse fields including meteorology, baseball, poker, finance, and politics, documenting estimates that either failed badly, or were extremely successful and the tactics employed by the forecasters who made them. Both the successful and the failed forecasts have much to teach us, and Silver distills the lessons from these examples into a cohesive message: we are inherently terrible at making predictions, but by adopting a few principles, we can improve our estimates and benefit at the personal and national level. In my view, there principles fall under three rules: It’s hard to make out much about these rules from their names, so let’s walk through each one in turn. Think Like a Fox At the outset of the book, Silver relates the contrasting viewpoints of the hedgehog and the fox as put forth by the philosopher Isiah Berlin. While the hedgehog has one big idea, the fox has many small ideas. Everyone tends to fall on one side or the other: we either believe in a black and white world with well-defined boundaries, or a gray world in which nothing is ever completely certain. Each point of view has its own merits, but when it comes to prediction, the foxes will be significantly more successful. A fox can draw on a diverse range of sources and assume different perspectives but the hedgehog is locked into a single line of thought. This is best related to predictions through a simple example. Let’s say we are in a country with only two political parties (how ludicrous a concept!), the bears and the tigers. We are vying against a rival forecaster trying to outperform him on predictions for the upcoming election. Whoever wins will be put on the prime news channel for the next four years. Our rival is a classic hedgehog; his one big idea is that the tigers will sweep every single one of the next elections. On the other hand, we have made the rational decision to be more fox-like. We believe that indeed the overall trend favors tigers, but we are willing to look at individual races where the story-line may not be straightforward. When it’s time to make our predictions, we examine each race in turn, looking at all the relevant news and averaging multiple polls to select a winner. Our rival confidently picks tigers for every race, backing up his picks with the assertion that his view of the world is infallible. In the end, we handily win and have the honor of appearing on the news every night for the next four years because we were willing to see the nuance whereas our rival was blinded by a single idea. This may be a contrived example, but it shows the dangers in adopting a simplistic right/wrong view of the world. When you are a hedgehog and hold only one belief, such as that the tigers will win all the elections, every piece of new information you hear will confirm your beliefs. You will gladly put all your faith in the pro-tiger papers and ignore any conflicting evidence. Meanwhile, as foxes, we have no existing biases and are willing to objectively evaluate each and every source of data. The hedgehogs suffer from confirmation bias, where there manipulate any evidence to fit their viewpoint. The foxes produce more accurate predictions because of their willingness to approach any problem with an open mind. In reality, it can be difficult to shed all of our pre-conceived beliefs, but we can work to counter them by collecting information from as many sources as possible as fivethirtyeight does when constructing election predictions. Unfortunately, in the real world as Silver points out, hedgehogs often are readily handed a bullhorn to broadcast their message. With tv news and the internet fracturing into two political sides with a massive no-man’s land in between, people still in the middle are overwhelmed by the fanatics at the extremes. When people tune into a news channel, they are not looking for objective coverage, they want to hear that their side is right. When it comes to predictions, they would rather listen to the hedgehog telling them their “team” will sweep the legislative branch than the fox with her wise forecasts detailing each race based on objective data. Thus, the majority of forecasters who appear on tv and in the news are hedgehogs who make bold claims and not foxes who make reasonable but less exciting predictions. Despite evidence that those who make more extreme predictions are the worst predictors, they get the majority of screen time. Nonetheless, this does not prevent us from being foxes in our day-to-day lives. In order to think like a fox, we need to shed our pre-conceived beliefs, collect information from diverse sources, listen to both sides of an argument, and make reasonable predictions that reflect the gray nature of the world. Think Like A Bayesian Before you are scared off by the strange sound of “Bayesian,” let me explain: Thomas Bayes was a statistician and minister in the 18th century known for formulating methods on how to update our beliefs about the world based on new evidence. When we approach any new prediction problem, we first need to form an initial estimate of the situation, called a prior. Let’s say we want to predict whether or not we will get a promotion at the start of the next quarter. It’s one month out, and because we have been more productive lately and are on good terms with our manager, we can put the initial odds at 50%. The idea behind Bayes’ theorem (the formal version of his ideas) is as we gather information related to the problem, we update our first estimate in accordance with the data. If our manager sends us an email praising our work, we might increase the probability of a promotion to 75%. As the date of the event grows closer, our prediction should converge on the true probability if we are incorporating all relevant information. Evidence is weighted according to how much it decreases our uncertainty. The prior is taken into account at each step, but as the amount of new information (observations) increases, the weight of the prior in the prediction decreases. Our final prediction is a combination of the initial estimate and the observed data. We use Bayesian reasoning all the time in our daily lives. When watching a sports event, we have an idea at the outset what the end result will be, and as the game progresses, we update that estimate, until by the end, we can be 100% sure of the result. Likewise, the stock market rises and falls as a result of news because investors believe the information reveals something about the worth of a company. The Bayes’ point of view is in contrast to what is called the frequentist worldview, where the chance of something occurring is based only on the observed frequency of the event in past data. When forecasting the chance of snow on January 21, 2018 from two weeks out, the frequentist would predict the average chance of snow on that day and stick with that estimate until the day in question. As a Bayesian however, we would make an initial forecast, perhaps using the historical occurrence as a starting point, and then revise our estimate as the day got closer based on new information. Warmer than average weather might decrease our predicted probability for snow while a snowstorm approaching from the west would increase it. The Bayesian is naturally at an advantage for making predictions because she is constantly changing her beliefs in response to evidence. Another critical aspect of Bayesian thinking is that predictions are expressed as a probability rather than a yes/no. Although most people tend to want a straight answer, we live in an uncertain world where a yes or no is not completely possible. Any prediction will have uncertainty, and when someone makes a prediction that the stock market will go up tomorrow, they are obscuring that there is actually a range of possible values the stock market could take. The most likely scenario may be an increase, but there will always be the chance of a decrease. Again, answering prediction problems with a range of values or a probability with likely not get us a spot on the news, but it will mean that we will on average be closer to the truth. Responsible data scientists and forecasters must communicate results with the attached uncertainty. Answering with a range of values should not be interpreted as a lack of confidence but rather a reflection of the shifting state of the world. Think Like a (Basketball) Shooter The optimal way to get good at anything is to fail at it many times and make adjustments. The best basketball shooters in the world fail about half the time, but they do not let that deter them from taking another shot. Each time they shoot and miss, they make minor adjustments, and then take another shot. While repeatedly failing may be difficult in some domains (it’s great that we invented airplane simulators to allow pilots the luxury of failing more than once), it is easy to implement in situations where the stakes are minor and we can receive immediate feedback. Predictions, at least on a personal level, fit these conditions perfectly. We can very easily (and safely) predict our weight a week from now, the score of our favorite team’s next game, or the optimal length of time to cook popcorn without suffering dire consequences. The world would have a severe shortage of meteorologists if everyone was held to perfect standards for predictions. Instead, every time we miss the mark, we examine the causes of our failure, and make adjustments we think will help us for the next time. Anyone can fail often, but it’s about examining and using mistakes to improve our performance. Silver’s team infamously failed at predicting the 2016 elections, and afterwards, they took a long look at what they had done wrong. Fivethirtyeight does not conduct polls, but aggregates a wide variety, weighting each one based on historical performance. In the case of 2016, it was clear that the polls were systematically biased towards Hillary Clinton which Silver and his team will undoubtedly take that into account for the next round of elections. Silver also documents a baseball model that made absolutely ridiculous predictions, which, on inspection was caused by a single mis-typed letter. Had the creators dumped the model after the failures, their development time would have been wasted, but they had the wisdom to keep making new predictions and mistakes to isolate the problem. Thinking like a shooter involves repeating the shoot — miss — adjust — shoot cycle continuously, improving performance with each iteration. Losing the Signal in the Noise There are limitations to any prediction model as Silver points out throughout the book. Humans have a tendency to see patterns where there are none, noticeably in the case of chance correlations. The Super Bowl indicator, which is supposed to predict the performance of the stock market based on which league wins the Super Bowl has been correct 40 out of the past 50 years. However, there is no macroeconomic effect whereby the winner of the NFL championship influences the markets and it is only a surprising correlation. Moreover, some systems, such as the weather, are extremely sensitive to initial conditions (these fall under the intriguing field of chaos theory). One minor change at the start of a weather simulation can lead to drastically different forecasts, which is why predicting the weather and climate is notoriously difficult. Nevertheless, thanks to many failures over time, weather prediction has improved remarkably in the past several decades. Weather services create hundreds of billions of dollars in value due to reduced damages from storms because of storm warnings and better harvests due to climate predictions. Although we normally tend to prefer a simple model, sometimes we need a massive, complicated model for accurate predictions. Another type of problem where predictions fail is out-of-sample situations. These are situations that have never before occurred in our data and therefore are nearly impossible to see coming. Consider a pilot who has made the flight from Houston to New York 800 times without incident in clear weather. On his next flight, a massive hurricane is impacting the East Coast and the airline must decide weather or not to cancel his flight. The pilot argues that he has never crashed before and therefore there is no chance of him running into trouble on this flight. However, this is an out-of-sample situation because each of his previous flights were made under perfect conditions. In this case the prudent measure would be to ground the flight because there is too much uncertainty. The attack on Pearl Harbor by the Japanese is often considered an out-of-sample event because no major attack by a foreign power of that scale had ever before occurred on US soil. Despite indications Japan was preparing for a large military operation, the US failed to predict the event. The signal was there but was ignored because an attack like Pearl Harbor had never before occurred. In this age of blind belief in big data and complex models, Silver takes a needed critical view of predictions that rely only on statistics. Drawing on his past experience with baseball models, Silver explains how computers alone often cannot capture all of the intricacies in many human pursuits. He found that his own numbers-only based models under-performed those built using both human intuition and data. Likewise, after computers beat the best chess players for the first time, it was predicted that humans had lost all relevance in the game. However, subsequent open world championships that allowed any combination of people and computers to compete were won by teams that used both a program and humans with knowledge of the game. In most fields, it is a good bet that domain knowledge plus computer models will surpass predictions relying on either alone. Recommendation My method for judging a book is whether I could get all the relevant information in a five-minute summary, or if reading the entire book is worthwhile for the extra insights. In the case of The Signal and the Noise, I recommend reading the whole work. In this post, I outlined the basics of the book but skipped over nearly all the real-world examples Silver uses to illustrate each one of his points. The book covers many statistical concepts in an intuitive style, and for an academic-seeming topic such as predictions, was quite readable. Anyone aspiring to be a data scientist or who wants to examine forecasts skeptically should read this informative work on the methods of accurate predictions. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Think like a Bayesian Think like a (basketball) shooter Think like a fox",A Theory of Prediction,4,published,2493,2817,0.04579339723109691,5,0,1,0,0,0
101,2600,502.7453307895602,417,https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74,23,Towards Data Science,2018-01-09 21:49:00,23.98,12,27835,2018-01-09 12:26:00,"['Machine Learning', 'Python', 'Data Science', 'Data']","Hyperparameter Tuning the Random Forest in Python Improving the Random Forest Part Two So we’ve built a random forest model to solve our machine learning problem (perhaps by following this end-to-end guide) but we’re not too impressed by the results. What are our options? As we saw in the first part of this series, our first step should be to gather more data and perform feature engineering. Gathering more data and feature engineering usually has the greatest payoff in terms of time invested versus improved performance, but when we have exhausted all data sources, it’s time to move on to model hyperparameter tuning. This post will focus on optimizing the random forest model in Python using Scikit-Learn tools. Although this article builds on part one, it fully stands on its own, and we will cover many widely-applicable machine learning concepts. I have included Python code in this article where it is most instructive. Full code and data to follow along can be found on the project Github page. A Brief Explanation of Hyperparameter Tuning The best way to think about hyperparameters is like the settings of an algorithm that can be adjusted to optimize performance, just as we might turn the knobs of an AM radio to get a clear signal (or your parents might have!). While model parameters are learned during training — such as the slope and intercept in a linear regression — hyperparameters must be set by the data scientist before training. In the case of a random forest, hyperparameters include the number of decision trees in the forest and the number of features considered by each tree when splitting a node. (The parameters of a random forest are the variables and thresholds used to split each node learned during training). Scikit-Learn implements a set of sensible default hyperparameters for all models, but these are not guaranteed to be optimal for a problem. The best hyperparameters are usually impossible to determine ahead of time, and tuning a model is where machine learning turns from a science into trial-and-error based engineering. Hyperparameter tuning relies more on experimental results than theory, and thus the best method to determine the optimal settings is to try many different combinations evaluate the performance of each model. However, evaluating each model only on the training set can lead to one of the most fundamental problems in machine learning: overfitting. If we optimize the model for the training data, then our model will score very well on the training set, but will not be able to generalize to new data, such as in a test set. When a model performs highly on the training set but poorly on the test set, this is known as overfitting, or essentially creating a model that knows the training set very well but cannot be applied to new problems. It’s like a student who has memorized the simple problems in the textbook but has no idea how to apply concepts in the messy real world. An overfit model may look impressive on the training set, but will be useless in a real application. Therefore, the standard procedure for hyperparameter optimization accounts for overfitting through cross validation. Cross Validation The technique of cross validation (CV) is best explained by example using the most common method, K-Fold CV. When we approach a machine learning problem, we make sure to split our data into a training and a testing set. In K-Fold CV, we further split our training set into K number of subsets, called folds. We then iteratively fit the model K times, each time training the data on K-1 of the folds and evaluating on the Kth fold (called the validation data). As an example, consider fitting a model with K = 5. The first iteration we train on the first four folds and evaluate on the fifth. The second time we train on the first, second, third, and fifth fold and evaluate on the fourth. We repeat this procedure 3 more times, each time evaluating on a different fold. At the very end of training, we average the performance on each of the folds to come up with final validation metrics for the model. For hyperparameter tuning, we perform many iterations of the entire K-Fold CV process, each time using different model settings. We then compare all of the models, select the best one, train it on the full training set, and then evaluate on the testing set. This sounds like an awfully tedious process! Each time we want to assess a different set of hyperparameters, we have to split our training data into K fold and train and evaluate K times. If we have 10 sets of hyperparameters and are using 5-Fold CV, that represents 50 training loops. Fortunately, as with most problems in machine learning, someone has solved our problem and model tuning with K-Fold CV can be automatically implemented in Scikit-Learn. Random Search Cross Validation in Scikit-Learn Usually, we only have a vague idea of the best hyperparameters and thus the best approach to narrow our search is to evaluate a wide range of values for each hyperparameter. Using Scikit-Learn’s RandomizedSearchCV method, we can define a grid of hyperparameter ranges, and randomly sample from the grid, performing K-Fold CV with each combination of values. As a brief recap before we get into model tuning, we are dealing with a supervised regression machine learning problem. We are trying to predict the temperature tomorrow in our city (Seattle, WA) using past historical weather data. We have 4.5 years of training data, 1.5 years of test data, and are using 6 different features (variables) to make our predictions. (To see the full code for data preparation, see the notebook). Let’s examine the features quickly. In previous posts, we checked the data to check for anomalies and we know our data is clean. Therefore, we can skip the data cleaning and jump straight into hyperparameter tuning. To look at the available hyperparameters, we can create a random forest and examine the default values. Wow, that is quite an overwhelming list! How do we know where to start? A good place is the documentation on the random forest in Scikit-Learn. This tells us the most important settings are the number of trees in the forest (n_estimators) and the number of features considered for splitting at each leaf node (max_features). We could go read the research papers on the random forest and try to theorize the best hyperparameters, but a more efficient use of our time is just to try out a wide range of values and see what works! We will try adjusting the following set of hyperparameters: To use RandomizedSearchCV, we first need to create a parameter grid to sample from during fitting: On each iteration, the algorithm will choose a difference combination of the features. Altogether, there are 2 * 12 * 2 * 3 * 3 * 10 = 4320 settings! However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values. Now, we instantiate the random search and fit it like any Scikit-Learn model: The most important arguments in RandomizedSearchCV are n_iter, which controls the number of different combinations to try, and cv which is the number of folds to use for cross validation (we use 100 and 3 respectively). More iterations will cover a wider search space and more cv folds reduces the chances of overfitting, but raising each will increase the run time. Machine learning is a field of trade-offs, and performance vs time is one of the most fundamental. We can view the best parameters from fitting the random search: From these results, we should be able to narrow the range of values for each hyperparameter. To determine if random search yielded a better model, we compare the base model with the best random search model. We achieved an unspectacular improvement in accuracy of 0.4%. Depending on the application though, this could be a significant benefit. We can further improve our results by using grid search to focus on the most promising hyperparameters ranges found in the random search. Grid Search with Cross Validation Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search: This will try out 1 * 4 * 2 * 3 * 3 * 4 = 288 combinations of settings. We can fit the model, display the best hyperparameters, and evaluate performance: It seems we have about maxed out performance, but we can give it one more try with a grid further refined from our previous results. The code is the same as before just with a different grid so I only present the results: A small decrease in performance indicates we have reached diminishing returns for hyperparameter tuning. We could continue, but the returns would be minimal at best. Comparisons We can make some quick comparisons between the different approaches used to improve performance showing the returns on each. The following table shows the final results from all the improvements we made (including those from the first part): Model is the (very unimaginative) names for the models, accuracy is the percentage accuracy, error is the average absolute error in degrees, n_features is the number of features in the dataset, n_trees is the number of decision trees in the forest, and time is the training and predicting time in seconds. The models are as follows: Overall, gathering more data and feature selection reduced the error by 17.69% while hyperparameter further reduced the error by 6.73%. In terms of programmer-hours, gathering data took about 6 hours while hyperparameter tuning took about 3 hours. As with any pursuit in life, there is a point at which pursuing further optimization is not worth the effort and knowing when to stop can be just as important as being able to keep going (sorry for getting all philosophical). Moreover, in any data problem, there is what is called the Bayes error rate, which is the absolute minimum possible error in a problem. Bayes error, also called reproducible error, is a combination of latent variables, the factors affecting a problem which we cannot measure, and inherent noise in any physical process. Creating a perfect model is therefore not possible. Nonetheless, in this example, we were able to significantly improve our model with hyperparameter tuning and we covered numerous machine learning topics which are broadly applicable. Training Visualizations To further analyze the process of hyperparameter optimization, we can change one setting at a time and see the effect on the model performance (essentially conducting a controlled experiment). For example, we can create a grid with a range of number of trees, perform grid search CV, and then plot the results. Plotting the training and testing error and the training time will allow us to inspect how changing one hyperparameter impacts the model. First we can look at the effect of changing the number of trees in the forest. (see notebook for training and plotting code) As the number of trees increases, our error decreases up to a point. There is not much benefit in accuracy to increasing the number of trees beyond 20 (our final model had 100) and the training time rises consistently. We can also examine curves for the number of features to split a node: As we increase the number of features retained, the model accuracy increases as expected. The training time also increases although not significantly. Together with the quantitative stats, these visuals can give us a good idea of the trade-offs we make with different combinations of hyperparameters. Although there is usually no way to know ahead of time what settings will work the best, this example has demonstrated the simple tools in Python that allow us to optimize our machine learning model. As always, I welcome feedback and constructive criticism. I can be reached at wjk68@case.edu Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. average = historical average max temperature ws_1 = average wind speed one day prior temp_2 = max temperature two days prior friend = prediction from our “trusty” friend year = calendar year max_features = max number of features considered for splitting a node max_depth = max number of levels in each decision tree min_samples_split = min number of data points placed in a node before the node is split min_samples_leaf = min number of data points allowed in a leaf node bootstrap = method for sampling data points (with or without replacement) one_year: model trained using a single year of data four_years_all: model trained using 4.5 years of data and expanded features (see Part One for details) four_years_red: model trained using 4.5 years of data and subset of most important features best_random: best model from random search with cross validation first_grid: best model from first grid search with cross validation (selected as the final model) second_grid: best model from second grid search n_estimators = number of trees in the foreset average: original baseline computed by predicting historical average max temperature for each day in test set",Hyperparameter Tuning the Random Forest in Python,7,published,116063,2456,1.0586319218241043,0,0,1,0,1,1
110,4500,499.7879477537732,920,https://towardsdatascience.com/time-series-analysis-in-python-an-introduction-70d5a5b1d52a,18,Towards Data Science,2018-01-12 20:48:00,25.11,14,32966,2018-01-11 08:45:00,"['Data Science', 'Data', 'Programming', 'Python', 'Towards Data Science']","Time Series Analysis in Python: An Introduction Additive models for time series modeling Time series are one of the most common data types encountered in daily life. Financial prices, weather, home energy usage, and even weight are all examples of data that can be collected at regular intervals. Almost every data scientist will encounter time series in their daily work and learning how to model them is an important skill in the data science toolbox. One powerful yet simple method for analyzing and predicting periodic data is the additive model. The idea is straightforward: represent a time-series as a combination of patterns at different scales such as daily, weekly, seasonally, and yearly, along with an overall trend. Your energy use might rise in the summer and decrease in the winter, but have an overall decreasing trend as you increase the energy efficiency of your home. An additive model can show us both patterns/trends and make predictions based on these observations. The following image shows an additive model decomposition of a time-series into an overall trend, yearly trend, and weekly trend. This post will walk through an introductory example of creating an additive model for financial time-series data using Python and the Prophet forecasting package developed by Facebook. Along the way, we will cover some data manipulation using pandas, accessing financial data using the Quandl library and, and plotting with matplotlib. I have included code where it is instructive, and I encourage anyone to check out the Jupyter Notebook on GitHub for the full analysis. This introduction will show you all the steps needed to start modeling time-series on your own! Disclaimer: Now comes the boring part when I have to mention that when it comes to financial data, past performance is no indicator of future performance and you cannot use the methods here to get rich. I chose to use stock data because it is easily available on a daily frequency and fun to play around with. If you really want to become wealthy, learning data science is a better choice than playing the stock market! Usually, about 80% of the time spent on a data science project is getting and cleaning data. Thanks to the quandl financial library, that was reduced to about 5% for this project. Quandl can be installed with pip from the command line, lets you access thousands of financial indicators with a single line of Python, and allows up to 50 requests a day without signing up. If you sign up for a free account, you get an api key that allows unlimited requests. First, we import the required libraries and get some data. Quandl automatically puts our data into a pandas dataframe, the data structure of choice for data science. (For other companies, just replace the ‘TSLA’ or ‘GM’ with the stock ticker. You can also specify a date range). There is an almost unlimited amount of data on quandl, but I wanted to focus on comparing two companies within the same industry, namely Tesla and General Motors. Tesla is a fascinating company not only because it is the first successful American car start-up in 111 years, but also because at times in 2017 it was the most valuable car company in America despite only selling 4 different cars. The other contender for the title of most valuable car company is General Motors which recently has shown signs of embracing the future of cars by building some pretty cool (but not cool-looking) all-electric vehicles. We could easily have spent hours searching for this data and downloading it as csv spreadsheet files, but instead, thanks to quandl, we have all the data we need in a few seconds! Data Exploration Before we can jump into modeling, it’s best to get an idea of the structure and ranges by making a few exploratory plots. This will also allows us to look for outliers or missing values that need to be corrected. Pandas dataframes can be easily plotted with matplotlib. If any of the graphing code looks intimidating, don’t worry. I also find matplotlib to be unintuitive and often copy and paste examples from Stack Overflow or documentation to get the graph I want. One of the rules of programming is don’t reinvent a solution that already exists! Comparing the two companies on stock prices alone does not show which is more valuable because the total value of a company (market capitalization) also depends on the number of shares (Market cap= share price * number of shares). Quandl does not have number of shares data, but I was able to find average yearly stock shares for both companies with a quick Google search. Is is not exact, but will be accurate enough for our analysis. Sometimes we have to make do with imperfect data! To create a column of market cap in our dataframe,we use a few tricks with pandas, such as moving the index to a column (reset_index) and simultaneously indexing and altering values in the dataframe using ix. This creates a ‘cap’ column for Tesla. We do the same process with the GM data and then merge the two. Merging is an essential part of a data science workflow because it allows us to join datasets on a shared column. In this case, we have stock prices for two different companies on the same dates and we therefore want to join the data on the date column. We perform an ‘inner’ merge to save only Date entries that are present in both dataframes. After merging, we rename the columns so we know which one goes with which car company. The market cap is in billions of dollars. We can see General Motors started off our period of analysis with a market cap about 30 times that of Tesla! Do things stay that way over the entire timeline? We observe a meteoric rise for Tesla and a minor increase for General Motors over the course of the data. Tesla even surpasses GM in value during 2017! During that period, Tesla sold about 48,000 cars while GM sold 1,500,000. GM was valued less than Tesla during a period in which it sold 30 times more cars! This definitely displays the power of a persuasive executive and a high-quality — if extremely low-quantity — product. Although the value of Tesla is now lower than GM, a good question might be, can we expect Tesla to again surpass GM? When will this happen? For that we turn to additive models for forecasting, or in other words, predicting the future. Modeling with Prophet The Facebook Prophet package was released in 2017 for Python and R, and data scientists around the world rejoiced. Prophet is designed for analyzing time series with daily observations that display patterns on different time scales. It also has advanced capabilities for modeling the effects of holidays on a time-series and implementing custom changepoints, but we will stick to the basic functions to get a model up and running. Prophet, like quandl, can be installed with pip from the command line. We first import prophet and rename the columns in our data to the correct format. The Date column must be called ‘ds’ and the value column we want to predict ‘y’. We then create prophet models and fit them to the data, much like a Scikit-Learn machine learning model: When creating the prophet models, I set the changepoint prior to 0.15, up from the default value of 0.05. This hyperparameter is used to control how sensitive the trend is to changes, with a higher value being more sensitive and a lower value less sensitive. This value is used to combat one of the most fundamental trade-offs in machine learning: bias vs. variance. If we fit too closely to our training data, called overfitting, we have too much variance and our model will not be able to generalize well to new data. On the other hand, if our model does not capture the trends in our training data it is underfitting and has too much bias. When a model is underfitting, increasing the changepoint prior allows more flexibility for the model to fit the data, and if the model is overfitting, decreasing the prior limits the amount of flexibility. The effect of the changepoint prior scale can be illustrated by graphing predictions made with a range of values: The higher the changepoint prior scale, the more flexible the model and the closer it fits to the training data. This may seem like exactly what we want, but learning the training data too well can lead to overfitting and an inability to accurately make predictions on new data. We therefore need to find the right balance of fitting the training data and being able to generalize to new data. As stocks vary from day-to-day, and we want our model to capture this, I increased the flexibility after experimenting with a range of values. In the call to create a prophet model, we can also specify changepoints, which occur when a time-series goes from increasing to decreasing, or from increasing slowly to increasing rapidly (they are located where the rate change in the time series is greatest). Changepoints can correspond to significant events such as product launches or macroeconomic swings in the market. If we do not specify changepoints, prophet will calculate them for us. To make forecasts, we need to create what is called a future dataframe. We specify the number of future periods to predict (two years) and the frequency of predictions (daily). We then make predictions with the prophet model we created and the future dataframe: Our future dataframes contain the estimated market cap of Tesla and GM for the next two years. We can visualize predictions with the prophet plot function. The black dots represent the actual values (notice how they stop at the beginning of 2018), the blue line indicates the forecasted values, and the light blue shaded region is the uncertainty (always a critical part of any prediction). The region of uncertainty increases the further out in the future the prediction is made because initial uncertainty propagates and grows over time. This is observed in weather forecasts which get less accurate the further out in time they are made. We can also inspect changepoints identified by the model. Again, changepoints represent when the time series growth rate significantly changes (goes from increasing to decreasing for example). For comparison, we can look at the Google Search Trends for Tesla over this time range to see if the changes line up. We plot the changepoints (vertical lines) and search trends on the same graph: Some of the changepoints in the market value of Tesla align with changes in frequency of Tesla searches, but not all of them. From this, I would say that relative Google search frequency is not a great indicator of stock changes. We still need to figure out when the market capitalization of Tesla will surpass that of General Motors. Since we have both predictions for the next two years we can plot both companies on the same graph after merging the dataframes. Before merging, we rename the columns to keep track of the data. First we will plot just the estimate. The estimate (called ‘yhat’ in the prophet package) smooths out some of the noise in the data so it looks a little different than the raw plots. The level of smoothness will depend on the changepoint prior scale — higher priors mean a more flexible model and more ups and downs. Our model thinks the brief surpassing of GM by Tesla in 2017 was just noise, and it is not until early 2018 that Tesla beats out GM for good in the forecast. The exact date is January 27, 2018, so if that happens, I will gladly take credit for predicting the future! When making the above graph, we left out the most important part of a forecast: the uncertainty! We can use matplotlib (see notebook) to show the regions of doubt: This is a better representation of the prediction. It shows the value of both companies is expected to increase, but Tesla will increase more rapidly than General Motors. Again, the uncertainty increases over time as expected for a prediction and the lower bound of Tesla is below the upper bound of GM in 2020 meaning GM might retain the lead. The last step of the market capitalization analysis is looking at the overall trend and patterns. Prophet allows us to easily visualize the overall trend and the component patterns: The trend is pretty clear: GM stock is rising and going to keep rising. The yearly pattern is interesting because it seems to suggest GM increases in value at the end of the year with a long slow decline into the summer. We can try to determine if there is a correlation between the yearly market cap and the average monthly sales of GM over the time period. I first gathered the monthly vehicle sales from Google and then averaged over the months using groupby. This is another critical data science operation, because often we want to compare stats between categories, such as users of a specific age group, or vehicles from one manufacturer. In this case, we want to calculate average sales in each month, so we group the months together and then average the sales. It does not look like monthly sales are correlated with the market cap. The monthly sales are second highest in August, which is right at the lowest point for the market cap! Looking at the weekly trend, there does not appear to be any meaningful signal (there are no stock prices recorded on the weekends so we look at the change during the week).This is to be expected as the random walk theory in economics states there is no predictable pattern in stock prices on a daily basis. As evidenced by our analysis, in the long run, stocks tend to increase, but on a day-to-day scale, there is almost no pattern that we can take advantage of even with the best models. A simple look at the Dow Jones Industrial Average (a market index of the 30 largest companies on the stock exchange) nicely illustrates this point: Clearly, the message is to go back to 1900 and invest your money! Or in reality, when the market drops, don’t withdraw because it will go back up according to history. On the overall scale, the day-to-day fluctuations are too small to even be seen and if we are thinking like data scientists, we realize that playing daily stocks is foolish compared to investing in the entire market and holding for long periods of time. Prophet can also be applied to larger-scale data measures, such as Gross Domestic Product, a measure of the overall size of a country’s economy. I made the following forecast by creating prophet models based on the historical GDP of the US and China. The exact date China will surpass the US in GDP is 2036! This model is limited because of the low frequency of the observations (GDP is measured once per quarter but prophet works best with daily data), but it provides a basic prediction with no macroeconomic knowledge required. There are many ways to model time-series, from simple linear regression to recurrent neural networks with LSTM cells. Additive Models are useful because they are quick to develop, fast to train, provide interpretable patterns, and make predictions with uncertainties. The capabilities of Prophet are impressive and we have only scratched the surface here. I encourage you to use this article and the notebook to explore some of the data offered by Quandl or your own time series. Stay tuned for future work on time series analysis, and for an application of prophet to my daily life, see my post on using these techniques to model and predict weight change. As a first step in exploring time-series, additive models in Python are the way to go! As always, I welcome feedback and constructive criticism. I can be reached at wjk68@case.edu. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Time Series Analysis in Python: An Introduction,7,published,131268,2974,1.5131136516476127,1,0,1,1,0,1
96,59,499.24362043917824,4,https://medium.com/p/the-failures-of-common-sense-1a4be757d09b,0,None,2018-01-13 09:51:00,41.77,6,66,2018-01-13 08:04:00,"['Books', 'Reading', 'Review', 'Psychology']","The Failures of Common Sense A review of Everything is Obvious: Once You Know the Answer The message of Everything is Obvious by Duncan Watts can be summed up as: “everything you tell yourself about the past is false.” We humans have a tendency to frame events as narratives and see patterns where none exist. Therefore, when we look at the past, we see a single story line leading to an inevitable conclusion when in reality, history is a tangled mess of individual actions. It is only many years after the fact, when we examine historical occurrences with the bias of hindsight does a clear narrative emerge. As an example, take some of the most famous battles in history: Gettysburg, the Battle of the Bulge, the D-Day landings. We think of these as a single event with one storyline — the heroic Allies storm the beaches and sweep through France — when to the soldiers on the ground at the time, everything was chaos. Our common sense — and popular media — have conditioned us to look for the story among the individual events, leading us to assume history is obvious. During a battle, a soldier is only conscious of his/her individual actions and has no sense of the broader state of the war. An Allied fighter on D-Day was concerned only with his survival and the eventual liberation of France was certainly not inevitable as it may appear to us today. Considered the current situation of political disagreements and fractured public opinion. 100 years from now, future historians will look back and tell a single narrative about these time, even though to us at the moment there is no clear story. Our natural inclination for patterns makes the storyline of the past obvious after the fact, but to everyone on the ground, nothing makes much sense. The phenomenon of concluding the outcome of past events was inevitable extends beyond battles and history. Looking back at Google, we think it’s obvious it would grow into a dominate technology company because clearly it had a great search engine, visionary executives, and top talent. However, it is only looking back at history that we construct this tale, and our explanation of how Google succeeded in lacking. In effect, we say that Google succeeded because it had the characteristics of Google, which is a prime example of circular reasoning. Infoseek could just as easily have become the dominant search provider and we would be telling the exact same story about the small team that grew into one of the most valuable companies in the world except with Google replaced by Infoseek. We have the same narrative bias when we think about popular media. We look at the success of Harry Potter and say that clearly in the 1990s-2000s the public had a taste for fictional stories of underdog boy wizards heading off to school with a cast of relatable characters. However, at the time no one would have made this prediction because it clearly was not obvious! Moreover, saying Harry Potter was massively successful because it had the elements of Harry Potter does not explain anything! Another failure of common sense is that we like to believe the world is ordered. This means we do not acknowledge the critical role played by randomness in determining which storyline plays out or which companies, books, songs, and even art become successful. The Mona Lisa is the most famous painting in the world and common opinion holds that it is revered because enigmatic subject or captivating style. Yet this overlooks the fact that over the centuries, there have been literally hundreds of thousands of other unique paintings that never caught the public imagination. There is nothing special about the Mona Lisa, and it was only because of a series of random events (including the stealing of the painting) 400 years after it was painted that it became popular. Studies that examine how songs become hits have shown the role of randomness in popularity. When subjects were not given any popularity information about the songs, the number of times they listened to each song varied among participants, and with no stand outs. However, when subjects were told ahead of time which songs were “popular”, they tended to listen to these more which then influenced other subjects to listen to these songs and so on. The researchers could effectively manipulate which songs would hit the top of the charts by assigning them arbitrary popularities! The phenomenon illustrated in this study is called cumulative advantage. Once a piece of media or a company gains a small advantage, its success becomes a self-reinforcing cycle. Google had a small lead in search, which led to more people using Google, which led to more traffic and better results which caused more people to search with Google until the company dominates. Individuals born into wealthier families start out with a slight lead, which they then can build on to accumulate a greater fortune until eventually we reach a point where the top 0.1% control more wealth than the bottom 90%. By downplaying the effects of randomness and initial advantage in success, we paint a false narrative that anyone can succeed with the right amount of effort. We all have heard stories of the business leader from an impoverished background who through her own sheer effort of will was able to lead a triumphant life. Yet these are noteworthy precisely because they are expectations! Especially in the individualist United States, we hear that people simply need to work hard to do well, which prevents us from addressing the real issues, such as education and segregated neighborhoods that lead to such wide disparities of wealth. Common sense is not a completely negative part of our lives, but it was more suited to making rapid decisions to avoid predators rather than rational choices where we must consider vast quantities of information. Once we come to realize where common sense fails us, we can start to think differently about the past and our own situation. Rather than one clear story leading to a predestined outcome, the past is a tangled collection of individual paths, any of which could have become the dominant storyline. Therefore, when we consider our current position, we have to realize there are more choices available to us than we can see. We are not doomed to environmental catastrophe or to an eventual collapse of the economy if we choose to take corrective actions. The people claiming that society is disintegrating are certainly not the ones working to make the world a better place to live! Recommendation I judge a book by whether or not I can get all the ideas in a five minute summary, or if reading it entirely is worth it for the details. Everything is Obvious makes the recommended list because it backs up all the phenomenon I’ve mentioned with real examples and studies. It is a rather short book, but manages to cover many important topics such as confirmation bias, the narrative fallacy, and cumulative advantage. In hindsight, this book was well worth my time, or at least that’s the story I like to tell myself! Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",The Failures of Common Sense,5,published,158,1314,0.04490106544901065,0,0,0,0,0,0
100,2900,495.01272793546303,561,https://towardsdatascience.com/stock-analysis-in-python-a0054e2c1a4c,17,Towards Data Science,2018-01-17 15:24:00,26.5,11,21761,2018-01-17 10:42:00,"['Data Science', 'Programming', 'Python', 'Analysis', 'Towards Data Science']","Stock Analysis in Python Exploring financial data with object-oriented programming and additive models It’s easy to get carried away with the wealth of data and free open-source tools available for data science. After spending a little bit of time with the quandl financial library and the prophet modeling library, I decided to try some simple stock data exploration. Several days and 1000 lines of Python later, I ended up with a complete stock analysis and prediction tool. Although I am not confident (or foolish) enough to use it to invest in individual stocks, I learned a ton of Python in the process and in the spirit of open-source, want to share my results and code so others can benefit. This article will show how to use Stocker, a Python class-based tool for stock analysis and prediction (the name was originally arbitrary, but I decided after the fact it nicely stands for “stock explorer”). I had tried several times to conquer classes, the foundation of object-oriented programming in Python, but as with most programming topics, they never quite made sense to me when I read the books. It was only when I was deep in a project faced with a problem I had not solved before that the concept finally clicked, showing once again that experience beats theoretical explanations! In addition to an exploration of Stocker, we will touch on some important topics including the basics of a Python class and additive models. For anyone wanting to use Stocker, the complete code can be found on GitHub along with documentation for usage. Stocker was designed to be easy to use (even for those new to Python), and I encourage anyone reading to try it out. Now, let’s take a look at the analysis capabilities of Stocker! Getting Started with Stocker After installing the required libraries, the first thing we do is import the Stocker class into our Python session. We can do this from an interactive Python session or a Jupyter Notebook started in the directory with the script. We now have the Stocker class in our Python session, and we can use it to create an instance of the class. In Python, an instance of a class is called an object, and the act of creating an object is sometimes called instantiation or construction. In order to make a Stocker object we need to pass in the name of a valid stock ticker (bold indicates output). Now, we have a microsoftobject with all the properties of the Stocker class. Stocker is built on the quandl WIKI database which gives us access to over 3000 US stocks with years of daily price data (full list). For this example, we will stick to Microsoft data. Although Microsoft might be seen as the opposite of open-source, they have recently made some changes that make me optimist they are embracing the open-source community (including Python). A class in Python is comprised of two main parts: attributes and methods. Without going into too much detail, attributes are values or data associated either with the class as a whole or with specific instances (objects) of the class. Methods are functions contained in the class which can act on that data. One attribute of a Stocker object is stock data for a specific company that is attribute is associated with the object when we construct it. We can access the attribute and assign it to another variable for inspection: The benefit of a Python class is that the methods (functions) and the data they act on are associated with the same object. We can use a method of the Stocker object to plot the entire history of the stock. The default value plotted is the Adjusted Closing price, which accounts for splits in the stock (when one stock is split into multiple stocks, say 2, with each new stock worth 1/2 of the original price). This is a pretty basic plot that we could have found from a Google Search, but there is something satisfying about doing it ourselves in a few lines of Python! The plot_stockfunction has a number of optional arguments. By default, this method plots the Adjusted Closing price for the entire date range, but we can choose the range, the stats to plot, and the type of plot. For example, if we want to compare the Daily Change in price with the Adjusted Volume (number of shares) traded, we can specify those in the function call. Notice the y-axis is in percentage change relative to the average value for the statistic. This scale is necessary because the daily volume is originally in shares, with a range in the hundreds of millions, while daily price change typically is a few dollars! By converting to percentage change we can look at both datasets on a similar scale. The plot shows there is no correlation between the number of shares traded and the daily change in price. This is surprising as we might have expected more shares to be traded on days with larger price changes as people rush to take advantage of the swings. However, the only real trend seems to be that the volume traded decreases over time. There is also a significant decrease in price on December 4, 2017 that we could try to correlate with news stories about Microsoft. A quick news search for December 3 yields the following: There certainly does not seem to be any indication that Microsoft stock is due for its largest price decrease in 10 years the next day! In fact, if we were playing the stock market based on news, we might have been tempted to buy stock because a deal with the NFL (second result) sounds like a positive! Using plot_stock,we can investigate any of the quantities in the data across any date range and look for correlations with real-world events (if there are any). For now, we will move on to one of the more enjoyable parts of Stocker: making fake money! Let’s pretend for a moment we had the presence of mind to invest in 100 shares of Microsoft at the company’s Initial Public Offering (IPO). How much richer would we be now? In addition to making us feel better, using these results will allow us to plan our trips back in time to maximize profits. If we are feeling too confident, we can try to tweak the results to lose money: Surprisingly, it is possible to lose money in the stock market! Additive Models Additive models are a powerful tool for analyzing and predicting time series, one of the most common types of real world data. The concept is straightforward: represent a time series as a combination of patterns on different time scales and an overall trend. We know the long-term trend of Microsoft stock is a steady increase, but there could also be patterns on a yearly or daily basis, such as an increase every Tuesday, that would be economically beneficial to know. A great library for analyzing time series with daily observations (such as stocks) is Prophet, developed by Facebook. Stocker does all the modeling work with Prophet behind the scenes for us, so we can use a simple method call to create and inspect a model. The additive model smooths out the noise in the data, which is why the modeled line does not exactly line up with the observations. Prophet models also calculate uncertainty, an essential part of modeling as we can never be sure of our predictions when dealing with fluctuating real life processes. We can also use a prophet model to make predictions for the future, but for now we are more concerned with past data. Notice that this method call returned two objects, a model and some data, which we assigned to variables. We now use these variables to plot the time series components. The overall trend is a definitive increase over the past three years. There also appears to be a noticeable yearly pattern (bottom graph), with prices bottoming out in September and October and reaching a peak in November and January. As the time-scale decreases, the data gets noisier. Over the course of a typical month, there is more signal than noise! If we believe there might be a weekly pattern, we can add that in to the prophet model by changing the weekly_seasonalityattribute of the Stocker object: The default value for weekly_seasonalityis False, but we changed the value to include a weekly pattern in our model. We then make another call to create_prophet_modeland graph the resulting components. Below is the weekly seasonality from the new model. We can ignore the weekends because the price only changes over the week (in reality the price changes by a small amount during after-hours training but it does not affect our analysis). Unfortunately, there is not a trend over the week that we can use and before we continue modeling, we will turn off the weekly seasonality. This behavior is expected: with stock data, as the time scale decreases, the noise starts to wash out the signal. On a day-to-day basis, the movements of stocks are essentially random, and it is only by zooming out to the yearly scale that we can see trends. Hopefully this serves as a good reminder of why not to play the daily stock game! Changepoints Changepoints occur when a time-series goes from increasing to decreasing or vice versa (in a more rigorous sense, they are located where the change in the rate of the time series is greatest). These times are extremely important because knowing when a stock will reach a peak or is about to take off could have significant economic benefits. Identifying the causes of changepoints might let us predict future swings in the value of a stock. The Stocker object can automatically find the 10 largest changepoints for us. The changepoints tend to line up with peaks and valleys in the stock price. Prophet only finds changepoints in the first 80% of the data, but nonetheless, these results are useful because we can attempt to correlate them with real-world events. We could repeat what we did earlier and manually search for Google News around these dates, but I thought it would be preferable if Stocker did that for us. You might have seen the Google Search Trends tool which allows you to see the popularity of any search term over time in Google searches. Stocker can automatically retrieve this data for any search term we specify and plot the result on the original data. To find and graph the frequency of a search term, we modify the previous method call. In addition to graphing the relative search frequency, Stocker displays the top related queries and the top rising queries for the date range of the graph. On the graph, the y-axis is normalized between 0 and 1 by dividing the values by their maximums, allowing us to compare two variables with different scales. From the figure, there does not appear to be a correlation between searches for “Microsoft profit” and the stock price of Microsoft. Had we found a correlation, there would still be the question of causation. We would not know if searches or news caused the price to change, or if the change in price caused the searches. There might be some useful information to be found, but there are also many chance correlations. (For a humorous take on such random relationships, check out spurious correlations). Feel free to try out some different terms to see if you can find any interesting trends! Looks like declining searches for Office leads to increasing stock prices. Maybe someone should let Microsoft know. Predictions We have only explored the first half of Stocker capabilities. The second half is designed for forecasting, or predicting future stock price. Although this might be a futile exercise (or at least will not pay off), there is still plenty to learn in the process! Stay tuned for a future article on prediction, or get started predicting with Stocker on your own (check out the documentation for details). For now, I’ll leave you with one more image. Although all the capabilities of Stocker might already be publically available, the process of creating this tool was enjoyable, and more importantly, taught me more about data science, Python, and the stock market than any college course could. We live in an incredible age of democratized knowledge where anyone can learn about programming or even state of the art fields like machine learning without formal instruction. If you have an idea for a project but think you do not know enough or find out it has been done before, don’t let that stop you. You might develop a better solution and even if you don’t, you’ll be better off and know more than if you had never started! As always, I welcome constructive criticism and feedback. I can be reached on twitter at @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Stock Analysis in Python,4,published,82117,2395,1.210855949895616,0,0,1,1,0,1
98,180,494.8333162942014,21,https://towardsdatascience.com/a-review-of-the-coursera-machine-learning-specialization-b889f5793648,2,Towards Data Science,2018-01-17 19:42:00,62.04,5,3239,2018-01-17 18:16:00,"['Data Science', 'Machine Learning', 'Programming', 'Coursera', 'Review']","A Review of the Coursera Machine Learning Specialization With so many high-quality options for studying machine learning, Coursera does not make the cut. Although I consider myself an ardent supporter of the democratization of education through online courses, I keep a healthy skeptical attitude towards what these classes can and cannot do. The Coursera Machine Learning Specialization from the University of Washington aims to help students “Build Intelligent Applications. Master machine learning fundamentals.” On these two self-declared criteria, the course fails. Ultimately, the specialization serves at most as a high-level overview of basic machine learning topics, but upon graduation, students will be hard-pressed to apply any of the concepts to real problems on their own. It’s difficult to assign a rating because the course might be better-suited for some individuals than others, but, comparing it to both the Udacity Machine Learning Nanodegree and college courses, I would give it 2 out of 5. The shallow assignments and lack of an involved project means the course does not provide students with transferable skills. When I signed up for the specialization, it was with the promise of 4 separate courses and a hands-on capstone project. The four courses were each expected to take about 6 weeks to complete with 5–8 hours of work per week and consisted of the following: fundamentals, regression, classification, and clustering/retrieval. Unbeknownst to me, the capstone was removed from the specialization, a fact difficult to figure out as numerous official Coursera emails and videos still referred to an application we would build ourselves. Had there been a capstone, my opinion might have changed, but I cannot recommend a class with no significant project requiring implementing course concepts. The specialization is broken into the four named sections, each with six or seven submodules. Each submodule consisted of a series of 5–10 minute lecture videos produced by the University of Washington. I found the videos informative and helpful as the instructors outlined concepts and usually followed them up with a worked out example. The two instructors, both professors at Washington, explained concepts in an easy to follow manner. However, a course cannot rely on lectures alone, and in order to be effective, the ideas covered in class must be reinforced with homework. The Coursera specialization did provide numerous assignments, but they did not succeed in teaching students how to apply the concepts to new problems. Every submodule generally had one or two conceptual multiple-choice quizzes as well as one or two Jupyter Notebook assignments. The Jupyter Notebook has become a popular teaching tool because it is an interactive environment that includes code (Python for this course), text, and images in the same document. The Coursera notebooks looked well done, but unfortunately, the work required by students to complete the assignments was too simplistic to have any learning value. The notebooks had a number of fill-in-the-blanks in the code that students had to complete to get a correct answer. The notebooks themselves were not graded, instead, there was a short online quiz to complete based on the code output. Each fill-in-the-blank in the notebooks usually took 1 or 2 lines, and generally was unchallenging and repetitive. There were a few confusing exceptions when the required code was complex, leading to an exercise in boredom punctuated by frustration. The class assumes moderate knowledge of Python, but if you are taking this specialization to improve your programming skills, you will be disappointed. Students are never required to write a complete function, only short snippets within pre-defined functions. Any difficult code is already written in the notebook or hidden away in a separate script. The Jupyter Notebook is a great teaching tool when used correctly, but having students make exceedingly minor changes in a wall of code written by other people is not the ideal way to study machine learning. Going by Coursera’s schedule, the specialization should take about 6 months working at 5–8 hours/week. At a cost of $50/month, that is $300 for the entire specialization. The way that the class works, you enroll in one of the four courses at a time and have certain deadlines you are supposed to meet. As far as I know, there is no actual penalty for missing the deadline other than increased cost as you can come back later and complete the work. I finished the class in 6 weeks, working about 10–15 hours per week. The largest part of the time was spent watching the videos, which I felt did provide me with some benefit, but not $100 worth. The Jupyter Notebooks do not receive a passing grade as they did not reinforce the video concepts. I may be a little harsh on the Coursera Specialization because my expectations were based on the Udacity Machine Learning Nanodegree (MLND) I recently completed. In contrast, the Udacity MLND, was a great learning experience. The videos were at least as good as those from Coursera, and were regularly followed up with conceptual and programming quizzes. This short reinforcement cycle helped to solidify the concepts in the videos. The Udacity degree also featured an extensive capstone project that was hand-reviewed by a human grader. Machine learning is studied best by doing, not watching, and Udacity has that figured out. The Udacity course (when I took it) was expected to last 6 months and cost $200/month. I completed it in 1 month doing 50 hours of work per week. Overall, I cannot recommend the Coursera Machine Learning Specialization. The class is not worth the time or cost investment without a capstone project. If you are self-motivated, for around $30, you can buy any number of great machine learning books, including Hands-On Machine Learning with Scikit-Learn and Tensorflow which taught me much more than this course! For people who want to make the investment, I do recommend Udacity’s MLND course, although there are also numerous free videos and example-based tutorials. In an age with so much quality content, we have to be selective, and Coursera’s Machine Learning Specialization does not make the cut. Have any questions about the Coursera Specialization or Udacity’s MLND? I can be reached on Twitter at @koehrsen_will Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",A Review of the Coursera Machine Learning Specialization,8,published,5221,1169,0.15397775876817793,0,0,1,0,1,0
95,27,494.2829392827546,7,https://medium.com/p/the-perils-of-rare-events-27dd1ae4e35e,0,None,2018-01-18 08:55:00,53.21,6,58,2018-01-18 07:16:00,"['Tech', 'Review', 'Books', 'Finance', 'Decision Making']","The Perils of Rare Events A review of The Black Swan: The Impact of the Highly Improbable Why is is that we would be shocked to find someone 55 feet tall, but we routinely hear of people making $5.6 million a year or more and think nothing of it? The former represents an individual only 10 times the average height, while the latter has a salary 100 times the average household income in the US. As Nassim Taleb explains in The Black Swan, these two quantities comes from fundamentally different realms. The first is generated by the natural world, what Taleb refers to as Mediocristan, while the second is a product of the recent technological world created by humans, Extremistan. For nearly all of human history, we have occupied Mediocristan, where quantities are normally distributed with few values far from the average and no extreme values. Almost all phenomenon in nature behave in this manner: there are no animals that live for 10,000 years, no trees miles tall, and no humans that can run 100 miles per hour. It is only in the past century as a result of science and technological advances that we have constructed Extremistan, where outlying values such as Bill Gates’ net worth of $80 billion (about 300,000 times the average American net worth) or books sales of 500 million (the first Harry Potter) are entirely possible. These professions, computer software and creative fields, are examples of jobs that are infinitely scalable, that is, the amount of gain added does not scale linearly with the amount of effort put in. If J.K. Rowling writes a successful book that sells 10,000 copies, she does not have to put in 10 times as much effort to write a book that sells 100,000 copies. She can add another zero to the total sales with a minimal amount of effort. In contrast, traditional professions, such as baking scale linearly. In order for a baker to sell another loaf of bread, he has to bake another loaf and 10 times more sales means making 10 times as many loaves. As Taleb points out, if you want to become very wealthy, choose an infinitely scalable profession rather than a linearly scaling one where you are paid in direct proportion to the amount of hours worked. Evolution has not prepared us for quantities which can take on such a vast array of values. We are therefore ill-equipped to handle or even process events far out of the ordinary. The 777 point drop in the stock market on September 9, 2008 sent the entire world into a panic because we were used to seeing moves on the order of a few points up or down in a day. Finance, Taleb’s area of expertise, is one of the best examples of a field that lives in Extremistan. The wealth of individuals, the GDP of nations, and the value of stocks are all values that differ by numerous orders of magnitude, unlike most quantities in the natural world. There is simply no way to comprehend how three individuals (Gates, Warren Buffet, and Jeff Bezos) are worth more than the bottom 50% of Americans combined. The massive range of possible values for these quantities mean they can change by an unthinkable amount in a few hours, leaving us totally unprepared for the consequences. Finance is not the only modern pursuit to create extreme swings. Science and technology have created many values that are not normally distributed. Book sales, social media followers, search engine users, and battle deaths all tend to have a few outliers that vastly exceed the other examples combined. The World Wars were so terrible because the destruction dwarfed that of any previous conflict. We do not prepare for these events because they are simply out of our consideration, which served us well for 99% of human history, but now does not know how to handle numbers outside of “reasonable” ranges. “Black Swan” refers to the theory that residents of Europe only believed that white swans existed until they encountered black swans in Australia. The Europeans, like most of us today, suffered from what is known as confirmation bias. Rather than seek out examples that would have disproved their white swan theory, such as a single black swan, they kept seeing only white swans which further entrenched their (false) belief. We do this often, especially in politics where we tend to follow only news sources that confirm our biases and ignore evidence that goes against our personal beliefs. However, in order to prepare for Black Swan events, we should look for disconfirmatory evidence, observations that disprove our theories. It only takes a single counterexample, like the stock market crash of 1929 to show our theory about the rationality of markets is incorrect. By seeking only confirmation, we blind ourselves to the possibility that our ideas may be wrong, often with disastrous results. When a black swan does occur we try to rationalize it after the fact, a process known as hindsight bias. As humans, we like to believe the world obeys certain rules, and for the most part this served us well in our carefree days in Mediocristan. However, by justifying past events with a well-told narrative, we downplay the chance of future extreme events and therefore do not take them into consideration when planning our daily lives. While it would be foolish to take this idea too far and live constantly in fear of an event we cannot see coming we should be more open to the concept that rare phenomenon actually occur all the time, particularly in the recent fields of finance, science, and technology. What to Do? How can we deal with events that we don’t see coming and are outside of our realm of experience? Well, first we should give up trying to predict black swans because, as Taleb states, while we think that intelligent individuals make accurate predictions, it is in fact the smartest people who admit their uncertainty about the future. Instead of making uncertain forecasts, we can build up our robustness. For banks, that might mean maintaining more low-risk bonds than high risk stocks on the chance that the market takes a turn for the worse. On a personal level, we should try to learn from our small failures in order to avoid potentially larger ones down the road. We can learn the perils of playing the daily stock market from investing a few hundred dollars in penny stocks and avoid the larger mistake of losing all of our retirement savings. Taleb has outlined 10 principles to build resilience against Black Swans which includes holding individuals and institutions responsible for past mistakes, and making systems as simple as possible so they can be easily monitored for problems. It is imprudent to attempt to predict specific Black Swans, but if we prepare for the possibility of any Black Swan, then we will be better equipped to deal with the consequences. Recommendation As always, I judge a book by whether or not I can get the whole picture from a five-minute summary or if I need to read the entire book. In the case of the Black Swan, I would not recommend reading all of the book. Taleb covers a lot of interesting concepts, such as human biases, but they are not topics unique to his writing. He does provide a few personal anecdotes which help to reinforce the ideas but he also constructs several fake narratives to illustrate his points which I found distracting. If you want to learn about biases, I recommend Thinking, Fast and Slow by Daniel Kahnemann (or any of his research papers written with Amos Tversky). There are a number of good web pages on the general Black Swan theory and the basic idea can be wrapped up in one sentence: rare events happen more than we think they will, and while we cannot predict them precisely in advance, we can construct our society to be more resilient to surprises. It’s a valuable concept, and if we want to enjoy the benefits of science, technology, and finance, then we must reckon with their unique tradeoffs for which our natural history has not prepared us. Have any book recommendations for me or want to chat about data science, programming, or ultramarathon running? I can be reached on twitter at @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",The Perils of Rare Events,5,published,109,1557,0.017341040462427744,0,0,0,0,0,0
108,7800,493.00869821723387,1573,https://towardsdatascience.com/stock-prediction-in-python-b66555171a2,22,Towards Data Science,2018-01-19 15:30:00,31.16,12,42301,2018-01-13 18:27:00,"['Machine Learning', 'Data Science', 'Programming', 'Python', 'Towards Data Science']","Stock Prediction in Python Make (and lose) fake fortunes while learning real Python Trying to predict the stock market is an enticing prospect to data scientists motivated not so much as a desire for material gain, but for the challenge.We see the daily up and downs of the market and imagine there must be patterns we, or our models, can learn in order to beat all those day traders with business degrees. Naturally, when I started using additive models for time series prediction, I had to test the method in the proving ground of the stock market with simulated funds. Inevitably, I joined the many others who have tried to beat the market on a day-to-day basis and failed. However, in the process, I learned a ton of Python including object-oriented programming, data manipulation, modeling, and visualization. I also found out why we should avoid playing the daily stock market without losing a single dollar (all I can say is play the long game)! When we don’t experience immediate success — in any task, not just data science — we have three options: While option three is the best choice on an individual and community level, it takes the most courage to implement. I can selectively choose ranges when my model delivers a handsome profit, or I can throw it away and pretend I never spent hours working on it. That seems pretty naive! We advance by repeatedly failing and learning rather than by only promoting our success. Moreover, Python code written for a difficult task is not Python code written in vain! This post documents the prediction capabilities of Stocker, the “stock explorer” tool I developed in Python. In a previous article, I showed how to use Stocker for analysis, and the complete code is available on GitHub for anyone wanting to use it themselves or contribute to the project. Stocker for Prediction Stocker is a Python tool for stock exploration. Once we have the required libraries installed (check out the documentation) we can start a Jupyter Notebook in the same folder as the script and import the Stocker class: The class is now accessible in our session. We construct an object of the Stocker class by passing it any valid stock ticker (bold is output): Just like that we have 20 years of daily Amazon stock data to explore! Stocker is built on the Quandl financial library and with over 3000 stocks to use. We can make a simple plot of the stock history using the plot_stockmethod: The analysis capabilities of Stocker can be used to find the overall trends and patterns within the data, but we will focus on predicting the future price. Predictions in Stocker are made using an additive model which considers a time series as a combination of an overall trend along with seasonalities on different time scales such as daily, weekly, and monthly. Stocker uses the prophet package developed by Facebook for additive modeling. Creating a model and making a prediction can be done with Stocker in a single line: Notice that the prediction, the green line, contains a confidence interval. This represents the model’s uncertainty in the forecast. In this case, the confidence interval width is set at 80%, meaning we expect that this range will contain the actual value 80% of the time. The confidence interval grows wide further out in time because the estimate has more uncertainty as it gets further away from the data. Any time we make a prediction we must include a confidence interval. Although most people tend to want a simple answer about the future, our forecast must reflect that we live in an uncertain world! Anyone can make stock predictions: simply pick a number and that’s your estimate (I might be wrong, but I’m pretty sure this is all people on Wall Street do). For us to trust our model we need to evaluate it for accuracy.There are a number of methods in Stocker for assessing model accuracy. Evaluate Predictions To calculate accuracy, we need a test set and a training set. We need to know the answers — the actual stock price — for the test set, so we will use the past one year of historical data (2017 in our case). When training, we do not let our model see the answers to the test set, so we use three years of data previous to the testing time frame (2014–2016). The basic idea of supervised learning is the model learns the patterns and relationships in the data from the training set and then is able to correctly reproduce them for the test data. We need to quantify our accuracy, so we using the predictions for the test set and the actual values, we calculate metrics including average dollar error on the testing and training set, the percentage of the time we correctly predicted the direction of a price change, and the percentage of the time the actual price fell within the predicted 80% confidence interval. All of these calculations are automatically done by Stocker with a nice visual: Those are abysmal stats! We might as well have flipped a coin. If we were using this to invest, we would probably be better off buying something sensible like lottery tickets. However, don’t give up on the model just yet. We usually expect a first model to be rather bad because we are using the default settings (called hyperparameters). If our initial attempts are not successful, we can turn these knobs to make a better model. There are a number of different settings to adjust in a Prophet model, with the most important the changepoint prior scale which controls the amount of weight the model places on shifts in the trend of the data. Changepoint Prior Selection Changepoints represent where a time series goes from increasing to decreasing or from increasing slowly to increasingly rapidly (or vice versa). They occur at the places with the greatest change in the rate of the time series. The changepoint prior scale represents the amount of emphasis given to the changepoints in the model. This is used to control overfitting vs. underfitting (also known as the bias vs. variance tradeoff). A higher prior creates a model with more weight on the changepoints and a more flexible fit. This may lead to overfitting because the model will closely stick to the training data and not be able to generalize to new test data. Lowering the prior decreases the model flexibility which can cause the opposite problem: underfitting. This occurs when our model does not follow the training data closely enough and fails to learn the underlying patterns. Figuring out the proper settings to achieve the right balance is more a matter of engineering than of theory, and here we must rely on empirical results. The Stocker class contains two different ways to choose an appropriate prior: visually and quantitatively. We can start off with the graphical method: Here, we are training on three years of data and then showing predictions for six months. We do not quantify the predictions here because we are just trying to understand the role of the changepoint prior. This graph does a great job of illustrating under- vs overfitting! The lowest prior, the blue line, does not follow the training data, the black observations , very closely. It kind of does its own thing and picks a route through the general vicinity of the data. In contrast, the highest prior, the yellow line, sticks to the training observations as closely as possible. The default value for the changepoint prior is 0.05 which falls somewhere in between the two extremes. Notice also the difference in uncertainty (shaded intervals) for the priors. The lowest prior has the largest uncertainty on the training data, but the smallest uncertainty on the test data. In contrast, the highest prior has the smallest uncertainty on the training data but the greatest uncertainty on the test data. The higher the prior, the more confident it is on the training data because it closely follows each observation. When it comes to the test data however, an overfit model is lost without any data points to anchor it. As stocks have quite a bit of variability, we probably want a more flexible model than the default so the model can capture as many patterns as possible. Now that we have an idea of the effect of the prior, we can numerically evaluate different values using a training and validation set: Here, we have to be careful that our validation data is not the same as our testing data. If this was the case, we would create the best model for the test data, but then we would just be overfitting the test data and our model could not translate to real world data. In total, as is commonly done in data science, we are using three different sets of data: a training set (2013–2015), a validation set (2016), and a testing set (2017). We evaluated four priors with four metrics: training error, training range (confidence interval), testing error, and testing range (confidence interval) with all values in dollars. As we saw in the graph, the higher the prior, the lower the training error and the lower the uncertainty on the training data. We also see that a higher prior decreases our testing error, backing up our intuition that closely fitting to the data is a good idea with stocks. In exchange for greater accuracy on the test set, we get a greater range of uncertainty on the test data with the increased prior. The Stocker prior validation also displays two plots illustrating these points: Since the highest prior produced the lowest testing error, we should try to increase the prior even higher to see if we get better performance. We can refine our search by passing in additional values to the validation method: The test set error is minimized at a prior of 0.5. We will set the changepoint prior attribute of the Stocker object appropriately. There are other settings of the model we can adjust, such as the patterns we expect to see, or the number of training years of data the model uses. Finding the best combination simply requires repeating the above procedure with a number of different values. Feel free to try out any settings! Now that our model is optimized, we can again evaluate it: That looks better! This shows the importance of model optimization. Using default values provides a reasonable first guess, but we need to be sure we are using the correct model “settings,” just like we try to optimize how a stereo sounds by adjusting balance and fade (sorry for the outdated reference). Playing the Stock Market Making predictions is an interesting exercise, but the real fun is looking at how well these forecasts would play out in the actual market. Using the evaluate_prediction method, we can “play” the stock market using our model over the evaluation period. We will use a strategy informed by our model which we can then compare to the simple strategy of buying and holding the stock over the entire period. The rules of our strategy are straightforward: We play this each day for the entire evaluation period which in our case is 2017. To play, add the number of shares to the method call. Stocker will inform us how the strategy played out in numbers and graphs: This shows us a valuable lesson: buy and hold! While we would have made a considerable sum playing our strategy, the better bet would simply have been to invest for the long term. We can try other test periods to see if there are times when our model strategy beats the buy and hold method. Our strategy is rather conservative because we do not play when we predict a market decrease, so we might expect to do better than a holding strategy when the stock takes a downturn. I knew our model could do it! However, our model only beat the market when we were had the benefit of hindsight to choose the test period. Now that we are satisfied we have a decent model, we can make future predictions using the predict_future() method. The model is overall bullish on Amazon as are most “professionals.” Additionally, the uncertainty increases the further out in time we make estimates as expected. In reality, if we were using this model to actively trade, we would train a new model every day and would make predictions for a maximum of one day in the future. While we might not get rich from the Stocker tool, the benefit is in the development rather than the end results! We can’t actually know if we can solve a problem until we try but it’s better to have tried and failed than to have never tried at all! For anyone interested in checking out the code or using Stocker themselves, it is available on GitHub. As always, I enjoy feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Hide the results so no one ever notices Show all our results and methods so that others (and ourselves) can learn how to do things better If we buy stock and the price increases over the day, we make the increase times the number of shares we bought. If we buy stock and the price decreases, we lose the decrease times the number of shares. Tweak the results to make it look like we were successful On each day the model predicts the stock to increase, we purchase the stock at the beginning of the day and sell at the end of the day. When the model predicts a decrease in price, we do not buy any stock.",Stock Prediction in Python,4,published,135743,2528,3.0854430379746836,5,0,1,1,1,1
105,1200,492.809801557176,210,https://towardsdatascience.com/correlation-vs-causation-a-real-world-example-9e939c85581e,4,Towards Data Science,2018-01-19 20:16:00,35.83,6,7438,2018-01-19 15:47:00,"['Data Science', 'Statistics', 'Education', 'Critical Thinking']","Correlation vs. Causation: An Example Viewing real world statistics skeptically It’s surprising the insights waiting to be discovered deep within the mass of emails we all receive. While mindlessly browsing my inbox, I briefly scanned a message from my University’s Study Abroad office with the following info about the benefits of studying overseas: What immediately caught my eye was those figures in the 90s — clearly, studying abroad makes you irresistible to grad schools and employers. I was surprised at just how large the academic and career benefits were that came as a result of studying in another country. My second thought was: it’s too bad I didn’t choose to take advantage of those benefits, and I quickly archived the email and before I came to regret any further life decisions. However, something about the information stuck with me. I have been trying to take more time to consciously think through claims and statistics in this fake-news dominated age, and while this wasn’t on the same society-degrading level, something seemed off about the conclusion I had drawn. A few days later while listening to a data skeptic podcast it hit me: I had assumed that studying abroad caused students to have better grades and career prospects, when all the statistics showed was that the two were correlated. Most of us regularly make the mistake of unwittingly confusing correlation with causation, a tendency reinforced by media headlines like music lessons boost student’s performance or that staying in school is the secret to a long life. Sometimes, especially with health, these tend towards the unbelievable like a Guardian headline claiming a diet of fish leads to less violence. The common problem in these articles is that they take two correlated trends and present it as one phenomenon causing the other. The real explanation is usually much less exciting. For example, students who take music lessons may perform better in school, but they are also more likely to have grown up in an environment with a large emphasis on education and the resources needed to succeed academically. These students would therefore have higher school achievement with or without the music lessons. Taking music classes and school performance happen to rise in tandem because they are both products of a similar background, but one does not necessarily cause the other. Likewise, people who stay in school longer typically have more resources which also means they can afford better health care. Most of the time these mistakes are not made out of a deliberate effort to deceive (although that does occur) but out of an honest misunderstanding of the idea of causation. What the statistics, especially those in the study abroad email, show is a selection bias. In each study, the individuals observed do not come from a representative slice of society, but instead are all drawn from similar groups, leading to a skewed result. Think of the statistics showing students who study abroad are 19% more likely to graduate on time. While it might be possible that studying abroad did somehow motivate lagging students to graduate on time, the more likely explanation is that students who choose to go abroad were those in a better position academically in the first place. They would graduate on time with high GPAs regardless of whether they went to another country. It takes a lot of work and preparation to go to another country to study for a year, and the students who feel confident enough to do so are the ones who are on top of their studies. In this real-world case, the selection bias is towards better students. The sample of students who study abroad is not indicative of students as a whole, rather, it includes only the best-prepared students and therefore it is no surprise that this group has significantly better academic and career outcomes. The study abroad experience may look great in hindsight, but if we selected only the best students and had them do anything, it would be misleading to say the phenomenon led to better grades. Say for example we own a bottled water company and we want to gather some positive stats to help with sales. We hire a few students to stand outside the honors class and only give our water to the top students. We then conduct a study that shows conclusively that students who drink our brand get better grades. Because we selected a specific group of subjects to include in our study, we can make it look as though our water caused an increase in grades. The study abroad statistics come from what is known as an observational study. Rather than constructing an experiment, an observational study observes some process in the real world with no cannot control over the independent variable, in this case the students who chose to study abroad. Observational studies cannot prove cause and effect, only associations between different factors (such as achievement and studying in another country). In order to prove one process caused another requires a randomized controlled trial with subjects represent the entire population. In this case, carrying out a randomized controlled trial would require selecting a random subset of students across the range of academic performance, sending some to study abroad, and keeping a control group home. We could then analyze the results to determine if there were significant differences between the two groups. If there was, then we would probably carry out more studies controlling for more variables, until eventually we were satisfied there was no hidden effects and we could establish a causal relationship. I pointed these observations out to the CWRU Study Abroad Office and what followed was a decent and productive conversation. By posting about this, I was not trying to call out the office. Although the email did state: “Here are just some of the benefits this New Year’s Resolution can bring,” it wasn’t claiming an exact cause and effect. However, when a single topic is presented surrounded by a sea of facts, our natural inclination is to draw a causal link, a tendency marketers and companies take advantage of with regularity. I believe all the statistics in this case are valid, but we still need to avoid assigning a cause and effect relationship. Without randomized controlled trials, we cannot say one activity caused another, and all we can claim is that two trends are correlated. This is a small example, but it illustrates an extremely critical point: all of us, even grad students who use these concepts every single day, can be fooled by statistics. Humans naturally see patterns where they don’t exist, and we like to tell a cohesive story about what we think is going on (the narrative fallacy). However, the world usually does not have defined causes and effects, and we must settle for correlations. This view of the world may make headlines less exciting (it turns out chocolate is not a miraculous food), but it means you will not be fooled into buying products or taking actions that are not in your best interest due to questionable evidence. Moreover, we can share our experiences with others and create a skeptical community in which we make sound decisions for our benefit and not for a company’s bottom line. As always, I welcome constructive criticism and feedback. I can be reached on Twitter at @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Correlation vs. Causation: An Example,6,published,20761,1361,0.881704628949302,0,1,1,0,0,0
92,355,490.05558305577546,49,https://medium.com/p/real-life-superpowers-c69d66b51ed4,0,None,2018-01-22 14:22:00,27.87,7,481,2018-01-11 21:05:00,"['Education', 'Self Image', 'Character', 'Technology']","Real Life Superpowers The three traits that turn ordinary people into modern superheroes It turns out Hollywood was right all along. Besides making great documentaries about space like The Martian, movie studios are also remarkably on point about superheroes. They just got the powers real people use to transform into legends slightly wrong. Rather than flight or invincibility, real superheroes share three traits: Take Elon Musk, founder and/or CEO of PayPal, SpaceX, Tesla, SolarCity, Neuralink, and the Boring Company with a net worth north of $20 billion. How was Musk able to create Tesla, the first successful American car startup in over one hundred years which, at times, has been worth more than GM despite selling less than 1/100 the number of cars? How was he able to start and keep alive SpaceX, a private spaceflight company that started out with three disastrous failures? Simple: Musk used all of these superpowers better than other any human alive. Teach Yourself, Free Yourself If you cannot teach yourself, then you will only learn from others and will be limited to what they want you to know. Autodidacticism, the ability to educate yourself, frees you from the constraints of a traditional education. Conventional studies tend to focus on a narrow topic. This severely limits career flexibility and innovation potential of those who cannot learn on their own. As Steven Johnson points out in his book Where Good Ideas Come From, all significant innovations have come from connections between fields, not from deep within a specific field. This blurring of boundaries is called the “adjacent possible”, and is created by the ability to cross from one topic to another, a trait not instilled in us by structured education. Further, we corner ourselves into a specific career definition because we focus on a single topic in school and do not feel comfortable trying to learn a new subject. Change is difficult, and we tell ourselves we can’t leave an awful job because we don’t know anything different. This not only makes you a terrible conversation partner (trust me, no one wants to hear the particulars of your mechanical engineering education at dinner), it also means you will be stuck working for others in unsatisfactory conditions your entire life. Education, specifically self-education, is the best way to ensure you get to live life on your terms. Musk, a business and physics major by training, took this lesson by heart. There was nothing in his college education that would have prepared him to start an online payments company, a rocket company, or an electric vehicle company. Musk has never been constrained by his education. Instead of backing away from the challenge of starting a rocket company despite knowing little rocket science, he defined his goal and then devoted himself to a study of aerospace engineering. He also surrounded himself with the brightest individuals in the field, demonstrating that a necessary part of self education is never being the smartest person in the room. By teaching himself what school would not, Musk was able to learn enough to figure out how to get stuff into space cheaper than anyone else — and build a multi-billion dollar company in the process. If Musk becomes the first man to set foot on Mars, it won’t be because a professor taught him how to get there in class. Make them Believe Anything Musk’s autodidacticism allowed him to create his companies, but his ability to convince people these businesses would prosper is what has allowed them to succeed. When Musk first announced the Tesla Model 3, over 500,000 people deposited $1000 for the opportunity to buy a car they hadn’t seen in person and wouldn’t be delivered for years. Tesla conducted no mass-marketing sales campaign, they merely gave Musk a stage and he did the rest. While Musk’s talks are not the polished deliveries of business school classes, he makes up for it with his exceptional portrayal of the future. He is persuasive because he sells not products, but a vision of utopia that will be obtained only by buying into his companies. Painting a vision of the future was a tactic also employed by another extraordinary CEO (and great salesman), the late Steve Jobs. Numerous companies developed mobile phones and tablets before the iPhone or iPad, but only Jobs was able to convince us we actually needed them. His speech announcing the first iPhone in 2007 makes the smartphone seem extraordinary even today. People do not willingly adopt a new argument based solely on its merits, you have to show them that your idea will improve their lives and is something they can’t live without. Writing clearly and persuasively is another critical aspect of achieving superhuman power. Think of a physicist. Who was the first person to come to mind? Probably Stephen Hawking or Neil DeGrasse Tyson, not because they are the most brilliant in their field, but because they have the best public outreach. Some people take the length of a work to be the measure of sophistication, but in fact, it's the opposite: those who can communicate their ideas in the shortest amount of time or words best understand those ideas. A study conducted at Princeton showed people rate papers with understandable words as more intelligent than papers with long, complex words. Moreover, they were more likely to accept the author’s ideas when they were presented in clear language. As the studies author’s conclude: “All in all, the effect is extremely robust: needless complexity leads to negative evaluations”. Great speakers and writers aim for the fewest words with the greatest impact. Whenever I write or speak I think of the following single word: cogent: clear, logical, and convincing There is no better summary of how to communicate. Harness Technology to do your Bidding When I say you need to be technically literate, I do not mean you need to understand how to build your own computer. A simple knowledge of how to make things easier with the appropriate tools can make you seem like a genius. While interning at NASA, I saw the country’s smartest individuals literally doing rocket science in Excel. I knew a little Python (a common coding language), and building on some code I copied from the internet, I was able to automate colleagues’ tasks and save them hundreds of hours. Instantly, I was transformed from a lowly intern into the hero of my division. Technical literacy is often not about inventing an answer from scratch, but rather adopting an existing solution to fit your problem. Our two model CEOs were no strangers to the idea of building upon tech that already existed. Apple was successful by creating a user-friendly product and acting at the right time when the technology — the PC, the smartphone, or the iPad — had matured to the point that anyone could figure it out. Often, the inventor of a technology does not reap the benefits because they are too early or do not see the potential applications. Finding ways to implement an invention is often more important than developing the idea. Musk did not create the internet or online commerce, but he was able to take advantage of existing platforms to found his first successful venture, PayPal. One doesn’t need to understand neural networks or artificial intelligence to be a tech wizard. Often, just being able to get a computer working by turning it off and on will win you immense adoration. One of the principles of technical literacy is to solve the problem using the easiest solution. Making a useful calendar with reminders does not require machine learning, only Google and a few minutes of tinkering. Technology is supposed to make our lives easier, and with a little trial and error, we can harness it for that purpose. While Musk and Jobs demonstrated all three traits, you don’t need to master all three to become a productive member of society. Simply keep these superpowers in mind as you go through daily actions. The next time you want to learn a topic, don’t look first at a list of college courses, but consider if you can teach it to yourself, preferably with real examples. If you have to give a presentation, work on making your point with as few slides as possible and try writing cogent emails. Finally, don’t be intimidated by our digital world and understand we control our technology. By harnessing these superpowers you might not get a movie made about you (although Jobs did), but you will be better equipped to construct a life on your own terms and shape the world to your liking. As always, I welcome feedback and constructive criticism. I can be reached on Twitter at @koehrsen_will. I would like to thank Taylor Koehrsen PharmD for her help in editing this post! Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Speaking and writing persuasively Technical literacy The ability to teach themselves",Real Life Superpowers,3,published,1726,1653,0.2147610405323654,10,1,0,0,0,0
94,1800,487.88570743173614,294,https://towardsdatascience.com/learn-by-sharing-4461cc93f8c1,16,Towards Data Science,2018-01-24 18:27:00,54.63,4,2290,2018-01-23 19:48:00,"['Data Science', 'Education', 'Technology', 'Studying', 'Towards Data Science']","Learn By Sharing Why I’m ditching the library to write a data science blog Traditional education is simple: sit down, shut up, and listen to the teacher. After class, go to the library to repeatedly read the same words, trying to figure out abstract topics with little meaning in our daily lives. Even as a graduate student, I still am routinely lectured at and expected to spend large portions of time outside of class alone in contemplating my studies. While this might work fine for subjects that require simple regurgitation of information on a test — looking at you history — it is entirely unsuited for modern technical topics such as data science. With that in mind, here’s a radical proposal: rather than hitting the books when you want to understand a concept, you should hit your blog and try to explain it clearly to others. The idea is simple: if you can’t teach a topic to someone else, then you truly don’t get it yourself. When I began grad classes, I decided to take a new approach to education. Instead of sitting passively in class, I aimed to ask at least one question every lecture. This small adjustment had a profound impact on my engagement in class. I focused my questions on how to implement concepts we covered which often were presented without any practical examples. This active participation made it easier to concentrate in class and to apply topics to problems both in my research and on assignments. Outside of class, I spent less time studying alone and more time in the lab implementing data science techniques. I also made an effort to engage in conversations with other students about what we covered in class. In the process, I was trying to understand the topics not by memorization, but by explaining them to others. Informed by these discussions, my colleagues and I would try to use the techniques on our problems. Whether we failed or succeeded, we would come back for more debates, creating a productive feedback loop. Fortunately, I am in a lab with students and professors smarter than myself — it’s a good idea to never be the smartest person in the room — and every day I learn something new through seeing it done in practice. With even more data science and Artificial Intelligence (AI) grad classes this semester, I need to step up my sharing game. My goal is to write at least one blog post explaining a topic covered in class each week. I don’t have as much time to develop cool side applications such as the stock exploration or weight tracker Python tools, but I can take the time normally spent reviewing class material and instead write about what I have learned. This serves both to test if I actually understand the material and to benefit others! Communities are best served when information — at least non-harmful info— is freely shared. Some people think because they worked hard to learn what they know, others must do the same and they refuse to divulge anything that would make learning easier for others. I ardently disagree with this view: just because we pay tens of thousands of dollars for an education does not mean we should keep it to ourselves. Instead, I believe in the democratization of education and in helping others to learn from my (many) mistakes and (limited but growing) experience. In technical fields, particularly data science, the internet has expanded access to information, and it is now possible for anyone to learn and practice cutting edge techniques. Formal institutions no longer have a monopoly on knowledge, and I want to play a small part in lowering the barriers to these exciting new fields. Trying to explain concepts helps us understand them better ourselves. It takes genuine understanding and not memorization to translate a topic for a general audience. We have all experienced the situation where we exhaustively study a topic and think we completely understand the idea, only to completely blank when we have to apply it in a basic situation. Moreover, the most successful individuals in a field tend to not be the smartest, but those who can best communicate findings and show how they are relevant. Neil deGrasse Tyson is the best-known physicist in the world, not because he publishes the most brilliant papers, but because he translates tough concepts for a wide audience. Clear written and spoken communication skills are a major advantage that cannot be taught in a classroom! These once a week posts will usually be about data science and machine learning with a focus on real-world examples and metaphors. A good indicator of my aim is this correlation vs causation post. While metaphors can oversimplify concepts, my intent is to provide a high-level framework for learning these concepts. It’s useful to have the basic ideas down before diving into the details. The specifics can be filled in by applying them to solve problems (or maybe in a book if you prefer that route). If you can’t wait for my posts, I suggest checking out the data skeptic podcast, which does a great job of distilling data science topics for a general audience. Better yet, start your own blog! Writing is the best form of thinking out your ideas and sharing knowledge benefits everyone in the community. As always, I welcome feedback and constructive criticism. I can be reached on Twitter at @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Learn By Sharing,3,published,4192,1014,1.7751479289940828,0,1,1,1,0,0
103,1800,484.80900174635417,332,https://towardsdatascience.com/overfitting-vs-underfitting-a-conceptual-explanation-d94ee20ca7f9,3,Towards Data Science,2018-01-27 20:17:00,41.77,6,7194,2018-01-26 18:22:00,"['Data Science', 'Machine Learning', 'Model', 'Education', 'Towards Data Science']","Overfitting vs. Underfitting: A Conceptual Explanation An example-based framework of a core data science concept Say you want to learn English. You have no prior knowledge of the language but you’ve heard the greatest English writer is William Shakespeare. A natural course of action surely must be locking yourself in a library and memorizing his works. After a year of study, you emerge from your studies, travel to New York City, and greet the first person you see with “Good dawning to thee, friend!” In response, you get a look of disdain and a muttered ‘crazy’. Unperturbed, you try again: “Dear gentlewoman, How fares our gracious lady?” Another failure and a hurried retreat. After a third unsuccessful attempt, you are distraught: “What shame what sorrow!”. Shame indeed: you have just committed one of the most basic mistakes in modeling, overfitting on the training data. In data science courses, an overfit model is explained as having high variance and low bias on the training set which leads to poor generalization on new testing data. Let’s break that perplexing definition down in terms of our attempt to learn English. The model we want build is a representation of how to communicate using the English language. Our training data is the entire works of Shakespeare and our testing set is New York. If we measure performance in terms of social acceptance, then our model fails to generalize, or translate, to the testing data. That seems straightforward so far, but what about variance and bias? Variance is how much a model changes in response to the training data. As we are simply memorizing the training set, our model has high variance: it is highly dependent on the training data. If we read the entire works of J.K. Rowling rather than Shakespeare, the model will be completely different. When a model with high variance is applied on a new testing set, it cannot perform well because all it is lost without the training data. It’s like a student that has memorized the problems in the textbook, only to be helpless when faced with real world faced problems. Bias is the flip side of variance as it represents the strength of our assumptions we make about our data. In our attempt to learn English, we formed no initial model hypotheses and trusted the Bard’s work to teach us everything about the language. This low bias may seem like a positive— why would we ever want to be biased towards our data? However, we should always be skeptical of data’s ability to tell us the complete story. Any natural process generates noise, and we cannot be confident our training data captures all of that noise. Often, we should make some initial assumptions about our data and leave room in our model for fluctuations not seen on the training data. Before we started reading, we should have decided that Shakespeare’s works could not literally teach us English on their own which would have led us to be cautious of memorizing the training data. To summarize so far: bias refers to how much we ignore the data, and variance refers to how dependent our model is on the data. In any modeling, there will always be a tradeoff between bias and variance and when we build models, we try to achieve the best balance. Bias vs variance is applicable to any model, from the simplest to the most complex and is a critical concept to understand for data scientists! We saw that a model that overfits has high variance and low bias. What about the reverse: low variance and high bias? This is known as underfitting: instead of following the training data too closely, a model that underfits the ignores the lessons from the training data and fails to learn the underlying relationship between inputs and outputs. Let’s think about this in terms of our example. Learning from our previous attempt to build a model of English, we decide to make a few assumptions about the model ahead of time. We also switch our training data and watch all episodes of the show Friends to teach ourselves English. To avoid repeating our mistakes from the first try, we make an assumption ahead of time that only sentences starting with the most common words in the language — the, be, to, of, and, a — are important. When we study, we do not pay attention to other sentences, confident we will build a better model. After a long period of training, we again journey out onto the streets of New York. This time we fare slightly better, but again, our conversations go nowhere and we are forced to admit defeat. While we know some English and can comprehend a limited number of sentences, we failed to learn the fundamental structure of the language due to our bias about the training data. The model does not suffer from high variance but we overcorrected from our initial attempt and underfit! What can we do? We paid strict attention to the data and we overfit. We ignored the data and we underfit. There has to be a way to find the optimal balance! Fortunately, there is a well-established solution in data science called validation. In our example, we used only a training set and a testing set. This meant we could not know ahead of time how our model would do in the real world. Ideally, we would have a “pre-test” set to evaluate our model and make improvements before the real test. This “pre-test” is known as a validation set and is a critical part of model development. Our two failures to learn English have made us much wiser and we now decide to use a validation set. We use both Shakespeare’s work and the Friends show because we have learned more data almost always improves a model. The difference this time is that after training and before we hit the streets, we evaluate our model on a group of friends that get together every week to discuss current events in English. The first week, we are nearly kicked out of the conversation because our model of the language is so bad. However, this is only the validation set, and each time we make mistakes we are able to adjust our model. Eventually, we can hold our own in conversation with the group and declare we are ready for the testing set. Venturing out in the real world once more, we are finally successful! Our model is now well suited for communication because we have a crucial element, a validation set for model development and optimization. This example is necessarily simplified. In data science models, we use numerous validation sets because otherwise we end up overfitting to the validation set! This is addressed by means of cross-validation, where we split the training data into different subsets, or we can use multiple validation sets if we have lots of data. This conceptual example stills covers all aspects of the problem. Now when you hear about overfitting vs. underfitting and bias vs. variance, you have a conceptual framework to understand the problem and how to fix it! Data science may seem complex but it is really built out of a series of basic building blocks. A few of those covered in this article are: Data science and other technical fields should not be divorced from our everyday lives. By explaining concepts with real-world examples, we can put them into context. If we understand the framework, then we can fill in the details by using the techniques on problems. The next post will provide an example using graphs and metrics, so if you want a more solid backing, check it out. Until then, fare you well dear readers! I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. I would like to thank Taylor Koehrsen (PharmD by the way) for helping me to sound less like an engineer in my writing! Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Underfitting: a failure to learn the relationships in the training data High Variance: model changes significantly based on training data High Bias: assumptions about model lead to ignoring training data Overfitting and underfitting cause poor generalization on the test set A validation set for model tuning can prevent under and overfitting Overfitting: too much reliance on the training data",Overfitting vs. Underfitting: A Conceptual Explanation,7,published,17223,1562,1.1523687580025608,1,1,1,1,1,0
91,1300,484.29280312907406,186,https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765,8,Towards Data Science,2018-01-28 08:40:00,30.9,11,11218,2018-01-26 20:32:00,"['Machine Learning', 'Data Science', 'Education', 'Technology']","Overfitting vs. Underfitting: A Complete Example Exploring and solving a fundamental data science problem When you study data science you come to realize there are no truly complex ideas, just many simple building blocks combined together. A neural network may seem extremely advanced, but it’s really just a combination of numerous small ideas. Rather than trying to learn everything at once when you want to develop a model, it’s more productive and less frustrating to work through one block at a time. This ensures you have a solid idea of the fundamentals and avoid many common mistakes that will hold up others. Moreover each piece opens up new concepts allowing you to continually build up knowledge until you can create a useful machine learning system and, just as importantly, understand how it works. This post walks through a complete example illustrating an essential data science building block: the underfitting vs overfitting problem. We’ll explore the problem and then implement a solution called cross-validation, another important principle of model development. If you’re looking for a conceptual framework on the topic, see my previous post. All of the graphs and results generated in this post are written in Python code which is on GitHub. I encourage anyone to go check out the code and make their own changes! Model Basics In order to talk about underfitting vs overfitting, we need to start with the basics: what is a model? A model is simply a system for mapping inputs to outputs. For example, if we want to predict house prices, we could make a model that takes in the square footage of a house and outputs a price. A model represents a theory about a problem: there is some connection between the square footage and the price and we make a model to learn that relationship. Models are useful because we can use them to predict the values of outputs for new data points given the inputs. A model learns relationships between the inputs, called features, and outputs, called labels, from a training dataset. During training the model is given both the features and the labels and learns how to map the former to the latter. A trained model is evaluated on a testing set, where we only give it the features and it makes predictions. We compare the predictions with the known labels for the testing set to calculate accuracy. Models can take many shapes, from simple linear regressions to deep neural networks, but all supervised models are based on the fundamental idea of learning relationships between inputs and outputs from training data. Training and Testing Data To make a model, we first need data that has an underlying relationship. For this example, we will create our own simple dataset with x-values (features) and y-values (labels). An important part of our data generation is adding random noise to the labels. In any real-world process, whether natural or man-made, the data does not exactly fit to a trend. There is always noise or other variables in the relationship we cannot measure. In the house price example, the trend between area and price is linear, but the prices do not lie exactly on a line because of other factors influencing house prices. Our data similarly has a trend (which we call the true function) and random noise to make it more realistic. After creating the data, we split it into random training and testing sets. The model will attempt to learn the relationship on the training data and be evaluated on the test data. In this case, 70% of the data is used for training and 30% for testing. The following graph shows the data we will explore. We can see that our data are distributed with some variation around the true function (a partial sine wave) because of the random noise we added (see code for details). During training, we want our model to learn the true function without being “distracted” by the noise. Choosing a model can seem intimidating, but a good rule is to start simple and then build your way up. The simplest model is a linear regression, where the outputs are a linearly weighted combination of the inputs. In our model, we will use an extension of linear regression called polynomial regression to learn the relationship between x and y. Polynomial regression, where the inputs are raised to different powers, is still considered a form of “linear” regression even though the graph does not form a straight line (this confused me at first as well!)The general equation for a polynomial is below. Here y represents the label and x is the feature. The beta terms are the model parameters which will be learned during training, and the epsilon is the error present in any model. Once the model has learned the beta values, we can plug in any value for x and get a corresponding prediction for y. A polynomial is defined by its order, which is the highest power of x in the equation. A straight line is a polynomial of degree 1 while a parabola has 2 degrees. Overfitting vs. Underfitting The problem of Overfitting vs Underfitting finally appears when we talk about the polynomial degree. The degree represents how much flexibility is in the model, with a higher power allowing the model freedom to hit as many data points as possible. An underfit model will be less flexible and cannot account for the data. The best way to understand the issue is to take a look at models demonstrating both situations. First up is an underfit model with a 1 degree polynomial fit. In the image on the left, model function in orange is shown on top of the true function and the training observations. On the right, the model predictions for the testing data are shown compared to the true function and testing data points. Our model passes straight through the training set with no regard for the data! This is because an underfit model has low variance and high bias. Variance refers to how much the model is dependent on the training data. For the case of a 1 degree polynomial, the model depends very little on the training data because it barely pays any attention to the points! Instead, the model has high bias, which means it makes a strong assumption about the data. For this example, the assumption is that the data is linear, which is evidently quite wrong. When the model makes test predictions, the bias leads it to make inaccurate estimates. The model failed to learn the relationship between x and y because of this bias, a clear example of underfitting. We saw a low degree leads to underfitting. A natural conclusion would be to learn the training data, we should just increase the degree of the model to capture every change in the data. This however is not the best decision! With such a high degree of flexibility, the model does its best to account for every single training point. This might seem like a good idea — don’t we want to learn from the data? Further, the model has a great score on the training data because it gets close to all the points. While this would be acceptable if the training observations perfectly represented the true function, because there is noise in the data, our model ends up fitting the noise. This is a model with a high variance, because it will change significantly depending on the training data. The predictions on the test set are better than the one degree model, but the twenty five degree model still does not learn the relationship because it essentially memorizes the training data and the noise. Our problem is that we want a model that does not “memorize” the training data, but learns the actual relationship! How can we find a balanced model with the right polynomial degree? If we choose the model with the best score on the training set, we will just select the overfitting model but this cannot generalize well to testing data. Fortunately, there is a well-established data science technique for developing the optimal model: validation. Validation We need to create a model with the best settings (the degree), but we don’t want to have to keep going through training and testing. There are no consequences in our example from poor test performance, but in a real application where we might be performing a critical task such as diagnosing cancer, there would be serious downsides to deploying a faulty model. We need some sort of pre-test to use for model optimization and evaluate. This pre-test is known as a validation set. A basic approach would be to use a validation set in addition to the training and testing set. This presents a few problems though: we could just end up overfitting to the validation set and we would have less training data. A smarter implementation of the validation concept is k-fold cross-validation. The idea is straightforward: rather than using a separate validation set, we split the training set into a number of subsets, called folds. Let’s use five folds as an example. We perform a series of train and evaluate cycles where each time we train on 4 of the folds and test on the 5th, called the hold-out set. We repeat this cycle 5 times, each time using a different fold for evaluation. At the end, we average the scores for each of the folds to determine the overall performance of a given model. This allows us to optimize the model before deployment without having to use additional data. For our problem, we can use cross-validation to select the best model by creating models with a range of different degrees, and evaluate each one using 5-fold cross-validation. The model with the lowest cross-validation score will perform best on the testing data and will achieve a balance between underfitting and overfitting. I choose to use models with degrees from 1 to 40 to cover a wide range. To compare models, we compute the mean-squared error, the average distance between the prediction and the real value squared. The following table shows the cross validation results ordered by lowest error and the graph shows all the results with error on the y-axis. The cross-validation error with the underfit and overfit models is off the chart! A model with 4 degrees appears to be optimal. To test out the results, we can make a 4-degree model and view the training and testing predictions. There is nothing more beautiful than a model that fits the data! Moreover, we know that our model not only closely follows the training data, it has actually learned the relationship between x and y. To verify we have the optimal model, we can also plot what are known as training and testing curves. These show the model setting we tuned on the x-axis and both the training and testing error on the y-axis. A model that is underfit will have high training and high testing error while an overfit model will have extremely low training error but a high testing error. This graph nicely summarizes the problem of overfitting and underfitting. As the flexibility in the model increases (by increasing the polynomial degree) the training error continually decreases due to increased flexibility. However, the error on the testing set only decreases as we add flexibility up to a certain point. In this case, that occurs at 5 degrees As the flexibility increases beyond this point, the training error increases because the model has memorized the training data and the noise. Cross-validation yielded the second best model on this testing data, but in the long run we expect our cross-validation model to perform best. The exact metrics depend on the testing set, but on average, the best model from cross-validation will outperform all other models. Conclusions Overfitting and underfitting is a fundamental problem that trips up even experienced data analysts. In my lab, I have seen many grad students fit a model with extremely low error to their data and then eagerly write a paper with the results. Their model looks great, but the problem is they never even used a testing set let alone a validation set! The model is nothing more than an overfit representation of the training data, a lesson the student soon learns when someone else tries to apply their model to new data. Fortunately, this is a mistake that we can easily avoid now that we have seen the importance of model evaluation and optimization using cross-validation. Once we understand the basic problems in data science and how to address them, we can feel confident in building up more complex models and helping others avoid mistakes. This post covered a lot of topics, but hopefully you now have an idea of the basics of modeling, overfitting vs underfitting, bias vs variance, and model optimization with cross-validation. Data science is all about being willing to learn and continually adding more tools to your skillset. The field is exciting both for its potential beneficial impacts and for the opportunity to constantly learn new techniques. I welcome feedback and constructive criticism. I can be reached on Twitter at @koehrsen_will. I would like to thank the contributors to Scikit-Learn for their excellent example on this subject. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Overfitting vs. Underfitting: A Complete Example,7,published,36301,2480,0.5241935483870968,1,1,1,0,1,0
93,4500,481.16772708158567,748,https://towardsdatascience.com/how-to-master-new-skills-656d42d0e09c,19,Towards Data Science,2018-01-31 11:41:00,47.39,6,7297,2018-01-31 07:36:00,"['Data Science', 'Education', 'Programming', 'Space', 'Towards Data Science']","How to Master New Skills Why trying to avoid spreadsheets is the best way to learn data science The best way to learn a new skill is using it to solve problems. In my previous life as an aerospace engineering student, I spent hours writing complicated formulas in Excel to do everything from designing wings to calculating reentry angles of spacecraft. After hours of labor, I would present results in the bland Excel charts that dominate so many powerpoints. Convinced this was not an optimal workflow, I decided to learn Python, a common coding language, solely to avoid spending all my waking hours in Excel. With the help of Automate the Boring Stuff with Python by Al Sweigart, I picked up enough of the language to cut the time I spent in Excel by 90%. Rather than memorizing the basics, I focused on efficiently solving aerospace problems. I was soon flying through assignments, and when I presented Python graphs in class, I was asked what sort of magic techniques I used. Without even trying, I gained a reputation as someone who could take a tangled spreadsheet and turn the data into illuminating images. Ironically, it was only when I had reached the top of my field that I decided to make the switch to data science. While working as an aerospace engineer intern at NASA, I was surrounded by some of the brightest people in the country. However, these individuals’ brilliance was severely constrained because they ended up spending most of their time poring over that curse of modern data: Excel spreadsheets. These were people working on expanding humanity’s knowledge of the universe literally doing rocket science in spreadsheets! It was at this moment that I realized I would have a greater impact as a data scientist helping people make sense from data than as an aerospace engineer. Although I was working on a mission to send a satellite into space (Near-Earth Asteroid Scout), I was frustrated with the work because it mostly involved hours of paging through numbers. Often, these spreadsheets had been used for years and people trusted the numbers despite not being able to understand the formulas. I eventually gave up trying to comprehend the spreadsheets and instead set about automating calculations using the little bit of Python I knew. Building on my limited knowledge I had from trying to escape spreadsheets in school, and using some code I copied and pasted from Stack Overflow, I was able to increase my work efficiency and quality. Tentatively, I mentioned to colleagues there was a better way to do their work. Although they were skeptical, they had seen my results, and were willing to listen. After showing them a few examples where I wrote simple code to replace editing spreadsheets by hand, they began to embrace the idea. At first, I wrote the scripts for them, a task I was happy to take on because my workload had been greatly reduced by a few lines of code, but eventually they were able to start writing their own scripts. (Although I did convince them to write code, it was almost always in Matlab or even Fortran. There are some things you just can’t change!) Over the course of a few weeks, I was transformed from a lowly intern into the go-to data consultant for my branch. This real-world application illustrates two critical points: Contrary to the traditional model of education, we first develop applications and then form theories, not the other way round. The internal combustion engine, one of the most influential technologies in history, came out of decades of tinkering by engineers not from theoreticians. Only after the engine had been invented did the theories underlying how it worked fall into place. I learned programming and data science not by memorizing the details of data structures and formulas, but by writing code to solve problems. Along the way, I picked up plenty of the basic knowledge I could have read in books. Now when I learn a new machine learning method, I go straight to using it on real data instead of studying the formulas. I get a better understanding of how a model works by seeing it in action instead of on the page. People usually think they need to be experts in a field before they can contribute. This prevents them from even starting to learn a new technique because they imagine it will be years before it will be useful. My experience with NASA provides a counterexample to this misconception. A limited amount of the correct knowledge, in this case data science, can be extremely influential. Rather than waiting until I knew everything possible about coding, I applied what I did know with beneficial results. The cliché that Rome was not built in a day implies large projects require years of work. However, there is a more important message here: an entire city does not need to be built before it is useful. Even with a few homes, a city still serves a purpose. Half of Rome is better than no Rome, just as some knowledge is preferable to no knowledge. Every new house built or every additional piece of knowledge inspires more growth in a positive feedback cycle. The key is to keep solving problems during the learning process instead of waiting until you master a subject to use it! These ideas should be encouraging to anyone wanting to pick up a new skill or switch fields. If you think it will take too long to learn a technique, or you are bored by the basics, don’t worry! The simple stuff is tedious and you’re better off learning by working on problems and picking up the fundamentals along the way. Furthermore, realize you can start getting useful results with a limited amount of the right knowledge. Solve a few basic problems and improve your skills as required when the complexity increases. Since we already used one cliché, here’s another: “The best time to plant a tree was 20 years ago. The second best time is today.” Don’t regret not learning programming — or any skill — while in school, but think about what you want to know in the future! Figure out the problems you (or your colleagues) need to solve and start learning. Looking back at it now, my road to data science was not smoothly planned, but evolved by solving problems and steadily acquiring tools. There has never been a better age to teach yourself anything, and the best way to expand your skill-set is to start working on problems! As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. I would like to thank Taylor Koehrsen (PharmD no less!) for her editing help. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. It doesn’t take much knowledge to achieve useful results New skills are learned from solving problems not from theory",How to Master New Skills,5,published,15398,1281,3.51288056206089,0,1,1,1,0,0
90,5300,479.2612080557639,917,https://towardsdatascience.com/statistical-significance-hypothesis-testing-the-normal-curve-and-p-values-93274fa32687,32,Towards Data Science,2018-02-02 09:26:00,29.79,10,15284,2018-01-30 20:38:00,"['Data Science', 'Hypothesis Testing', 'P Value', 'Statistics', 'Towards Data Science']","Statistical Significance Explained What does it mean to prove something with data? As the dean at a major university, you receive a concerning report showing your students get an average of 6.80 hours of sleep per night compared to the national college average of 7.02 hours. The student body president is worried about the health of students and points to this study as proof that homework must be reduced. The university president on the other hand dismisses the study as nonsense: “Back in my day we got four hours of sleep a night and considered ourselves lucky.” You have to decide if this is a serious issue. Fortunately, you’re well-versed in statistics and finally see a chance to put your education to use! Statistical significance is one of those terms we often hear without really understanding. When someone claims data proves their point, we nod and accept it, assuming statisticians have done complex operations that yielded a result which cannot be questioned. In fact, statistical significance is not a complicated phenomenon requiring years of study to master, but a straightforward idea that everyone can — and should — understand. Like with most technical concepts, statistical significance is built on a few simple ideas: hypothesis testing, the normal distribution, and p values. In this article, we will briefly touch on all of these concepts (with further resources provided) as we work up to solving the conundrum presented above. Author’s Note: An earlier edition of this post oversimplified the definition of the p-value. I would like to thank Professor Timothy Bates for correcting my mistake. This was a great example of the type of collaborative learning possible online and I encourage any feedback, corrections, or discussion! The first idea we have to discuss is hypothesis testing, a technique for evaluating a theory using data. The “hypothesis” refers to the researcher’s initial belief about the situation before the study. This initial theory is known as the alternative hypothesis and the opposite is known as the null hypothesis. In our example these are: Notice how careful we have to be about the wording: we are looking for a very specific effect, which needs to be formalized in the hypotheses so after the fact we cannot claim to have been testing something else! (This is an example of a one-sided hypothesis test because we are concerned with a change in only one direction.) Hypothesis tests are one of the foundations of statistics and are used to assess the results of most studies. These studies can be anything from a medical trial to assess drug effectiveness to an observational study evaluating an exercise plan. What all studies have in common is that they are concerned with making comparisons, either between two groups or between one group and the entire population. In the medical example, we might compare the average time to recover between groups taking two different drugs, or, in our problem as dean, we want to compare sleep between our students and all the students in the country. The testing part of hypothesis tests allows us to determine which theory, the null or alternative, is better supported by the evidence. There are many hypothesis tests and we will use one called the z-test. However, before we can get to testing our data, we need to talk about two more crucial ideas. The second building block of statistical significance is the normal distribution, also called the Gaussian or bell curve. The normal distribution is used to represent how data from a process is distributed and is defined by the mean, given the Greek letter μ (mu), and the standard deviation, given the letter σ (sigma). The mean shows the location of the center of the data and the standard deviation is the spread in the data. The application of the normal distribution comes from assessing data points in terms of the standard deviation. We can determine how anomalous a data point is based on how many standard deviations it is from the mean. The normal distribution has the following helpful properties: If we have a normal distribution for a statistic, we can characterize any point in terms of standard deviations from the mean. For example, average female height in the US is 65 inches (5' 5"") with a standard deviation of 4 inches. If we meet a new acquaintance who is 73 inches tall, we can say she is two standard deviations above the mean and is in the tallest 2.5% of females. (2.5% of females will be shorter than μ — 2σ (57 in) and 2.5% will be taller than μ+2σ). In statistics, instead of saying our data is two standard deviations from the mean, we assess it in terms of a z-score, which just represents the number of standard deviations a point is from the mean. Conversion to a z-score is done by subtracting the mean of the distribution from the data point and dividing by the standard deviation. In the height example, you can check that our friend would have a z-score of 2. If we do this to all the data points the new distribution is called the standard normal with a mean of 0 and a standard deviation of 1 as shown below. Every time we do a hypothesis test, we need to assume a distribution for the test statistic, which in our case is the average (mean) hours of sleep for our students. For a z-test, the normal curve is used as an approximation for the distribution of the test statistic. Generally, according to the central limit theorem, as we take more averages from a data distribution, the averages will tend towards a normal distribution. However, this will always be an estimate because real-world data never perfectly follows a normal distribution. Assuming a normal distribution lets us determine how meaningful the result we observe in a study is. The higher or lower the z-score, the more unlikely the result is to happen by chance and the more likely the result is meaningful. To quantify just how meaningful the results are, we use one more concept. The final core idea is that of p-values. A p-value is the probability of observing results at least as extreme as those measured when the null hypothesis is true. That might seem a little convoluted, so let’s look at an example. Say we are measuring average IQ in the US states of Florida and Washington. Our null hypothesis is that average IQs in Washington are not higher than average IQs in Florida. We perform the study and find IQs in Washington are higher by 2.2 points with a p-value of 0.346. This means, in a world where the null hypothesis — average IQs in Washington are not higher than average IQs in Florida — is true, there is a 34.6% chance we would measure IQs at least 2.2 points higher in Washington. So, if IQs in Washington are not actually higher, we would still measure they are higher by at least 2.2 points about 1/3 of the time due to random noise. Subsequently, the lower the p-value, the more meaningful the result because it is less likely to be caused by noise. Whether or not the result can be called statistically significant depends on the p-value (known as alpha) we establish for significance before we begin the experiment . If the observed p-value is less than alpha, then the results are statistically significant. We need to choose alpha before the experiment because if we waited until after, we could just select a number that proves our results are significant no matter what the data shows! The choice of alpha depends on the situation and the field of study, but the most commonly used value is 0.05, corresponding to a 5% chance the results occurred at random. In my lab, I see values from 0.1 to 0.001 commonly in use. As an extreme example, the physicists who discovered the Higgs Boson particle used a p-value of 0.0000003, or a 1 in 3.5 million chance the discovery occurred because of noise. (Statisticians are loathe to admit that a p-value of 0.05 is arbitrary. R.A. Fischer, the father of modern statistics, choose a p-value of 0.05 for indeterminate reasons and it stuck)! To get from a z-score on the normal distribution to a p-value, we can use a table or statistical software like R. The result will show us the probability of a z-score lower than the calculated value. For example, with a z-score of 2, the p-value is 0.977, which means there is only a 2.3% probability we observe a z-score higher than 2 at random. As a summary so far, we have covered three ideas: Now, let’s put the pieces together in our example. Here are the basics: First, we need to convert our measurement into a z-score, or the number of standard deviations it is away from the mean. We do this by subtracting the population mean (the national average) from our measured value and dividing by the standard deviation over the square root of the number of samples. (As the number of samples increases, the standard deviation and hence the variation decreases. We account for this by dividing the standard deviation by the square root of the number of samples.) The z-score is called our test-statistic. Once we have a test-statistic, we can use a table or a programming language such as R to calculate the p-value. I use code here not to intimidate but to show how easy it is to implement our solution with free tools! (# are comments and bold is output) Based on the p-value of 0.02116, we can reject the null hypothesis. (Statisticians like us to say reject the null rather than accept the alternative.) There is statistically significant evidence our students get less sleep on average than college students in the US at a significance level of 0.05. The p-value shows there is a 2.12% chance that our results occurred because of random noise. In this battle of the presidents, the student was right. Before we ban all homework, we need to be careful not to assign too much to this result. Notice that our p-value, 0.02116, would not be significant if we had used a threshold of 0.01. Someone who wants to prove the opposite point in our study can simply manipulate the p-value. Anytime we examine a study, we should think about the p-value and the sample size in addition to the conclusion. With a relatively small sample size of 202, our study might have statistical significance, but that does mean it is practically meaningful. Further, this was an observational study, which means there is only evidence for correlation and not causation. We showed there is a correlation between students at our school and less average sleep, but not that going to our school causes a decrease in sleep. There could be other factors at play that affect sleep and only a randomized controlled study is able to prove causation. As with most technical concepts, statistical significance is not that complex and is just a combination of many small ideas. Most of the trouble comes with learning the vocabulary! Once you put the pieces together, you can start applying these statistical concepts. As you learn the basics of stats, you become better prepared to view studies and the news with a healthy skepticism. You can see what the data actually says rather than what someone tells you it means. The best tactic against dishonest politicians and corporations is a skeptical, well-educated public! As always, I welcome constructive criticism and feedback. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Null Hypothesis: The average amount of sleep by students at our university is not below the national average for college students. 95% of data is within ± 2 standard deviations from the mean 99.7% of data is within ± 3 standard deviations from the mean Normal Distribution: An approximate representation of the data in a hypothesis test. p-value: The probability a result at least as extreme at that observed would have occurred if the null hypothesis is true. In a poll of 202 students at our university the average hours of sleep per night was 6.90 hours with a standard deviation of 0.84 hours. Our alternative hypothesis is the average sleep of students at our university is below the national average for college students. We will use an alpha value of 0.05 which means the results are significant f the p-value is below 0.05. Alternative Hypothesis: The average amount of sleep by students at our university is below the national average for college student. 68% of data is within ± 1 standard deviations from the mean Hypothesis Testing: A technique used to test a theory Students across the country average 7.02 hours of sleep per night according to the National Sleep Foundation",Statistical Significance Explained,3,published,51312,2450,2.163265306122449,2,0,1,1,0,0
87,13000,477.27096707535884,2235,https://towardsdatascience.com/python-is-the-perfect-tool-for-any-problem-f2ba42889a85,60,Towards Data Science,2018-02-04 09:12:00,62.51,3,30140,2018-02-04 07:49:00,"['Python', 'Programming', 'Education', 'Data Science', 'Towards Data Science']","Python is the Perfect Tool for any Problem Reflecting on my first Python program Reflection is always a helpful (and sometimes entertaining ) exercise. For nostalgia’s sake — if one can be nostalgic for something 2 years old— I wanted to share my first Python program. I initially picked up Python as an aerospace engineering student to avoid spreadsheets and little did I know how good a decision this would turn out to be. My Python education began with the book Automate the Boring Stuff with Python by Al Sweigart, an excellent application-based book with simple programs to do useful tasks. When I learn a new topic, I look for any chances to use it and I needed a problem to solve in Python. Fortunately, I found one in the form of a $200 textbook required for a class. My personal limit for textbooks is about $20 (Automate the Boring Stuff is free online) and I refused to even rent this book. Desperate to get the book before the first assignment, I saw it was available for a free one-week trial through Amazon with a new account. I got the book for one week and was able to do the first assignment. While I could have kept creating new accounts one week at a time, I needed a better solution. Enter Python and my first programming application. One of many useful libraries in Automate the Boring Stuff is pyautogui which allows you to control the keyboard and mouse through Python. They say when you have a hammer, every problem looks like a nail, and that was definitely the case here. Python and pyautogui would allow me to press the arrow keys and take screenshots, and I put the two together to come up with a solution to the book issue. I wrote my first program to automatically turn through every page in the book and take a screenshot. The end program was only 10 lines long yet I was nearly as proud of it as anything I had done in aerospace engineering! Following is the program in its entirety: Running the program is pretty simple (I encourage anyone to try). I saved the script as book_screenshot.py, then pulled up a command prompt in the same folder and typed: Then I would have 5 seconds to flip to the book and put it into fullscreen. The program would do the rest, flipping through every page and taking a screenshot that was saved as a pdf. I could then join all the pdfs together into one file, and have a (questionably legal) copy of the book! Granted, this was a pretty awful copy because it could not be searched, but I made any excuse possible to use my “book”. This example demonstrates two key points that have stuck with me as I continue my data science education: With just a few lines of code and a free online book, I wrote a program that I actually put to use. Learning the basics can be tedious, and my first attempts to learn Python failed within a few hours as I got stuck with ideas like data structures and loops. Changing tactics, I started to develop solutions to real problems and ended up learning the fundamentals along the way. There is so much to master in programming and data science, but you don’t need to learn everything at once. Pick a problem you need to solve and get started! Since then, I have made a few more sophisticated programs, but I still remember this first script with fondness! Share your first program! I welcome discussion, feedback, and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. You don’t need to fully master a skill before it is useful. The best way to learn a new skill is to find a problem you need to solve!",Python is the Perfect Tool for any Problem,8,published,48216,721,18.030513176144243,0,1,1,1,0,1
83,278,474.27326303195605,75,https://towardsdatascience.com/the-multiple-comparisons-problem-e5573e8b9578,0,Towards Data Science,2018-02-07 09:09:00,43.12,6,1366,2018-01-30 20:38:00,"['Data Science', 'Education', 'Statistics', 'Research']","The Misleading Effect of Noise: The Multiple Comparisons Problem How to avoid being fooled by randomness The CEO of a major drug company has a problem. The new miracle drug developed by his chemists to do increase willpower has failed in every trial. The CEO cannot believe these results, but the researchers tell him there is no evidence of an effect on willpower at a significance level (p-value) of 0.05. Convinced the drug must be beneficial in some way, the CEO has a brilliant idea: instead of testing the drug for just one effect, test it for 1000 different effects at the same time, all at the same p-value. Even if it doesn’t increase willpower, it must do something, like reduce anxiety or boost memory. Skeptical, the researchers redo the trials exactly as the CEO says, monitoring 1000 different health measures of subjects on the drug. The researchers come back with astounding news: the drug had a significant effect on 50 of the measured values! Miraculous right? Actually, it would be more surprising if they had found no significant effects with this experimental analysis. The CEO’s mistake is an example of the multiple comparisons problem. The issue comes down to the noisiness of data in the real world. While the chance of noise affecting one result may be small, the more measurements we make, the larger the probability that a random fluctuation is mis-classified as a meaningful result. While this affects researchers performing objective studies, it can also be used for nefarious purposes. The CEO has a drug that he wants to sell but it doesn’t do what it was designed for. Instead of admitting failure, he instructs his researchers to keep looking until they find some vital sign the drug improves. Even if the drug has absolutely no effect on any health markers, the researchers will eventually find it does improve some measures because of random noise in the data. For this reason, the multiple comparisons problem is also called the look-elsewhere effect: if a researcher doesn’t find the result she wants, she can just keep looking until she finds some beneficial effect! Fortunately, most statisticians and researchers are honest and use methods to account for the multiple comparisons problem. The most common technique is called the Bonferroni Correction, but before we can explain it, we need to talk about p-values. The p-value represents the probability that in a world where the null hypothesis is true, the test statistic would be at least as extreme as the measured value. In the drug example, for the initial trial, the null hypothesis is that the drug does not increase the average motivation of individuals. The alternative hypothesis, or the researcher’s belief, is the drug increases average motivation. With a p-value of 0.05, this means in a world where the drug does not increase average motivation, the researchers would measure the drug does increase motivation 5% of the time due to random noise. Before a study is run, researchers select a p-value (known as alpha or the significance level) for establishing statistical significance. If the measured p-value falls below the the threshold, the null hypothesis is rejected and the results are statistically significant. A lower measured p-value is considered better because it shows the results are less likely to occur by chance. Once we know what a p-value represents, we can spot the CEO’s mistake. He ordered the trial to be run again with the same p-value of 0.05, but instead of testing for just one effect, he wanted to test for 1000. If we perform 1000 hypothesis tests with a p-value of 0.05, we would expect on average to find 50 significant results due to random noise (5% of 1000). Some of the results might actually be meaningful, but it would be unethical to declare they all are and sell the drug based on this study! The figure below illustrates the issue. I generated 1000 random numbers from a standard normal distribution and plotted them on the corresponding probability density function. We can ignore the y-axis here and focus on the x-axis which is in terms of the z-score. If we perform a hypothesis test and assume the mean of our test statistic (measured value) comes from a normal distribution, then we calculate our test-statistic in terms of a z-score. Using our selected p-value (alpha) for the hypothesis test, we can then find the z-score needed for statistical significance. These thresholds for a p-value of 0.05 are shown as the red vertical lines, with observations outside the lines considered to be statistically significant. The black dots are insignificant randomly generated observations, while the red dots are “significant” randomly generated data points. We can see with random observations and using an uncorrected p-value, we classify a number of these results as significant! This might be great news if we have a drug to sell, but as responsible data scientists, we need to account for performing multiple tests so we are not misled by noise. A simple fix to the multiple comparisons problem is the Bonferroni Correction. To compensate for many hypothesis tests, we take the p-value for a single comparison and divide it by the number of tests. In the case of the drug company trial, the original p-value of 0.05 should be divided by 1000, resulting in a new significance threshold of 0.00005. This means results must be more extreme for them to be considered significant, decreasing the probability that random noise is characterized as meaningful. We can apply the Bonferroni Correction to the graph made above and see how it affects the classification of random points. The graph has the same structure, but now the significance thresholds account for multiple tests. No random data points are now considered significant! There are criticisms that the Bonferroni Correction is too conservative and it may lead us to reject some results which actually are meaningful. However, the important concept behind the method is the significance value needs to be adjusted when we make many comparisons. The problem of multiple comparisons extends to more than just hypothesis tests. If we compare enough data sets, we can find correlations that are nothing more than random noise. This is humorously illustrated by the website Spurious Correlations which has examples of completely unrelated trends that happen to closely follow each other. The multiple comparisons issue is important to keep in mind when we examine studies, but it can also be used in our daily lives. If we look hard enough, we can find correlations anywhere, but that does not mean we should make lifestyle changes because of them. Maybe we weigh ourselves every day and find our weight is negatively correlated with the number of text messages we send. It would be foolish to send more text messages in the hopes of losing weight. Humans are adept at spotting patterns, and when we look hard enough and make enough comparisons, we can convince ourselves there is meaning in random noise. Once we are aware of this tendency, we are prepared to analyze dubious claims and make rational choices. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",The Misleading Effect of Noise: The Multiple Comparisons Problem,9,published,3168,1375,0.20218181818181818,7,1,1,0,0,0
88,5700,471.71749344475694,1000,https://towardsdatascience.com/markov-chain-monte-carlo-in-python-44f7e609be98,25,Towards Data Science,2018-02-09 22:29:00,29.0,12,16917,2018-02-09 07:18:00,"['Machine Learning', 'Data Science', 'Programming', 'Python', 'Education']","Markov Chain Monte Carlo in Python A Complete Real-World Implementation The past few months, I encountered one term again and again in the data science world: Markov Chain Monte Carlo. In my research lab, in podcasts, in articles, every time I heard the phrase I would nod and think that sounds pretty cool with only a vague idea of what anyone was talking about. Several times I tried to learn MCMC and Bayesian inference, but every time I started reading the books, I soon gave up. Exasperated, I turned to the best method to learn any new skill: apply it to a problem. Using some of my sleep data I had been meaning to explore and a hands-on application-based book (Bayesian Methods for Hackers, available free online), I finally learned Markov Chain Monte Carlo through a real-world project. As usual, it was much easier (and more enjoyable) to understand the technical concepts when I applied them to a problem rather than reading them as abstract ideas on a page. This article walks through the introductory implementation of Markov Chain Monte Carlo in Python that finally taught me this powerful modeling and analysis tool. The full code and data for this project is on GitHub. I encourage anyone to take a look and use it on their own data. This article focuses on applications and results, so there are a lot of topics covered at a high level, but I have tried to provide links for those wanting to learn more! Introduction My Garmin Vivosmart watch tracks when I fall asleep and wake up based on heart rate and motion. It’s not 100% accurate, but real-world data is never perfect, and we can still extract useful knowledge from noisy data with the right model! The objective of this project was to use the sleep data to create a model that specifies the posterior probability of sleep as a function of time. As time is a continuous variable, specifying the entire posterior distribution is intractable, and we turn to methods to approximate a distribution, such as Markov Chain Monte Carlo (MCMC). Before we can start with MCMC, we need to determine an appropriate function for modeling the posterior probability distribution of sleep. One simple way to do this is to visually inspect the data. The observations for when I fall asleep as a function of time are shown below. Every data point is represented as a dot, with the intensity of the dot showing the number of observations at the specific time. My watch records only the minute at which I fall asleep, so to expand the data, I added points to every minute on both sides of the precise time. If my watch says I fell asleep at 10:05 PM, then every minute before is represented as a 0 (awake) and every minute after gets a 1 (asleep). This expanded the roughly 60 nights of observations into 11340 data points. We can see that I tend to fall asleep a little after 10:00 PM but we want to create a model that captures the transition from awake to asleep in terms of a probability. We could use a simple step function for our model that changes from awake (0) to asleep (1) at one precise time, but this would not represent the uncertainty in the data. I do not go to sleep at the same time every night, and we need a function to that models the transition as a gradual process to show the variability. The best choice given the data is a logistic function which is smoothly transitions between the bounds of 0 and 1. Following is a logistic equation for the probability of sleep as a function of time Here, β (beta) and α (alpha) are the parameters of the model that we must learn during MCMC. A logistic function with varying parameters is shown below. A logistic function fits the data because the probability of being asleep transitions gradually, capturing the variability in my sleep patterns. We want to be able to plug in a time t to the function and get out the probability of sleep, which must be between 0 and 1. Rather than a straight yes or no answer to the question am I asleep at 10:00 PM, we can get a probability. To create this model, we use the data to find the best alpha and beta parameters through one of the techniques classified as Markov Chain Monte Carlo. Markov Chain Monte Carlo Markov Chain Monte Carlo refers to a class of methods for sampling from a probability distribution in order to construct the most likely distribution. We cannot directly calculate the logistic distribution, so instead we generate thousands of values — called samples — for the parameters of the function (alpha and beta) to create an approximation of the distribution. The idea behind MCMC is that as we generate more samples, our approximation gets closer and closer to the actual true distribution. There are two parts to a Markov Chain Monte Carlo method. Monte Carlo refers to a general technique of using repeated random samples to obtain a numerical answer. Monte Carlo can be thought of as carrying out many experiments, each time changing the variables in a model and observing the response. By choosing random values, we can explore a large portion of the parameter space, the range of possible values for the variables. A parameter space for our problem using normal priors for the variables (more on this in a moment) is shown below. Clearly we cannot try every single point in these plots, but by randomly sampling from regions of higher probability (red) we can create the most likely model for our problem. A Markov Chain is a process where the next state depends only on the current state. (A state in this context refers to the assignment of values to the parameters). A Markov Chain is memoryless because only the current state matters and not how it arrived in that state. If that’s a little difficult to understand, consider an everyday phenomenon, the weather. If we want to predict the weather tomorrow we can get a reasonable estimate using only the weather today. If it snowed today, we look at historical data showing the distribution of weather on the day after it snows to estimate probabilities of the weather tomorrow. The concept of a Markov Chain is that we do not need to know the entire history of a process to predict the next output, an approximation that works well in many real-world situations. Putting together the ideas of Markov Chain and Monte Carlo, MCMC is a method that repeatedly draws random values for the parameters of a distribution based on the current values. Each sample of values is random, but the choices for the values are limited by the current state and the assumed prior distribution of the parameters. MCMC can be considered as a random walk that gradually converges to the true distribution. In order to draw random values of alpha and beta, we need to assume a prior distribution for these values. As we have no assumptions about the parameters ahead of time, we can use a normal distribution. The normal, or Gaussian distribution, is defined by the mean, showing the location of the data, and the variance, showing the spread. Several normal distributions with different means and spreads are below: The specific MCMC algorithm we are using is called Metropolis Hastings. In order to connect our observed data to the model, every time a set of random values are drawn, the algorithm evaluates them against the data. If they do not agree with the data (I’m simplifying a little here), the values are rejected and the model remains in the current state. If the random values are in agreement with the data, the values are assigned to the parameters and become the current state. This process continues for a specified number of steps, with the accuracy of the model improving with the number of steps. Putting it all together, the basic procedure for Markov Chain Monte Carlo in our problem is as follows: The algorithm returns all of the values it generates for alpha and beta. We can then use the average of these values as the most likely final values for alpha and beta in the logistic function. MCMC cannot return the “True” value but rather an approximation for the distribution. The final model for the probability of sleep given the data will be the logistic function with the average values of alpha and beta. Python Implementation The above details went over my head many times until I applied them in Python! Seeing the results first-hand is a lot more helpful than reading someone else describe. To implement MCMC in Python, we will use the PyMC3 Bayesian inference library. It abstracts away most of the details, allowing us to create models without getting lost in the theory. The following code creates the full model with the parameters, alpha and beta, the probability, p, and the observations, observed The step variable refers to the specific algorithm, and the sleep_trace holds all of the values of the parameters generated by the model. (Check out the notebook for the full code) To get a sense of what occurs when we run this code, we can look at all the value of alpha and beta generated during the model run. These are called trace plots. We can see that each state is correlated to the previous — the Markov Chain — but the values oscillate significantly — the Monte Carlo sampling. In MCMC, it is common practice to discard up to 90% of the trace. The algorithm does not immediately converge to the true distribution and the initial values are often inaccurate. The later values for the parameters are generally better which means they are what we should use for building our model. We used 10000 samples and discarded the first 50%, but an industry application would likely use hundreds of thousands or millions of samples. MCMC converges to the true value given enough steps, but assessing convergence can be difficult. I will leave that topic out of this post (one way is by measuring the auto-correlation of the traces) but it is an important consideration if we want the most accurate results. PyMC3 has built in functions for assessing the quality of models, including trace and autocorrelation plots. Sleep Model After finally building and running the model, it’s time to use the results. We will the the average of the last 5000 alpha and beta samples as the most likely values for the parameters which allows us to create a single curve modeling the posterior sleep probability: The model represents the data well. Moreover, it captures the inherent variability in my sleep patterns. Rather than a single yes or no answer, the model gives us a probability. For example, we can query the model to find out the probability I am asleep at a given time and find the time at which the probability of being asleep passes 50%: Although I try to go to bed at 10:00 PM, that clearly does not happen most nights! We can see that the average time I go to bed is around 10:14 PM. These values are the most likely estimates given the data. However, there is uncertainty associated with these probabilities because the model is approximate. To represent this uncertainty, we can make predictions of the sleep probability at a given time using all of the alpha and beta samples instead of the average and then plot a histogram of the results. These results give a better indicator of what an MCMC model really does. The method does not find a single answer, but rather a sample of possible values. Bayesian Inference is useful in the real-world because it expresses predictions in terms of probabilities. We can say there is one most likely answer, but the more accurate response is that there are a range of values for any prediction. I can use the waking data to find a similar model for when I wake up in the morning. I try to always be up at 6:00 AM with my alarm, but we can see that does not always happen! The following image shows the final model for the transition from sleeping to waking along with the observations. We can query the model to find the probability I’m asleep at a given time and the most likely time for me to wake up. Looks like I have some work to do with that alarm! Duration of Sleep A final model I wanted to create — both out of curiosity and for the practice — was my duration of sleep. First, we need to find a function to model the distribution of the data. Ahead of time, I think it would be normal, but we can only find out by examining the data! A normal distribution would work, but it would not capture the outlying points on the right side (times when I severely slept in). We could use two separate normal distributions to represent the two modes, but instead, I will use a skewed normal. The skewed normal has three parameters, the mean, the variance, and alpha, the skew. All three of these must be learned from the MCMC algorithm. The following code creates the model and implements the Metropolis Hastings sampling. Now, we can use the average values of the three parameters to construct the most likely distribution. Following is the final skewed normal distribution on top of the data. It looks like a nice fit! We can query the model to find the likelihood I get at least a certain amount of sleep and the most likely duration of sleep: I’m not entirely pleased with those results, but what can you expect as a graduate student? Conclusions Once again, completing this project showed me the importance of solving problems, preferably ones with real world applications! Along the way to building an end-to-end implementation of Bayesian Inference using Markov Chain Monte Carlo, I picked up many of the fundamentals and enjoyed myself in the process. Not only did I learn a little bit about my habits (and what I need to improve), but now I can finally understand what everyone is talking about when they say MCMC and Bayesian Inference. Data science is about constantly adding tools to your repertoire and the most effective way to do that is to find a problem and get started! As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Randomly assign new values to alpha and beta based on the current state. Check if the new random values agree with the observations. If they do not, reject the values and return to the previous state. If they do, accept the values as the new current state. Repeat steps 2 and 3 for the specified number of iterations. Select an initial set of values for alpha and beta, the parameters of the logistic function.",Markov Chain Monte Carlo in Python,6,published,58343,2772,2.0562770562770565,0,1,1,0,1,1
74,101,469.8645766654514,23,https://medium.com/p/slow-tech-take-back-your-mind-5d142dc3ccb9,1,None,2018-02-11 18:57:00,57.94,3,135,2018-02-10 08:15:00,"['Social Media', 'Technology', 'Education', 'Focus']","Slow Tech: Take Back Your Mind Technology should improve real life not replace it Lately, I have noticed a disturbing phenomenon. I unlock my phone to complete a basic task — say check my calendar — notice that I have a social media or email notification, and without even thinking or really wanting to, click on the notification. The next few minutes I’ll spend mindlessly scrolling through tweets that only frustrate me or reading irrelevant emails. Eventually, with considerable effort, I’ll pull my gaze away from my phone and bring myself back into the real world. Five minutes later, I’ll realize I never did the task I unlocked my phone for in the first place. As a graduate student working in a data science lab, I used to pride myself on how long I could stare at a screen every day. Working as long as possible is almost a competition among the students, and anyone leaving early or unplugging from their computers is looked down upon. However, as events like the one described above became more common, I started to take a critical examination of the role of tech in my life. What does looking at a screen for 12 hours a day do to me and my attention, and is it really something that I want to spend the rest of my life doing? Eventually, I decided I needed to take a more balanced approach to my technology use. Inspired by the principle of the slow movement, which aims to experience life more fully by slowing down the hectic pace of modernity, I drew up the following set of guidelines for a new relationship with technology. My goal is to restore the original purpose of technology: improve and simplify real life rather than replace it. Slow Tech Manifesto Expecting perfection is a good way to fail to stick to your resolutions, and I don’t always expect to uphold these guidelines. This will be a gradual shift rather than an overnight fix to establish a healthy relationship with technology. Moreover, this is not an anti-tech stance or about abandoning technology, but about realizing it is a tool, and as with any other tool, can be used for both good and bad. As a data scientist, I try to be skeptical about data and I need to also start thinking critically about the expanding role of technology in my life. By adopting a slow-tech attitude, we can continue to take advantage of the benefits of tech while regaining control of our attention. As always, I welcome feedback, discussion, and constructive criticism. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator No notifications on my phone except calendar events, texts, and calls: we should choose when we want to check social media Use social media to actually be social and connect with other humans Before opening any social media app or visiting any news site ask yourself “does this enrich my life?” Social media posts should be about engaging and encouraging discussion not about constructing a fake image of yourself In real life and online, respond to praise, respond to constructive criticism, ignore hateful comments People take precedence over screens: look at others when talking to them A Technology Sabbath: one full (or half) day of no screens per week No more mindless consumption: write responses (private or public) to everything you read to force yourself to think not just consume When possible, have discussions in person rather than through screens",Slow Tech: Take Back Your Mind,6,published,233,634,0.15930599369085174,1,1,0,0,0,0
89,6300,467.17733991232643,1096,https://towardsdatascience.com/bayes-rule-applied-75965e4482ff,18,Towards Data Science,2018-02-14 11:27:00,32.4,9,11571,2018-02-13 19:06:00,"['Statistics', 'Programming', 'Education', 'Data Analysis', 'Towards Data Science']","Bayes’ Rule Applied Using Bayesian Inference on a real-world problem The fundamental idea of Bayesian inference is to become “less wrong” with more data. The process is straightforward: we have an initial belief, known as a prior, which we update as we gain additional information. Although we don’t think about it as Bayesian Inference, we use this technique all the time. For example, we might initially think there is a 50% chance we will get a promotion at the end of the quarter. If we receive positive feedback from our manager, we adjust our estimate upwards, and conversely, we might decrease the probability if we make a mess with the coffee machine. As we continually gather information, we refine our estimate to get closer to the “true” answer. Our intuitive actions are formalized in the simple yet powerful equation known as Bayes’ Rule: We read the left side, called the posterior, as the conditional probability of event A given event B. On the right side, P(A) is our prior, or the initial belief of the probability of event A, P(B|A) is the likelihood (also a conditional probability), which we derive from our data, and P(B) is a normalization constant to make the probability distribution sum to 1. The general form of Bayes’ Rule in statistical language is the posterior probability equals the likelihood times the prior divided by the normalization constant. This short equation leads to the entire field of Bayesian Inference, an effective method for reasoning about the world. While A’s and B’s may be good placeholders, they aren’t very helpful for letting us see how we can use this concept. To do that, we can apply Bayes’ Rule to a problem with real world data. One problem I have been exploring in my own life is sleeping patterns. I have more than 2 months of data from my Garmin Vivosmart watch showing when I fall asleep and wake up. In a previous post, I figured out the probability I am asleep at a given time using Markov Chain Monte Carlo (MCMC) methods.The final model showing the most likely distribution of sleep as a function of time (MCMC is an approximate method) is below. This is the probability I am asleep taking into account only the time. What if we know the time and have additional evidence? How would knowing that my bedroom light is on change the probability that I am asleep? This is where we use Bayes’ Rule to update our estimate. For a specific time, if we know information about my bedroom light, we can use the probability from the distribution above as the prior and then apply Bayes’ equation: The left side is the posterior, the conditional probability of sleep given the status of my bedroom light (either on or off). The probability at a given time will serve as our prior, P(sleep) , or the estimate we use if we have no additional information. For example, at 10:00 PM, the prior probability I am asleep is 27.34%. If we do have more information, we can update this using the likelihood, P(bedroom light |sleep) , which is derived from observed data. Based on my habits, I know the probability my bedroom light is on given that I am asleep is about 1%. That is: The probability that my light is off given I am asleep is 1–0.01 = 0.99(Here I am using the minus sign (-) to indicate the opposite condition.) This is because conditional probability distributions must sum to 1. If we know I am asleep, either my bedroom light has to be on or it has to be off! The final piece of the equation is the normalization constant P(light) . This represents the total probability my light is on. There are two conditions for which my light is on: I am asleep or I am awake. Therefore, if we know the prior probability of sleep, we can calculate the normalization constant as: The total probability my light is on takes into account both the chance I am asleep and my light is on and the chance I am awake and my light is on. (P(-sleep) = 1 — P(sleep) is the probability I am awake.) The probability my light is on given that I am not asleep, P(light | — sleep), is also determined from observations. In my case, I know there is around a 80% probability my bedroom light is on if I am awake (which means there is a 20% chance my light is not on if I’m awake). Using the total probability for my light being on, Bayes’ equation is: This represents the probability I am asleep given that my light is on. If my light is off, then we replace every P(light|... with P(-light|.... That’s more than enough equations with only words, let’s see how we can use this with numbers! We will walk through applying the equation for a time of 10:30 PM if we know my light is on. First, we calculate the prior probability I am asleep using the time and get an answer of 73.90%. The prior provides a good starting point for our estimate, but we can improve it by incorporating info about my light. Knowing that my light is on, we can fill in Bayes’ Equation with the relevant numbers: The knowledge that my light is on drastically changes our estimate of the probability I am asleep from over 70% to 3.42%. This shows the power of Bayes’ Rule: we were able to update our initial estimate for the situation by incorporating more information. While we might have intuitively done this anyway, thinking about it in terms of formal equations allows us to update our beliefs in a rigorous manner. Let’s try another example. What if it is 9:45 PM, and my light is off? Try to work this one out starting with the prior probability of 0.1206. Instead of always doing this inference by hand, I wrote some simple Python code to do these calculations which you can play with in the Jupyter Notebook. The code outputs the following answer: We again see the extra information changes our estimate. Now, if my sister wants to call me at 9:45 PM and she somehow knows that my light is on, she can consult this equation to determine if I will pick up (assuming I always pick up when I’m awake)! Who says you can’t use stats in your daily life? Seeing the numerical results is helpful, but visualizations can also be useful for making the point clearer. I always try to incorporate plots to communicate ideas if they don’t come across just by looking at equations. Here, we can visualize the prior and the conditional probability distribution of sleep using the extra data. When my light is on, the curve is shifted to the right, indicating there is a lower probability I am asleep at a given time. Likewise, the curve is shifted to the left if my light is off. It can be difficult to conceptually understand a statistical concept, but this illustration demonstrates precisely why we use Bayes’ Rule. If we want to be less wrong about the world, then additional information should change our beliefs, and Bayesian Inference updates our estimates using a systematic method. Using More Evidence! Why stop with my bedroom light? We can use as much information in the model as we like and it will continue to get more precise (as long as the data tells us something useful about the situation). For example, if I know the likelihood my phone is charging given that I am asleep is 95%, we can incorporate that knowledge into the model. Here, we will assume the probability my phone is charging is conditionally independent of the probability my light is on given the information of whether or not I am sleeping (independence is a little more advanced concept, but it allows us to simplify many problems). Bayes’ equation using the extra information is expressed: That might look intimidating, but using a little Python code, we can make a function to do the calculation for us. We feed in any time, and any combination of whether or not my light is on and phone is charging and the function returns the updated probability I am asleep. I’ll skip the math (I let my computer do it anyway) and show the results: At 11:00 PM, with no additional information, we would guess with almost certainty that I am asleep. However, once we have the additional information that my light is on and phone is not charging, we conclude there is only a miniscule chance I am asleep. Here’s another query: The probability shifts lower or higher depending on the exact situation. To demonstrate this, we can look at the four configurations of light and phone evidence and how they change the probability distribution: There is a lot of information in this graph, but the critical idea is that the probability curve changes depending on the evidence. As we get more data, we can further refine our estimate. Conclusion Bayes’ Rule and other statistical concepts can be difficult to understand when presented with abstract equations using only letters or made-up situations. I’ve been through many classes where Bayes Rule was shown in terms of not very useful examples like coin flips or drawing colored balls from an urn, but it wasn’t until this project that I finally understood the applicability of Bayesian inference. If you’re struggling with a concept, seek out a situation where you can apply it, or look at cases where others have already done so! Actual learning comes when we translate concepts to problems. Not only can we learn new skills this way, but we can also do some pretty cool projects! Success in data science is about continuously learning, adding new techniques to your skillset, and figuring out the best method for different tasks. Here we showed how Bayesian Inference lets us update our beliefs to account for new evidence in order to better model reality. We constantly need to adjust our predictions as we gather more evidence, and Bayes’ Equation provides us with the appropriate framework. As always, I welcome feedback, discussion, and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Bayes’ Rule Applied,3,published,35713,1944,3.240740740740741,0,1,0,1,0,0
84,726,461.1199734354167,140,https://towardsdatascience.com/data-visualization-hackathon-style-c6dcaabbf626,2,Towards Data Science,2018-02-20 12:49:00,42.86,8,2141,2018-02-19 07:47:00,"['Data Visualization', 'Data Science', 'Education', 'JavaScript', 'Carbon Emissions']","Data Visualization Hackathon Style My effort to liberate data from spreadsheets Everyone — corporations, governments, individuals — has data, but few people know how to use it effectively. Data can tell us much about how to make better decisions, but often this knowledge is hidden within the numbers. One problem is that most of the data looks something like this: Although the information here, global CO2 emissions, is “open data” in the sense that it’s publicly available for anyone to download, it might as well be locked away for all the good it is doing anyone in a spreadsheet. At its core, data science is about taking these meaningless pages of numbers and turning them into useful knowledge. One of the most effective ways of revealing insights within numbers is through data visualization. For HackCWRU 2018, a hackathon hosted at Case Western Reserve University, I wanted to explore the public CO2 data and make it accessible to a general audience. For those who haven’t had the experience, a hackathon is where a bunch of passionate makers — coders, artists, hardware specialists, and occasionally data scientists — get together for a weekend to work on projects for 24 or 36 straight hours. Sometimes there are specific problems to solve, but in other cases, such as with HackCWRU, you are free to choose your team and project. With a limited amount of time to accomplish your goal, sleeping is generally discouraged! There were all sorts of projects done at HackCWRU: phone apps, virtual reality games, motion capture programs, and I choose to do what I enjoy the most: data visualization. I went solo, under the project name emissions explorer, sticking with my habit of adding explorer to all my projects (see the stock explorer and weight explorer). My objective for the weekend was to show how data visualization can take millions of numbers and turn them into meaningful knowledge by building an interactive data exploration website. The entire website is available online and the code is also freely available on GitHub for anyone to use or improve. (The website works best on desktop as I didn’t quite have time to make it mobile-responsive. That would be a great fix for anyone to implement!). I’ll present most of the results below as I go through my process, but I highly recommend anyone check out the website to get the full experience. Development Process Before we can get to the fun part of making visuals, we have to find and clean some data. In a normal data science workflow, this make take up to 90% of the time spent on a project. However, working on a limited time schedule, I needed clean data and fast. I pulled together data from three sources: GapMinder (curated by Tom Carden), the climate data online tool, and the World Bank, all in the familiar csv file format. The data was in good shape, but there were some inconsistencies, mostly to do with country names. I spent quite a bit of time figuring out that South Korea was coded as ‘Korea, Rep.’ and North Korea as ‘Korea, Dem. Rep.’ All the data formatting was done in Python (in Jupyter Notebooks), making use of Pandas and Numpy. Final versions of the data were saved as json (Javascript Object Notation), a machine readable data-interchange format. While normally I work with Python and R for analysis and visualization, part of data science is knowing the right tools to use for the job, and one of the best options for interactive plots is d3.js. D3, Data-Driven Documents is a JavaScript library that lets you control every aspect of a chart by building it up one element at a time. It has a very steep learning curve, but the end results are worth it! For my first chart, I wanted to do something with a map because I enjoy seeing comparisons between countries in a geographic context. My idea was to create a world map and place circles on countries sized proportional to the amount of CO2 they emitted. The color of the circles would reflect the country’s rank in terms of emissions. As CO2 emissions have changed considerably, especially in the past few decades, I included a time dimension with options to visualize the entire history of the data (1850–2014) and a selection box so users can choose a specific year. The final results are presented in the following animation: As a finishing touch, I added tooltips showing the exact amount emitted by each country and the country’s ranking in the given year when you mouse over the country. It’s the little details like this that I love about professionally produced charts so I tried to do a little of that on my own! After finishing up the map, I wanted to make a better way to compare the top countries over time. There are a lot of details hidden in the map animation, and it’s not great for visualizing trajectories over time. The most common way to show time series is with simple line plots, and I saw no reason to invent a different solution. Again, I kept an interactive element in the chart by allowing users to select specific countries. The most difficult part of making this chart was the auto-scaling on the y-axis. Luckily, d3 lets you control every single aspect of a graph, so I was able to specify that the axis scale should update to match the country with the largest emissions. I thought this visual was better for comparing the top 15 countries than the map and playing around with it reveals a number of insights. For example, China surpasses the US in 2005 (not an area in which you want to be #1), and although the United Kingdom initially started out as the largest emitter, by 2014, their output pales in comparison to industrialized nations. One of the most influential data visualization inspirations to me is the late Hans Rosling, the founder of GapMinder. In particular, his talk showing the demographic changes in countries over time has stuck with me many years after I first watched it, in a way that no spreadsheet ever could. For a final visual, I wanted to try and recreate the GapMinder style of chart and use some additional socio-economic data in the process. Drawing heavily on Mike Bostock’s The Wealth and Health of Nations Chart, I made my own take on the animated, interactive visual. There were four different variables to represent on the same graph, which I broke up into the following The final version takes some time to absorb because it presents quite a bit of information! This chart also allows users to change the year by mousing over the label. When I was showcasing this, I had a number of people compare this to Hans Rosling’s work, which might be the highest data visualization complement I’ve gotten! Once I had all the charts together, I put together a quick home page and about page by copying and pasting existing html/css templates. I then hosted my files and created a static website on Amazon S3, a cloud storage service. (This tutorial was helpful). My formal mechanical engineering/data science education did not include any instruction in web design, so the overall aesthetic clearly needs some work. Nonetheless, the three charts accomplish the goal I set out to achieve: liberate this valuable public data from spreadsheets where it is not benefitting anyone! Conclusions One element I made sure to include in all the charts was interactivity. When I look at charts, I enjoy playing with the data myself and changing the parameters to find all the nuances. As this was an exploratory data analysis (EDA) project, I wanted to let users come to some conclusions on their own. While the overall message is that CO2 emissions have undoubtedly been increasing over time, there are other interesting takeaways in the charts that people can find on their own. We think of data as containing objective truth, but even with numbers, different individuals will bring their own interpretations that affect the knowledge they derive from the data. There are certainly times when we want to use data to prove a point, such as with medical trials, but in the case of this public data, I wanted to make it more accessible for people to use as they see fit. Although I did end up winning two prizes — the civic track and best use of Amazon Web Services — the swag isn’t as important as working on a cool project. Furthermore, the hackathon challenged me to learn another skill, d3.js, that I can add to the toolbox I use for data science. Successful data scientists are always willing to learn and there is no stage at which you have mastered everything. D3.js might not be worth the steep learning curve — plot.ly can be used in Python and is built on d3, and Tableau allows you to make professional charts from clean data very quickly — but I enjoyed the project nonetheless. Hackathons are a great way to get a lot of experience on real problems and I recommend them to anyone who loves to solve problems/ create unique projects. While I choose to take a data science approach, hackathons welcome people from all backgrounds, and I encourage anyone to try one out! If anyone wants to improve my website, or has any other feedback and constructive criticism, I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Y-axis shows GDP per Capita, a measure of a country’s wealth The area of the circles is proportional to the CO2 emissions The color of the circles represents the rank of the country X-axis shows the population of a country on a log scale",Data Visualization Hackathon Style,4,published,4995,1827,0.3973727422003284,1,1,1,0,0,0
79,1000,457.25625475234955,195,https://towardsdatascience.com/unintended-consequences-and-goodharts-law-68d60a94705c,6,Towards Data Science,2018-02-24 09:33:00,43.28,4,5923,2018-02-22 20:38:00,"['Machine Learning', 'Education', 'Data Science', 'Measurement', 'Towards Data Science']","Unintended Consequences and Goodhart’s Law The importance of using the right metrics In order to increase revenue, the manager of a customer service call center starts a new policy: rather than being paid an hourly wage, every employee is compensated solely based on the number of calls they make. After the first week, the experiment seems like a resounding success: the call center is processing twice the number of calls per day! The manager, who never bothers to listen to his employees’ conversations as long as their numbers are good, is quite pleased. However, when the boss stops by, she insists on going out to the floor and when she does so, both she and the manager are shocked by what they hear: the employees pick up the phone, issue a series of one word answers, and slam the phone down without waiting for a good-bye. No wonder the number of completed calls has doubled! Without intending to, by judging performance only by the volume of calls, the manager has incentivized employees to value speed over courtesy. Unknowingly, he has fallen for the phenomenon known as Goodhart’s Law. Goodhart’s Law is expressed simply as: “When a measure becomes a target, it ceases to be a good measure.” In other words, when we set one specific goal, people will tend to optimize for that objective regardless of the consequences. This leads to problems when other equally important aspects of a situation are neglected. Our call center manager thought that increasing the number of calls processed was a good objective, and his employees dutifully strove to increase their numbers. However by choosing only one metric to measure success, he motivated employees to sacrifice courtesy in the name of quantity. People respond to incentives, and our natural inclination is to maximize the standards by which we are judged. Once we are aware of Goodhart’s Law, we can its effect in many areas of our lives. In school, we are given one objective: maximize our grade. This focus on one number can be detrimental to actual learning. High school seemed like one long series of memorizing content for a test, then promptly forgetting it all so I could stuff my brain full of info for the next one, without any consideration of whether I really knew the concepts. This strategy worked quite well given how success was measured in school, but I doubt it is the best approach for a great education. Another area in which we see the detrimental effects of Goodhart’s Law is in the academic world where there is an emphasis on publishing as indicated by the phrase “publish or perish.” Publishing is often dependent on achieving a positive result in a study, which leads to the technique known as “p-hacking” where researchers manipulate or subset experimental results to achieve statistical significance. Both memorizing rather than learning content and p-hacking are unintended consequences that arise when a single number is used to gauge success. From a data science perspective, the application of Goodhart’s Law is that it reminds of us of the need for proper metrics. When we design a machine learning model or make changes to the interface of a website, we need a way to determine if our solution is effective. Often, we will use one statistic, such as mean-squared error for regression or F1 score for classification problems. If we realize there may be detrimental consequences of using only a single measure, we might think again about how we classify success. Much as the call center manager would be better off judging employee performance based on a combination of the number of calls handled and customer satisfaction, we can create better models by taking into account several factors. Instead of assessing a machine learning method only by accuracy, we might also consider interpretability so we create understandable models. Although most people want to hear a single number to summarize an analysis, in most situations we are better off reporting multiple measures (with uncertainty intervals). There are times when a single well-designed metric can encourage the behavior we want, such as in increasing savings rates for retirement, but, it is important to keep in mind that people will try to maximize whatever measurement we choose. If we end up achieving a single goal at the expense of other, equally important factors, then our solution might not help the situation. One of the first steps in solving a problem — data science or otherwise — is determining the right measure to gauge success. When we want to objectively find the best solution, we should recall the concept of Goodhart’s Law and realize that rather than using a single number, the best assessment is usually a set of measurements. By choosing multiple metrics, we can design a solution without the unintended consequences that occur when optimizing for a narrow objective. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Unintended Consequences and Goodhart’s Law,6,published,13684,933,1.0718113612004287,1,1,1,1,1,0
86,8500,450.18930983115746,1350,https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c,37,Towards Data Science,2018-03-03 11:10:00,20.45,11,26703,2018-02-27 14:32:00,"['Machine Learning', 'Data Science', 'Education', 'Statistics', 'Towards Data Science']","Beyond Accuracy: Precision and Recall Choosing the right metrics for classification tasks Would you believe someone who claimed to create a model entirely in their head to identify terrorists trying to board flights with greater than 99% accuracy? Well, here is the model: simply label every single person flying from a US airport as not a terrorist. Given the 800 million average passengers on US flights per year and the 19 (confirmed) terrorists who boarded US flights from 2000–2017, this model achieves an astounding accuracy of 99.9999999%! That might sound impressive, but I have a suspicion the US Department of Homeland Security will not be calling anytime soon to buy this model. While this solution has nearly-perfect accuracy, this problem is one in which accuracy is clearly not an adequate metric! The terrorist detection task is an imbalanced classification problem: we have two classes we need to identify — terrorists and not terrorists — with one category representing the overwhelming majority of the data points. Another imbalanced classification problem occurs in disease detection when the rate of the disease in the public is very low. In both these cases the positive class — disease or terrorist — is greatly outnumbered by the negative class. These types of problems are examples of the fairly common case in data science when accuracy is not a good measure for assessing model performance. Intuitively, we know that proclaiming all data points as negative in the terrorist detection problem is not helpful and, instead, we should focus on identifying the positive cases. The metric our intuition tells us we should maximize is known in statistics as recall, or the ability of a model to find all the relevant cases within a dataset. The precise definition of recall is the number of true positives divided by the number of true positives plus the number of false negatives. True positives are data point classified as positive by the model that actually are positive (meaning they are correct), and false negatives are data points the model identifies as negative that actually are positive (incorrect). In the terrorism case, true positives are correctly identified terrorists, and false negatives would be individuals the model labels as not terrorists that actually were terrorists. Recall can be thought as of a model’s ability to find all the data points of interest in a dataset. You might notice something about this equation: if we label all individuals as terrorists, then our recall goes to 1.0! We have a perfect classifier right? Well, not exactly. As with most concepts in data science, there is a trade-off in the metrics we choose to maximize. In the case of recall, when we increase the recall, we decrease the precision. Again, we intuitively know that a model that labels 100% of passengers as terrorists is probably not useful because we would then have to ban every single person from flying. Statistics provides us with the vocabulary to express our intuition: this new model would suffer from low precision, or the ability of a classification model to identify only the relevant data points. Precision is defined as the number of true positives divided by the number of true positives plus the number of false positives. False positives are cases the model incorrectly labels as positive that are actually negative, or in our example, individuals the model classifies as terrorists that are not. While recall expresses the ability to find all relevant instances in a dataset, precision expresses the proportion of the data points our model says was relevant actually were relevant. Now, we can see that our first model which labeled all individuals as not terrorists wasn’t very useful. Although it had near-perfect accuracy, it had 0 precision and 0 recall because there were no true positives! Say we modify the model slightly, and identify a single individual correctly as a terrorist. Now, our precision will be 1.0 (no false positives) but our recall will be very low because we will still have many false negatives. If we go to the other extreme and classify all passengers as terrorists, we will have a recall of 1.0 — we’ll catch every terrorist — but our precision will be very low and we’ll detain many innocent individuals. In other words, as we increase precision we decrease recall and vice-versa. Combining Precision and Recall In some situations, we might know that we want to maximize either recall or precision at the expense of the other metric. For example, in preliminary disease screening of patients for follow-up examinations, we would probably want a recall near 1.0 — we want to find all patients who actually have the disease — and we can accept a low precision if the cost of the follow-up examination is not significant. However, in cases where we want to find an optimal blend of precision and recall we can combine the two metrics using what is called the F1 score. The F1 score is the harmonic mean of precision and recall taking both metrics into account in the following equation: We use the harmonic mean instead of a simple average because it punishes extreme values. A classifier with a precision of 1.0 and a recall of 0.0 has a simple average of 0.5 but an F1 score of 0. The F1 score gives equal weight to both measures and is a specific example of the general Fβ metric where β can be adjusted to give more weight to either recall or precision. (There are other metrics for combining precision and recall, such as the Geometric Mean of precision and recall, but the F1 score is the most commonly used.) If we want to create a balanced classification model with the optimal balance of recall and precision, then we try to maximize the F1 score. Visualizing Precision and Recall I’ve thrown a couple new terms at you and we’ll walk through an example to show how they are used in practice. Before we can get there though we need to briefly talk about tw concepts used for showing precision and recall. First up is the confusion matrix which is useful for quickly calculating precision and recall given the predicted labels from a model. A confusion matrix for binary classification shows the four different outcomes: true positive, false positive, true negative, and false negative. The actual values form the columns, and the predicted values (labels) form the rows. The intersection of the rows and columns show one of the four outcomes. For example, if we predict a data point is positive, but it actually is negative, this is a false positive. Going from the confusion matrix to the recall and precision requires finding the respective values in the matrix and applying the equations: The other main visualization technique for showing the performance of a classification model is the Receiver Operating Characteristic (ROC) curve. Don’t let the complicated name scare you off! The idea is relatively simple: the ROC curve shows how the recall vs precision relationship changes as we vary the threshold for identifying a positive in our model. The threshold represents the value above which a data point is considered in the positive class. If we have a model for identifying a disease, our model might output a score for each patient between 0 and 1 and we can set a threshold in this range for labeling a patient as having the disease (a positive label). By altering the threshold, we can try to achieve the right precision vs recall balance. An ROC curve plots the true positive rate on the y-axis versus the false positive rate on the x-axis. The true positive rate (TPR) is the recall and the false positive rate (FPR) is the probability of a false alarm. Both of these can be calculated from the confusion matrix: A typical ROC curve is shown below: The black diagonal line indicates a random classifier and the red and blue curves show two different classification models. For a given model, we can only stay on one curve, but we can move along the curve by adjusting our threshold for classifying a positive case. Generally, as we decrease the threshold, we move to the right and upwards along the curve. With a threshold of 1.0, we would be in the lower left of the graph because we identify no data points as positives leading to no true positives and no false positives (TPR = FPR = 0). As we decrease the threshold, we identify more data points as positive, leading to more true positives, but also more false positives (the TPR and FPR increase). Eventually, at a threshold of 0.0 we identify all data points as positive and find ourselves in the upper right corner of the ROC curve (TPR = FPR = 1.0). Finally, we can quantify a model’s ROC curve by calculating the total Area Under the Curve (AUC), a metric which falls between 0 and 1 with a higher number indicating better classification performance. In the graph above, the AUC for the blue curve will be greater than that for the red curve, meaning the blue model is better at achieving a blend of precision and recall. A random classifier (the black line) achieves an AUC of 0.5. Recap We’ve covered a few terms, none of which are difficult on their own, but which combined can be a little overwhelming! Let’s do a quick recap and then walk through an example to solidly the new ideas we learned. Four Outcomes of Binary Classification Recall and Precision Metrics Visualizing Recall and Precision Example Application Our task will be to diagnose 100 patients with a disease present in 50% of the general population. We will assume a black box model, where we put in information about patients and receive a score between 0 and 1. We can alter the threshold for labeling a patient as positive (has the disease) to maximize the classifier performance. We will evaluate thresholds from 0.0 to 1.0 in increments of 0.1, at each step calculating the precision, recall, F1, and location on the ROC curve. Following are the classification outcomes at each threshold: We’ll do one sample calculation of the recall, precision, true positive rate, and false positive rate at athreshold of 0.5. First we make the confusion matrix: We can use the numbers in the matrix to calculate the recall, precision, and F1 score: Then we calculate the true positive and false positive rate to find the y and x coordinates for the ROC curve. To make the entire ROC curve, we carry out this process at each threshold. As you might think, this is pretty tedious, so instead of doing it by hand, we use a language like Python to do it for us! The Jupyter Notebook with the calculations is on GitHub for anyone to see the implementation. The final ROC curve is shown below with the thresholds above the points. Here we can see all the concepts come together! At a threshold of 1.0, we classify no patients as having the disease and hence have a recall and precision of 0.0. As the threshold decreases, the recall increases because we identify more patients that have the disease. However, as our recall increases, our precision decreases because in addition to increasing the true positives, we increase the false positives. At a threshold of 0.0, our recall is perfect — we find all patients with the disease — but our precision is low because we have many false positives. We can move along the curve for a given model by changing the threshold and select the threshold that maximizes the F1 score. To shift the entire curve, we would need to build a different model. Final model statistics at each threshold are below: Based on the F1 score, the overall best model occurs at a threshold of 0.5. If we wanted to emphasize precision or recall to a greater extent, we could choose the corresponding model that performs best on those measures. Conclusions We tend to use accuracy because everyone has an idea of what it means rather than because it is the best tool for the task! Although better-suited metrics such as recall and precision may seem foreign, we already have an intuitive sense of why they work better for some problems such as imbalanced classification tasks. Statistics provides us with the formal definitions and the equations to calculate these measures. Data science is about knowing the right tools to use for a job, and often we need to go beyond accuracy when developing classification models. Knowing about recall, precision, F1, and the ROC curve allows us to assess classification models and should make us think skeptically about anyone touting only the accuracy of a model, especially for imbalanced problems. As we have seen, accuracy does not provide a useful assessment on several crucial problems, but now we know how to employ smarter metrics! As always, I welcome constructive criticism, feedback, and discussion. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. False positives: data points labeled as positive that are actually negative True negatives: data points labeled as negative that are actually negative False negatives: data points labeled as negative that are actually positive Precision: ability of a classification model to return only relevant instances F1 score: single metric that combines recall and precision using the harmonic mean Receiver operating characteristic (ROC) curve: plots the true positive rate (TPR) versus the false positive rate (FPR) as a function of the model’s threshold for classifying a positive Area under the curve (AUC): metric to calculate the overall performance of a classification model based on area under the ROC curve True positives: data points labeled as positive that are actually positive Recall: ability of a classification model to identify all relevant instances Confusion matrix: shows the actual and predicted labels from a classification problem",Beyond Accuracy: Precision and Recall,5,published,130587,2565,3.313840155945419,3,1,1,1,1,0
77,7400,442.9764902898843,1238,https://towardsdatascience.com/controlling-the-web-with-python-6fceb22c5f08,34,Towards Data Science,2018-03-10 16:16:00,22.44,9,14756,2018-03-10 12:22:00,"['Python', 'Education', 'Programming', 'Internet', 'Towards Data Science']","Controlling the Web with Python An adventure in simple web automation Problem: Submitting class assignments requires navigating a maze of web pages so complex that several times I’ve turned an assignment in to the wrong place. Also, while this process only takes 1–2 minutes, it sometimes seems like an insurmountable barrier (like when I’ve finished an assignment way too late at night and I can barely remember my password). Solution: Use Python to automatically submit completed assignments! Ideally, I would be able to save an assignment, type a few keys, and have my work uploaded in a matter of seconds. At first this sounded too good to be true, but then I discovered selenium, a tool which can be used with Python to navigate the web for you. Anytime we find ourselves repeating tedious actions on the web with the same sequence of steps, this is a great chance to write a program to automate the process for us. With selenium and Python, we just need to write a script once, and which then we can run it as many times and save ourselves from repeating monotonous tasks (and in my case, eliminate the chance of submitting an assignment in the wrong place)! Here, I’ll walk through the solution I developed to automatically (and correctly) submit my assignments. Along the way, we’ll cover the basics of using Python and selenium to programmatically control the web. While this program does work (I’m using it every day!) it’s pretty custom so you won’t be able to copy and paste the code for your application. Nonetheless, the general techniques here can be applied to a limitless number of situations. (If you want to see the complete code, it’s available on GitHub). Approach Before we can get to the fun part of automating the web, we need to figure out the general structure of our solution. Jumping right into programming without a plan is a great way to waste many hours in frustration. I want to write a program to submit completed course assignments to the correct location on Canvas (my university’s “learning management system”). Starting with the basics, I need a way to tell the program the name of the assignment to submit and the class. I went with a simple approach and created a folder to hold completed assignments with child folders for each class. In the child folders, I place the completed document named for the particular assignment. The program can figure out the name of the class from the folder, and the name of the assignment by the document title. Here’s an example where the name of the class is EECS491 and the assignment is “Assignment 3 — Inference in Larger Graphical Models”. The first part of the program is a loop to go through the folders to find the assignment and class, which we store in a Python tuple: This takes care of file management and the program now knows the program and the assignment to turn in. The next step is to use selenium to navigate to the correct webpage and upload the assignment. To get started with selenium, we import the library and create a web driver, which is a browser that is controlled by our program. In this case, I’ll use Chrome as my browser and send the driver to the Canvas website where I submit assignments. When we open the Canvas webpage, we are greeted with our first obstacle, a login box! To get past this, we will need to fill in an id and a password and click the login button. Imagine the web driver as a person who has never seen a web page before: we need to tell it exactly where to click, what to type, and which buttons to press. There are a number of ways to tell our web driver what elements to find, all of which use selectors. A selector is a unique identifier for an element on a webpage. To find the selector for a specific element, say the CWRU ID box above, we need to inspect the webpage. In Chrome, this is done by pressing “ctrl + shift + i” or right clicking on any element and selecting “Inspect”. This brings up the Chrome developer tools, an extremely useful application which shows the HTML underlying any webpage. To find a selector for the “CWRU ID” box, I right clicked in the box, hit “Inspect” and saw the following in developer tools. The highlighted line corresponds to the id box element (this line is called an HTML tag). This HTML might look overwhelming, but we can ignore the majority of the information and focus on the id = ""username"" and name=""username"" parts. (these are known as attributes of the HTML tag). To select the id box with our web driver, we can use either the id or name attribute we found in the developer tools. Web drivers in selenium have many different methods for selecting elements on a webpage and there are often multiple ways to select the exact same item: Our program now has access to the id_box and we can interact with it in various ways, such as typing in keys, or clicking (if we have selected a button). We carry out the same process for the password box and login button, selecting each based on what we see in the Chrome developer tools. Then, we send information to the elements or click on them as needed. Once we are logged in, we are greeted by this slightly intimidating dashboard: We again need to guide the program through the webpage by specifying exactly the elements to click on and the information to enter. In this case, I tell the program to select courses from the menu on the left, and then the class corresponding to the assignment I need to turn in: The program finds the correct class using the name of the folder we stored in the first step. In this case, I use the selection method find_element_by_link_text to find the specific class. The “link text” for an element is just another selector we can find by inspecting the page. : This workflow may seem a little tedious, but remember, we only have to do it once when we write our program! After that, we can hit run as many times as we want and the program will navigate through all these pages for us. We use the same ‘inspect page — select element — interact with element’ process to get through a couple more screens. Finally, we reach the assignment submission page: At this point, I could see the finish line, but initially this screen perplexed me. I could click on the “Choose File” box pretty easily, but how was I supposed to select the actual file I need to upload? The answer turns out to be incredibly simple! We locate the Choose File box using a selector, and use the send_keys method to pass the exact path of the file (called file_location in the code below) to the box: That’s it! By sending the exact path of the file to the button, we can skip the whole process of navigating through folders to find the right file. After sending the location, we are rewarded with the following screen showing that our file is uploaded and ready for submission. Now, we select the “Submit Assignment” button, click, and our assignment is turned in! File management is always a critical step and I want to make sure I don’t re-submit or lose old assignments. I decided the best solution was to store a single file to be submitted in the completed_assignments folder at any one time and move files to asubmitted_assignments folder once they had been turned in. The final bit of code uses the os module to move the completed assignment by renaming it with the desired location: All of the proceeding code gets wrapped up in a single script, which I can run from the command line. To limit opportunities for mistakes, I only submit one assignment at a time, which isn’t a big deal given that it only takes about 5 seconds to run the program! Here’s what it looks like when I start the program: The program provides me with a chance to make sure this is the correct assignment before uploading. After the program has completed, I get the following output: While the program is running, I can watch Python go to work for me: Conclusions The technique of automating the web with Python works great for many tasks, both general and in my field of data science. For example, we could use selenium to automatically download new data files every day (assuming the website doesn’t have an API). While it might seem like a lot of work to write the script initially, the benefit comes from the fact that we can have the computer repeat this sequence as many times as want in exactly the same manner. The program will never lose focus and wander off to Twitter. It will faithfully carry out the same exact series of steps with perfect consistency (which works great until the website changes). I should mention you do want to be careful before you automate critical tasks. This example is relatively low-risk as I can always go back and re-submit assignments and I usually double-check the program’s handiwork. Websites change, and if you don’t change the program in response you might end up with a script that does something completely different than what you originally intended! In terms of paying off, this program saves me about 30 seconds for every assignment and took 2 hours to write. So, if I use it to turn in 240 assignments, then I come out ahead on time! However, the payoff of this program is in designing a cool solution to a problem and learning a lot in the process. While my time might have been more effectively spent working on assignments rather than figuring out how to automatically turn them in, I thoroughly enjoyed this challenge. There are few things as satisfying as solving problems, and Python turns out to be a pretty good tool for doing exactly that. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Controlling the Web with Python,5,published,65762,1906,3.882476390346275,0,1,0,1,0,1
76,3000,436.21711827540514,530,https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-one-getting-started-a11655a467d4,17,Towards Data Science,2018-03-17 10:29:00,36.75,11,20727,2018-03-16 12:40:00,"['Data Science', 'Python', 'Education', 'Visualization', 'Towards Data Science']","Data Visualization with Bokeh in Python, Part I: Getting Started Elevate your visualization game The most sophisticated statistical analysis can be meaningless without an effective means for communicating the results. This point was driven home by a recent experience I had on my research project, where we use data science to improve building energy efficiency. For the past several months, one of my team members has been working on a technique called wavelet transforms which is used to analyze the frequency components of a time-series. The method achieves positive results, but she was having trouble explaining it without getting lost in the technical details. Exasperated, she asked me if I could make a visual showing the transformation. In a couple minutes using an R package called gganimate, I made a simple animation showing how the method transforms a time-series. Now, instead of struggling to explain wavelets, my team member can show the clip to provide an intuitive idea of how the technique works. My conclusion was we can do the most rigorous analysis, but at the end of the day, all people want to see is a gif! While this statement is meant to be humorous, it has an element of truth: results will have little impact if they cannot be clearly communicated, and often the best way for presenting the results of an analysis is with visualizations. The resources available for data science are advancing rapidly which is especially pronounced in the realm of visualization where it seems there is another option to try every week. With all these advances there is one common trend: increased interactivity. People like to see data in static graphs but what they enjoy even more is playing with the data to see how changing parameters affects the results. With regards to my research, a report telling a building owner how much electricity they can save by changing their AC schedule is nice, but it’s more effective to give them an interactive graph where they can choose different schedules and see how their choice affects electricity consumption. Recently, inspired by the trend towards interactive plots and a desire to keep learning new tools, I have been working with Bokeh, a Python library. An example of the interactive capabilities of Bokeh are shown in this dashboard I built for my research project: While I can’t share the code behind this project, I can walk through an example of building a fully-interactive Bokeh application using publicly available data. This series of articles will cover the entire process of creating an application using Bokeh. For this first post, we’ll cover the basic elements of Bokeh, which we’ll build upon in subsequent posts. Throughout this series, we’ll be working with the nycflights13 dataset, which has records of over 300,000 flights from 2013. We will first concentrate on visualizing a single variable, in this case the arrival delay of flights in minutes and we’ll start by constructing a basic histogram, a classic method for display the spread and location of one continuous variable. The full code is accessible on GitHub and the first Jupyter notebook can be found here. This post focuses on the visuals so I encourage anyone to check out the code if they want to see the unglamorous, but necessary steps of data cleaning and formatting! Basics of Bokeh The major concept of Bokeh is that graphs are built up one layer at a time. We start out by creating a figure, and then we add elements, called glyphs, to the figure. (For those who have used ggplot, the idea of glyphs is essentially the same as that of geoms which are added to a graph one ‘layer’ at a time.) Glyphs can take on many shapes depending on the desired use: circles, lines, patches, bars, arcs, and so on. Let’s illustrate the idea of glyphs by making a basic chart with squares and circles. First, we make a plot using the figure method and then we append our glyphs to the plot by calling the appropriate method and passing in data. Finally, we show our plot (I’m using a Jupyter Notebook which lets you see the plots right below the code if you use the output_notebook call). This generates the slightly uninspiring plot below: While we could have easily made this chart in any plotting library, we get a few tools for free with any Bokeh plot which are on the right side and include panning, zooming, selection, and plot saving abilities. These tools are configurable and will come in handy when we want to investigate our data. Let’s now get to work on showing our flight delay data. Before we can jump right into the graph, we should load in the data and give it a brief inspection (bold is code output): The summary stats give us information to inform our plotting decisions: we have 327,346 flights, with a minimum delay of -86 minutes (meaning the flight was early by 86 minutes) and a maximum delay of 1272 minutes, an astounding 21 hours! The 75% quantile is only at 14 minutes, so we can assume that numbers over 1000 minutes are likely outliers (which does not mean they are illegitimate, just extreme). I will focus on delays between -60 minutes and +120 minutes for our histogram. A histogram is a common choice for an initial visualization of a single variable because it shows the distribution of the data. The x-position is the value of the variable grouped into intervals called bins, and the height of each bar represents the count (number) of data points in each interval. In our case, the x-position will represent the arrival delay in minutes and the height is the number of flights in the corresponding bin. Bokeh does not have a built-in histogram glyph, but we can make our own using the quad glyph which allows us to specify the bottom, top, left, and right edges of each bar. To create the data for the bars, we will use the numpy histogram function which calculates the number of data points in each specified bin. We will use bins of 5 minute length which means the function will count the number of flights in each five minute delay interval. After generating the data, we put it in a pandas dataframe to keep all the data in one object. The code here is not crucial for understanding Bokeh, but it’s useful nonetheless because of the prevalence of numpy and pandas in data science! Our data looks like this: The flightscolumn is the count of the number of flights within each delay interval from leftto right. From here, we can make a new Bokeh figure and add a quad glpyh specifying the appropriate parameters: Most of the work in producing this graph comes in the data formatting which is not an unusual occurrence in data science! From our plot, we see that arrival delays are nearly normally distributed with a slight positive skew or heavy tail on the right side. There are easier ways to create a basic histogram in Python, and the same result could be done using a few lines of matplotlib. However, the payoff in the development required for a Bokeh plot comes in the tools and ways to interact with the data that we can now easily add to the graph. Adding Interactivity The first type of interactivity we will cover in this series is passive interactions. These are actions the viewer can take which do not alter the data displayed. These are referred to as inspectors because they allow viewers to “investigate” the data in more detail . A useful inspector is the tooltip which appears when a user mouses over data points and is called the HoverTool in Bokeh. In order to add tooltips, we need to change our data source from a dataframe to a ColumnDataSource, a key concept in Bokeh. This is an object specifically used for plotting that includes data along with several methods and attributes. The ColumnDataSource allows us to add annotations and interactivity to our graphs, and can be constructed from a pandas dataframe. The actual data itself is held in a dictionary accessible through the data attribute of the ColumnDataSource. Here, we create the source from our dataframe and look at the keys of the data dictionary which correspond to the columns of our dataframe. When we add glyphs using a ColumnDataSource, we pass in the ColumnDataSource as the source parameter and refer to the column names using strings: Notice how the code refers to the specific data columns, such as ‘flights’, ‘left’, and ‘right’ by a single string instead of the df['column'] format as before. The syntax of a HoverTool may seem a little convoluted at first, but with practice they are quite easy to create. We pass our HoverToolinstance a list of tooltips as Python tuples where the first element is the label for the data and the second references the specific data we want to highlight. We can reference either attributes of the graph, such as x or y position using ‘$’ or specific fields in our source using ‘@’. That probably sounds a little confusing so here’s an example of a HoverTool where we do both: Here, we reference the left data field in the ColumnDataSource (which corresponds to the ‘left’ column of the original dataframe) using ‘@’ and we reference the (x,y) position of the cursor using ‘$’. The result is below: The (x,y) position is that of the mouse on the graph and is not very helpful for our histogram, because we to find the find the number of flights in a given bar which corresponds to the top of the bar. To fix that we will alter our tooltip instance to refer to the correct column. Formatting the data shown in a tooltip can be frustrating, so I usually create another column in my dataframe with the correct formatting. For example, if I want my tooltip to show the entire interval for a given bar, I create a formatted column in my dataframe: Then I convert this dataframe into a ColumnDataSource and access this column in my HoverTool call. The following code creates the plot with a hover tool referring to two formatted columns and adds the tool to the plot: In the Bokeh style, we include elements in our chart by adding them to the original figure. Notice in the p.quad glyph call, there are a few additional parameters, hover_fill_alpha and hover_fill_color, that change the look of the glyph when we mouse over the bar. I also added in styling using a style function (see the notebook for the code). Aesthetics are tedious to type, so I usually write a function that I can apply to any plot. When I use styling, I keep things simple and focus on readability of labels. The main point of a plot is to show the data, and adding unnecessary elements only detracts from the usefulness of a figure! The final plot is presented below: As we mouse over different bars, we get the precise statistics for that bar showing the interval and the number of flights within that interval. If we are proud of our plot, we can save it to an html file to share: Further Steps and Conclusions It took me more than one plot to get the basic workflow of Bokeh so don’t worry if it seems there is a lot to learn. We’ll get plenty more practice over the course of this series! While it might seem like Bokeh is a lot of work, the benefits come when we want to extend our visuals beyond simple static figures. Once we have a basic chart, we can increase the effectiveness of the visual by adding more elements. For example, if we want to look at the arrival delay by airline, we can make an interactive chart allowing users to select and compare airlines. We will leave active interactions, those that change the data displayed, to the next post, but here’s a look at what we can do: Active interactions require a bit more involved scripting, but that gives us a chance to work on our Python! (If anyone wants to have a look at the code for this plot before the next article, here it is.) Throughout this series, I want to emphasize that Bokeh or any one library tool will never be a one stop tool for all your plotting needs. Bokeh is great for allowing users to explore graphs, but for other uses, like simple exploratory data analysis, a lightweight library such asmatplotliblikely will be more efficient. This series is meant to show the capabilities of Bokeh to give you another plotting tool you can rely on as needed. The more libraries you know, the better equipped you will be to use the right visualization tool for the task. As always, I welcome constructive criticism and feedback. I can be reached on Twitter @koehrsen_will . Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.","Data Visualization with Bokeh in Python, Part I: Getting Started",11,published,56402,2394,1.2531328320802004,0,1,1,1,0,1
81,1200,433.2465668597801,206,https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-ii-interactions-a4cf994e2512,7,Towards Data Science,2018-03-20 09:47:00,36.97,10,11418,2018-03-18 12:22:00,"['Data Visualization', 'Python', 'Education', 'Programming']","Data Visualization with Bokeh in Python, Part II: Interactions Moving beyond static plots In the first part of this series, we walked through creating a basic histogram in Bokeh, a powerful Python visualization library. The final result, which shows the distribution of arrival delays of flights departing New York City in 2013 is shown below (with a nice tooltip!): This chart gets the job done, but it’s not very engaging! Viewers can see the distribution of flight delays is nearly normal (with a slight positive skew), but there’s no reason for them to spend more than a few seconds with the figure. If we want to create more engaging visualization, we can allow users to explore the data on their own through interactions. For example, in this histogram, one valuable feature would be the ability to select specific airlines to make comparisons or the option to change the width of the bins to examine the data in finer detail. Fortunately, these are both features we can add on top of our existing plot using Bokeh. The initial development of the histogram may have seemed involved for a simple plot, but now we get to see the payoff of using a powerful library like Bokeh! All the code for this series is available on GitHub. I encourage anyone to check it out for all the data cleaning details (an uninspiring but necessary part of data science) and to experiment with the code!(For interactive Bokeh plots, we can still use a Jupyter Notebook to show the results or we can write Python scripts and run a Bokeh server. For development, I usually work in a Jupyter Notebook because it is easier to rapidly iterate and change plots without having to restart the server. I then move to a server to display the final results. You can see both a standalone script and the full notebook on GitHub.) Active Interactions There are two classes of interactions in Bokeh: passive and active. Passive interactions, covered in Part I, are also known as inspectors because they allow users to examine a plot in more detail but do not change the information displayed. One example is a tooltip that appears when a user hovers over a data point: The second class of interaction is called active because it changes the actual data displayed on the plot. This can be anything from selecting a subset of the data (such as specific airlines) to changing the degree of a polynomial regression fit. There are multiple types of active interactions in Bokeh, but here we will focus on what are called “widgets”, elements that can be clicked on and that give the user control over some aspect of the plot. When I view graphs, I enjoy playing with active interactions (such as those on FlowingData) because they allow me to do my own exploration of the data. I find it more insightful to discover conclusions from the data on my own (with some direction from the designer) rather than from a completely static chart. Moreover, giving users some amount of freedom allows them to come away with slightly different interpretations that can generate beneficial discussion about the dataset. Interaction Outline Once we start adding active interactions, we need to move beyond single lines of code and into functions that encapsulate specific actions. For a Bokeh widget interaction, there are three main functions that to implement: Before we can make the plot, we need to plan out the data that will be displayed. For our interactive histogram, we will offer users three controllable parameters: For the function that makes the dataset for the plot, we need to allow each of these parameters to be specified. To inform how we will transform the data in our make_dataset function, lets load in all the relevant data and inspect. In this dataset, each row is one separate flight. The arr_delaycolumn is the arrival delay of the flight in minutes (negative numbers means the flight was early). In part I, we did some data exploration and know there are 327,236 flights with a minimum delay of -86 minutes and a maximum delay of +1272 minutes. In the make_datasetfunction, we will want to select airlines based on the name column in the dataframe and limit the flights by the arr_delay column. To make the data for the histogram, we use the numpy function histogram which counts the number of data points in each bin. In our case, this is the number of flights in each specified delay interval. For part I, we made a histogram for all flights, but now we will do it by each carrier. As the number of flights for each carrier varies significantly, we can display the delays not in raw counts but in proportions. That is, the height on the plot corresponds to the fraction of all flights for a specific airline with a delay in the corresponding bin. To go from counts to a proportion, we divide the count by the total count for the airline. Below is the full code for making the dataset. The function takes in a list of carriers that we want to include, the minimum and maximum delays to be plotted, and the specified bin width in minutes. (I know this is a post about Bokeh, but you can’t make a graph without formatted data, so I included the code to demonstrate my methods!) The results of running the function with all of the carriers is below: As a reminder, we are using the Bokeh quad glyphs to make the histogram and so we need to provide the left, right, and top of the glyph (the bottom will be fixed at 0). These are in the left, right, and proportion columns respectively. The color column gives each carrier a unique color and the f_ columns provide formatted text for the tooltips. The next function to implement is make_plot. The function should take in a ColumnDataSource (a specific type of object used in Bokeh for plotting) and return the plot object: If we pass in a source with all airlines, this code gives us the following plot: This histogram is very cluttered because there are 16 airlines plotted on the same graph! If we want to compare airlines, it’s nearly impossible because of the overlapping information. Luckily, we can add widgets to make the plot clearer and enable quick comparisons. Once we create a basic figure in Bokeh adding in interactions via widgets is relatively straightforward. The first widget we want is a selection box that allows viewers to select airlines to display. This control will be a check box which allows as many selections as desired and is known in Bokeh as a CheckboxGroup. To make the selection tool, we import the CheckboxGroup class and create an instance with two parameters, labels: the values we want displayed next to each box and active: the initial boxes which are checked. Here is the code to create a CheckboxGroup with all carriers. The labels in a Bokeh checkbox must be strings, while the active values are integers. This means that in the image ‘AirTran Airways Corporation’ maps to the active value of 0 and ‘Alaska Airlines Inc.’ maps to the active value of 1. When we want to match the selected checkboxes to the airlines, we need to make sure to find the string names associated with the selected integer active values. We can do this using the .labels and .active attributes of the widget: After making the selection widget, we now need to link the selected airline checkboxes to the information displayed on the graph. This is accomplished using the .on_change method of the CheckboxGroup and an update function that we define. The update function always takes three arguments: attr, old, new and updates the plot based on the selection controls. The way we change the data displayed on the graph is by altering the data source that we passed to the glyph(s) in the make_plot function. That might sound a little abstract, so here’s an example of an update function that changes the histogram to display the selected airlines: Here, we are retrieving the list of airlines to display based on the selected airlines from the CheckboxGroup. This list is passed to the make_datasetfunction which returns a new column data source. We update the data of the source used in the glyphs by calling src.data.update and passing in the data from the new source. Finally, in order to link changes in the carrier_selection widget to the update function, we have to use the .on_change method (called an event handler). This calls the update function any time a different airline is selected or unselected. The end result is that only glyphs corresponding to the selected airlines are drawn on the histogram, which can be seen below: Now that we know the basic workflow for creating a control we can add in more elements. Each time, we create the widget, write an update function to change the data displayed on the plot, and link the update function to the widget with an event handler. We can even use the same update function for multiple elements by rewriting the function to extract the values we need from the widgets. To practice, we will add two additional controls: a Slider which selects the bin width for the histogram, and a RangeSlider that sets the minimum and maximum delays to display. Here’s the code to make both of these widgets and the new update function: The standard slider and the range slider are shown here: If we want, we can also change other aspects of the plot besides the data displayed using the update function. For example, to change the title text to match the bin width we can do: There are many other types of interactions in Bokeh, but for now, our three controls allow users plenty to “play” with on the chart! Putting it all together All the elements for our interactive plot are in place. We have the three necessary functions: make_dataset, make_plot, and update to change the plot based on the controls and the widgets themselves. We join all of these elements onto one page by defining a layout. I put the entire layout onto a tab, and when we make a full application, we can put each plot on a separate tab. The final result of all this work is below: Feel free to check out the code and plot for yourself on GitHub. Next Steps and Conclusions The next part of this series will look at how we can make a complete application with multiple plots. We will be able to show our work on a server and access it in a browser, creating a full dashboard to explore the dataset. We can see that the final interactive plot is much more useful than the original! We can now compare delays between airlines and change the bin widths/ranges to see how the distribution is affected. Adding interactivity raises the value of a plot because it increases engagement with the data and allows users to arrive at conclusions through their own explorations. Although setting up the initial plot was involved, we saw how we could easily add elements and control widgets to an existing figure. The customizability of plots and interactions are the benefits of using a heavier plotting library like Bokeh compared to something quick and simple like matplotlib. Different visualization libraries have different advantages and use-cases, but when we want to add the extra dimension of interaction, Bokeh is a great choice. Hopefully at this point you are confident enough to start developing your own visualizations, and please share anything you create! I welcome feedback and constructive criticism and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. make_plot()Draw the plot with the specified data update() Update the plot based on user selections Range of delays on the plot, for example: -60 to +120 minutes Width of histogram bin, 5 minutes by default make_dataset() Format the specific data to be displayed Airlines displayed (called carriers in the code)","Data Visualization with Bokeh in Python, Part II: Interactions",10,published,30881,2220,0.5405405405405406,1,1,0,0,0,1
80,3500,430.123667052963,575,https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0,20,Towards Data Science,2018-03-23 12:44:00,27.09,11,32661,2018-03-22 21:21:00,"['Data Science', 'Programming', 'Education', 'Python', 'Statistics']","Histograms and Density Plots in Python Visualizing One-Dimensional Data in Python Plotting a single variable seems like it should be easy. With only one dimension how hard can it be to effectively display the data? For a long time, I got by using the simple histogram which shows the location of values, the spread of the data, and the shape of the data (normal, skewed, bimodal, etc.) However, I recently ran into some problems where a histogram failed and I knew it was time to broaden my plotting knowledge. I found an excellent free online book on data visualization, and implemented some of the techniques. Rather than keep everything I learned to myself, I decided it would helpful (to myself and to others) to write a Python guide to histograms and an alternative that has proven immensely useful, density plots. This article will take a comprehensive look at using histograms and density plots in Python using the matplotlib and seaborn libraries. Throughout, we will explore a real-world dataset because with the wealth of sources available online, there is no excuse for not using actual data! We will visualize the NYCflights13 data, which contains over 300,000 observations of flights departing NYC in 2013. We will focus on displaying a single variable, the arrival delay of flights in minutes. The full code for this article is available as a Jupyter Notebook on GitHub. It’s always a good idea to examine our data before we get started plotting. We can read the data into a pandas dataframe and display the first 10 rows: The flight arrival delays are in minutes and negative values mean the flight was early (it turns out flights often tend to arrive early, just never when we’re on them!) There are over 300,000 flights with a minimum delay of -60 minutes and a maximum delay of 120 minutes. The other column in the dataframe is the name of the airline which we can use for comparisons. Histograms A great way to get started exploring a single variable is with the histogram. A histogram divides the variable into bins, counts the data points in each bin, and shows the bins on the x-axis and the counts on the y-axis. In our case, the bins will be an interval of time representing the delay of the flights and the count will be the number of flights falling into that interval. The binwidth is the most important parameter for a histogram and we should always try out a few different values of binwidth to select the best one for our data. To make a basic histogram in Python, we can use either matplotlib or seaborn. The code below shows function calls in both libraries that create equivalent figures. For the plot calls, we specify the binwidth by the number of bins. For this plot, I will use bins that are 5 minutes in length, which means that the number of bins will be the range of the data (from -60 to 120 minutes) divided by the binwidth, 5 minutes ( bins = int(180/5)). For most basic histograms, I would go with the matplotlib code because it is simpler, but we will use the seaborn distplot function later on to create different distributions and it’s good to be familiar with the different options. How did I come up with 5 minutes for the binwidth? The only way to figure out an optimal binwidth is to try out multiple values! Below is code to make the same figure in matplotlib with a range of binwidths. Ultimately, there is no right or wrong answer to the binwidth, but I choose 5 minutes because I think it best represents the distribution. The choice of binwidth significantly affects the resulting plot. Smaller binwidths can make the plot cluttered, but larger binwidths may obscure nuances in the data. Matplotlib will automatically choose a reasonable binwidth for you, but I like to specify the binwidth myself after trying out several values. There is no true right or wrong answer, so try a few options and see which works best for your particular data. When Histograms Fail Histograms are a great way to start exploring a single variable drawn from one category. However, when we want to compare the distributions of one variable across multiple categories, histograms have issues with readability. For example, if we want to compare arrival delay distributions between airlines, an approach that doesn’t work well is to to create histograms for each airline on the same plot: (Notice that the y-axis has been normalized to account for the differing number of flights between airlines. To do this, pass in the argument norm_hist = True to the sns.distplot function call.) This plot is not very helpful! All the overlapping bars make it nearly impossible to make comparisons between the airlines. Let’s look at a few possible solutions to this common problem. Instead of overlapping the airline histograms, we can place them side-by-side. To do this, we create a list of the arrival delays for each airline, and then pass this into the plt.hist function call as a list of lists. We have to specify different colors to use for each airline and a label so we can tell them apart. The code, including creating the lists for each airline is below: By default, if we pass in a list of lists, matplotlib will put the bars side-by-side. Here, I have changed the binwidth to 15 minutes because otherwise the plot is too cluttered, but even with this modification, this is not an effective figure. There is too much information to process at once, the bars don’t align with the labels, and it’s still hard to compare distributions between airlines. When we make a plot, we want it to be as easy for the viewer to understand as possible, and this figure fails by that criteria! Let’s look at a second potential solution. Instead of plotting the bars for each airline side-by-side, we can stack them by passing in the parameter stacked = True to the histogram call: Well, that definitely is not any better! Here, each airline is represented as a section of the whole for each bin, but it’s nearly impossible to make comparisons. For example, at a delay of -15 to 0 minutes, does United Air Lines or JetBlue Airlines have a larger size of the bar? I can’t tell and viewers won’t be able to either. I generally am not a proponent of stacked bars because they can be difficult to interpret (although there are use cases such as when visualizing proportions). Both of the solutions we tried using histograms were not successful, and so it’s time to move to the density plot. Density Plots First, what is a density plot? A density plot is a smoothed, continuous version of a histogram estimated from the data. The most common form of estimation is known as kernel density estimation. In this method, a continuous curve (the kernel) is drawn at every individual data point and all of these curves are then added together to make a single smooth density estimation. The kernel most often used is a Gaussian (which produces a Gaussian bell curve at each data point). If, like me, you find that description a little confusing, take a look at the following plot: Here, each small black vertical line on the x-axis represents a data point. The individual kernels (Gaussians in this example) are shown drawn in dashed red lines above each point. The solid blue curve is created by summing the individual Gaussians and forms the overall density plot. The x-axis is the value of the variable just like in a histogram, but what exactly does the y-axis represent? The y-axis in a density plot is the probability density function for the kernel density estimation. However, we need to be careful to specify this is a probability density and not a probability. The difference is the probability density is the probability per unit on the x-axis. To convert to an actual probability, we need to find the area under the curve for a specific interval on the x-axis. Somewhat confusingly, because this is a probability density and not a probability, the y-axis can take values greater than one. The only requirement of the density plot is that the total area under the curve integrates to one. I generally tend to think of the y-axis on a density plot as a value only for relative comparisons between different categories. To make density plots in seaborn, we can use either the distplot or kdeplot function. I will continue to use the distplot function because it lets us make multiple distributions with one function call. For example, we can make a density plot showing all arrival delays on top of the corresponding histogram: The curve shows the density plot which is essentially a smooth version of the histogram. The y-axis is in terms of density, and the histogram is normalized by default so that it has the same y-scale as the density plot. Analogous to the binwidth of a histogram, a density plot has a parameter called the bandwidth that changes the individual kernels and significantly affects the final result of the plot. The plotting library will choose a reasonable value of the bandwidth for us (by default using the ‘scott’ estimate), and unlike the binwidth of a histogram, I usually use the default bandwidth. However, we can look at using different bandwidths to see if there is a better choice. In the plot, ‘scott’ is the default, which looks like the best option. Notice that a wider bandwidth results in more smoothing of the distribution. We also see that even though we limited our data to -60 to 120 minutes, the density plot extends beyond these limits. This is one potential issue with a density plot: because it calculates a distribution at each data point, it can generate data that falls outside the bounds of the original data. This might mean that we end up with impossible values on the x-axis that were never present in the original data! As a note, we can also change the kernel, which changes the distribution drawn at each data point and thus the overall distribution. However, for most applications, the default kernel, Gaussian, and the default bandwidth estimation work very well. Now that we understand how a density plot is made and what it represents, let’s see how it can solve our problem of visualizing the arrival delays of multiple airlines. To show the distributions on the same plot, we can iterate through the airlines, each time calling distplot with the kernel density estimate set to True and the histogram set to False. The code to draw the density plot with multiple airlines is below: Finally, we have arrived at an effective solution! With the density plot, we can easily make comparisons between airlines because the plot is less cluttered. Now that we finally have the plot we want, we come to the conclusion that all these airlines have nearly identical arrival delay distributions! However, there are other airlines in the dataset, and we can plot one that is a little different to illustrate another optional parameter for density plots, shading the graph. Filling in the density plot can help us to distinguish between overlapping distributions. Although this is not always a good approach, it can help to emphasize the difference between distributions. To shade the density plots, we pass in shade = True to the kde_kws argument in the distplot call. Whether or not to shade the plot is, like other plotting options, a question that depends on the problem! For this graph, I think it makes sense because the shading helps us distinguish the plots in the regions where they overlap. Now, we finally have some useful information: Alaska Airlines flights tend to be earlier more often than United Airlines. The next time you have the option, you know which airline to choose! If you want to show every value in a distribution and not just the smoothed density, you can add a rug plot. This shows every single data point on the x-axis, allowing us to visualize all of the actual values. The benefit of using seaborn’s distplot is that we can add the rug plot with a single parameter call of rug = True (with some formatting as well). With many data points the rug plot can become overcrowded, but for some datasets, it can be helpful to view every data point. The rug plot also lets us see how the density plot “creates” data where none exists because it makes a kernel distribution at each data point. These distributions can leak over the range of the original data and give the impression that Alaska Airlines has delays that are both shorter and longer than actually recorded. We need to be careful about this artifact of density plots and point it out to viewers! Conclusions This post has hopefully given you a range of options for visualizing a single variable from one or multiple categories. There are even more univariate (single variable) plots we can make such as empirical cumulative density plots and quantile-quantile plots, but for now we will leave it at histograms and density plots (and rug plots too!). Don’t worry if the options seem overwhelming: with practice, making a good choice will become easier, and you can always ask for help if needed. Moreover, often there isn’t an optimal choice and the “right” decision will come down to preference and the objectives of the visualization. The good thing is, no matter what plot you want to make, there is going to be a way to do it in Python! Visualizations are an effective means for communicating results, and knowing all the options available allows us to choose the right figure for our data. I welcome feedback and constructive criticism and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Histograms and Density Plots in Python,6,published,120556,2614,1.3389441469013006,0,1,1,0,0,1
82,2800,422.2958793939352,564,https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-iii-a-complete-dashboard-dc6a86aa6e23,16,Towards Data Science,2018-03-31 08:36:00,33.36,10,18986,2018-03-30 09:33:00,"['Python', 'Data Visualization', 'Programming', 'Bokeh', 'Towards Data Science']","Data Visualization with Bokeh in Python, Part III: Making a Complete Dashboard Creating an interactive visualization application in Bokeh Sometimes I learn a data science technique to solve a specific problem. Other times, as with Bokeh, I try out a new tool because I see some cool projects on Twitter and think: “That looks pretty neat. I’m not sure when I’ll use it, but it could come in handy.” Nearly every time I say this, I end up finding a use for the tool. Data science requires knowledge of many different skills and you never know where that next idea you will use will come from! In the case of Bokeh, several weeks after trying it out, I found a perfect use case in my work as a data science researcher. My research project involves increasing the energy efficiency of commercial buildings using data science, and, for a recent conference, we needed a way to show off the results of the many techniques we apply. The usual suggestion of a powerpoint gets the job done, but doesn’t really stand out. By the time most people at a conference see their third slide deck, they have already stopped paying attention. Although I didn’t yet know Bokeh very well, I volunteered to try and make an interactive application with the library, thinking it would allow me to expand my skill-set and create an engaging way to show off our project. Skeptical, our team prepared a back-up presentation, but after I showed them some prototypes, they gave it their full support. The final interactive dashboard was a stand-out at the conference and will be adopted by our team for future use: While not every idea you see on Twitter is probably going to be helpful to your career, I think it’s safe to say that knowing more data science techniques can’t possibly hurt. Along these lines, I started this series to share the capabilities of Bokeh, a powerful plotting library in Python that allows you to make interactive plots and dashboards. Although I can’t share the dashboard for my research, I can show the basics of building visualizations in Bokeh using a publicly available dataset. This third post is a continuation of my Bokeh series, with Part I focused on building a simple graph, and Part II showing how to add interactions to a Bokeh plot. In this post, we will see how to set up a full Bokeh application and run a local Bokeh server accessible in your browser! This article will focus on the structure of a Bokeh application rather than the plot details, but the full code for everything can be found on GitHub. We will continue to use the NYCFlights13 dataset, a real collection of flight information from flights departing 3 NYC airports in 2013. There are over 300,000 flights in the dataset, and for our dashboard, we will focus primarily on exploring the arrival delay information. To run the full application for yourself, make sure you have Bokeh installed ( using pip install bokeh), download the bokeh_app.zip folder from GitHub, unzip it, open a command window in the directory, and type bokeh serve --show bokeh_app. This will set-up a local Bokeh server and open the application in your browser (you can also make Bokeh plots available publicly online, but for now we will stick to local hosting). Final Product Before we get into the details, let’s take a look at the end product we’re aiming for so we can see how the pieces fit together. Following is a short clip showing how we can interact with the complete dashboard: Here I am using the Bokeh application in a browser (in Chrome’s fullscreen mode) that is running on a local server. At the top we see a number of tabs, each of which contains a different section of the application. The idea of a dashboard is that while each tab can stand on its own, we can join many of them together to enable a complete exploration of the data. The video shows the range of charts we can make with Bokeh, from histograms and density plots, to data tables that we can sort by column, to fully interactive maps. Besides the range of figures we can create in Bokeh, another benefit of using this library is interactions. Each tab has an interactive element which lets users engage with the data and make their own discoveries. From experience, when exploring a dataset, people like to come to insights on their own, which we can allow by letting them select and filter data through various controls. Now that we have an idea of the dashboard we are aiming for, let’s take a look at how to create a Bokeh application. I highly recommend downloading the code for yourself to follow along! Structure of a Bokeh Application Before writing any code, it’s important to establish a framework for our application. In any project, it’s easy to get carried away coding and soon become lost in a mess of half-finished scripts and out-of-place data files, so we want to create a structure beforehand for all our codes and data to slot into. This organization will help us keep track of all the elements in our application and assist in debugging when things inevitably go wrong. Also, we can re-use this framework for future projects so our initial investment in the planning stage will pay off down the road. To set up a Bokeh application, I create one parent directory to hold everything called bokeh_app . Within this directory, we will have a sub-directory for our data (called data), a sub-directory for our scripts (scripts), and a main.py script to pull everything together. Generally, to manage all the code, I have found it best to keep the code for each tab in a separate Python script and call them all from a single main script. Following is the file structure I use for a Bokeh application, adapted from the official documentation. For the flights application, the structure follows the general outline: There are three main parts: data, scripts, and main.py, under one parentbokeh_app directory. When it comes time to run the server, we tell Bokeh to serve the bokeh_app directory and it will automatically search for and run the main.py script. With the general structure in place, let’s take a look at main.py which is what I like to call the executive of the Bokeh application (not a technical term)! main.py The main.py script is like the executive of a Bokeh application. It loads in the data, passes it out to the other scripts, gets back the resulting plots, and organizes them into one single display. This will be the only script I show in its entirety because of how critical it is to the application: We start out with the necessary imports including the functions to make the tabs, each of which is stored in a separate script within the scripts directory. If you look at the file structure, notice that there is an __init__.py file in the scripts directory. This is a completely blank file that needs to be placed in the directory to allow us to import the appropriate functions using relative statements (e.g. from scripts.histogram import histogram_tab ). I’m not quite sure why this is needed, but it works (here’s the Stack Overflow answer I used to figure this out). After the library and script imports, we read in the necessary data with help from the Python __file__ attribute. In this case, we are using two pandas dataframes ( flights and map_data ) as well as US states data that is included in Bokeh. Once the data has been read in, the script proceeds to delegation: it passes the appropriate data to each function, the functions each draw and return a tab, and the main script organizes all these tabs in a single layout called tabs. As an example of what each of these separate tab functions does, let’s look at the function that draws the map_tab. This function takes in map_data (a formatted version of the flights data) and the US state data and produces a map of flight routes for selected airlines: We covered interactive plots in Part II of this series, and this plot is just an implementation of that idea. The overall structure of the function is: We see the familiar make_dataset, make_plot, and update functions used to draw the plot with interactive controls. Once we have the plot set up, the final line returns the entire plot to the main script. Each individual script (there are 5 for the 5 tabs) follows the same pattern. Returning to the main script, the final touch is to gather the tabs and add them to a single document. The tabs appear at the top of the application, and much like tabs in any browser, we can easily switch between them to explore the data. Running the Bokeh Server After all the set-up and coding required to make the plots, running the Bokeh server locally is quite simple. We open up a command line interface (I prefer Git Bash but any one will work), change to the directory containing bokeh_app and run bokeh serve --show bokeh_app. Assuming everything is coded correctly, the application will automatically open in our browser at the address http://localhost:5006/bokeh_app. We can then access the application and explore our dashboard! If something goes wrong (as it undoubtedly will the first few times we write a dashboard) it can be frustrating to have to stop the server, make changes to the files, and restart the server to see if our changes had the desired effect. To quickly iterate and resolve problems, I generally develop plots in a Jupyter Notebook. The Jupyter Notebook is a great environment for Bokeh development because you can create and test fully interactive plots from within the notebook. The syntax is a little different, but once you have a completed plot, the code just needs to be slightly modified and can then be copied and pasted into a standalone .py script. To see this in action, take a look at the Jupyter Notebook I used to develop the application. Conclusions A fully interactive Bokeh dashboard makes any data science project stand out. Oftentimes, I see my colleagues do a lot of great statistical work but then fail to clearly communicate the results, which means all that work doesn’t get the recognition it deserves. From personal experience, I have also seen how effective Bokeh applications can be in communicating results. While making a full dashboard is a lot of work (this one is over 600 lines of code!) the results are worthwhile. Moreover, once we have an application, we can quickly share it using GitHub and if we are smart about our structure, we can re-use the framework for additional projects. The key points to take away from this project are applicable to many data science projects in general: That’s all for this post and for this series, although I plan on releasing additional stand-alone tutorials on Bokeh in the future. With libraries like Bokeh and plot.ly it’s becoming easier to make interactive figures and having a way to present your data science results in a compelling manner is crucial. Check out this Bokeh GitHub repo for all my work and feel free to fork and get started with your own projects. For now, I’m eager to see what everyone else can create! As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Finding a debugging cycle that allows you to quickly iterate through ideas is crucial. The write code —see results — fix errors loop allowed by the Jupyter Notebook makes for a productive development cycle (at least for small scale projects). Interactive applications in Bokeh will elevate your project and encourage user engagement. A dashboard can be a stand alone exploratory project, or highlight all the tough analysis work you’ve already done! You never know where you will find the next tool you will use in your work or side projects. Keep your eyes open, and don’t be afraid to experiment with new software and techniques! Having the proper framework/structure in place before you start on a data science task — Bokeh or anything else — is crucial. This way, you won’t find yourself lost in a forest of code trying to find errors. Also, once we develop a framework that works, it can be re-used with minimal effort, leading to dividends far down the road.","Data Visualization with Bokeh in Python, Part III: Making a Complete Dashboard",13,published,56905,2365,1.1839323467230445,0,0,0,1,0,1
75,3400,416.0628522097454,628,https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166,10,Towards Data Science,2018-04-06 14:12:00,31.39,8,15987,2018-04-03 20:06:00,"['Data Science', 'Education', 'Programming', 'Data Visualization', 'Data Analysis']","Visualizing Data with Pairs Plots in Python How to quickly create a powerful exploratory data analysis visualization Once you’ve got yourself a nice cleaned dataset, the next step is Exploratory Data Analysis (EDA). EDA is the process of figuring out what the data can tell us and we use EDA to find patterns, relationships, or anomalies to inform our subsequent analysis. While there are an almost overwhelming number of methods to use in EDA, one of the most effective starting tools is the pairs plot (also called a scatterplot matrix). A pairs plot allows us to see both distribution of single variables and relationships between two variables. Pair plots are a great method to identify trends for follow-up analysis and, fortunately, are easily implemented in Python! In this article we will walk through getting up and running with pairs plots in Python using the seaborn visualization library. We will see how to create a default pairs plot for a rapid examination of our data and how to customize the visualization for deeper insights. The code for this project is available as a Jupyter Notebook on GitHub. We will explore a real-world dataset, comprised of country-level socioeconomic data collected by GapMinder. Pairs Plots in Seaborn To get started we need to know what data we have. We can load in the socioeconomic data as a pandas dataframe and look at the columns: Each row of the data represents an observation for one country in one year and the columns hold the variables (data in this format is known as tidy data). There are 2 categorical columns (country and continent) and 4 numerical columns. The columns are fairly self-explanatory: life_exp is life expectancy at birth in years, popis population, and gdp_per_cap is gross domestic product per person in units of international dollars. The default pairs plot in seaborn only plots numerical columns although later we will use the categorical variables for coloring. Creating the default pairs plot is simple: we load in the seaborn library and call the pairplot function, passing it our dataframe: I’m still amazed that one simple line of code gives us this entire plot! The pairs plot builds on two basic figures, the histogram and the scatter plot. The histogram on the diagonal allows us to see the distribution of a single variable while the scatter plots on the upper and lower triangles show the relationship (or lack thereof) between two variables. For example, the left-most plot in the second row shows the scatter plot of life_exp versus year. The default pairs plot by itself often gives us valuable insights. We see that life expectancy and gdp per capita are positively correlated showing that people in higher income countries tend to live longer (although this of course does not prove that one causes the other). It also appears that (thankfully) life expectancies worldwide are on the rise over time. From the histograms, we learn that the population and gdp variables are heavily right-skewed. To better show these variables in future plots, we can transform these columns by taking the logarithm of the values: While this plot alone can be useful in an analysis, we can find make it more valuable by coloring the figures based on a categorical variable such as continent. This is also extremely simple in seaborn! All we need to do is use the hue keyword in the sns.pairplot function call: Now we see that Oceania and Europe tend to have the highest life expectancies and Asia has the largest population. Notice that our log transformation of the population and gdp made these variables normally distributed which gives a more thorough representation of the values. This graph is more informative, but there are still some issues: I tend not to find stacked histograms, as on the diagonals, to be very interpretable. A better method for showing univariate (single variable) distributions from multiple categories is the density plot. We can exchange the histogram for a density plot in the function call. While we are at it, we will pass in some keywords to the scatter plots to change the transparency, size, and edgecolor of the points. The density plots on the diagonal make it easier to compare distributions between the continents than stacked bars. Changing the transparency of the scatter plots increases readability because there is considerable overlap (known as overplotting) on these figures. As a final example of the default pairplot, let’s reduce the clutter by plotting only the years after 2000. We will still color by continent, but now we won’t plot the year column. To limit the columns plotted, we pass in a list of vars to the function. To clarify the plot, we can also add a title. This is starting to look pretty nice! If we were going to do modeling, we could use information from these plots to inform our choices. For example, we know that log_gdp_per_cap is positively correlated with life_exp, so we could create a linear model to quantify this relationship. For this post we’ll stick to plotting, and, if we want to explore our data even more, we can customize the pairplots using the PairGrid class. Customization with PairGrid In contrast to the sns.pairplot function, sns.PairGrid is a class which means that it does not automatically fill in the plots for us. Instead, we create a class instance and then we map specific functions to the different sections of the grid. To create a PairGrid instance with our data, we use the following code which also limits the variables we will show: If we were to display this, we would get a blank graph because we have not mapped any functions to the grid sections. There are three grid sections to fill in for a PairGrid: the upper triangle, lower triangle, and the diagonal. To map plots to these sections, we use the grid.map method on the section. For example, to map a scatter plot to the upper triangle we use: The map_upper method takes in any function that accepts two arrays of variables (such as plt.scatter)and associated keywords (such as color). The map_lower method is the exact same but fills in the lower triangle of the grid. The map_diag is slightly different because it takes in a function that accepts a single array (remember the diagonal shows only one variable). An example is plt.hist which we use to fill in the diagonal section below: In this case, we are using a kernel density estimate in 2-D (a density plot) on the lower triangle. Put together, this code gives us the following plot: The real benefits of using the PairGrid class come when we want to create custom functions to map different information onto the plot. For example, I might want to add the Pearson Correlation Coefficient between two variables onto the scatterplot. To do so, I would write a function that takes in two arrays, calculates the statistic, and then draws it on the graph. The following code shows how this is done (credit to this Stack Overflow answer): Our new function is mapped to the upper triangle because we need two arrays to calculate a correlation coefficient (notice also that we can map multiple functions to grid sections). This produces the following plot: The correlation coefficient now appears above the scatterplot. This is a relatively straightforward example, but we can use PairGrid to map any function we want onto the plot. We can add as much information as needed provided we can figure out how to write the function! As a final example, here is a plot that shows the summary statistics on the diagonal instead of a plot. This needs a little cleaning up, but it shows the general idea; in addition to using any existing function in a library such as matplotlib to map data onto the figure, we can write our own function to show custom information. Conclusion Pairs plots are a powerful tool to quickly explore distributions and relationships in a dataset. Seaborn provides a simple default method for making pair plots that can be customized and extended through the Pair Grid class. In a data analysis project, a major portion of the value often comes not in the flashy machine learning, but in the straightforward visualization of data. A pairs plot is provides us with a comprehensive first look at our data and is a great starting point in data analysis projects. I welcome feedback and constructive criticism and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Visualizing Data with Pairs Plots in Python,7,published,50934,1577,2.1559923906150917,2,1,1,0,0,0
85,6000,408.7945738315047,983,https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7,19,Towards Data Science,2018-04-13 20:38:00,33.88,10,19994,2018-04-12 09:58:00,"['Data Science', 'Education', 'Statistics', 'Learning', 'Towards Data Science']","Introduction to Bayesian Linear Regression An explanation of the Bayesian approach to linear modeling The Bayesian vs Frequentist debate is one of those academic arguments that I find more interesting to watch than engage in. Rather than enthusiastically jump in on one side, I think it’s more productive to learn both methods of statistical inference and apply them where appropriate. In that line of thinking, recently, I have been working to learn and apply Bayesian inference methods to supplement the frequentist statistics covered in my grad classes. One of my first areas of focus in applied Bayesian Inference was Bayesian Linear modeling. The most important part of the learning process might just be explaining an idea to others, and this post is my attempt to introduce the concept of Bayesian Linear Regression. We’ll do a brief review of the frequentist approach to linear regression, introduce the Bayesian interpretation, and look at some results applied to a simple dataset. I kept the code out of this article, but it can be found on GitHub in a Jupyter Notebook. Recap of Frequentist Linear Regression The frequentist view of linear regression is probably the one you are familiar with from school: the model assumes that the response variable (y) is a linear combination of weights multiplied by a set of predictor variables (x). The full formula also includes an error term to account for random sampling noise. For example, if we have two predictors, the equation is: y is the response variable (also called the dependent variable), β’s are the weights (known as the model parameters), x’s are the values of the predictor variables, and ε is an error term representing random sampling noise or the effect of variables not included in the model. Linear Regression is a simple model which makes it easily interpretable: β_0 is the intercept term and the other weights, β’s, show the effect on the response of increasing a predictor variable. For example, if β_1 is 1.2, then for every unit increase in x_1,the response will increase by 1.2. We can generalize the linear model to any number of predictors using matrix equations. Adding a constant term of 1 to the predictor matrix to account for the intercept, we can write the matrix formula as: The goal of learning a linear model from training data is to find the coefficients, β, that best explain the data. In frequentist linear regression, the best explanation is taken to mean the coefficients, β, that minimize the residual sum of squares (RSS). RSS is the total of the squared differences between the known values (y) and the predicted model outputs (ŷ, pronounced y-hat indicating an estimate). The residual sum of squares is a function of the model parameters: The summation is taken over the N data points in the training set. We won’t go into the details here (check out this reference for the derivation), but this equation has a closed form solution for the model parameters, β, that minimize the error. This is known as the maximum likelihood estimate of β because it is the value that is the most probable given the inputs, X, and outputs, y. The closed form solution expressed in matrix form is: (Again, we have to put the ‘hat’ on β because it represents an estimate for the model parameters.) Don’t let the matrix math scare you off! Thanks to libraries like Scikit-learn in Python, we generally don’t have to calculate this by hand (although it is good practice to code a linear regression). This method of fitting the model parameters by minimizing the RSS is called Ordinary Least Squares (OLS). What we obtain from frequentist linear regression is a single estimate for the model parameters based only on the training data. Our model is completely informed by the data: in this view, everything that we need to know for our model is encoded in the training data we have available. Once we have β-hat, we can estimate the output value of any new data point by applying our model equation: As an example of OLS, we can perform a linear regression on real-world data which has duration and calories burned for 15000 exercise observations. Below is the data and OLS model obtained by solving the above matrix equation for the model parameters: With OLS, we get a single estimate of the model parameters, in this case, the intercept and slope of the line.We can write the equation produced by OLS: From the slope, we can say that every additional minute of exercise results in 7.17 additional calories burned. The intercept in this case is not as helpful, because it tells us that if we exercise for 0 minutes, we will burn -21.86 calories! This is just an artifact of the OLS fitting procedure, which finds the line that minimizes the error on the training data regardless of whether it physically makes sense. If we have a new datapoint, say an exercise duration of 15.5 minutes, we can plug it into the equation to get a point estimate of calories burned: calories = -21.83 + 7.17 * 15.5 = 89.2 Ordinary least squares gives us a single point estimate for the output, which we can interpret as the most likely estimate given the data. However, if we have a small dataset we might like to express our estimate as a distribution of possible values. This is where Bayesian Linear Regression comes in. Bayesian Linear Regression In the Bayesian viewpoint, we formulate linear regression using probability distributions rather than point estimates. The response, y, is not estimated as a single value, but is assumed to be drawn from a probability distribution. The model for Bayesian Linear Regression with the response sampled from a normal distribution is: The output, y is generated from a normal (Gaussian) Distribution characterized by a mean and variance. The mean for linear regression is the transpose of the weight matrix multiplied by the predictor matrix. The variance is the square of the standard deviation σ (multiplied by the Identity matrix because this is a multi-dimensional formulation of the model). The aim of Bayesian Linear Regression is not to find the single “best” value of the model parameters, but rather to determine the posterior distribution for the model parameters. Not only is the response generated from a probability distribution, but the model parameters are assumed to come from a distribution as well. The posterior probability of the model parameters is conditional upon the training inputs and outputs: Here, P(β|y, X) is the posterior probability distribution of the model parameters given the inputs and outputs. This is equal to the likelihood of the data, P(y|β, X), multiplied by the prior probability of the parameters and divided by a normalization constant. This is a simple expression of Bayes Theorem, the fundamental underpinning of Bayesian Inference: Let’s stop and think about what this means. In contrast to OLS, we have a posterior distribution for the model parameters that is proportional to the likelihood of the data multiplied by the prior probability of the parameters. Here we can observe the two primary benefits of Bayesian Linear Regression. As the amount of data points increases, the likelihood washes out the prior, and in the case of infinite data, the outputs for the parameters converge to the values obtained from OLS. The formulation of model parameters as distributions encapsulates the Bayesian worldview: we start out with an initial estimate, our prior, and as we gather more evidence, our model becomes less wrong. Bayesian reasoning is a natural extension of our intuition. Often, we have an initial hypothesis, and as we collect data that either supports or disproves our ideas, we change our model of the world (ideally this is how we would reason)! Implementing Bayesian Linear Regression In practice, evaluating the posterior distribution for the model parameters is intractable for continuous variables, so we use sampling methods to draw samples from the posterior in order to approximate the posterior. The technique of drawing random samples from a distribution to approximate the distribution is one application of Monte Carlo methods. There are a number of algorithms for Monte Carlo sampling, with the most common being variants of Markov Chain Monte Carlo (see this post for an application in Python). I’ll skip the code for this post (see the notebook for the implementation in PyMC3) but the basic procedure for implementing Bayesian Linear Regression is: specify priors for the model parameters (I used normal distributions in this example), creating a model mapping the training inputs to the training outputs, and then have a Markov Chain Monte Carlo (MCMC) algorithm draw samples from the posterior distribution for the model parameters. The end result will be posterior distributions for the parameters. We can inspect these distributions to get a sense of what is occurring. The first plots show the approximations of the posterior distributions of model parameters. These are the result of 1000 steps of MCMC, meaning the algorithm drew 1000 steps from the posterior distribution. If we compare the mean values for the slope and intercept to those obtained from OLS (the intercept from OLS was -21.83 and the slope was 7.17), we see that they are very similar. However, while we can use the mean as a single point estimate, we also have a range of possible values for the model parameters. As the number of data points increases, this range will shrink and converge one a single value representing greater confidence in the model parameters. (In Bayesian inference a range for a variable is called a credible interval and which has a slightly different interpretation from a confidence interval in frequentist inference). When we want show the linear fit from a Bayesian model, instead of showing only estimate, we can draw a range of lines, with each one representing a different estimate of the model parameters. As the number of datapoints increases, the lines begin to overlap because there is less uncertainty in the model parameters. In order to demonstrate the effect of the number of datapoints in the model, I used two models, the first, with the resulting fits shown on the left, used 500 datapoints and the one on the right used 15000 datapoints. Each graph shows 100 possible models drawn from the model parameter posteriors. There is much more variation in the fits when using fewer data points, which represents a greater uncertainty in the model. With all of the data points, the OLS and Bayesian Fits are nearly identical because the priors are washed out by the likelihoods from the data. When predicting the output for a single datapoint using our Bayesian Linear Model, we also do not get a single value but a distribution. Following is the probability density plot for the number of calories burned exercising for 15.5 minutes. The red vertical line indicates the point estimate from OLS. We see that the probability of the number of calories burned peaks around 89.3, but the full estimate is a range of possible values. Conclusions Instead of taking sides in the Bayesian vs Frequentist debate (or any argument), it is more constructive to learn both approaches. That way, we can apply them in the right situation. In problems where we have limited data or have some prior knowledge that we want to use in our model, the Bayesian Linear Regression approach can both incorporate prior information and show our uncertainty. Bayesian Linear Regression reflects the Bayesian framework: we form an initial estimate and improve our estimate as we gather more data. The Bayesian viewpoint is an intuitive way of looking at the world and Bayesian Inference can be a useful alternative to its frequentist counterpart. Data science is not about taking sides, but about figuring out the best tool for the job, and having more techniques in your repertoire only makes you more effective! As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Sources Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Posterior: The result of performing Bayesian Linear Regression is a distribution of possible model parameters based on the data and the prior. This allows us to quantify our uncertainty about the model: if we have fewer data points, the posterior distribution will be more spread out. http://twiecki.github.io/blog/2013/08/12/bayesian-glms-1/ https://wiseodd.github.io/techblog/2017/01/05/bayesian-regression/ Priors: If we have domain knowledge, or a guess for what the model parameters should be, we can include them in our model, unlike in the frequentist approach which assumes everything there is to know about the parameters comes from the data. If we don’t have any estimates ahead of time, we can use non-informative priors for the parameters such as a normal distribution. https://www.quantstart.com/articles/Bayesian-Linear-Regression-Models-with-PyMC3",Introduction to Bayesian Linear Regression,5,published,59012,2394,2.506265664160401,1,1,1,1,0,0
73,1500,402.09134036238424,268,https://towardsdatascience.com/bayesian-linear-regression-in-python-using-machine-learning-to-predict-student-grades-part-1-7d0ad817fca5,9,Towards Data Science,2018-04-20 13:31:00,31.15,12,6874,2018-04-02 19:57:00,"['Machine Learning', 'Education', 'Data Analysis', 'Python', 'Towards Data Science']","Bayesian Linear Regression in Python: Using Machine Learning to Predict Student Grades Part 1 Exploratory Data Analysis, Feature Selection, and Benchmarks Even after struggling with the theory of Bayesian Linear Modeling for a couple weeks and writing a blog plot covering it, I couldn’t say I completely understood the concept. So, with the mindset that learn by doing is the most effective technique, I set out to do a data science project using Bayesian Linear Regression as my machine learning model of choice. This post is the first of two documenting the project. I wanted to show an example of a complete data science pipeline, so this first post will concentrate on defining the problem, exploratory data analysis, and setting benchmarks. The second part will focus entirely on implementing Bayesian Linear Regression and interpreting the results, so if you already have EDA down, head on over there. If not, or if you just want to see some nice plots, stay here and we’ll walk through how to get started on a data science problem. The complete code for this project is available as a Jupyter Notebook on GitHub. I encourage anyone interested to check it out and put their own spin on this project. Feel free to use, build on, and distribute the code in any way! I like to focus on using real-world data, and in this project, we will be exploring student performance data collected from a Portuguese secondary (high) school. The data includes personal and academic characteristics of students along with final class grades. Our objective will be to create a model that can predict grades based on the student’s information. This dataset, along with many other useful ones for testing models or trying out data science techniques, is available on the UCI Machine Learning Repository. Exploratory Data Analysis The first step in solving a data science problem (once you have cleaned data) is exploratory data analysis (EDA). This is an open-ended process where we look for anomalies, interesting trends or patterns, and correlations in a dataset. These may be interesting in their own right and they can inform our modeling. Basically, we use EDA to find out what our data can tell us! First off, let’s look at a snapshot of the data as a pandas dataframe: There are a total of 633 observations with 33 variables. Each row is one student with each column containing a difference characteristic. The Grade column is our target variable (also known as the response), which makes this a supervised, regression machine learning task. It’s supervised because we have a set of training data with known targets and, during training, we want our model to learn to predict the grade from the other variables. We will treat the grade as continuous which makes this a regression problem (technically the grade only takes on integer values so it is a nominal variable). The primary variable of interest is the grade, so let’s take a look at the distribution to check for skew: The grades are close to normally distributed with a mode at 11 (the grading scale in this school goes from 0–20). While the overall grades do not have a noticeable skew, it’s possible that students from certain categories will have skewed grades. To look at the effect of categorical variables on the grade we can make density plots of the grade distribution colored by the value of the categorical variable. For this we use the seaborn library and the kdeplot function. Here is the code to plot the distribution by location (urban or rural): We can use similar code to plot the distribution of grades by guardian with the resulting plots shown below: The actual values on a density plot are difficult to interpret, but we can use the shape of the plots for comparisons. The location does not seem to have a substantial impact on the student grades and neither does the guardian. Plots such as these can inform our modeling because they tell us if knowing the location or the guardian might be helpful for predicting final grades. Of course, we want to use a measure more rigorous than a single plot to make these conclusions, and later we will use statistics to back up our intuition! Feature Selection As we saw from the plots, we don’t expect every variable to be related to the final grade, so we need to perform feature selection (also called dimensionality reduction) to choose only the “relevant” variables. This depends on the problem, but because we will be doing linear modeling in this project, we can use a simple measure called the Correlation Coefficient to determine the most useful variables for predicting a grade. This is a value between -1 and +1 that measures the direction and strength of a linear relationship between two variables. To select a limited number of variables, we can find those that have the greatest correlation (either negative or positive) with the final grade. Finding correlations in pandas is extremely simple: These correlations seem to make sense at least by my rudimentary social science knowledge! failures is the number of previous class failures and is negatively correlated with the grade, as is absences, the number of absences from school. This negative correlation indicates that as these variables increase, the final grade tends to decrease (although we can only say this is a correlation and not that one variable causes another to decrease). On the other hand, both studytime, the amount of studying per week, and Medu the mother’s level of education, are positively correlated with the grade. Correlations can only be calculated between numerical variables, so to find the relationship between categorical variables and grade, we have to one-hot encode the categorical variable and then calculate the correlation coefficient. One-hot encoding is a process that creates one column for every category within a categorical variable. Here is an example categorical column before and after one-hot encoding: One-hot encoding is a standard step in machine learning pipelines and is very easy to do with the pandas library: We again see relationships that intuitively make sense: higher_no represents the student does not want to go on to higher education and is negatively correlated with the grade with higher_yes indicating the student does want higher education and showing a positive correlation.Mjob_at_home means the mother stays at home, and is negatively correlated with the grade while Mjob_teacher indicates the mother teaches and has a positive correlation. In this problem we will use these results to perform feature selection by retaining only the 6 variables that are most highly correlated with the final grade. 6 is sort of an arbitrary number that I found works well in the model, which shows that a lot of machine learning is just experimentation! The final six variables we end up with after feature selection (see the Notebook for details) are shown in the snapshot of the new dataframe. (I renamed the columns so they are more intuitive): The complete descriptions of the variables are on the UCI machine learning repository, but here is a brief overview: While we are performing feature selection, we also split the data into a training and testing set using a Scikit-learn function. This is necessary because we need to have a hold-out test set to evaluate our model and make sure it is not overfitting to the testing data: This leaves us with 474 training observations and 159 testing data points. One of my favorite figures is the Pairs Plot, which is great for showing both distribution of variables and relations between pairs of variables. Here I use the seaborn PairGrid function to show a Pairs Plot for the selected features: There is a lot of information encoded in this plot! On the upper triangle, we have scatterplots of every variable plotted against one another. Notice that most variables are discrete integers, meaning they only take on certain values. On the diagonal, we have histograms showing the distribution of a single variable. The lower right has both 2-D density plots and the correlation coefficient between variables. To interpret the plot, we can select a variable and look at the row and column to find the relationships with all the other variables. For example, the first row shows the scatterplots of Grade , our target, with the other variables. The first column shows the correlation coefficient between the Grade and the other variables. We see that failures has the greatest correlation with the final grade in terms of absolute magnitude. As another exploration of the selected data, we can make distribution plots of each variable, coloring the plot by if the grade is above the median score of 12. To make these plot, we create a column in our dataframe comparing the grade to to 12 and then plot all the values in density plots. This yields the following plots: The green distributions represent students with grades at or above the median, and the red is students below. We can see that some variables are more positively correlated with grades (such as studytime), while others are indicators of low grades, such as low father_edu. The EDA has given us a good sense of our dataset. We made figures, found relationships between variables, and used these to perform feature selection to retain only the variables most relevant for our task. While EDA is a precursor to modeling, it’s also useful on its own, and many data science problems can be solved solely through the plots and statistics we made here. One of the most overlooked aspects of the machine learning pipeline is establishing a baseline. Yes, it might look impressive if your classification model achieves 99% accuracy, but what if we could get 98% accuracy just by guessing the same class every time? Would we really want to spend our time building a model for that problem? A good baseline allows us to assess whether or not our model (or any model) is applicable to the task. For regression, a good naive baseline is simply to guess the median value of the target for every observation in the test data. In our problem, the median is 12, so let’s assess the accuracy of a model that naively predicts 12 for every student on the test set. We will use 2 metrics to evaluate predictions: The mean absolute error is easily interpretable, as it represents how far off we are on average from the correct value. The root mean squared error penalizes larger errors more heavily and is commonly used in regression tasks. Either metric may be appropriate depending on the situation and we will use both for comparison. (Here is a discussion of the merits of these metrics.) When we predict 12 for every example on the test set we get these results: If our machine learning model cannot beat these metrics, then we either need to get more data, try another approach, or conclude that machine learning is not applicable to our problem! Our modeling focus is on Bayesian Linear Regression, but it will be helpful to compare our results to those from standard techniques such as Linear Regression, Support Vector Machines, or tree-based methods. We will evaluate several of these methods on our dataset. Luckily, these are all very easy to implement with Python libraries such as Scikit-Learn. Check the Notebook for the code, but here are the results for 6 different models along with the naive baseline: Fortunately, we see that all models best the baseline indicating that machine learning will work for this problem. Overall, the gradient boosted regression method performs the best although Ordinary Least Squares (OLS) Linear Regression (the frequentist approach to linear modeling) also does well. As a final exercise, we can interpret the OLS Linear Regression model. Linear Regression is the simplest machine learning technique, and does not perform well on complex, non-linear problems with lots of features, but it has the benefit of being easily explained. We can extract the prediction formula from the linear regression using the trained model. Following is the formula: The intercept, 9.19, represents our guess if every variable of a student is 0. The coefficients (also known as the weights or model parameters) indicate the effect of a unit increase in the respective variable. For example, with every additional previous failure, the student’s score is expected to decrease by 1.32 points, and with every additional point increase in the mother’s education, the student’s grade increases by 0.26 points. I often like to start off with a linear regression when solving a problem, because if it works well enough, we have a completely explainable model that we can use to make predictions. Conclusions While machine learning gets all the attention, it often comprises a small part of a data science project. Most of the work — and most of the value — comes in obtaining, cleaning, and exploring the data. Only once we have a firm grasp on the structure of our data and the relationships within it should we proceed to building machine learning models. I wanted to show the entire process in for this project to demonstrate a typical data science workflow. In the first half of this project we: These techniques apply equally well to any machine learning problem, so feel free to use these as a starting point for your next project. While the exact implementation details of a particular machine learning model may differ, the general structure of a data science problem is fairly consistent. In the next half of this series, we will implement a Bayesian Linear Regression model using PyMC3 in Python. We’ll build the model, train the model (which in this case means sampling from the posterior), inspect the model for inferences, and make predictions using the results. I’ll see you there! As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. higher_edu: binary for whether the student will pursue higher education mother_edu: Mother’s level of education studytime: Amount of studying per week father_edu: Father’s level of education absences: Absences from school in the semester Root Mean Squared Error (RMSE): The square root of the average of the squared differences between the predictions and true values. Examined correlations between the features and the target Performed feature selection using correlation values Established a baseline and benchmarked machine learning models failures: previous class failures Mean Absolute Error (MAE): The average of the absolute value of the differences between the predictions and true values. Explored the data to find interesting patterns, trends, or anomalies",Bayesian Linear Regression in Python: Using Machine Learning to Predict Student Grades Part 1,14,published,22066,2690,0.5576208178438662,17,1,0,1,1,1
72,1100,402.0886080019445,227,https://towardsdatascience.com/bayesian-linear-regression-in-python-using-machine-learning-to-predict-student-grades-part-2-b72059a8ac7e,7,Towards Data Science,2018-04-20 13:34:00,26.69,12,6783,2018-04-11 11:02:00,"['Machine Learning', 'Education', 'Data Analysis', 'Python', 'Towards Data Science']","Bayesian Linear Regression in Python: Using Machine Learning to Predict Student Grades Part 2 Implementing a Model, Interpreting Results, and Making Predictions In Part One of this Bayesian Machine Learning project, we outlined our problem, performed a full exploratory data analysis, selected our features, and established benchmarks. Here we will implement Bayesian Linear Regression in Python to build a model. After we have trained our model, we will interpret the model parameters and use the model to make predictions. The entire code for this project is available as a Jupyter Notebook on GitHub and I encourage anyone to check it out! As a reminder, we are working on a supervised, regression machine learning problem. Using a dataset of student grades, we want to build a model that can predict a final student’s score from personal and academic characteristics of the student. The final dataset after feature selection is: We have 6 features (explanatory variables) that we use to predict the target (response variable), in this case the grade. There are 474 students in the training set and 159 in the test set. To get a sense of the variable distributions (and because I really enjoy this plot) here is a Pairs plot of the variables showing scatter plots, histograms, density plots, and correlation coefficients. For details about this plot and the meaning of all the variables check out part one and the notebook. Now, let’s move on to implementing Bayesian Linear Regression in Python. Bayesian Linear Regression Let’s briefly recap Frequentist and Bayesian linear regression. The Frequentist view of linear regression assumes data is generated from the following model: Where the response, y, is generated from the model parameters, β, times the input matrix, X, plus error due to random sampling noise or latent variables. In the ordinary least squares (OLS) method, the model parameters, β, are calculated by finding the parameters which minimize the sum of squared errors on the training data. The output from OLS is single point estimates for the “best” model parameters given the training data. These parameters can then be used to make predictions for new data points. In contrast, Bayesian Linear Regression assumes the responses are sampled from a probability distribution such as the normal (Gaussian) distribution: The mean of the Gaussian is the product of the parameters, β and the inputs, X, and the standard deviation is σ. In Bayesian Models, not only is the response assumed to be sampled from a distribution, but so are the parameters. The objective is to determine the posterior probability distribution for the model parameters given the inputs, X, and outputs, y: The posterior is equal to the likelihood of the data times the prior for the model parameters divided by a normalization constant. If we have some domain knowledge, we can use it to assign priors for the model parameters, or we can use non-informative priors: distributions with large standard deviations that do not assume anything about the variable. Using a non-informative prior means we “let the data speak.” A common prior choice is to use a normal distribution for β and a half-cauchy distribution for σ. In practice, calculating the exact posterior distribution is computationally intractable for continuous values and so we turn to sampling methods such as Markov Chain Monte Carlo (MCMC) to draw samples from the posterior in order to approximate the posterior. Monte Carlo refers to the general technique of drawing random samples, and Markov Chain means the next sample drawn is based only on the previous sample value. The concept is that as we draw more samples, the approximation of the posterior will eventually converge on the true posterior distribution for the model parameters. The end result of Bayesian Linear Modeling is not a single estimate for the model parameters, but a distribution that we can use to make inferences about new observations. This distribution allows us to demonstrate our uncertainty in the model and is one of the benefits of Bayesian Modeling methods. As the number of data points increases, the uncertainty should decrease, showing a higher level of certainty in our estimates. The best library for probabilistic programming and Bayesian Inference in Python is currently PyMC3. It includes numerous utilities for constructing Bayesian Models and using MCMC methods to infer the model parameters. We will be using the Generalized Linear Models (GLM) module of PyMC3, in particular, the GLM.from_formula function which makes constructing Bayesian Linear Models extremely simple. There are only two steps we need to do to perform Bayesian Linear Regression with this module: Instead of having to define probability distributions for each of the model parameters separately, we pass in an R-style formula relating the features (input) to the target (output). Here is the formula relating the grade to the student characteristics: In this syntax, ~, is read as “is a function of”. We are telling the model that Grade is a linear combination of the six features on the right side of the tilde. The model is built in a context using the with statement. In the call to GLM.from_formula we pass the formula, the data, and the data likelihood family (this actually is optional and defaults to a normal distribution). The function parses the formula, adds random variables for each feature (along with the standard deviation), adds the likelihood for the data, and initializes the parameters to a reasonable starting estimate. By default, the model parameters priors are modeled as a normal distribution. Once the GLM model is built, we sample from the posterior using a MCMC algorithm. If we do not specify which method, PyMC3 will automatically choose the best for us. In the code below, I let PyMC3 choose the sampler and specify the number of samples, 2000, the number of chains, 2, and the number of tuning steps, 500. In this case, PyMC3 chose the No-U-Turn Sampler and intialized the sampler with jitter+adapt_diag. To be honest, I don’t really know the full details of what these mean, but I assume someone much smarter than myself implemented them correctly. Sometimes just knowing how to use the tool is more important than understanding every detail of the implementation! The sampler runs for a few minutes and our results are stored in normal_trace. This contains all the samples for every one of the model parameters (except the tuning samples which are discarded). The trace is essentially our model because it contains all the information we need to perform inference. To get an idea of what Bayesian Linear Regression does, we can examine the trace using built-in functions in PyMC3. A traceplot shows the posterior distribution for the model parameters on the left and the progression of the samples drawn in the trace for the variable on the right. The two colors represent the two difference chains sampled. Here we can see that our model parameters are not point estimates but distributions. The mean of each distribution can be taken as the most likely estimate, but we also use the entire range of values to show we are uncertain about the true values. Another way to look at the posterior distributions is as histograms: Here we can see the mean, which we can use as most likely estimate, and also the entire distribution. 95% HPD stands for the 95% Highest Posterior Density and is a credible interval for our parameters. A credible interval is the Bayesian equivalent of a confidence interval in Frequentist statistics (although with different interpretations). We can also see a summary of all the model parameters: We can interpret these weights in much the same way as those of OLS linear regression. For example in the model: The standard deviation column and hpd limits give us a sense of how confident we are in the model parameters. For example, the father_edu feature has a 95% hpd that goes from -0.22 to 0.27 meaning that we are not entirely sure if the effect in the model is either negative or positive! There is also a large standard deviation (the sd row) for the data likelihood, indicating large uncertainty in the targets. Overall, we see considerable uncertainty in the model because we are dealing with a small number of samples. With only several hundred students, we do not have enough data to pin down the model parameters precisely. Interpret Variable Effects In order to see the effect of a single variable on the grade, we can change the value of this variable while holding the others constant and look at how the estimated grades change. To do this, we use the plot_posterior_predictive function and assume that all variables except for the one of interest (the query variable) are at the median value. We generate a range of values for the query variable and the function estimates the grade across this range by drawing model parameters from the posterior distribution. Here’s the code: The results show the estimated grade versus the range of the query variable for 100 samples from the posterior: Each line (there are 100 in each plot) is drawn by picking one set of model parameters from the posterior trace and evaluating the predicted grade across a range of the query variable. The distribution of the lines shows uncertainty in the model parameters: the more spread out the lines, the less sure the model is about the effect of that variable. For one variable, the father’s education, our model is not even sure if the effect of increasing the variable is positive or negative! If we were using this model to make decisions, we might want to think twice about deploying it without first gathering more data to form more certain estimates. With only several hundred students, there is considerable uncertainty in the model parameters. For example, we should not make claims such as “the father’s level of education positively impacts the grade” because the results show there is little certainly about this conclusion. If we were using Frequentist methods and saw only a point estimate, we might make faulty decisions because of the limited amount of data. In cases where we have a limited dataset, Bayesian models are a great choice for showing our uncertainty in the model. Making Predictions When it comes to predicting, the Bayesian model can be used to estimate distributions. We remember that the model for Bayesian Linear Regression is: Where β is the coefficient matrix (model parameters), X is the data matrix, and σ is the standard deviation. If we want to make a prediction for a new data point, we can find a normal distribution of estimated outputs by multiplying the model parameters by our data point to find the mean and using the standard deviation from the model parameters. In this case, we will take the mean of each model parameter from the trace to serve as the best estimate of the parameter. If we take the mean of the parameters in the trace, then the distribution for a prediction becomes: For a new data point, we substitute in the value of the variables and construct the probability density function for the grade. As an example, here is an observation from the test set along with the probability density function (see the Notebook for the code to build this distribution): For this data point, the mean estimate lines up well with the actual grade, but there is also a wide estimated interval. If we had more students, the uncertainty in the estimates should be lower. We can also make predictions for any new point that is not in the test set: In the first part of this series, we calculated benchmarks for a number of standard machine learning models as well as a naive baseline. To calculate the MAE and RMSE metrics, we need to make a single point estimate for all the data points in the test set. We can make a “most likely” prediction using the means value from the estimated distributed. The resulting metrics, along with those of the benchmarks, are shown below: Bayesian Linear Regression achieves nearly the same performance as the best standard models! However, the main benefits of Bayesian Linear Modeling are not in the accuracy, but in the interpretability and the quantification of our uncertainty. Any model is only an estimate of the real world, and here we have seen how little confidence we should have in models trained on limited data. For anyone looking to get started with Bayesian Modeling, I recommend checking out the notebook. In this project, I only explored half of the student data (I used math scores and the other half contains Portuguese class scores) so feel free to carry out the same analysis on the other half. In addition, we can change the distribution for the data likelihood—for example to a Student’s T distribution — and see how that changes the model. As with most machine learning, there is a considerable amount that can be learned just by experimenting with different settings and often no single right answer! Conclusions In this series of articles, we walked through the complete machine learning process used to solve a data science problem. We started with exploratory data analysis, moved to establishing a baseline, tried out several different models, implemented our model of choice, interpreted the results, and used the model to make new predictions. While the model implementation details may change, this general structure will serve you well for most data science projects. Moreover, hopefully this project has given you an idea of the unique capabilities of Bayesian Machine Learning and has added another tool to your skillset. Learning new skills is the most exciting aspect of data science and now you have one more to deploy to solve your data problems. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Sample from the parameter posterior distribution using MCMC Higher Education plans and studying time have a positive weight The mother’s and father’s education have a positive weight (although the mother’s is much more positive) Build a formula relating the features to the target and decide on a prior distribution for the data likelihood Previous class failures and absences have a negative weight",Bayesian Linear Regression in Python: Using Machine Learning to Predict Student Grades Part 2,14,published,25417,2620,0.4198473282442748,9,1,0,1,1,1
78,8500,394.01016209946766,1655,https://towardsdatascience.com/web-scraping-regular-expressions-and-data-visualization-doing-it-all-in-python-37a1aade7924,28,Towards Data Science,2018-04-28 15:27:00,31.63,7,21705,2018-04-28 13:02:00,"['Data Science', 'Programming', 'Education', 'Computer Science', 'Towards Data Science']","Web Scraping, Regular Expressions, and Data Visualization: Doing it all in Python A Small Real-World Project for Learning Three Invaluable Data Science Skills As with most interesting projects, this one started with a simple question asked half-seriously: how much tuition do I pay for five minutes of my college president’s time? After a chance pleasant discussion with the president of my school (CWRU), I wondered just how much my conversation had cost me. My search led to this article, which along with my president’s salary, had this table showing the salaries of private college presidents in Ohio: While I could have found the answer for my president, (SPOILER ALERT, it’s $48 / five minutes), and been satisfied, I wanted to take the idea further using this table. I had been looking for a chance to practice web scraping and regular expressions in Python and decided this was a great short project. Although it almost certainly would have been faster to manually enter the data in Excel, then I would not have had the invaluable opportunity to practice a few skills! Data science is about solving problems using a diverse set of tools, and web scraping and regular expressions are two areas I need some work on (not to mention that making plots is always fun). The result was a very short — but complete — project showing how we can bring together these three techniques to solve a data science problem. The complete code for this project is available as a Jupyter Notebook on Google Colaboratory (this is a new service I’m trying out where you can share and collaborate on Jupyter Notebooks in the cloud. It feels like the future!) To edit the notebook, open it up in Colaboratory, select file > save a copy in drive and then you can make any changes and run the Notebook. Web Scraping While most data used in classes and textbooks just appears ready-to-use in a clean format, in reality, the world does not play so nice. Getting data usually means getting our hands dirty, in this case pulling (also known as scraping) data from the web. Python has great tools for doing this, namely the requests library for retrieving content from a webpage, and bs4 (BeautifulSoup) for extracting the relevant information. These two libraries are often used together in the following manner: first, we make a GET request to a website. Then, we create a Beautiful Soup object from the content that is returned and parse it using several methods. The resulting soup object is quite intimidating: Our data is in there somewhere, but we need to extract it. To select our table from the soup, we need to find the right CSS selectors. One way to do this is by going to the webpage and inspecting the element. In this case, we can also just look at the soup and see that our table resides under a <div> HTML tag with the attribute class = ""entry-content"" . Using this info and the .find method of our soup object, we can pull out the main article content. This returns another soup object which is not quite specific enough. To select the table, we need to find the <ul> tag (see above image). We also want to deal with only the text in the table, so we use the .text attribute of the soup. We now have the exact text of the table as a string, but clearly is it not of much use to us yet! To extract specific parts of a text string, we need to move on to regular expressions. I don’t have space in this article (nor do I have the experience!) to completely explain regular expressions, so here I only give a brief overview and show the results. I’m still learning myself, and I have found the only way to get better is practice. Feel free to go over this notebook for some practice, and check out the Python re documentation to get started (documentation is usually dry but extremely helpful). Regular Expressions The basic idea of regular expressions is we define a pattern (the “regular expression” or “regex”) that we want to match in a text string and then search in the string to return matches. Some of these patterns look pretty strange because they contain both the content we want to match and special characters that change how the pattern is interpreted. Regular expressions come up all the time when parsing string information and are a vital tool to learn at least at a basic level! There are 3 pieces of info we need to extract from the text table: First up is the name. In this regular expression, I make use of the fact that each name is at the start of a line and ends with a comma. The code below creates a regular expression pattern, and then searches through the string to find all occurrences of the pattern: Like I said, the pattern is pretty complex, but it does exactly what we want! Don’t worry about the details of the pattern, but just think about the general process: first define a pattern, and then search a string to find the pattern. We repeat the procedure with the colleges and the salary: Unfortunately the salary is in a format that no computer would understand as numbers. Fortunately, this gives us a chance to practice using a Python list comprehension to convert the string salaries into numbers. The following code illustrates how to use string slicing, split , and join, all within a list comprehension to achieve the results we want: We apply this transformation to our salaries and finally have the all info we want. Let’s put everything into a pandas dataframe. At this point, I manually insert the information for my college (CWRU) because it was not in the main table. It’s important to know when it’s more efficient to do things by hand rather than writing a complicated program (although this whole article kind of goes against this point!). Visualization This project is indicative of data science because the majority of time was spent collecting and formatting the data. However, now that we have a clean dataset, we get to make some plots! We can use both matplotlib and seaborn to visualize the data. If we aren’t too concerned about aesthetics, we can use the built in dataframe plot method to quickly show results: To get a better plot we have to do some work. Plotting code in Python, like regular expressions, can be a little complex, and it takes some practice to get used to. Mostly, I learn by building on answers on sites like Stack Overflow or by reading official documentation. After a bit of work, we get the following plot (see notebook for the details): Much better, but this still doesn’t answer my original question! To show how much students are paying for 5 minutes of their president’s time we can convert salaries into $ / five minutes assuming 2000 work hours per year. This is not necessarily a publication-worthy plot, but it’s a nice way to wrap up a small project. Conclusions The most effective way to learn technical skills is by doing. While this whole project could have been done manually inserting values into Excel, I like to take the long view and think about how the skills learned here will help in the future. The process of learning is more important than the final result, and in this project we were able to see how to use 3 critical skills for data science: Now, get out there and start your own project and remember: it doesn’t have to be world-changing to be worthwhile. I welcome feedback and discussion and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. The names of the colleges The salaries Regular Expressions: Parsing our data to extract information Visualization: Showcasing all our hard work The names of the presidents Web Scraping: Retrieving online data","Web Scraping, Regular Expressions, and Data Visualization: Doing it all in Python",14,published,68615,1480,5.743243243243243,0,1,1,1,0,0
71,362,392.19284667715283,64,https://towardsdatascience.com/if-your-files-are-saved-only-on-your-laptop-they-might-as-well-not-exist-29f3503750d5,4,Towards Data Science,2018-04-30 11:04:00,63.28,3,1508,2018-04-29 19:27:00,"['Programming', 'Education', 'Computers', 'Storage']","If your files are saved only on your laptop they might as well not exist! How to avert computer catastrophes Last week, as I was working on one of my three final graduate course projects, my laptop decided it was a good time to give out. I spent a futile 15 minutes resetting the battery and holding down the power button trying to get a response, but to no avail: my laptop was done for good. At this point a year ago, I would have been sobbing uncontrollably, my semester wreaked in the final week. However, this time, I set down my laptop, walked to the school library, logged onto a computer, downloaded my files from Google Drive where they had been synced up until the minute my laptop went dark, and was working on my final projects within 30 minutes. All in all, thanks to automatic back-ups, instead of losing an entire semester, I lost two lines of one report. This near-tragedy illustrates two points that anyone who does any work on a computer must keep in mind: Having witnessed firsthand the destruction wreaked on fellow students by computer failures, I finally installed Google Drive backup and sync a few months ago. This is one of a number of services that will run in the background on your computer, saving all files (or select ones you choose) to the cloud where you can access them from any computer in the world. There’s a saying I’m quite fond of known as the Rule of Two: “Two is one and one is none.” (I first heard this from CGPGrey on the Cortex Podcast). What these means is that if you only have one of some necessity, it might as well not exist because you will probably inevitably lose it. When it comes to file storage, if your files are located only on your laptop, they might as well not be saved at all for how vulnerable you are. Signing up for a back-up service tends to be one of those things that everyone says they will get around to but never actually implements (I was in this group for a long time). However, having proper back-ups should be a necessity before you start doing any work you don’t want to lose! In today’s world of ridiculously cheap storage, there is no excuse for not having multiple copies of your files available in the cloud. For students, you might have free unlimited storage through Google Drive which means you can store anything you want (I haven’t tested the limits of unlimited, but have friends with multiple terabytes saved). For everyone else, 100 GB of storage is only $2/month on Google Drive and other options are equally reasonable. I know the moment my screen went blank I would gladly have paid $100 / year to know that my files were safe. The exact backup route you choose does not matter, but what is important is ensuring that your files live in multiple places (usb sticks are better than nothing, but cloud backup is the best option). I prefer automatic syncing because humans are fallible. With a service that syncs your files by itself, there is little risk you will get in the way of technology helping you out! This is one area where you should be content to let acomputer think for you. (Along the same lines, if a program offers the option to auto-save files, make sure to enable it to save as often as possible!) Whenever I see the little Google Drive sync icon whirring away on my laptop, I take a moment to appreciate the wonders of automatic file backups. The next time my laptop inevitably fails, I know that it will be only a minor, recoverable inconvenience. Can you say the same? I welcome feedback and discussion and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. This can be either a soul-crushing loss or no big deal depending on the safeguards you have in place You will have a complete computer failure sometime soon",If your files are saved only on your laptop they might as well not exist!,16,published,2383,765,0.47320261437908495,0,1,0,0,0,0
69,14300,376.13140005438663,2600,https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420,32,Towards Data Science,2018-05-16 12:33:00,18.9,15,23740,2018-05-07 13:38:00,"['Machine Learning', 'Education', 'Data Science', 'Python', 'Towards Data Science']","A Complete Machine Learning Project Walk-Through in Python: Part One Putting the machine learning pieces together Reading through a data science book or taking a course, it can feel like you have the individual pieces, but don’t quite know how to put them together. Taking the next step and solving a complete machine learning problem can be daunting, but preserving and completing a first project will give you the confidence to tackle any data science problem. This series of articles will walk through a complete machine learning solution with a real-world dataset to let you see how all the pieces come together. We’ll follow the general machine learning workflow step-by-step: Along the way, we’ll see how each step flows into the next and how to specifically implement each part in Python. The complete project is available on GitHub, with the first notebook here. This first article will cover steps 1–3 with the rest addressed in subsequent posts. (As a note, this problem was originally given to me as an “assignment” for a job screen at a start-up. After completing the work, I was offered the job, but then the CTO of the company quit and they weren’t able to bring on any new employees. I guess that’s how things go on the start-up scene!) Problem Definition The first step before we get coding is to understand the problem we are trying to solve and the available data. In this project, we will work with publicly available building energy data from New York City. The objective is to use the energy data to build a model that can predict the Energy Star Score of a building and interpret the results to find the factors which influence the score. The data includes the Energy Star Score, which makes this a supervised regression machine learning task: We want to develop a model that is both accurate — it can predict the Energy Star Score close to the true value — and interpretable — we can understand the model predictions. Once we know the goal, we can use it to guide our decisions as we dig into the data and build models. Data Cleaning Contrary to what most data science courses would have you believe, not every dataset is a perfectly curated group of observations with no missing values or anomalies (looking at you mtcars and iris datasets). Real-world data is messy which means we need to clean and wrangle it into an acceptable format before we can even start the analysis. Data cleaning is an un-glamorous, but necessary part of most actual data science problems. First, we can load in the data as a Pandas DataFrame and take a look: This is a subset of the full data which contains 60 columns. Already, we can see a couple issues: first, we know that we want to predict the ENERGY STAR Score but we don’t know what any of the columns mean. While this isn’t necessarily an issue — we can often make an accurate model without any knowledge of the variables — we want to focus on interpretability, and it might be important to understand at least some of the columns. When I originally got the assignment from the start-up, I didn’t want to ask what all the column names meant, so I looked at the name of the file, and decided to search for “Local Law 84”. That led me to this page which explains this is an NYC law requiring all buildings of a certain size to report their energy use. More searching brought me to all the definitions of the columns. Maybe looking at a file name is an obvious place to start, but for me this was a reminder to go slow so you don’t miss anything important! We don’t need to study all of the columns, but we should at least understand the Energy Star Score, which is described as: A 1-to-100 percentile ranking based on self-reported energy usage for the reporting year. The Energy Star score is a relative measure used for comparing the energy efficiency of buildings. That clears up the first problem, but the second issue is that missing values are encoded as “Not Available”. This is a string in Python which means that even the columns with numbers will be stored as object datatypes because Pandas converts a column with any strings into a column of all strings. We can see the datatypes of the columns using the dataframe.info()method: Sure enough, some of the columns that clearly contain numbers (such as ft²), are stored as objects. We can’t do numerical analysis on strings, so these will have to be converted to number (specifically float) data types! Here’s a little Python code that replaces all the “Not Available” entries with not a number ( np.nan), which can be interpreted as numbers, and then converts the relevant columns to the float datatype: Once the correct columns are numbers, we can start to investigate the data. In addition to incorrect datatypes, another common problem when dealing with real-world data is missing values. These can arise for many reasons and have to be either filled in or removed before we train a machine learning model. First, let’s get a sense of how many missing values are in each column (see the notebook for code). (To create this table, I used a function from this Stack Overflow Forum). While we always want to be careful about removing information, if a column has a high percentage of missing values, then it probably will not be useful to our model. The threshold for removing columns should depend on the problem (here is a discussion), and for this project, we will remove any columns with more than 50% missing values. At this point, we may also want to remove outliers. These can be due to typos in data entry, mistakes in units, or they could be legitimate but extreme values. For this project, we will remove anomalies based on the definition of extreme outliers: (For the code to remove the columns and the anomalies, see the notebook). At the end of the data cleaning and anomaly removal process, we are left with over 11,000 buildings and 49 features. Exploratory Data Analysis Now that the tedious — but necessary — step of data cleaning is complete, we can move on to exploring our data! Exploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. In short, the goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find interesting parts of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use. The goal is to predict the Energy Star Score (renamed to score in our data) so a reasonable place to start is examining the distribution of this variable. A histogram is a simple yet effective way to visualize the distribution of a single variable and is easy to make using matplotlib. This looks quite suspicious! The Energy Star score is a percentile rank, which means we would expect to see a uniform distribution, with each score assigned to the same number of buildings. However, a disproportionate number of buildings have either the highest, 100, or the lowest, 1, score (higher is better for the Energy Star score). If we go back to the definition of the score, we see that it is based on “self-reported energy usage” which might explain the very high scores. Asking building owners to report their own energy usage is like asking students to report their own scores on a test! As a result, this probably is not the most objective measure of a building’s energy efficiency. If we had an unlimited amount of time, we might want to investigate why so many buildings have very high and very low scores which we could by selecting these buildings and seeing what they have in common. However, our objective is only to predict the score and not to devise a better method of scoring buildings! We can make a note in our report that the scores have a suspect distribution, but our main focus in on predicting the score. A major part of EDA is searching for relationships between the features and the target. Variables that are correlated with the target are useful to a model because they can be used to predict the target. One way to examine the effect of a categorical variable (which takes on only a limited set of values) on the target is through a density plot using the seaborn library. A density plot can be thought of as a smoothed histogram because it shows the distribution of a single variable. We can color a density plot by class to see how a categorical variable changes the distribution. The following code makes a density plot of the Energy Star Score colored by the the type of building (limited to building types with more than 100 data points): We can see that the building type has a significant impact on the Energy Star Score. Office buildings tend to have a higher score while Hotels have a lower score. This tells us that we should include the building type in our modeling because it does have an impact on the target. As a categorical variable, we will have to one-hot encode the building type. A similar plot can be used to show the Energy Star Score by borough: The borough does not seem to have as large of an impact on the score as the building type. Nonetheless, we might want to include it in our model because there are slight differences between the boroughs. To quantify relationships between variables, we can use the Pearson Correlation Coefficient. This is a measure of the strength and direction of a linear relationship between two variables. A score of +1 is a perfectly linear positive relationship and a score of -1 is a perfectly negative linear relationship. Several values of the correlation coefficient are shown below: While the correlation coefficient cannot capture non-linear relationships, it is a good way to start figuring out how variables are related. In Pandas, we can easily calculate the correlations between any columns in a dataframe: The most negative (left) and positive (right) correlations with the target: There are several strong negative correlations between the features and the target with the most negative the different categories of EUI (these measures vary slightly in how they are calculated). The EUI — Energy Use Intensity — is the amount of energy used by a building divided by the square footage of the buildings. It is meant to be a measure of the efficiency of a building with a lower score being better. Intuitively, these correlations make sense: as the EUI increases, the Energy Star Score tends to decrease. To visualize relationships between two continuous variables, we use scatterplots. We can include additional information, such as a categorical variable, in the color of the points. For example, the following plot shows the Energy Star Score vs. Site EUI colored by the building type: This plot lets us visualize what a correlation coefficient of -0.7 looks like. As the Site EUI decreases, the Energy Star Score increases, a relationship that holds steady across the building types. The final exploratory plot we will make is known as the Pairs Plot. This is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle. To see interactions between variables, we look for where a row intersects with a column. For example, to see the correlation of Weather Norm EUI with score, we look in the Weather Norm EUI row and the score column and see a correlation coefficient of -0.67. In addition to looking cool, plots such as these can help us decide which variables to include in modeling. Feature Engineering and Selection Feature engineering and selection often provide the greatest return on time invested in a machine learning problem. First of all, let’s define what these two tasks are: A machine learning model can only learn from the data we provide it, so ensuring that data includes all the relevant information for our task is crucial. If we don’t feed a model the correct data, then we are setting it up to fail and we should not expect it to learn! For this project, we will take the following feature engineering steps: One-hot encoding is necessary to include categorical variables in a model. A machine learning algorithm cannot understand a building type of “office”, so we have to record it as a 1 if the building is an office and a 0 otherwise. Adding transformed features can help our model learn non-linear relationships within the data. Taking the square root, natural log, or various powers of features is common practice in data science and can be based on domain knowledge or what works best in practice. Here we will include the natural log of all numerical features. The following code selects the numeric features, takes log transformations of these features, selects the two categorical features, one-hot encodes these features, and joins the two sets together. This seems like a lot of work, but it is relatively straightforward in Pandas! After this process we have over 11,000 observations (buildings) with 110 columns (features). Not all of these features are likely to be useful for predicting the Energy Star Score, so now we will turn to feature selection to remove some of the variables. Many of the 110 features we have in our data are redundant because they are highly correlated with one another. For example, here is a plot of Site EUI vs Weather Normalized Site EUI which have a correlation coefficient of 0.997. Features that are strongly correlated with each other are known as collinear and removing one of the variables in these pairs of features can often help a machine learning model generalize and be more interpretable. (I should point out we are talking about correlations of features with other features, not correlations with the target, which help our model!) There are a number of methods to calculate collinearity between features, with one of the most common the variance inflation factor. In this project, we will use thebcorrelation coefficient to identify and remove collinear features. We will drop one of a pair of features if the correlation coefficient between them is greater than 0.6. For the implementation, take a look at the notebook (and this Stack Overflow answer) While this value may seem arbitrary, I tried several different thresholds, and this choice yielded the best model. Machine learning is an empirical field and is often about experimenting and finding what performs best! After feature selection, we are left with 64 total features and 1 target. Establishing a Baseline We have now completed data cleaning, exploratory data analysis, and feature engineering. The final step to take before getting started with modeling is establishing a naive baseline. This is essentially a guess against which we can compare our results. If the machine learning models do not beat this guess, then we might have to conclude that machine learning is not acceptable for the task or we might need to try a different approach. For regression problems, a reasonable naive baseline is to guess the median value of the target on the training set for all the examples in the test set. This sets a relatively low bar for any model to surpass. The metric we will use is mean absolute error (mae) which measures the average absolute error on the predictions. There are many metrics for regression, but I like Andrew Ng’s advice to pick a single metric and then stick to it when evaluating models. The mean absolute error is easy to calculate and is interpretable. Before calculating the baseline, we need to split our data into a training and a testing set: We will use 70% of the data for training and 30% for testing: Now we can calculate the naive baseline performance: The naive estimate is off by about 25 points on the test set. The score ranges from 1–100, so this represents an error of 25%, quite a low bar to surpass! Conclusions In this article we walked through the first three steps of a machine learning problem. After defining the question, we: Finally, we also completed the crucial step of establishing a baseline against which we can judge our machine learning algorithms. The second post (available here) will show how to evaluate machine learning models using Scikit-Learn, select the best model, and perform hyperparameter tuning to optimize the model. The third post, dealing with model interpretation and reporting results, is here. As always, I welcome feedback and constructive criticism and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Exploratory data analysis Feature engineering and selection Compare several machine learning models on a performance metric Perform hyperparameter tuning on the best model Evaluate the best model on the testing set Interpret the model results Draw conclusions and document work Regression: The Energy Star score is a continuous variable Above the third quartile + 3 ∗ interquartile range Feature selection: The process of choosing the most relevant features in the data. In feature selection, we remove features to help the model generalize better to new data and create a more interpretable model. Generally, I think of feature selection as subtracting features so we are left with only those that are most important. Add in the natural log transformation of the numerical variables The testing set of features is used to evaluate the trained model. The model is not allowed to see the answers for the testing set and must make predictions using only the features. We know the answers for the test set so we can compare the test predictions to the answers. Performed an exploratory data analysis to learn about the dataset Developed a set of features that we will use for our models Data cleaning and formatting Supervised: we have access to both the features and the target and our goal is to train a model that can learn a mapping between the two Below the first quartile − 3 ∗ interquartile range Feature engineering: The process of taking raw data and extracting or creating new features. This might mean taking transformations of variables, such as a natural log and square root, or one-hot encoding categorical variables so they can be used in a model. Generally, I think of feature engineering as creating additional features from the raw data. One-hot encode categorical variables (borough and property use type) The training set of features is what we provide to our model during training along with the answers. The goal is for the model to learn a mapping between the features and the target. Cleaned and formatted the raw data",A Complete Machine Learning Project Walk-Through in Python: Part One,11,published,125638,3553,4.024767801857585,8,1,1,1,1,1
68,4200,375.255836551794,848,https://towardsdatascience.com/a-complete-machine-learning-project-walk-through-in-python-part-two-300f1f8147e2,10,Towards Data Science,2018-05-17 09:34:00,20.86,13,9695,2018-05-16 13:59:00,"['Machine Learning', 'Education', 'Towards Data Science', 'Programming', 'Data Science']","A Complete Machine Learning Walk-Through in Python: Part Two Model Selection, Hyperparameter Tuning, and Evaluation Assembling all the machine learning pieces needed to solve a problem can be a daunting task. In this series of articles, we are walking through implementing a machine learning workflow using a real-world dataset to see how the individual techniques come together. In the first post, we cleaned and structured the data, performed an exploratory data analysis, developed a set of features to use in our model, and established a baseline against which we can measure performance. In this article, we will look at how to implement and compare several machine learning models in Python, perform hyperparameter tuning to optimize the best model, and evaluate the final model on the test set. The full code for this project is on GitHub and the second notebook corresponding to this article is here. Feel free to use, share, and modify the code in any way you want! Model Evaluation and Selection As a reminder, we are working on a supervised regression task: using New York City building energy data, we want to develop a model that can predict the Energy Star Score of a building. Our focus is on both accuracy of the predictions and interpretability of the model. There are a ton of machine learning models to choose from and deciding where to start can be intimidating. While there are some charts that try to show you which algorithm to use, I prefer to just try out several and see which one works best! Machine learning is still a field driven primarily by empirical (experimental) rather than theoretical results, and it’s almost impossible to know ahead of time which model will do the best. Generally, it’s a good idea to start out with simple, interpretable models such as linear regression, and if the performance is not adequate, move on to more complex, but usually more accurate methods. The following chart shows a (highly unscientific) version of the accuracy vs interpretability trade-off: We will evaluate five different models covering the complexity spectrum: In this post we will focus on implementing these methods rather than the theory behind them. For anyone interesting in learning the background, I highly recommend An Introduction to Statistical Learning (available free online) or Hands-On Machine Learning with Scikit-Learn and TensorFlow. Both of these textbooks do a great job of explaining the theory and showing how to effectively use the methods in R and Python respectively. While we dropped the columns with more than 50% missing values when we cleaned the data, there are still quite a few missing observations. Machine learning models cannot deal with any absent values, so we have to fill them in, a process known as imputation. First, we’ll read in all the data and remind ourselves what it looks like: Every value that is NaN represents a missing observation. While there are a number of ways to fill in missing data, we will use a relatively simple method, median imputation. This replaces all the missing values in a column with the median value of the column. In the following code, we create a Scikit-Learn Imputer object with the strategy set to median. We then train this object on the training data (using imputer.fit) and use it to fill in the missing values in both the training and testing data (using imputer.transform). This means missing values in the test data are filled in with the corresponding median value from the training data. (We have to do imputation this way rather than training on all the data to avoid the problem of test data leakage, where information from the testing dataset spills over into the training data.) All of the features now have real, finite values with no missing examples. Scaling refers to the general process of changing the range of a feature. This is necessary because features are measured in different units, and therefore cover different ranges. Methods such as support vector machines and K-nearest neighbors that take into account distance measures between observations are significantly affected by the range of the features and scaling allows them to learn. While methods such as Linear Regression and Random Forest do not actually require feature scaling, it is still best practice to take this step when we are comparing multiple algorithms. We will scale the features by putting each one in a range between 0 and 1. This is done by taking each value of a feature, subtracting the minimum value of the feature, and dividing by the maximum minus the minimum (the range). This specific version of scaling is often called normalization and the other main version is known as standardization. While this process would be easy to implement by hand, we can do it using a MinMaxScaler object in Scikit-Learn. The code for this method is identical to that for imputation except with a scaler instead of imputer! Again, we make sure to train only using training data and then transform all the data. Every feature now has a minimum value of 0 and a maximum value of 1. Missing value imputation and feature scaling are two steps required in nearly any machine learning pipeline so it’s a good idea to understand how they work! After all the work we spent cleaning and formatting the data, actually creating, training, and predicting with the models is relatively simple. We will use the Scikit-Learn library in Python, which has great documentation and a consistent model building syntax. Once you know how to make one model in Scikit-Learn, you can quickly implement a diverse range of algorithms. We can illustrate one example of model creation, training (using .fit ) and testing (using .predict ) with the Gradient Boosting Regressor: Model creation, training, and testing are each one line! To build the other models, we use the same syntax, with the only change the name of the algorithm. The results are presented below: To put these figures in perspective, the naive baseline calculated using the median value of the target was 24.5. Clearly, machine learning is applicable to our problem because of the significant improvement over the baseline! The gradient boosted regressor (MAE = 10.013) slightly beats out the random forest (10.014 MAE). These results aren’t entirely fair because we are mostly using the default values for the hyperparameters. Especially in models such as the support vector machine, the performance is highly dependent on these settings. Nonetheless, from these results we will select the gradient boosted regressor for model optimization. Hyperparameter Tuning for Model Optimization In machine learning, after we have selected a model, we can optimize it for our problem by tuning the model hyperparameters. First off, what are hyperparameters and how do they differ from parameters? Controlling the hyperparameters affects the model performance by altering the balance between underfitting and overfitting in a model. Underfitting is when our model is not complex enough (it does not have enough degrees of freedom) to learn the mapping from features to target. An underfit model has high bias, which we can correct by making our model more complex. Overfitting is when our model essentially memorizes the training data. An overfit model has high variance, which we can correct by limiting the complexity of the model through regularization. Both an underfit and an overfit model will not be able to generalize well to the testing data. The problem with choosing the right hyperparameters is that the optimal set will be different for every machine learning problem! Therefore, the only way to find the best settings is to try out a number of them on each new dataset. Luckily, Scikit-Learn has a number of methods to allow us to efficiently evaluate hyperparameters. Moreover, projects such as TPOT by Epistasis Lab are trying to optimize the hyperparameter search using methods like genetic programming. In this project, we will stick to doing this with Scikit-Learn, but stayed tuned for more work on the auto-ML scene! The particular hyperparameter tuning method we will implement is called random search with cross validation: The idea of K-Fold cross validation with K = 5 is shown below: The entire process of performing random search with cross validation is: Of course, we don’t do actually do this manually, but rather let Scikit-Learn’s RandomizedSearchCV handle all the work! Since we will be using the Gradient Boosted Regression model, I should give at least a little background! This model is an ensemble method, meaning that it is built out of many weak learners, in this case individual decision trees. While a bagging algorithm such as random forest trains the weak learners in parallel and has them vote to make a prediction, a boosting method like Gradient Boosting, trains the learners in sequence, with each learner “concentrating” on the mistakes made by the previous ones. Boosting methods have become popular in recent years and frequently win machine learning competitions. The Gradient Boosting Method is one particular implementation that uses Gradient Descent to minimize the cost function by sequentially training learners on the residuals of previous ones. The Scikit-Learn implementation of Gradient Boosting is generally regarded as less efficient than other libraries such as XGBoost , but it will work well enough for our small dataset and is quite accurate. There are many hyperparameters to tune in a Gradient Boosted Regressor and you can look at the Scikit-Learn documentation for the details. We will optimize the following hyperparameters: I’m not sure if there is anyone who truly understands how all of these interact, and the only way to find the best combination is to try them out! In the following code, we build a hyperparameter grid, create a RandomizedSearchCV object, and perform hyperparameter search using 4-fold cross validation over 25 different combinations of hyperparameters: After performing the search, we can inspect the RandomizedSearchCV object to find the best model: We can then use these results to perform grid search by choosing parameters for our grid that are close to these optimal values. However, further tuning is unlikely to significant improve our model. As a general rule, proper feature engineering will have a much larger impact on model performance than even the most extensive hyperparameter tuning. It’s the law of diminishing returns applied to machine learning: feature engineering gets you most of the way there, and hyperparameter tuning generally only provides a small benefit. One experiment we can try is to change the number of estimators (decision trees) while holding the rest of the hyperparameters steady. This directly lets us observe the effect of this particular setting. See the notebook for the implementation, but here are the results: As the number of trees used by the model increases, both the training and the testing error decrease. However, the training error decreases much more rapidly than the testing error and we can see that our model is overfitting: it performs very well on the training data, but is not able to achieve that same performance on the testing set. We always expect at least some decrease in performance on the testing set (after all, the model can see the true answers for the training set), but a significant gap indicates overfitting. We can address overfitting by getting more training data, or decreasing the complexity of our model through the hyerparameters. In this case, we will leave the hyperparameters where they are, but I encourage anyone to try and reduce the overfitting. For the final model, we will use 800 estimators because that resulted in the lowest error in cross validation. Now, time to test out this model! Evaluating on the Test Set As responsible machine learning engineers, we made sure to not let our model see the test set at any point of training. Therefore, we can use the test set performance as an indicator of how well our model would perform when deployed in the real world. Making predictions on the test set and calculating the performance is relatively straightforward. Here, we compare the performance of the default Gradient Boosted Regressor to the tuned model: Hyperparameter tuning improved the accuracy of the model by about 10%. Depending on the use case, 10% could be a massive improvement, but it came at a significant time investment! We can also time how long it takes to train the two models using the %timeit magic command in Jupyter Notebooks. First is the default model: 1 second to train seems very reasonable. The final tuned model is not so fast: This demonstrates a fundamental aspect of machine learning: it is always a game of trade-offs. We constantly have to balance accuracy vs interpretability, bias vs variance, accuracy vs run time, and so on. The right blend will ultimately depend on the problem. In our case, a 12 times increase in run-time is large in relative terms, but in absolute terms it’s not that significant. Once we have the final predictions, we can investigate them to see if they exhibit any noticeable skew. On the left is a density plot of the predicted and actual values, and on the right is a histogram of the residuals: The model predictions seem to follow the distribution of the actual values although the peak in the density occurs closer to the median value (66) on the training set than to the true peak in density (which is near 100). The residuals are nearly normally distribution, although we see a few large negative values where the model predictions were far below the true values. We will take a deeper look at interpreting the results of the model in the next post. Conclusions In this article we covered several steps in the machine learning workflow: The results of this work showed us that machine learning is applicable to the task of predicting a building’s Energy Star Score using the available data. Using a gradient boosted regressor we were able to predict the scores on the test set to within 9.1 points of the true value. Moreover, we saw that hyperparameter tuning can increase the performance of a model at a significant cost in terms of time invested. This is one of many trade-offs we have to consider when developing a machine learning solution. In the third post (available here), we will look at peering into the black box we have created and try to understand how our model makes predictions. We also will determine the greatest factors influencing the Energy Star Score. While we know that our model is accurate, we want to know why it makes the predictions it does and what this tells us about the problem! As always, I welcome feedback and constructive criticism and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. K-Nearest Neighbors Regression Random Forest Regression Gradient Boosted Regression Support Vector Machine Regression Model parameters are what the model learns during training, such as weights in a linear regression. Cross Validation is the technique we use to evaluate a selected combination of hyperparameters. Rather than splitting the training set up into separate training and validation sets, which reduces the amount of training data we can use, we use K-Fold Cross Validation. This involves dividing the training data into K number of folds, and then going through an iterative process where we first train on K-1 of the folds and then evaluate performance on the Kth fold. We repeat this process K times and at the end of K-fold cross validation, we take the average error on each of the K iterations as the final performance measure. Randomly sample a combination of hyperparameters Create a model with the selected combination Evaluate the model using K-fold cross validation Decide which hyperparameters worked the best n_estimators: the number of weak learners (decision trees) to use max_depth: the maximum depth of each decision tree min_samples_leaf: the minimum number of examples required at a leaf node of the decision tree min_samples_split: the minimum number of examples required to split a node of the decision tree max_features: the maximum number of features to use for splitting nodes Evaluating and comparing several machine learning models Hyperparameter tuning using random grid search and cross validation Evaluating the best model on the test set Linear Regression Model hyperparameters are best thought of as settings for a machine learning algorithm that are set by the data scientist before training. Examples would be the number of trees in a random forest or the number of neighbors used in K-nearest neighbors algorithm. Random Search refers to the technique we will use to select hyperparameters. We define a grid and then randomly sample different combinations, rather than grid search where we exhaustively try out every single combination. (Surprisingly, random search performs nearly as well as grid search with a drastic reduction in run time.) Set up a grid of hyperparameters to evaluate loss: the loss function to minimize Imputation of missing values and scaling of features",A Complete Machine Learning Walk-Through in Python: Part Two,10,published,46472,3106,1.3522215067611076,0,1,1,1,1,0
66,2300,374.2457351864815,490,https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-three-388834e8804b,12,Towards Data Science,2018-05-18 09:48:00,23.74,11,6555,2018-05-17 09:36:00,"['Machine Learning', 'Towards Data Science', 'Education', 'Data Science', 'Programming']","A Complete Machine Learning Walk-Through in Python: Part Three Interpreting a machine learning model and presenting results Machine learning models are often criticized as black boxes: we put data in one side, and get out answers — often very accurate answers — with no explanations on the other. In the third part of this series showing a complete machine learning solution, we will peer into the model we developed to try and understand how it makes predictions and what it can teach us about the problem. We will wrap up by discussing perhaps the most important part of a machine learning project: documenting our work and presenting results. Part one of the series covered data cleaning, exploratory data analysis, feature engineering, and feature selection. Part two covered imputing missing values, implementing and comparing machine learning models, hyperparameter tuning using random search with cross validation, and evaluating a model. All the code for this project is on GitHub. The third Jupyter Notebook, corresponding to this post, is here. I encourage anyone to share, use, and build on this code! As a reminder, we are working through a supervised regression machine learning problem. Using New York City building energy data, we have developed a model which can predict the Energy Star Score of a building. The final model we built is a Gradient Boosted Regressor which is able to predict the Energy Star Score on the test data to within 9.1 points (on a 1–100 scale). Model Interpretation The gradient boosted regressor sits somewhere in the middle on the scale of model interpretability: the entire model is complex, but it is made up of hundreds of decision trees, which by themselves are quite understandable. We will look at three ways to understand how our model makes predictions: The first two methods are specific to ensembles of trees, while the third — as you might have guessed from the name — can be applied to any machine learning model. LIME is a relatively new package and represents an exciting step in the ongoing effort to explain machine learning predictions. Feature importances attempt to show the relevance of each feature to the task of predicting the target. The technical details of feature importances are complex (they measure the mean decrease impurity, or the reduction in error from including the feature), but we can use the relative values to compare which features are the most relevant. In Scikit-Learn, we can extract the feature importances from any ensemble of tree-based learners. With model as our trained model, we can find the feature importances usingmodel.feature_importances_ . Then we can put them into a pandas DataFrame and display or plot the top ten most important: The Site EUI(Energy Use Intensity) and the Weather Normalized Site Electricity Intensity are by far the most important features, accounting for over 66% of the total importance. After the top two features, the importance drops off significantly, which indicates we might not need to retain all 64 features in the data to achieve high performance. (In the Jupyter notebook, I take a look at using only the top 10 features and discover that the model is not quite as accurate.) Based on these results, we can finally answer one of our initial questions: the most important indicators of a building’s Energy Star Score are the Site EUI and the Weather Normalized Site Electricity Intensity. While we do want to be careful about reading too much into the feature importances, they are a useful way to start to understand how the model makes its predictions. While the entire gradient boosting regressor may be difficult to understand, any one individual decision tree is quite intuitive. We can visualize any tree in the forest using the Scikit-Learn function export_graphviz. We first extract a tree from the ensemble then save it as a dot file: Using the Graphviz visualization software we can convert the dot file to a png from the command line: The result is a complete decision tree: This is a little overwhelming! Even though this tree only has a depth of 6 (the number of layers), it’s difficult to follow. We can modify the call to export_graphviz and limit our tree to a more reasonable depth of 2: Each node (box) in the tree has four pieces of information: (Leaf nodes only have 2.–4. because they represent the final estimate and do not have any children). A decision tree makes a prediction for a data point by starting at the top node, called the root, and working its way down through the tree. At each node, a yes-or-no question is asked of the data point. For example, the question for the node above is: Does the building have a Site EUI less than or equal to 68.95? If the answer is yes, the building is placed in the right child node, and if the answer is no, the building goes to the left child node. This process is repeated at each layer of the tree until the data point is placed in a leaf node, at the bottom of the tree (the leaf nodes are cropped from the small tree image). The prediction for all the data points in a leaf node is the value. If there are multiple data points ( samples ) in a leaf node, they all get the same prediction. As the depth of the tree is increased, the error on the training set will decrease because there are more leaf nodes and the examples can be more finely divided. However, a tree that is too deep will overfit to the training data and will not be able to generalize to new testing data. In the second article, we tuned a number of the model hyperparameters, which control aspects of each tree such as the maximum depth of the tree and the minimum number of samples required in a leaf node. These both have a significant impact on the balance of under vs over-fitting, and visualizing a single decision tree allows us to see how these settings work. Although we cannot examine every tree in the model, looking at one lets us understand how each individual learner makes a prediction. This flowchart-based method seems much like how a human makes decisions, answering one question about a single value at a time. Decision-tree-based ensembles combine the predictions of many individual decision trees in order to create a more accurate model with less variance. Ensembles of trees tend to be very accurate, and also are intuitive to explain. The final tool we will explore for trying to understand how our model “thinks” is a new entry into the field of model explanations. LIME aims to explain a single prediction from any machine learning model by creating a approximation of the model locally near the data point using a simple model such as linear regression (the full details can be found in the paper ). Here we will use LIME to examine a prediction the model gets completely wrong to see what it might tell us about why the model makes mistakes. First we need to find the observation our model gets most wrong. We do this by training and predicting with the model and extracting the example on which the model has the greatest error: Next, we create the LIME explainer object passing it our training data, the mode, the training labels, and the names of the features in our data. Finally, we ask the explainer object to explain the wrong prediction, passing it the observation and the prediction function. The plot explaining this prediction is below: Here’s how to interpret the plot: Each entry on the y-axis indicates one value of a variable and the red and green bars show the effect this value has on the prediction. For example, the top entry says the Site EUI is greater than 95.90 which subtracts about 40 points from the prediction. The second entry says the Weather Normalized Site Electricity Intensity is less than 3.80 which adds about 10 points to the prediction. The final prediction is an intercept term plus the sum of each of these individual contributions. We can get another look at the same information by calling the explainer .show_in_notebook() method: This shows the reasoning process of the model on the left by displaying the contributions of each variable to the prediction. The table on the right shows the actual values of the variables for the data point. For this example, the model prediction was about 12 and the actual value was 100! While initially this prediction may be puzzling, looking at the explanation, we can see this was not an extreme guess, but a reasonable estimate given the values for the data point. The Site EUI was relatively high and we would expect the Energy Star Score to be low (because EUI is strongly negatively correlated with the score), a conclusion shared by our model. In this case, the logic was faulty because the building had a perfect score of 100. It can be frustrating when a model is wrong, but explanations such as these help us to understand why the model is incorrect. Moreover, based on the explanation, we might want to investigate why the building has a perfect score despite such a high Site EUI. Perhaps we can learn something new about the problem that would have escaped us without investigating the model. Tools such as this are not perfect, but they go a long way towards helping us understand the model which in turn can allow us to make better decisions. Documenting Work and Reporting Results An often under-looked part of any technical project is documentation and reporting. We can do the best analysis in the world, but if we do not clearly communicate the results, then they will not have any impact! When we document a data science project, we take all the versions of the data and code and package it so it our project can be reproduced or built on by other data scientists. It’s important to remember that code is read more often than it is written, and we want to make sure our work is understandable both for others and for ourselves if we come back to it a few months later. This means putting in helpful comments in the code and explaining your reasoning. I find Jupyter Notebooks to be a great tool for documentation because they allow for explanations and code one after the other. Jupyter Notebooks can also be a good platform for communicating findings to others. Using notebook extensions, we can hide the code from our final report , because although it’s hard to believe, not everyone wants to see a bunch of Python code in a document! Personally, I struggle with succinctly summarizing my work because I like to go through all the details. However, it’s important to understand your audience when you are presenting and tailor the message accordingly. With that in mind, here is my 30-second takeaway from the project: Originally, I was given this project as a job-screening “assignment” by a start-up. For the final report, they wanted to see both my work and my conclusions, so I developed a Jupyter Notebook to turn in. However, instead of converting directly to PDF in Jupyter, I converted it to a Latex .tex file that I then edited in texStudio before rendering to a PDF for the final version. The default PDF output from Jupyter has a decent appearance, but it can be significantly improved with a few minutes of editing. Moreover, Latex is a powerful document preparation system and it’s good to know the basics. At the end of the day, our work is only as valuable as the decisions it enables, and being able to present results is a crucial skill. Furthermore, by properly documenting work, we allow others to reproduce our results, give us feedback so we can become better data scientists, and build on our work for the future. Conclusions Throughout this series of posts, we’ve walked through a complete end-to-end machine learning project. We started by cleaning the data, moved into model building, and finally looked at how to interpret a machine learning model. As a reminder, the general structure of a machine learning project is below: While the exact steps vary by project, and machine learning is often an iterative rather than linear process, this guide should serve you well as you tackle future machine learning projects. I hope this series has given you confidence to be able to implement your own machine learning solutions, but remember, none of us do this by ourselves! If you want any help, there are many incredibly supportive communities where you can look for advice. A few resources I have found helpful throughout my learning process: As always, I welcome feedback and discussion and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Visualizing a single decision tree LIME: Local Interpretable Model-Agnostic Explainations The mse which is a measure of the error of the node The samples which is the number of examples (data points) in the node The value is the estimate of the target for all the samples in the node The Site EUI and Weather Normalized Electricity Intensity are the most relevant factors for predicting the Energy Star Score. Exploratory data analysis Feature engineering and selection Compare several machine learning models on a performance metric Perform hyperparameter tuning on the best model Evaluate the best model on the testing set Interpret the model results to the extent possible Draw conclusions and write a well-documented report An Introduction to Statistical Learning Kaggle: The Home of Data Science and Machine Learning Datacamp: Good beginner tutorials for practicing data science coding Coursera: Free and paid courses in many subjects Udacity: Paid programming and data science courses Feature importances The question asked about the value of one feature of the data point: this determines if we go right or left out of the node Using the New York City energy data, it is possible to build a model that can predict the Energy Star Score of buildings to within 9.1 points. Data cleaning and formatting Hands-On Machine Learning with Scikit-Learn and Tensorflow (Jupyter Notebooks for this book are available online for free)!",A Complete Machine Learning Walk-Through in Python: Part Three,10,published,27616,2622,0.8771929824561403,1,1,1,1,1,0
67,4100,371.0597670176042,862,https://towardsdatascience.com/automated-machine-learning-on-the-cloud-in-python-47cf568859f,13,Towards Data Science,2018-05-21 14:16:00,25.07,9,8494,2018-05-18 12:08:00,"['Machine Learning', 'Towards Data Science', 'Data Science', 'Education', 'Programming']","Automated Machine Learning on the Cloud in Python An introduction to the future of data science Two trends have recently become apparent in data science: This article will cover a brief introduction to these topics and show how to implement them, using Google Colaboratory to do automated machine learning on the cloud in Python. Cloud Computing using Google Colab Originally, all computing was done on a mainframe. You logged in via a terminal, and connected to a central machine where users simultaneously shared a single large computer. Then, along came microprocessors and the personal computer revolution and everyone got their own machine. Laptops and desktops work fine for routine tasks, but with the recent increase in size of datasets and computing power needed to run machine learning models, taking advantage of cloud resources is a necessity for data science. Cloud computing in general refers to the “delivery of computing services over the Internet”. This covers a wide range of services, from databases to servers to software, but in this article we will run a simple data science workload on the cloud in the form of a Jupyter Notebook. We will use the relatively new Google Colaboratory service: online Jupyter Notebooks in Python which run on Google’s servers, can be accessed from anywhere with an internet connection, are free to use, and are shareable like any Google Doc. Google Colab has made the process of using cloud computing a breeze. In the past, I spent dozens of hours configuring an Amazon EC2 instance so I could run a Jupyter Notebook on the cloud and had to pay by the hour! Fortunately, last year, Google announced you can now run Jupyter Notebooks on their Colab servers for up to 12 hours at a time completely free. (If that’s not enough, Google recently began letting users add a NVIDIA Tesla K80 GPU to the notebooks). The best part is these notebooks come pre-installed with most data science packages, and more can be easily added, so you don’t have to worry about the technical details of getting set up on your own machine. To use Colab, all you need is an internet connection and a Google account. If you just want an introduction, head to colab.research.google.com and create a new notebook, or explore the tutorial Google has developed (called Hello, Colaboratory). To follow along with this article, get the notebook here. Sign into your Google account, open the notebook in Colaboratory, click File > save a copy in Drive, and you will then have your own version to edit and run. Data science is becoming increasingly accessible with the wealth of resources online, and the Colab project has significantly lowered the barrier to cloud computing. For those who have done prior work in Jupyter Notebooks, it’s a completely natural transition, and for those who haven’t, it’s a great opportunity to get started with this commonly used data science tool! Automated Machine Learning using TPOT Automated machine learning (abbreviated auto-ml) aims to algorithmically design and optimize a machine learning pipeline for a particular problem. In this context, the machine learning pipeline consists of: There are an almost infinite number of ways these steps can be combined together, and the optimal solution will change for every problem! Designing a machine learning pipeline can be a time-consuming and frustrating process, and at the end, you will never know if the solution you developed is even close to optimal. Auto-ml can help by evaluating thousands of possible pipelines to try and find the best (or near-optimal) solution for a particular problem. It’s important to remember that machine learning is only one part of the data science process, and automated machine learning is not meant to replace the data scientist. Instead, auto-ml is meant to free the data scientist so she can work on more valuable aspects of the process, such as gathering data or interpreting a model. There are a number of auto-ml tools — H20, auto-sklearn, Google Cloud AutoML — and we will focus on TPOT: Tree-based Pipeline Optimization Tool developed by Randy Olson. TPOT (your “data-science assistant”) uses genetic programming to find the best machine learning pipeline. To use TPOT, it’s not really necessary to know the details of genetic programming, so you can skip this section. For those who are curious, at a high level, genetic programming for machine learning works as follows: (For more details on genetic programming, check out this short article.) The primary benefit of genetic programming for building machine learning models is exploration. Even a human with no time restraints will not be able to try out all combinations of preprocessing, models, and hyperparameters because of limited knowledge and imagination. Genetic programming does not display an initial bias towards any particular sequence of machine learning steps, and with each generation, new pipelines are evaluated. Furthermore, the fitness function means that the most promising areas of the search space are explored more thoroughly than poorer-performing areas. Putting it together: Automated Machine Learning on the Cloud With the background in place, we can now walk through using TPOT in a Google Colab notebook to automatically design a machine learning pipeline. (Follow along with the notebook here). Our task is a supervised regression problem: given New York City energy data, we want to predict the Energy Star Score of a building. In a previous series of articles (part one, part two, part three, code on GitHub), we built a complete machine learning solution for this problem. Using manual feature engineering, dimensionality reduction, model selection, and hyperparameter tuning, we designed a Gradient Boosting Regressor model that achieved a mean absolute error of 9.06 points (on a scale from 1–100) on the test set. The data contains several dozen continuous numeric variables (such as energy use and area of the building) and two one-hot encoded categorical variables (borough and building type) for a total of 82 features. The score is the target for regression. All of the missing values have been encoded as np.nan and no feature preprocessing has been done to the data. To get started, we first need to make sure TPOT is installed in the Google Colab environment. Most data science packages are already installed, but we can add any new ones using system commands (preceded with a ! in Jupyter): After reading in the data, we would normally fill in the missing values (imputation) and normalize the features to a range (scaling). However, in addition to feature engineering, model selection, and hyperparameter tuning, TPOT will automatically impute the missing values and do feature scaling! So, our next step is to create the TPOT optimizer: The default parameters for TPOT optimizers will evaluate 100 populations of pipelines, each with 100 generations for a total of 10,000 pipelines. Using 10-fold cross validation, this represents 100,000 training runs! Even though we are using Google’s resources, we do not have unlimited time for training. To avoid running out of time on the Colab server (we get a max of 12 hours of continuous run time), we will set a maximum of 8 hours (480 minutes) for evaluation. TPOT is designed to be run for days, but we can still get good results from a few hours of optimization. We set the following parameters in the call to the optimizer: There are other parameters that control details of the genetic programming method, but leaving them at the default works well for most cases. (If you want to play around with the parameters, check out the documentation.) The syntax for TPOT optimizers is designed to be identical to that for Scikit-Learn models so we can train the optimizer using the .fit method. During training, we get some information displayed: Due to the time limit, our model was only able to get through 15 generations. With 100 populations, this still represents 1500 different individual pipelines that were evaluated, quite a few more than we could have tried by hand! Once the model has trained, we can see the optimal pipeline using tpot.fitted_pipeline_. We can also save the model to a Python script: Since we are in a Google Colab notebook, to get the pipeline onto a local machine from the server, we have to use the Google Colab library: We can then open the file (available here) and look at the completed pipeline: We see that the optimizer imputed the missing values for us and built a complete model pipeline! The final estimator is a stacked model meaning that it uses two machine learning algorithms ( LassoLarsCV and GradientBoostingRegressor ), the second of which is trained on the predictions of the first (If you run the notebook again, you may get a different model because the optimization process is stochastic). This is a complex method that I probably would not have been able to develop on my own! Now, the moment of truth: performance on the testing set. To find the mean absolute error, we can use the .score method: In the series of articles where we developed a solution manually, after many hours of development, we built a Gradient Boosting Regressor model that achieved a mean absolute error of 9.06. Automated machine learning has significantly improved on the performance with a drastic reduction in the amount of development time. From here, we can use the optimized pipeline and try to further refine the solution, or we can move on to other important phases of the data science pipeline. If we use this as our final model, we could try and interpret the model (such as by using LIME: Local Interpretable Model-Agnostic Explainations) or write a well-documented report. Conclusions In this post, we got a brief introduction to both the capabilities of the cloud and automated machine learning. With only a Google account and an internet connection, we can use Google Colab to develop, run, and share machine learning or data science work loads. Using TPOT, we can automatically develop an optimized machine learning pipeline with feature preprocessing, model selection, and hyperparameter tuning. Moreover, we saw that auto-ml will not replace the data scientist, but it will allow her to spend more time on higher value parts of the workflow. While being an early adopter does not always pay off, in this case, TPOT is mature enough to be easy to use and relatively issue-free, yet also new enough that learning it will put you ahead of the curve. With that in mind, find a machine learning problem (perhaps through Kaggle) and try to solve it! Running automatic machine learning in a notebook on Google Colab feels like the future and with such a low barrier to entry, there’s never been a better time to get started! As always, I welcome feedback and discussion and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Machine learning pipelines are algorithmically developed and optimized Feature selection: dimensionality reduction Model Selection: evaluating many machine learning models Hyperparameter tuning: finding the optimal model settings Train each of these pipelines (called an individual) and evaluate on a performance metric using cross validation. The cross validation performance represents the “fitness” of the individual. Each training run of a population is known as a generation. After one round of training — the first generation — create a second generation of 100 individuals by reproduction, mutation, and crossover. Reproduction means keeping the same steps in the pipeline, chosen with a probability proportional to the fitness score. Mutation refers to random changes within an individual from one generation to the next. Crossover is random changes between individuals from one generation to the next. Together, these three strategies will result in 100 new pipelines, each slightly different, but with the steps that worked the best according to the fitness function more likely to be retained. Repeat this process for a suitable number of generations, each time creating new individuals through reproduction, mutation, and crossover. At the end of optimization, select the best-performing individual pipeline. max_time_minutes = 480: Limit evaluation to 8 hours n_jobs = -1: Use all available cores on the machine verbosity = 2: Show a limited amount of information while training cv = 5: Use 5-fold cross validation (default is 10) Data analysis and model training is done using cloud resources Feature Preprocessing: imputation, scaling, and constructing new features Start with an initial population of randomly generated machine learning pipelines, say 100, each of which is composed of functions for feature preprocessing, model selection, and hyperparameter tuning. scoring = neg_mean_absolute error: Our regression performance metric",Automated Machine Learning on the Cloud in Python,8,published,33884,2338,1.7536355859709154,3,1,1,1,1,0
64,7000,364.1688309876389,1217,https://towardsdatascience.com/machine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426,12,Towards Data Science,2018-05-28 11:39:00,23.86,12,10457,2018-05-24 18:11:00,"['Data Science', 'Machine Learning', 'Towards Data Science', 'Programming', 'Education']","Machine Learning Kaggle Competition Part One: Getting Started Learning the Kaggle Environment and an Introductory Notebook In the field of data science, there are almost too many resources available: from Datacamp to Udacity to KDnuggets, there are thousands of places online to learn about data science. However, if you are someone who likes to jump in and learn by doing, Kaggle might be the single best location for expanding your skills through hands-on data science projects. While it originally was known as a place for machine learning competitions, Kaggle — which bills itself as “Your Home for Data Science” — now offers an array of data science resources. Although this series of articles will focus on a competition, it’s worth pointing out the main aspects of Kaggle: Overall, Kaggle is a great place to learn, whether that’s through the more traditional learning tracks or by competing in competitions. When I want to find out about the latest machine learning method, I could go read a book, or, I could go on Kaggle, find a competition, and see how people use it in practice. Personally, I find this much more enjoyable and a more effective teaching method. Moreover, the community is extremely supportive and always willing to answer questions or provide feedback on a project. In this article, we’ll focus on getting started with a Kaggle machine learning competition: the Home Credit Default Risk problem. This is a fairly straightforward competition with a reasonable sized dataset (which can’t be said for all of the competitions) which means we can compete entirely using Kaggle’s kernels. This significantly lowers the barrier to entry because you don’t have to worry about any software on your computer and you don’t even have to download the data! As long as you have a Kaggle account and an Internet connection, you can connect to a kernel and run the code. I plan to do the entire competition on Kaggle and the kernel (a Python Jupyter Notebook) for this post is available here. To get the most from this article, copy the kernel by creating a Kaggle account, then hitting the blue Fork Notebook button. This will open up the notebook for editing and running in the kernel environment. Competition Description The Home Credit Default Risk competition is a standard supervised machine learning task where the goal is to use historical loan application data to predict whether or not an applicant will repay a loan. During training, we provide our model with the features — the variables describing a loan application — and the label — a binary 0 if the loan was repaid and a 1 if the loan was not repaid — and the model learns a mapping from the features to the label. Then, during testing, we feed the model the features for a new batch of applications and ask it to predict the label. All the data for this competition is structured meaning it exists in neat rows and columns — think a spreadsheet. This means we won’t need to use any convolutional neural networks (which excel at processing image data ) and it will give us great practice on a real-world dataset. Home Credit, the host of the competition, is a finance provider that focuses on serving the unbanked population. Predicting whether or not an application will repay a loan is a vital business need, and Home Credit has developed this competition in the hopes that the Kaggle community can develop an effective algorithm for this task. This competition follows the general idea of most Kaggle competitions: a company has data and a problem to solve, and rather than (or in addition to) hiring internal data scientists to build a model, they put up a modest prize to entice the entire world to contribute solutions. A community of thousands of skilled data scientists (Kagglers) then work on the problem for basically no charge to come up with the best solution. As far as cost effective business plans go, this seems like a brilliant idea! Kaggle Competition Environment When you go to the competition homepage, you’ll see this: Here’s a quick run through of the tabs Although they are called competitions, Kaggle machine learning events should really be termed “collaborative projects” because the main goal is not necessarily to win but to practice and learn from fellow data scientists. Once you realize that it’s not so much about beating others but about expanding your own skills, you will get a lot more out of the competitions. When you sign up for Kaggle, you not only get all the resources , you also get to be part of a community of data scientists with thousands of years of collective experience. Take advantage of all that experience by trying to be an active part of the community! That means anything from sharing a kernel to asking a question in a discussion forum. While it can be intimidating to make your work public, we learn best by making mistakes, receiving feedback, and improving so we don’t make the same mistake again. Everyone starts out a beginner, and the community is very supportive of data scientists of all skill levels. In that mindset, I want to emphasize that discussion with others and building on others’ code is not only acceptable, but encouraged! In school, working with others is called cheating and gets you a zero, but in the real world, it’s called collaboration and an extremely important skill. A great method for throwing yourself into a competition is to find a kernel someone has shared with a good leaderboard score, fork the kernel, edit it to try and improve the score, and then run it to see the results. Then, make the kernel public so others can use your work. Data scientists stand not on the shoulders of giants, but on the backs of thousands of individuals who have made their work public for the benefit of all. (Sorry for getting philosophical, but this is why I love data science!) Working Through a First Notebook Once you have a basic understanding of how Kaggle works and the philosophy of how to get the most out of a competition, it’s time to get started. Here, I’ll briefly outline a Python Jupyter Notebook I put together in a kernel for the Home Credit Default Risk problem, but to get the full benefit, you’ll want to fork the notebook on Kaggle and run it yourself (you don’t have to download or set-up anything so I’d highly encourage checking it out). When you open the notebook in a kernel, you’ll see this environment: Think of this as a standard Jupyter Notebook with slightly different aesthetics. You can write Python code and text (using Markdown syntax) just like in Jupyter and run the code completely in the cloud on Kaggle’s servers. However, Kaggle kernels have some unique features not available in Jupyter Notebook. Hit the leftward facing arrow in the upper right to expand the kernel control panel which brings up three tabs (if the notebook is not in fullscreen, then these three tabs may already be visible next to the code). In the data tab, we can view the datasets to which our Kernel is connected. In this case, we have the entire competition data, but we can also connect to any other dataset on Kaggle or upload our own data and access it in the kernel. Data files are available in the ../input/ directory from within the code: The Settings tab lets us control different technical aspects of the kernel. Here we can add a GPU to our session, change the visibility, and install any Python package which is not already in the environment. Finally, the Versions tab lets us see any previous committed runs of the code. We can view changes to the code, look at log files of a run, see the notebook generated by a run, and download the files that are output from a run. To run the entire notebook and record a new Version, hit the blue Commit & Run button in the upper right of the kernel. This executes all the code, shows us the completed notebook (or any errors if there are mistakes), and saves any files that are created during the run. When we commit the notebook, we can then access any predictions our models made and submit them for scoring. The first notebook is meant to get you familiar with the problem. We start off much the same way as any data science problem: understanding the data and the task. For this problem, there is 1 main training data file (with the labels included), 1 main testing data file, and 6 additional data files. In this first notebook, we use only the main data, which will get us a decent score, but later work will have to incorporate all the data in order to be competitive. To understand the data, it’s best to take a couple minutes away from the keyboard and read through the problem documentation, such as the column descriptions of each data file. Because there are multiple files, we need to know how they are all linked together, although for this first notebook we only use the main file to keep things simple. Reading through other kernels can also help us get familiar with the data and which variables are important. Once we understand the data and the problem, we can start structuring it for a machine learning task This means dealing with categorical variables (through one-hot encoding), filling in the missing values (imputation), and scaling the variables to a range. We can do exploratory data analysis, such as finding correlations with the label, and graphing these relationships. We can use these relationships later on for modeling decisions, such as including which variables to use. (See the notebook for implementation). Of course, no exploratory data analysis is complete without my favorite plot, the Pairs Plot. After thoroughly exploring the data and making sure it’s acceptable for machine learning, we move on to creating baseline models. However, before we quite get to the modeling stage, it’s critical we understand the performance metric for the competition. In a Kaggle competition, it all comes down to a single number, the metric on the test data. While it might make intuitive sense to use accuracy for a binary classification task, that is a poor choice because we are dealing with an imbalanced class problem. Instead of accuracy, submissions are judged in terms of ROC AUC or Receiver Operating Characteristic curve Area Under the Curve. I’ll let you do the research on this one, or read the explanation in the notebook. Just know that higher is better, with a random model scoring 0.5 and a perfect model scoring 1.0. To calculate a ROC AUC, we need to make predictions in terms of probabilities rather than a binary 0 or 1. The ROC then shows the True Positive Rate versus the False Positive Rate as a function of the threshold according to which we classify an instance as positive. Usually we like to make a naive baseline prediction, but in this case, we already know that random guessing on the task would get an ROC AUC of 0.5. Therefore, for our baseline model, we will use a slightly more sophisticated method, Logistic Regression. This is a popular simple algorithm for binary classification problems and it will set a low bar for future models to surpass. After implementing the logistic regression, we can save the results to a csv file for submission. When the notebook is committed, any results we write will show up in the Output sub-tab on the Versions tab: From this tab, we can download the submissions to our computer and then upload them to the competition. In this notebook, we make four different models with scores as follows: These scores don’t get us anywhere close to the top of the leaderboard, but they leave room for plenty of future improvement! We also get an idea of the performance we can expect using only a single source of data. (Not surprisingly, the extraordinary Gradient Boosting Machine (using the LightGBM library) performs the best. This model wins nearly every structured Kaggle competition (where the data is in table format) and we will likely need to use some form of this model if we want to seriously compete!) Conclusions This article and introductory kernel demonstrated a basic start to a Kaggle competition. It’s not meant to win, but rather to show you the basics of how to approach a machine learning competition and also a few models to get you off the ground (although the LightGBM model is like jumping off the deep end). Furthermore, I laid out my philosophy for machine learning competitions, which is to learn as much as possible by taking part in discussions, building on other’s code, and sharing your own work. It’s enjoyable to best your past scores, but I view doing well not as the main focus but as a positive side effect of learning new data science techniques. While these are known as competitions, they are really collaborative projects where everyone is welcome to participate and hone their abilities. There remains a ton of work to be done, but thankfully we don’t have to do it alone. In later articles and notebooks we’ll see how to build on the work of others to make even better models. I hope this article (and the notebook kernel) has given you the confidence to start competing on Kaggle or taking on any data science project. As always, I welcome constructive criticism and discussion and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Machine Learning Competitions: once the heart of Kaggle, these tests of modeling skill are a great way to learn cutting edge machine learning techniques and hone your abilities on interesting problems using real data. Learn: A series of data science learning tracks covering SQL to Deep Learning taught in Jupyter Notebooks. Discussion: A place to ask questions and get advice from the thousands of data scientists in the Kaggle community. Kernels: Online programming environments running on Kaggle’s servers where you can write Python/R scripts, or Jupyter Notebooks. These kernels are entirely free to run (you can even add a GPU) and are a great resource because you don’t have to worry about setting up a data science environment on your own computer. The kernels can be used to analyze any dataset, compete in machine learning competitions, or complete the learning tracks. You can copy and build on existing kernels from other users and share your kernels with the community for feedback. Data: all of the data needed for the competition as external data is not allowed. You can download all of the data, but we won’t need to do that because we will be using a Kaggle Kernel which can connect to the data. Kernels: Previous work done by you and other competitors. This —in my opinion — is the most valuable resource for a competition. You can read through other scripts and notebooks and then copy the code (called “Forking”) to edit and run. Discussion: another helpful resource where you can find conversations both from the competition hosts and from other competitors. A great place to ask questions and learn from the answers of others. Leaderboard: who’s on top and where you stand Rules: not very interesting, but good to understand Team: manage the members of your team if you decide to form a team My Submissions: view your previous submissions and select the final ones to be used for the competition Random Forest: 0.678 Random Forest with Constructed Features: 0.678 Light Gradient Boosting Machine: 0.729 Datasets: Tens of thousands of datasets of all different types and sizes that you can download and use for free. This is a great place to go if you are looking for interesting data to explore or to test your modeling skills. Overview: a brief description of the problem, the evaluation metric, the prizes, and the timeline Logistic Regression: 0.671",Machine Learning Kaggle Competition Part One: Getting Started,8,published,43826,2975,2.3529411764705883,3,1,1,1,1,0
65,9300,359.23664285651626,1874,https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219,24,Towards Data Science,2018-06-02 10:01:00,21.94,11,25366,2018-06-01 06:51:00,"['Machine Learning', 'Towards Data Science', 'Data Science', 'Education', 'Python']","Automated Feature Engineering in Python How to automatically create machine learning features Machine learning is increasingly moving from hand-designed models to automatically optimized pipelines using tools such as H20, TPOT, and auto-sklearn. These libraries, along with methods such as random search, aim to simplify the model selection and tuning parts of machine learning by finding the best model for a dataset with little to no manual intervention. However, feature engineering, an arguably more valuable aspect of the machine learning pipeline, remains almost entirely a human labor. Feature engineering, also known as feature creation, is the process of constructing new features from existing data to train a machine learning model. This step can be more important than the actual model used because a machine learning algorithm only learns from the data we give it, and creating features that are relevant to a task is absolutely crucial (see the excellent paper “A Few Useful Things to Know about Machine Learning”). Typically, feature engineering is a drawn-out manual process, relying on domain knowledge, intuition, and data manipulation. This process can be extremely tedious and the final features will be limited both by human subjectivity and time. Automated feature engineering aims to help the data scientist by automatically creating many candidate features out of a dataset from which the best can be selected and used for training. In this article, we will walk through an example of using automated feature engineering with the featuretools Python library. We will use an example dataset to show the basics (stay tuned for future posts using real-world data). The complete code for this article is available on GitHub. Feature Engineering Basics Feature engineering means building additional features out of existing data which is often spread across multiple related tables. Feature engineering requires extracting the relevant information from the data and getting it into a single table which can then be used to train a machine learning model. The process of constructing features is very time-consuming because each new feature usually requires several steps to build, especially when using information from more than one table. We can group the operations of feature creation into two categories: transformations and aggregations. Let’s look at a few examples to see these concepts in action. A transformation acts on a single table (thinking in terms of Python, a table is just a Pandas DataFrame ) by creating new features out of one or more of the existing columns. As an example, if we have the table of clients below we can create features by finding the month of the joined column or taking the natural log of the income column. These are both transformations because they use information from only one table. On the other hand, aggregations are performed across tables, and use a one-to-many relationship to group observations and then calculate statistics. For example, if we have another table with information on the loans of clients, where each client may have multiple loans, we can calculate statistics such as the average, maximum, and minimum of loans for each client. This process involves grouping the loans table by the client, calculating the aggregations, and then merging the resulting data into the client data. Here’s how we would do that in Python using the language of Pandas. These operations are not difficult by themselves, but if we have hundreds of variables spread across dozens of tables, this process is not feasible to do by hand. Ideally, we want a solution that can automatically perform transformations and aggregations across multiple tables and combine the resulting data into a single table. Although Pandas is a great resource, there’s only so much data manipulation we want to do by hand! (For more on manual feature engineering check out the excellent Python Data Science Handbook). Featuretools Fortunately, featuretools is exactly the solution we are looking for. This open-source Python library will automatically create many features from a set of related tables. Featuretools is based on a method known as “Deep Feature Synthesis”, which sounds a lot more imposing than it actually is (the name comes from stacking multiple features not because it uses deep learning!). Deep feature synthesis stacks multiple transformation and aggregation operations (which are called feature primitives in the vocab of featuretools) to create features from data spread across many tables. Like most ideas in machine learning, it’s a complex method built on a foundation of simple concepts. By learning one building block at a time, we can form a good understanding of this powerful method. First, let’s take a look at our example data. We already saw some of the dataset above, and the complete collection of tables is as follows: If we have a machine learning task, such as predicting whether a client will repay a future loan, we will want to combine all the information about clients into a single table. The tables are related (through the client_id and the loan_id variables) and we could use a series of transformations and aggregations to do this process by hand. However, we will shortly see that we can instead use featuretools to automate the process. The first two concepts of featuretools are entities and entitysets. An entity is simply a table (or a DataFrame if you think in Pandas). An EntitySet is a collection of tables and the relationships between them. Think of an entityset as just another Python data structure, with its own methods and attributes. We can create an empty entityset in featuretools using the following: Now we have to add entities. Each entity must have an index, which is a column with all unique elements. That is, each value in the index must appear in the table only once. The index in the clients dataframe is the client_idbecause each client has only one row in this dataframe. We add an entity with an existing index to an entityset using the following syntax: The loans dataframe also has a unique index, loan_id and the syntax to add this to the entityset is the same as for clients. However, for the payments dataframe, there is no unique index. When we add this entity to the entityset, we need to pass in the parameter make_index = True and specify the name of the index. Also, although featuretools will automatically infer the data type of each column in an entity, we can override this by passing in a dictionary of column types to the parameter variable_types . For this dataframe, even though missed is an integer, this is not a numeric variable since it can only take on 2 discrete values, so we tell featuretools to treat is as a categorical variable. After adding the dataframes to the entityset, we inspect any of them: The column types have been correctly inferred with the modification we specified. Next, we need to specify how the tables in the entityset are related. The best way to think of a relationship between two tables is the analogy of parent to child. This is a one-to-many relationship: each parent can have multiple children. In the realm of tables, a parent table has one row for every parent, but the child table may have multiple rows corresponding to multiple children of the same parent. For example, in our dataset, the clients dataframe is a parent of the loans dataframe. Each client has only one row in clients but may have multiple rows in loans. Likewise, loans is the parent of payments because each loan will have multiple payments. The parents are linked to their children by a shared variable. When we perform aggregations, we group the child table by the parent variable and calculate statistics across the children of each parent. To formalize a relationship in featuretools, we only need to specify the variable that links two tables together. The clients and the loans table are linked via the client_id variable and loans and payments are linked with the loan_id. The syntax for creating a relationship and adding it to the entityset are shown below: The entityset now contains the three entities (tables) and the relationships that link these entities together. After adding entities and formalizing relationships, our entityset is complete and we are ready to make features. Before we can quite get to deep feature synthesis, we need to understand feature primitives. We already know what these are, but we have just been calling them by different names! These are simply the basic operations that we use to form new features: New features are created in featuretools using these primitives either by themselves or stacking multiple primitives. Below is a list of some of the feature primitives in featuretools (we can also define custom primitives): These primitives can be used by themselves or combined to create features. To make features with specified primitives we use the ft.dfs function (standing for deep feature synthesis). We pass in the entityset, the target_entity , which is the table where we want to add the features, the selected trans_primitives (transformations), and agg_primitives (aggregations): The result is a dataframe of new features for each client (because we made clients the target_entity). For example, we have the month each client joined which is a transformation feature primitive: We also have a number of aggregation primitives such as the average payment amounts for each client: Even though we specified only a few feature primitives, featuretools created many new features by combining and stacking these primitives. The complete dataframe has 793 columns of new features! We now have all the pieces in place to understand deep feature synthesis (dfs). In fact, we already performed dfs in the previous function call! A deep feature is simply a feature made of stacking multiple primitives and dfs is the name of process that makes these features. The depth of a deep feature is the number of primitives required to make the feature. For example, the MEAN(payments.payment_amount) column is a deep feature with a depth of 1 because it was created using a single aggregation. A feature with a depth of two is LAST(loans(MEAN(payments.payment_amount)) This is made by stacking two aggregations: LAST (most recent) on top of MEAN. This represents the average payment size of the most recent loan for each client. We can stack features to any depth we want, but in practice, I have never gone beyond a depth of 2. After this point, the features are difficult to interpret, but I encourage anyone interested to try “going deeper”. We do not have to manually specify the feature primitives, but instead can let featuretools automatically choose features for us. To do this, we use the same ft.dfs function call but do not pass in any feature primitives: Featuretools has built many new features for us to use. While this process does automatically create new features, it will not replace the data scientist because we still have to figure out what to do with all these features. For example, if our goal is to predict whether or not a client will repay a loan, we could look for the features most correlated with a specified outcome. Moreover, if we have domain knowledge, we can use that to choose specific feature primitives or seed deep feature synthesis with candidate features. Automated feature engineering has solved one problem, but created another: too many features. Although it’s difficult to say before fitting a model which of these features will be important, it’s likely not all of them will be relevant to a task we want to train our model on. Moreover, having too many features can lead to poor model performance because the less useful features drown out those that are more important. The problem of too many features is known as the curse of dimensionality. As the number of features increases (the dimension of the data grows) it becomes more and more difficult for a model to learn the mapping between features and targets. In fact, the amount of data needed for the model to perform well scales exponentially with the number of features. The curse of dimensionality is combated with feature reduction (also known as feature selection): the process of removing irrelevant features. This can take on many forms: Principal Component Analysis (PCA), SelectKBest, using feature importances from a model, or auto-encoding using deep neural networks. However, feature reduction is a different topic for another article. For now, we know that we can use featuretools to create numerous features from many tables with minimal effort! Conclusions Like many topics in machine learning, automated feature engineering with featuretools is a complicated concept built on simple ideas. Using concepts of entitysets, entities, and relationships, featuretools can perform deep feature synthesis to create new features. Deep feature synthesis in turn stacks feature primitives — aggregations, which act across a one-to-many relationship between tables, and transformations, functions applied to one or more columns in a single table — to build new features from multiple tables. In future articles, I’ll show how to use this technique on a real world problem, the Home Credit Default Risk competition currently being hosted on Kaggle. Stay tuned for that post, and in the meantime, read this introduction to get started in the competition! I hope that you can now use automated feature engineering as an aid in a data science pipeline. Our models are only as good as the data we give them, and automated feature engineering can help to make the feature creation process more efficient. For more information on featuretools, including advanced usage, check out the online documentation. To see how featuretools is used in practice, read about the work of Feature Labs, the company behind the open-source library. As always, I welcome feedback and constructive criticism and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Transformations: operations done on a single table to one or more columns. An example is taking the difference between two columns in one table or taking the absolute value of a column. clients : basic information about clients at a credit union. Each client has only one row in this dataframe Aggregations: operations completed across a parent-to-child (one-to-many) relationship that group by the parent and calculate stats for the children. An example is grouping the loan table by the client_id and finding the maximum loan amount for each client.",Automated Feature Engineering in Python,5,published,115638,2648,3.512084592145015,1,1,1,1,1,1
63,1500,341.796500950301,260,https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8,4,Towards Data Science,2018-06-19 20:35:00,16.85,20,1908,2018-05-24 18:12:00,"['Machine Learning', 'Education', 'Data Science', 'Towards Data Science', 'Programming']","Machine Learning Kaggle Competition Part Two: Improving Feature engineering, feature selection, and model evaluation Like most problems in life, there are several potential approaches to a Kaggle competition: I recommend against the “lone genius” path, not only because it’s exceedingly lonely, but also because you will miss out on the most important part of a Kaggle competition: learning from other data scientists. If you work by yourself, you end up relying on the same old methods while the rest of the world adopts more efficient and accurate techniques. As a concrete example, I recently have been dependent on the random forest model, automatically applying it to any supervised machine learning task. This competition finally made me realize that although the random forest is a decent starting model, everyone else has moved on to the superior gradient boosting machine. The other extreme approach is also limiting: 2. Copy one of the leader’s scripts (called “kernels” on Kaggle), run it, and shoot up the leaderboard without writing a single line of code I also don’t recommend the “copy and paste” approach, not because I’m against using other’s code (with proper attribution), but because you are still limiting your chances to learn. Instead, what I do recommend is a hybrid approach: read what others have done, understand and even use their code, and build on other’s work with your own ideas. Then, release your code to the public so others can do the same process, expanding the collective knowledge of the community. In the second part of this series about competing in a Kaggle machine learning competition, we will walk through improving on our initial submission that we developed in the first part. The major results documented in this article are: We will walk through how we achieve these results — covering a number of major ideas in machine learning and building on other’s code where applicable. We’ll focus on three crucial steps of any machine learning project: To get the most out of this post, you’ll want to follow the Python notebooks on Kaggle (which will be linked to as they come up). These notebooks can be run on Kaggle without downloading anything on your computer so there’s little barrier to entry! I’ll hit the major points at a high-level in this article, with the full details in the notebooks. If you’re new to the competition, I highly recommend starting with this article and this notebook to get up to speed. The Home Credit Default Risk competition on Kaggle is a standard machine learning classification problem. Given a dataset of historical loans, along with clients’ socioeconomic and financial information, our task is to build a model that can predict the probability of a client defaulting on a loan. In the first part of this series, we went through the basics of the problem, explored the data, tried some feature engineering, and established a baseline model. Using a random forest and only one of the seven data tables, we scored a 0.678 ROC AUC (Receiver Operating Characteristic Area Under the Curve) on the public leaderboard. (The public leaderboard is calculated with only 20% of the test data and the final standings usually change significantly.) To improve our score, in this article and a series of accompanying notebooks on Kaggle, we will concentrate primarily on feature engineering and then on feature selection. Generally, the largest benefit relative to time invested in a machine learning problem will come in the feature engineering stage. Before we even start trying to build a better model, we need to focus on using all of the data in the most effective manner! Much of this article will seem exploratory (or maybe even arbitrary) and I don’t claim to have made the best decisions! There are a lot of knobs to tune in machine learning, and often the only approach is to try out different combinations until we find the one that works best. Machine learning is more empirical than theoretical and relies on testing rather than working from first principles or a set of hard rules. In a great blog post, Pete Warden explained that machine learning is a little like banging on the side of the TV until it works. This is perfectly acceptable as long as we write down the exact “bangs” we made on the TV and the result each time. Then, we can analyze the choices we made, look for any patterns to influence future decisions, and find which method works the best. My goal with this series is to get others involved with machine learning, put my methods out there for feedback, and document my work so I can remember what I did for the next time! Any comments or questions, here or on Kaggle, are much appreciated. Feature Engineering Feature engineering is the process of creating new features from existing data. The objective is to build useful features that can help our model learn the relationship between the information in a dataset and a given target. In many cases — including this problem — the data is spread across multiple tables. Because a machine learning model must be trained with a single table, feature engineering requires us to summarize all of the data in one table. This competition has a total of 7 data files. In the first part, we used only a single source of data, the main file with socioeconomic information about each client and characteristics of the loan application. We will call this table app.(For those used to Pandas, a table is just a dataframe). We can tell this is the training data because it includes the label, TARGET. A TARGET value of 1 indicates a loan which was not repaid. The app dataframe is tidy structured data: there is one row for every observation — a client’s application for a loan — with the columns containing the features (also known as the explanatory or predictor variables). Each client’s application — which we will just call a “client” — has a single row in this dataframe identified by the SK_ID_CURR. Because each client has a unique row in this dataframe, it is the parent of all the other tables in the dataset as indicated by this diagram showing how the tables are related: When we make our features, we want to add them to this main dataframe. At the end of feature engineering, each client will still have only a single row, but with many more columns capturing information from the other data tables. The six other tables contain information about clients’ previous loans, both with Home Credit (the institution running the competition), and other credit agencies. For example, here is the bureau dataframe, containing client’s previous loans at other financial institutions: This dataframe is a child table of the parentapp: for each client (identified by SK_ID_CURR) in the parent, there may be many observations in the child. These rows correspond to multiple previous loans for a single client. The bureau dataframe in turn is the parent of the bureau_balance dataframe where we have monthly information for each previous loan. Let’s look at an example of creating a new feature from a child dataframe: the count of the number of previous loans for each client at other institutions. Even though I wrote a post about automated feature engineering, for this article we will stick to doing it by hand. The first Kaggle notebook to look at is here: is a comprehensive guide to manual feature engineering. Calculating this one feature requires grouping (using groupby)the bureau dataframe by the client id, calculating an aggregation statistic (using agg with count) and then merging (using merge) the resulting table with the main dataframe. This means that for each client, we are gathering up all of their previous loans and counting the total number. Here it is in Python: Now our model can use the information of the number of previous loans as a predictor for whether a client will repay a loan. To inspect the new variable, we can make a kernel density estimate (kde) plot. This shows the distribution of a single variable and can be thought of as a smoothed histogram. To see if the distribution of this feature varies based on whether the client repaid her/his loan, we can color the kde by the value of the TARGET: There does not appear to be much of a difference in the distribution, although the peak of the TARGET==1 distribution is slightly to the left of the TARGET==0 distribution. This could indicate clients who did not repay the loan tend to have had fewer previous loans at other institutions. Based on my extremely limited domain knowledge, this relationship would make sense! Generally, we do not know whether a feature will be useful in a model until we build the model and test it. Therefore, our approach is to build as many features as possible, and then keep only those that are the most relevant. “Most relevant” does not have a strict definition, but we will see some ways we can try to measure this in the feature selection section. Now let’s look at capturing information not from a direct child of the app dataframe, but from a child of a child of app! The bureau_balance dataframe contains monthly information about each previous loan. This is a child of the bureau dataframe so to get this information into the main dataframe, we will have to do two groupbys and aggregates: first by the loan id (SK_ID_BUREAU) and then by the client id. As an example, if we want to calculate for each client the average of the max number of MONTHS_BALANCE for each previous loan in the bureau_balance dataframe, we can do this: This was a lot of code for a single feature, and you can easily imagine that the manual feature engineering process gets tedious after a few features! That’s why we want to write functions that take these individual steps and repeat them for us on each dataframe. Instead of repeating code over and over, we put it into a function — called refactoring — and then call the function every time we want to perform the same operation. Writing functions saves us time and allows for more reproducible workflows because it will execute the same actions in exactly the same way every time. Below is a function based on the above steps that can be used on any child dataframe to compute aggregation statistics on the numeric columns. It first groups the columns by a grouping variable (such as the client id), calculates the mean, max, min, sum of each of these columns, renames the columns, and returns the resulting dataframe. We can then merge this dataframe with the main app data. (This function draws heavily on this kernel from olivier on Kaggle). (Half of the lines of code for this function is documentation. Writing proper docstrings is crucial not only for others to understand our code, but so we can understand our own code when we come back to it!) To see this in action, refer to the notebook, but clearly we can see this will save us a lot of work, especially with 6 children dataframes to process. This function handles the numeric variables, but that still leaves the categorical variables. Categorical variables, often represented as strings, can only take on a limited number of values (in contrast to continuous variables which can be any numeric value). Machine learning models cannot handle string data types, so we have to find a way to capture the information in these variables in a numeric form. As an example of a categorical variable, the bureau table has a column called CREDIT_ACTIVE that has the status of each previous loan: We can represent this data in a numeric form by counting the number of each type of loan that each client has had. Moreover, we can calculate the normalized count of each loan type by dividing the count for one particular type of loan by the total count. We end up with this: Now these categorical features can be passed into a machine learning model. The idea is that we capture not only the number of each type of previous loan, but also the relative frequency of that type of loan. As before, we don’t actually know whether these new features will be useful and the only way to say for sure is to make the features and then test them in a model! Rather than doing this by hand for every child dataframe, we again can write a function to calculate the counts of categorical variables for us. Initially, I developed a really complicated method for doing this involving pivot tables and all sorts of aggregations, but then I saw other code where someone had done the same thing in about two lines using one-hot encoding. I promptly discarded my hours of work and used this version of the function instead! This function once again saves us massive amounts of time and allows us to apply the same exact steps to every dataframe. Once we write these two functions, we use them to pull all the data from the seven separate files into one single training (and one testing dataframe). If you want to see this implemented, you can look at the first and second manual engineering notebooks. Here’s a sample of the final data: Using information from all seven tables, we end up with a grand total of 1465 features! (From an original 122). How do we know if any of these features are helpful? One method is to calculate the Pearson correlation coefficient between the variables and the TARGET. This is a relatively crude measure of importance, but it can serve as an approximation of which variables are related to a client’s ability to repay a loan. Below are the most correlated variables with the TARGET: The EXT_SOURCE_ variables were from the original features, but some of the variables we created are among the top correlations. However, we want to avoid reading too much into these numbers. Anytime we make a ton of features, we can run into the multiple comparisons problem: the more comparisons we make — in this case correlations with the target — the more likely some of them are to be large due to random noise. With correlations this small, we need to be especially careful when interpreting the numbers. The most negatively correlated variable we made, client_bureau_balance_counts_mean, represents the average for each client of the count of the number of times a loan appeared in the bureau_balance data. In other words, it is the average number of monthly records per previous loan for each client. The kde plot is below: Now that we have 1465 features, we run into the problem of too many features! More menacingly, this is known as the curse of dimensionality, and it is addressed through the crucial step of feature selection. Feature Selection Too many features can slow down training, make a model less interpretable, and, most critically, reduce the model’s generalization performance on the test set. When we have irrelevant features, these drown out the important variables and as the number of features increases, the number of data points needed for the model to learn the relationship between the data and the target grows exponentially (curse of dimensionality explained). After going to all the work of making these features, we now have to select only those that are “most important” or equivalently, discard those that are irrelevant. The next notebook to go through is here: a guide to feature selection which is fairly comprehensive although it still does not cover every possible method! There are many ways to reduce the number of features, and here we will go through three methods: Collinear variables are variables that are highly correlated with one another. These variables are redundant in the sense that we only need to keep one out of each pair of collinear features in order to retain most of the information in both. The definition of highly correlated can vary and this is another of those numbers where there are no set rules! As an example of collinear variables, here is a plot of the median apartment size vs average apartment size: To identify highly correlated variables, we can calculate the correlation of every variable in the data with every other variable (this is quite a computationally expensive process)! Then we select the upper triangle of the correlation matrix and remove one variable from every pair of highly correlated variables based on a threshold. This is implemented in code below: (This code is adapted from this blog post.) In this implementation, I use a correlation coefficient threshold of 0.9 to remove collinear variables. So, for each pair of features with a correlation greater than 0.9, we remove one of the pair of features. Out of 1465 total features, this removes 583, indicating many of the variables we created were redundant. Of all the feature selection methods, this seems the most simple: just eliminate any columns above a certain percentage of missing values. However, even this operation brings in another choice to make, the threshold percentage of missing values for removing a column. Moreover, some models, such as the Gradient Boosting Machine in LightGBM, can handle missing values with no imputation and then we might not want to remove any columns at all! However, because we’ll eventually test several models requiring missing values to be imputed, we’ll remove any columns with more than 75% missing values in either the training or testing set. This threshold is not based on any theory or rule of thumb, rather it’s based on trying several options and seeing which worked best in practice. The most important point to remember when making these choices is that they don’t have to be made once and then forgotten. They can be revisited again later if the model is not performing as well as expected. Just make sure to record the steps you took and the performance metrics so you can see which works best! Dropping columns with more than 75% missing values removes 19 columns from the data, leaving us with 863 features. The last method we will use to select features is based on the results from a machine learning model. With decision tree based classifiers, such as ensembles of decision trees (random forests, extra trees, gradient boosting machines), we can extract and use a metric called the feature importances. The technical details of this is complicated (it has to do with the reduction in impurity from including the feature in the model), but we can use the relative importances to determine which features are the most helpful to a model. We can also use the feature importances to identify and remove the least helpful features to the model, including any with 0 importance. To find the feature importances, we will use a gradient boosting machine (GBM) from the LightGBM library. The model is trained using early stopping with two training iterations and the feature importances are averaged across the training runs to reduce the variance. Running this on the features identifies 308 features with 0.0 importance. Removing features with 0 importance is a pretty safe choice because these are features that are literally never used for splitting a node in any of the decision trees. Therefore, removing these features will have no impact on the model results (at least for this particular model). This isn’t necessary for feature selection, but because we have the feature importances, we can see which are the most relevant. To try and get an idea of what the model considers to make a prediction, we can visualize the top 15 most important features: We see that a number of the features we built made it into the top 15 which should give us some confidence that all our hard work was worthwhile! One of our features even made it into the top 5. This feature, client_installments_AMT_PAYMENT_min_sum represents the sum of the minimum installment payment for each client of their previous loans at Home Credit. That is, for each client,it is the sum of all the minimum payments they made on each of their previous loans. The feature importance don’t tell us whether a lower value of this variable corresponds to lower rates of default, it only lets us know that this feature is useful for making splits of decision trees nodes. Feature importances are useful, but they do not offer a completely clear interpretation of the model! After removing the 0 importance features, we have 536 features and another choice to make. If we think we still have too many features, we can start removing features that have a minimal amount of importance. In this case, I continued with feature selection because I wanted to test models besides the gbm that do not do as well with a large number of features. The final feature selection step we do is to retain only the features needed to account for 95% of the importance. According to the gradient boosting machine, 342 features are enough to cover 95% of the importance. The following plot shows the cumulative importance vs the number of features. There are a number of other dimensionality reduction techniques we can use, such as principal components analysis (PCA). This method is effective at reducing the number of dimensions, but it also transforms the features to a lower-dimension feature space where they have no physical representation, meaning that PCA features cannot be interpreted. Moreover, PCA assumes the data is normally distributed, which might not be a valid assumption for human-generated data. In the notebook I show how to use pca, but don’t actually apply it to the data. We can however use pca for visualizations. If we graph the first two principal components colored by the value of the TARGET , we get the following image: The two classes are not cleanly separated with only two principal components and clearly we need more than two features to identify customers who will repay a loan versus those who will not. Before moving on, we should record the feature selection steps we took so we remember them for future use: The final dataset has 342 features. If it seems like there are a few arbitrary choices made during feature selection, that’s because there were! At a later point, we might want to revisit some of these choices if we are not happy with our performance. Fortunately, because we wrote functions and documented our decisions, we can easily change some of the parameters and then reassess performance. Model Selection Generally, at this point in a machine learning problem, we would move on to evaluating a number of models. No model is better than all others at every task (the “no free lunch theorem”), and so we need to try out a range of models to determine which one to use. However, in recent years, one model has become increasingly successful for problems with medium-sized structured data: the gradient boosting machine. (There are a number of reasons why this model works so well, and for a comprehensive guide, this Master’s Thesis is a great read.) Model selection is one area where I relied heavily on the work of others. As mentioned at the beginning of the post, prior to this competition, my go-to model was the random forest. Very early on in this competition though, it was clear from reading the notebooks of others that I would need to implement some version of the gradient boosting machine in order to compete. Nearly every submission at the top of the leaderboard on Kaggle uses some variation (or multiple versions) of the Gradient Boosting Machine. (Some of the libraries you might see used are LightGBM, CatBoost, and XGBoost.) Over the past few weeks, I have read through a number of kernels (see here and here) and now feel pretty confident deploying the Gradient Boosting Machine using the LightGBM library (Scikit-Learn does have a GBM, but its not as efficient or as accurate as other libraries). Nonetheless, mostly for curiosity’s sake, I wanted to try several other methods to see just how much is gained from the GBM. The code for this testing can be found on Kaggle here. This isn’t entirely a fair comparison because I was using mostly the default hyperparameters in Scikit-Learn, but it should give us a first approximation of the capabilities of several different models. Using the dataset after applying all of the feature engineering and the feature selection, below are the modeling results with the public leaderboard scores. All of the models except for the LightGBM are built in Scikit-Learn: It turns out everyone else was right: the gradient boosting machine is the way to go. It returns the best performance out of the box and has a number of hyperparameters that we can adjust for even better scores. That does not mean we should forget about other models, because sometimes adding the predictions of multiple models together (called ensembling) can perform better than a single model by itself. In fact, many winners of Kaggle competitions used some form of ensembling in their final models. We didn’t spend too much time here on the models, but that is where our focus will shift in the next notebooks and articles. Next we can work on optimizing the best model, the gradient boosting machine, using hyperparameter optimization. We may also look at averaging models together or even stacking multiple models to make predictions. We might even go back and redo feature engineering! The most important points are that we need to keep experimenting to find what works best, and we can read what others have done to try and build on their work. Conclusions Important character traits of being a data scientist are curiosity and admitting you don’t know everything! From my place on the leaderboard, I clearly don’t know the best approach to this problem, but I’m willing to keep trying different things and learn from others. Kaggle competitions are just toy problems, but that doesn’t prevent us from using them to learn and practice concepts to apply to real projects. In this article we covered a number of important machine learning topics: After going through all this work, we were able to improve our leaderboard score from 0.678 to 0.779, in the process moving over a 1000 spots up the leaderboard. Next, our focus will shift to optimizing our selected algorithm, but we also won’t hesitate to revisit feature engineering/selection. If you want to stay up-to-date on my machine learning progress, you can check out my work on Kaggle: the notebooks are coming a little faster than the articles at this point! Feel free to get started on Kaggle using these notebooks and start contributing to the community. I’ll be using this Kaggle competition to explore a few interesting machine learning ideas such as Automated Feature Engineering and Bayesian Hyperparameter Optimization. I plan on learning as much from this competition as possible, and I’m looking forward to exploring and sharing these new techniques! As always, I welcome constructive criticism and feedback and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Over 1000 places gained on the leaderboard Feature engineering to go from 122 features to 1465 Feature selection to reduce the final number of features to 342 Decision to use a gradient boosting machine learning model Feature selection Model evaluation Removing variables with many missing values Using feature importances to keep only “important” variables Remove columns with more than 75% missing values: 19 features removed Remove 0.0 importance features according to a GBM: 308 features removed Keep only features needed for 95% of feature importance: 193 features removed Random Forest with 1000 trees = 0.708 Extra Trees with 1000 trees = 0.725 Gradient Boosting Machine in Scikit-Learn with 1000 trees = 0.761 Gradient Boosting Machine in LightGBM with 1000 trees = 0.779 Average of all Models = 0.771 Applying feature selection to remove irrelevant features Evaluating several machine learning models for applicability to the task Lock yourself away from the outside world and work in isolation An increase in ROC AUC from a baseline of 0.678 to 0.779 Feature engineering Removing collinear variables Remove collinear variables with a correlation coefficient greater than 0.9: 583 features removed Logistic Regression = 0.768 Using feature engineering to construct new features from multiple related tables of information",Machine Learning Kaggle Competition Part Two: Improving,7,published,11321,5228,0.2869166029074216,26,1,1,1,1,0
61,6400,339.2290511584491,1205,https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0,35,Towards Data Science,2018-06-22 10:12:00,25.54,10,20779,2018-06-21 16:17:00,"['Machine Learning', 'Python', 'Education', 'Programming', 'Towards Data Science']","A Feature Selection Tool for Machine Learning in Python Using the FeatureSelector for efficient machine learning workflows Feature selection, the process of finding and selecting the most useful features in a dataset, is a crucial step of the machine learning pipeline. Unnecessary features decrease training speed, decrease model interpretability, and, most importantly, decrease generalization performance on the test set. Frustrated by the ad-hoc feature selection methods I found myself applying over and over again for machine learning problems, I built a class for feature selection in Python available on GitHub. The FeatureSelector includes some of the most common feature selection methods: In this article we will walk through using the FeatureSelector on an example machine learning dataset. We’ll see how it allows us to rapidly implement these methods, allowing for a more efficient workflow. The complete code is available on GitHub and I encourage any contributions. The Feature Selector is a work in progress and will continue to improve based on the community needs! For this example, we will use a sample of data from the Home Credit Default Risk machine learning competition on Kaggle. (To get started with the competition, see this article). The entire dataset is available for download and here we will use a sample for illustration purposes. The competition is a supervised classification problem and this is a good dataset to use because it has many missing values, numerous highly correlated (collinear) features, and a number of irrelevant features that do not help a machine learning model. To create an instance of the FeatureSelector class, we need to pass in a structured dataset with observations in the rows and features in the columns. We can use some of the methods with only features, but the importance-based methods also require training labels. Since we have a supervised classification task, we will use a set of features and a set of labels. (Make sure to run this in the same directory as feature_selector.py ) The feature selector has five methods for finding features to remove. We can access any of the identified features and remove them from the data manually, or use the remove function in the Feature Selector. Here we will go through each of the identification methods and also show how all 5 can be run at once. The FeatureSelector additionally has several plotting capabilities because visually inspecting data is a crucial component of machine learning. Missing Values The first method for finding features to remove is straightforward: find features with a fraction of missing values above a specified threshold. The call below identifies features with more than 60% missing values (bold is output). We can see the fraction of missing values in every column in a dataframe: To see the features identified for removal, we access the ops attribute of the FeatureSelector , a Python dict with features as lists in the values. Finally, we have a plot of the distribution of missing values in all feature: Collinear Features Collinear features are features that are highly correlated with one another. In machine learning, these lead to decreased generalization performance on the test set due to high variance and less model interpretability. The identify_collinear method finds collinear features based on a specified correlation coefficient value. For each pair of correlated features, it identifies one of the features for removal (since we only need to remove one): A neat visualization we can make with correlations is a heatmap. This shows all the features that have at least one correlation above the threshold: As before, we can access the entire list of correlated features that will be removed, or see the highly correlated pairs of features in a dataframe. If we want to investigate our dataset, we can also make a plot of all the correlations in the data by passing in plot_all = True to the call: Zero Importance Features The previous two methods can be applied to any structured dataset and are deterministic — the results will be the same every time for a given threshold. The next method is designed only for supervised machine learning problems where we have labels for training a model and is non-deterministic. The identify_zero_importance function finds features that have zero importance according to a gradient boosting machine (GBM) learning model. With tree-based machine learning models, such as a boosting ensemble, we can find feature importances. The absolute value of the importance is not as important as the relative values, which we can use to determine the most relevant features for a task. We can also use feature importances for feature selection by removing zero importance features. In a tree-based model, the features with zero importance are not used to split any nodes, and so we can remove them without affecting model performance. The FeatureSelector finds feature importances using the gradient boosting machine from the LightGBM library. The feature importances are averaged over 10 training runs of the GBM in order to reduce variance. Also, the model is trained using early stopping with a validation set (there is an option to turn this off) to prevent overfitting to the training data. The code below calls the method and extracts the zero importance features: The parameters we pass in are as follows: This time we get two plots with plot_feature_importances: On the left we have the plot_n most important features (plotted in terms of normalized importance where the total sums to 1). On the right we have the cumulative importance versus the number of features. The vertical line is drawn at threshold of the cumulative importance, in this case 99%. Two notes are good to remember for the importance-based methods: This should not have a major impact (the most important features will not suddenly become the least) but it will change the ordering of some of the features. It also can affect the number of zero importance features identified. Don’t be surprised if the feature importances change every time! When we get to the feature removal stage, there is an option to remove any added one-hot encoded features. However, if we are doing machine learning after feature selection, we will have to one-hot encode the features anyway! Low Importance Features The next method builds on zero importance function, using the feature importances from the model for further selection. The function identify_low_importance finds the lowest importance features that do not contribute to a specified total importance. For example, the call below finds the least important features that are not required for achieving 99% of the total importance: Based on the plot of cumulative importance and this information, the gradient boosting machine considers many of the features to be irrelevant for learning. Again, the results of this method will change on each training run. To view all the feature importances in a dataframe: The low_importance method borrows from one of the methods of using Principal Components Analysis (PCA) where it is common to keep only the PC needed to retain a certain percentage of the variance (such as 95%). The percentage of total importance accounted for is based on the same idea. The feature importance based methods are really only applicable if we are going to use a tree-based model for making predictions. Besides being stochastic, the importance-based methods are a black-box approach in that we don’t really know why the model considers the features to be irrelevant. If using these methods, run them several times to see how the results change, and perhaps create multiple datasets with different parameters to test! Single Unique Value Features The final method is fairly basic: find any columns that have a single unique value. A feature with only one unique value cannot be useful for machine learning because this feature has zero variance. For example, a tree-based model can never make a split on a feature with only one value (since there are no groups to divide the observations into). There are no parameters here to select, unlike the other methods: We can plot a histogram of the number of unique values in each category: One point to remember is NaNs are dropped before calculating unique values in Pandas by default. Removing Features Once we’ve identified the features to discard, we have two options for removing them. All of the features to remove are stored in the ops dict of the FeatureSelector and we can use the lists to remove features manually. Another option is to use the remove built-in function. For this method, we pass in the methods to use to remove features. If we want to use all the methods implemented, we just pass in methods = 'all'. This method returns a dataframe with the features removed. To also remove the one-hot encoded features that are created during machine learning: It might be a good idea to check the features that will be removed before going ahead with the operation! The original dataset is stored in the data attribute of the FeatureSelector as a back-up! Running all Methods at Once Rather than using the methods individually, we can use all of them with identify_all. This takes a dictionary of the parameters for each method: Notice that the number of total features will change because we re-ran the model. The remove function can then be called to discard these features. Conclusions The Feature Selector class implements several common operations for removing features before training a machine learning model. It offers functions for identifying features for removal as well as visualizations. Methods can be run individually or all at once for efficient workflows. The missing, collinear, and single_unique methods are deterministic while the feature importance-based methods will change with each run. Feature selection, much like the field of machine learning, is largely empirical and requires testing multiple combinations to find the optimal answer. It’s best practice to try several configurations in a pipeline, and the Feature Selector offers a way to rapidly evaluate parameters for feature selection. As always, I welcome feedback and constructive criticism. I want to emphasis that I’m looking for help on the FeatureSelector. Anyone can contribute on GitHub and I appreciate advice from those who just uses the tool! I can also be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Collinear (highly correlated) features Features with zero importance in a tree-based model Features with low importance Features with a single unique value eval_metric: metric to use for early stopping (not necessary if early stopping is disabled) n_iterations : number of training runs to average the feature importances over early_stopping: whether or not use early stopping for training the model Features with a high percentage of missing values task : either “classification” or “regression” corresponding to our problem Training the gradient boosting machine is stochastic meaning the feature importances will change every time the model is run To train the machine learning model, the features are first one-hot encoded. This means some of the features identified as having 0 importance might be one-hot encoded features added during modeling.",A Feature Selection Tool for Machine Learning in Python,9,published,81352,1998,3.2032032032032034,0,1,0,1,1,1
60,2900,337.3028329137385,405,https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f,14,Towards Data Science,2018-06-24 08:26:00,22.08,14,5990,2018-06-23 10:55:00,"['Machine Learning', 'Education', 'Bayesian Machine Learning', 'Computer Science', 'Towards Data Science']","A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning The concepts behind efficient hyperparameter tuning using Bayesian optimization Following are four common methods of hyperparameter optimization for machine learning in order of increasing efficiency: (There are also other methods such as evolutionary and gradient-based.) I was pretty proud that I’d recently moved up the ladder from manual to random search until I found this image deep in a paper by Bergstra et al.: These figures compare validation error for hyperparameter optimization of an image classification neural network with random search in grey and Bayesian Optimization (using the Tree Parzen Estimator or TPE) in green. Lower is better: a smaller validation set error generally means better test set performance, and a smaller number of trials means less time invested. Clearly, there are significant advantages to Bayesian methods, and these graphs, along with other impressive results, convinced me it was time to take the next step and learn model-based hyperparameter optimization. The one-sentence summary of Bayesian hyperparameter optimization is: build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function. If you like to operate at a very high level, then this sentence may be all you need. However, if you want to understand the details, this article is my attempt to outline the concepts behind Bayesian optimization, in particular Sequential Model-Based Optimization (SMBO) with the Tree Parzen Estimator (TPE). With the mindset that you don’t know a concept until you can explain it to others, I went through several academic papers and will try to communicate the results in a (relatively) easy to understand format. Although we can often implement machine learning methods without understanding how they work, I like to try and get an idea of what is going on so I can use the technique as effectively as possible. In later articles I’ll walk through using these methods in Python using libraries such as Hyperopt, so this article will lay the conceptual groundwork for implementations to come! Update: Here is a brief Jupyter Notebook showing the basics of using Bayesian Model-Based Optimization in the Hyperopt Python library. Hyperparameter Optimization The aim of hyperparameter optimization in machine learning is to find the hyperparameters of a given machine learning algorithm that return the best performance as measured on a validation set. (Hyperparameters, in contrast to model parameters, are set by the machine learning engineer before training. The number of trees in a random forest is a hyperparameter while the weights in a neural network are model parameters learned during training. I like to think of hyperparameters as the model settings to be tuned.) Hyperparameter optimization is represented in equation form as: Here f(x) represents an objective score to minimize— such as RMSE or error rate— evaluated on the validation set; x* is the set of hyperparameters that yields the lowest value of the score, and x can take on any value in the domain X. In simple terms, we want to find the model hyperparameters that yield the best score on the validation set metric. The problem with hyperparameter optimization is that evaluating the objective function to find the score is extremely expensive. Each time we try different hyperparameters, we have to train a model on the training data, make predictions on the validation data, and then calculate the validation metric. With a large number of hyperparameters and complex models such as ensembles or deep neural networks that can take days to train, this process quickly becomes intractable to do by hand! Grid search and random search are slightly better than manual tuning because we set up a grid of model hyperparameters and run the train-predict -evaluate cycle automatically in a loop while we do more productive things (like feature engineering). However, even these methods are relatively inefficient because they do not choose the next hyperparameters to evaluate based on previous results. Grid and random search are completely uninformed by past evaluations, and as a result, often spend a significant amount of time evaluating “bad” hyperparameters. For example, if we have the following graph with a lower score being better, where does it make sense to concentrate our search? If you said below 200 estimators, then you already have the idea of Bayesian optimization! We want to focus on the most promising hyperparameters, and if we have a record of evaluations, then it makes sense to use this information for our next choice. Random and grid search pay no attention to past results at all and would keep searching across the entire range of the number of estimators even though it’s clear the optimal answer (probably) lies in a small region! Bayesian Optimization Bayesian approaches, in contrast to random or grid search, keep track of past evaluation results which they use to form a probabilistic model mapping hyperparameters to a probability of a score on the objective function: In the literature, this model is called a “surrogate” for the objective function and is represented as p(y | x). The surrogate is much easier to optimize than the objective function and Bayesian methods work by finding the next set of hyperparameters to evaluate on the actual objective function by selecting hyperparameters that perform best on the surrogate function. In other words: The aim of Bayesian reasoning is to become “less wrong” with more data which these approaches do by continually updating the surrogate probability model after each evaluation of the objective function. At a high-level, Bayesian optimization methods are efficient because they choose the next hyperparameters in an informed manner. The basic idea is: spend a little more time selecting the next hyperparameters in order to make fewer calls to the objective function. In practice, the time spent selecting the next hyperparameters is inconsequential compared to the time spent in the objective function. By evaluating hyperparameters that appear more promising from past results, Bayesian methods can find better model settings than random search in fewer iterations. If there’s one thing to take away from this article it’s that Bayesian model-based methods can find better hyperparameters in less time because they reason about the best set of hyperparameters to evaluate based on past trials. As a good visual description of what is occurring in Bayesian Optimization take a look at the images below (source). The first shows an initial estimate of the surrogate model — in black with associated uncertainty in gray — after two evaluations. Clearly, the surrogate model is a poor approximation of the actual objective function in red: The next image shows the surrogate function after 8 evaluations. Now the surrogate almost exactly matches the true function. Therefore, if the algorithm selects the hyperparameters that maximize the surrogate, they will likely yield very good results on the true evaluation function. Bayesian methods have always made sense to me because they operate in much the same way we do: we form an initial view of the world (called a prior) and then we update our model based on new experiences (the updated model is called a posterior). Bayesian hyperparameter optimization takes that framework and applies it to finding the best value of model settings! Sequential model-based optimization (SMBO) methods (SMBO) are a formalization of Bayesian optimization. The sequential refers to running trials one after another, each time trying better hyperparameters by applying Bayesian reasoning and updating a probability model (surrogate). There are five aspects of model-based hyperparameter optimization: There are several variants of SMBO methods that differ in steps 3–4, namely, how they build a surrogate of the objective function and the criteria used to select the next hyperparameters. Several common choices for the surrogate model are Gaussian Processes, Random Forest Regressions, and Tree Parzen Estimators (TPE) while the most common choice for step 4 is Expected Improvement. In this post, we will focus on TPE and Expected Improvement. In the case of random search and grid search, the domain of hyperparameters we search is a grid. An example for a random forest is shown below: For a model-based approach, the domain consists of probability distributions. As with a grid, this lets us encode domain knowledge into the search process by placing greater probability in regions where we think the true best hyperparameters lie. If we wanted to express the above grid as a probability distribution, it may look something like this: Here we have a uniform, log-normal, and normal distribution. These are informed by prior practice/knowledge (for example the learning rate domain is usually a log-normal distribution over several orders of magnitude). The objective function takes in hyperparameters and outputs a single real-valued score that we want to minimize (or maximize). As an example, let’s consider the case of building a random forest for a regression problem. The hyperparameters we want to optimize are shown in the hyperparameter grid above and the score to minimize is the Root Mean Squared Error. Our objective function would then look like (in Python): While the objective function looks simple, it is very expensive to compute! If the objective function could be quickly calculated, then we could try every single possible hyperparameter combination (like in grid search). If we are using a simple model, a small hyperparameter grid, and a small dataset, then this might be the best way to go. However, in cases where the objective function may take hours or even days to evaluate, we want to limit calls to it. The entire concept of Bayesian model-based optimization is to reduce the number of times the objective function needs to be run by choosing only the most promising set of hyperparameters to evaluate based on previous calls to the evaluation function. The next set of hyperparameters are selected based on a model of the objective function called a surrogate. The surrogate function, also called the response surface, is the probability representation of the objective function built using previous evaluations. This is called sometimes called a response surface because it is a high-dimensional mapping of hyperparameters to the probability of a score on the objective function. Below is a simple example with only two hyperparameters: There are several different forms of the surrogate function including Gaussian Processes and Random Forest regression. However, in this post we will focus on the Tree-structured Parzen Estimator as put forward by Bergstra et al in the paper “Algorithms for Hyper-Parameter Optimization”. These methods differ in how they construct the surrogate function which we’ll explain in just a bit. First we need to talk about the selection function. The selection function is the criteria by which the next set of hyperparameters are chosen from the surrogate function. The most common choice of criteria is Expected Improvement: Here y* is a threshold value of the objective function, x is the proposed set of hyperparameters, y is the actual value of the objective function using hyperparameters x, and p(y | x) is the surrogate probability model expressing the probability of y given x. If that’s all a little much, in simpler terms, the aim is to maximize the Expected Improvement with respect to x. This means finding the best hyperparameters under the surrogate function p (y | x). If p (y | x) is zero everywhere that y < y*, then the hyperparameters x are not expected to yield any improvement. If the integral is positive, then it means that the hyperparameters x are expected to yield a better result than the threshold value. Tree-structured Parzen Estimator (TPE) Now let’s get back to the surrogate function. The methods of SMBO differ in how they construct the surrogate model p(y | x). The Tree-structured Parzen Estimator builds a model by applying Bayes rule. Instead of directly representing p( y | x), it instead uses: p (x | y), which is the probability of the hyperparameters given the score on the objective function, in turn is expressed: where y < y* represents a lower value of the objective function than the threshold. The explanation of this equation is that we make two different distributions for the hyperparameters: one where the value of the objective function is less than the threshold, l(x), and one where the value of the objective function is greater than the threshold, g(x). Let’s update our Random Forest graph to include a threshold: Now we construct two probability distributions for the number of estimators, one using the estimators that yielded values under the threshold and one using the estimators that yielded values above the threshold. Intuitively, it seems that we want to draw values of x from l(x) and not from g(x) because this distribution is based only on values of x that yielded lower scores than the threshold. It turns out this is exactly what the math says as well! With Bayes Rule, and a few substitutions, the expected improvement equation (which we are trying to maximize) becomes: The term on the far right is the most important part. What this says is that the Expected Improvement is proportional to the ratio l(x) / g(x) and therefore, to maximize the Expected Improvement, we should maximize this ratio. Our intuition was correct: we should draw values of the hyperparameters which are more likely under l(x) than under g(x)! The Tree-structured Parzen Estimator works by drawing sample hyperparameters from l(x), evaluating them in terms of l(x) / g(x), and returning the set that yields the highest value under l(x) / g(x) corresponding to the greatest expected improvement. These hyperparameters are then evaluated on the objective function. If the surrogate function is correct, then these hyperparameters should yield a better value when evaluated! The expected improvement criteria allows the model to balance exploration versus exploitation. l(x) is a distribution and not a single value which means that the hyperparameters drawn are likely close but not exactly at the maximum of the expected improvement. Moreover, because the surrogate is just an estimate of the objective function, the selected hyperparameters may not actually yield an improvement when evaluated and the surrogate model will have to be updated. This updating is done based on the current surrogate model and the history of objective function evaluations. Each time the algorithm proposes a new set of candidate hyperparameters, it evaluates them with the actual objective function and records the result in a pair (score, hyperparameters). These records form the history. The algorithm builds l(x) and g(x) using the history to come up with a probability model of the objective function that improves with each iteration. This is Bayes’ Rule at work: we have an initial estimate for the surrogate of the objective function that we update as we gather more evidence. Eventually, with enough evaluations of the objective function, we hope that our model accurately reflects the objective function and the hyperparameters that yield the greatest Expected Improvement correspond to the hyperparameters that maximize the objective function. Putting it All Together How do Sequential Model-Based Methods help us more efficiently search the hyperparameter space? Because the algorithm is proposing better candidate hyperparameters for evaluation, the score on the objective function improves much more rapidly than with random or grid search leading to fewer overall evaluations of the objective function. Even though the algorithm spends more time selecting the next hyperparameters by maximizing the Expected Improvement, this is much cheaper in terms of computational cost than evaluating the objective function. In a paper about using SMBO with TPE, the authors reported that finding the next proposed set of candidate hyperparameters took several seconds, while evaluating the actual objective function took hours. If we are using better-informed methods to choose the next hyperparameters, that means we can spend less time evaluating poor hyperparameter choices. Furthermore, sequential model-based optimization using tree-structured Parzen estimators is able to find better hyperparameters than random search in the same number of trials. In other words, we get Hopefully, this has convinced you Bayesian model-based optimization is a technique worth trying! Fortunately for us, there are now a number of libraries that can do SMBO in Python. Spearmint and MOE use a Gaussian Process for the surrogate, Hyperopt uses the Tree-structured Parzen Estimator, and SMAC uses a Random Forest regression. These libraries all use the Expected Improvement criterion to select the next hyperparameters from the surrogate model. In later articles we will take a look at using Hyperopt in Python and there are already several good articles and code examples for learning. Conclusions Bayesian model-based optimization methods build a probability model of the objective function to propose smarter choices for the next set of hyperparameters to evaluate. SMBO is a formalization of Bayesian optimization which is more efficient at finding the best hyperparameters for a machine learning model than random or grid search. Sequential model-based optimization methods differ in they build the surrogate, but they all rely on information from previous trials to propose better hyperparameters for the next evaluation. The Tree Parzen Estimator is one algorithm that uses Bayesian reasoning to construct the surrogate model and can select the next hyperparameters using Expected Improvement. There are a number of libraries to implement SMBO in Python which we will explore in further articles. The concepts are a little tough at first, but understanding them will allow us to use the tools built on them more effectively. I’d like to mention I’m still trying to work my way through the details and if I’ve made a mistake, please let me know (civilly)! For more details, the following articles are extremely helpful: As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Grid search Random search Bayesian model-based optimization Find the hyperparameters that perform best on the surrogate Apply these hyperparameters to the true objective function Update the surrogate model incorporating the new results Repeat steps 2–4 until max iterations or time is reached An objective function which takes in hyperparameters and outputs a score that we want to minimize (or maximize) The surrogate model of the objective function A criteria, called a selection function, for evaluating which hyperparameters to choose next from the surrogate model A history consisting of (score, hyperparameter) pairs used by the algorithm to update the surrogate model Better scores on the testing set Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures [Link] Bayesian Optimization Primer [Link] Taking the Human Out of the Loop: A Review of Bayesian Optimization [Link] Manual Build a surrogate probability model of the objective function A domain of hyperparameters over which to search Reduced running time of hyperparameter tuning Algorithms for Hyper-Parameter Optimization [Link]",A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning,10,published,27133,3393,0.8547008547008547,0,1,0,1,1,0
57,3100,333.2982867714815,482,https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0,13,Towards Data Science,2018-06-28 08:33:00,27.46,9,9053,2018-06-27 16:00:00,"['Machine Learning', 'Education', 'Towards Data Science', 'Data Science', 'Python']","An Introductory Example of Bayesian Optimization in Python with Hyperopt A hands-on example for learning the foundations of a powerful optimization framework Although finding the minimum of a function might seem mundane, it’s a critical problem that extends to many domains. For example, optimizing the hyperparameters of a machine learning model is just a minimization problem: it means searching for the hyperparameters with the lowest validation loss. Bayesian optimization is a probabilistic model based approach for finding the minimum of any function that returns a real-value metric. This function may be as simple as f(x) = x², or it can be as complex as the validation error of a deep neural network with respect to hundreds of model architecture and hyperparameter choices. Recent results suggest Bayesian hyperparameter optimization of machine learning models is more efficient than manual, random, or grid search with: Clearly, a method this powerful has to be extremely hard to use right? Fortunately, there are a number of Python libraries such as Hyperopt that allow for simple applications of Bayesian optimization. In fact, we can do basic Bayesian optimization in one line! If you can understand everything in the above code, then you can probably stop reading and start using this method. If you want a little more explanation, in this article, we’ll go through the basic structure of a Hyperopt program so later we can expand this framework to more complex problems, such as machine learning hyperparameter optimization. The code for this article is available in a Jupyter Notebook on GitHub. Optimization is finding the input value or set of values to an objective function that yields the lowest output value, called a “loss”. The objective function f(x) = x² has a single input and is a 1-D optimization problem. Typically, in machine learning, our objective function is many-dimensional because it takes in a set of model hyperparameters. For simple functions in low dimensions, we can find the minimum loss by trying many input values and seeing which one yields the lowest loss. We could create a grid of input values and try all of them — grid search — or randomly pick some values — random search. As long as evaluations of the objective function (“evals”) are cheap, these uninformed methods might be adequate. However, for complex objective functions like the 5-fold cross validation error of a neural network, each eval of the objective function means training the network 5 times! For models that take days to train, we want a way to limit calls to the evaluation function. Random search is actually more efficient than grid search for problems with high dimensions, but is still an uniformed method where the search does not use previous results to pick the next input values to try. Let’s see if you’re smarter than random search. Say we have the following results from training a random forest for a regression task: If you were picking the next number of trees to evaluate, where would you concentrate? Clearly, the best option is around 100 trees because a smaller number of trees has tended to yield a lower loss. You’ve basically just done Bayesian optimization in your head: using the previous results, you formed a probabilistic model of the objective function which said that a smaller number of trees is likely to result in a lower error. Bayesian optimization, also called Sequential Model-Based Optimization (SMBO), implements this idea by building a probability model of the objective function that maps input values to a probability of a loss: p (loss | input values). The probability model, also called the surrogate or response surface, is easier to optimize than the actual objective function. Bayesian methods select the next values to evaluate by applying a criteria (usually Expected Improvement) to the surrogate. The concept is to limit evals of the objective function by spending more time choosing the next values to try. Bayesian Reasoning means updating a model based on new evidence, and, with each eval, the surrogate is re-calculated to incorporate the latest information. The longer the algorithm runs, the closer the surrogate function comes to resembling the actual objective function. Bayesian Optimization methods differ in how they construct the surrogate function: common choices include Gaussian Processes, Random Forest Regression, and, the choice in Hyperopt, the Tree Parzen Estimator (TPE). The details of these methods can be a little tough to understand (I wrote a high-level overview here), and it’s also difficult to figure out which works the best: if you read articles by the designers of the algorithms, each claim their method is superior! However, the particular algorithms does not matter as much as upgrading from random/grid search to Bayesian Optimization. Using any library (Spearmint, Hyperopt, SMAC) will be fine for getting started! With that in mind, let’s see how to put Bayesian optimization into practice. Formulating an optimization problem in Hyperopt requires four parts: Once we know how to specify these four parts, they can be applied to any optimization problem. For now, we will walk through a basic problem. The objective function can be any function that returns a real value that we want to minimize. (If we have a value that we want to maximize, such as accuracy, then we just have our function return the negative of that metric.) Here we will use polynomial function with the code and graph shown below: This problem is 1-D because we are optimizing over a single value, x. In Hyperopt, the objective function can take in any number of inputs but must return a single loss to minimize. The domain space is the input values over which we want to search. As a first try, we can use a uniform distribution over the range that our function is defined: To visualize the domain, we can draw samples from the space and plot the histogram: If we have an idea where the best values are, then we can make a smarter domain that places more probability in higher scoring regions. (See the notebook for an example of using a normal distribution on this problem.) While this is technically the most difficult concept, in Hyperopt creating an optimization algorithm only requires one line. We are using the Tree-structured Parzen Estimator model, and we can have Hyperopt configure it for us using the suggest method. There’s a lot of theory going on behind the scenes we don’t have to worry about! In the notebook, we also use a random search algorithm for comparison. This is not strictly necessary as Hyperopt keeps track of the results for the algorithm internally. However, if we want to inspect the progression of the alogorithm, we need to create a Trials object that will record the values and the scores: Now that the problem is defined, we can minimize our objective function! To do so, we use the fmin function that takes the four parts above, as well as a maximum number of trials: For this run, the algorithm found the best value of x (the one which minimizes the loss) in just under 1000 trials. The best object only returns the input value that minimizes the function. While this is what we are looking for, it doesn’t give us much insight into the method. To get more details, we can get the results from the trials object: Visualizations are useful for an intuitive understanding of what is occurring. For example, let’s plot the values of x evaluated in order: Over time, the input values cluster around the optimal indicated by the red line. This is a simple problem, so the algorithm does not have much trouble finding the best value of x. To contrast with what a naive search looks like, if we run the same problem with random search we get the following figure: The random search basically tries values, well, at random! The differences between the values become even more apparent when we look at the histogram of values for x of the TPE algorithm and random search: Here we see the main benefit of Bayesian model-based optimization: more concentration on promising input values. When we are searching over dozens of parameters and each eval takes hours or days, reducing the number of evals is critical. Bayesian optimization minimizes the number of evals by reasoning based on previous results what input values should be tried in the future. (In this case, random search actually finds a value of x very close to the optimal because of the basic 1-D objective function and the number of evals.) Once we have mastered how to minimize a simple function, we can extend this to any problem where we need to optimize a function that returns a real value. For example, to tune the hyperparameters of a machine learning model requires only a few adjustments to the basic framework: the objective function must take in the model hyperparameters and return the validation loss, and the domain space will need to be specific to the model. For an idea what this looks like, I wrote a notebook where I tune the hyperparameters of a gradient boosting machine which will be the next article! Conclusions Bayesian model-based optimization is intuitive: choose the next input values to evaluate based on the past results to concentrate the search on more promising values. The end outcome is a reduction in the total number of search iterations compared to uninformed random or grid search methods. Although this was only a simple example, we can take the concepts here and use them in a wide variety of useful situations. The takeaways from this article are: As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Less time required for optimization Domain space: the range of input values to evaluate Optimization Algorithm: the method used to construct the surrogate function and choose the next values to evaluate Results: score, value pairs that the algorithm uses to build the model The surrogate is informed by past search results and, by choosing the next values from this model, the search is concentrated on promising values The overall outcome of these method is reduced search time and better values These powerful techniques can be implemented easily in Python libraries like Hyperopt The Bayesian optimization framework can be extended to complex problems including hyperparameter tuning of machine learning models Better overall performance on the test set Objective Function: takes in an input and returns a loss to minimize Bayesian Optimization is an efficient method for finding the minimum of a function that works by constructing a probabilistic (surrogate) model of the objective function",An Introductory Example of Bayesian Optimization in Python with Hyperopt,10,published,32963,1957,1.5840572304547778,0,1,1,1,1,1
56,2900,328.1929925208449,467,https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a,15,Towards Data Science,2018-07-03 11:04:00,14.42,18,7115,2018-06-22 19:07:00,"['Machine Learning', 'Data Science', 'Towards Data Science', 'Education', 'Automation']","Automated Machine Learning Hyperparameter Tuning in Python A complete walk through using Bayesian optimization for automated hyperparameter tuning in Python Tuning machine learning hyperparameters is a tedious yet crucial task, as the performance of an algorithm can be highly dependent on the choice of hyperparameters. Manual tuning takes time away from important steps of the machine learning pipeline like feature engineering and interpreting results. Grid and random search are hands-off, but require long run times because they waste time evaluating unpromising areas of the search space. Increasingly, hyperparameter tuning is done by automated methods that aim to find optimal hyperparameters in less time using an informed search with no manual effort necessary beyond the initial set-up. Bayesian optimization, a model-based method for finding the minimum of a function, has recently been applied to machine learning hyperparameter tuning, with results suggesting this approach can achieve better performance on the test set while requiring fewer iterations than random search. Moreover, there are now a number of Python libraries that make implementing Bayesian hyperparameter tuning simple for any machine learning model. In this article, we will walk through a complete example of Bayesian hyperparameter tuning of a gradient boosting machine using the Hyperopt library. In an earlier article I outlined the concepts behind this method, so here we will stick to the implementation. Like with most machine learning topics, it’s not necessary to understand all the details, but knowing the basic idea can help you use the technique more effectively! All the code for this article is available as a Jupyter Notebook on GitHub. As a brief primer, Bayesian optimization finds the value that minimizes an objective function by building a surrogate function (probability model) based on past evaluation results of the objective. The surrogate is cheaper to optimize than the objective, so the next input values to evaluate are selected by applying a criterion to the surrogate (often Expected Improvement). Bayesian methods differ from random or grid search in that they use past evaluation results to choose the next values to evaluate. The concept is: limit expensive evaluations of the objective function by choosing the next input values based on those that have done well in the past. In the case of hyperparameter optimization, the objective function is the validation error of a machine learning model using a set of hyperparameters. The aim is to find the hyperparameters that yield the lowest error on the validation set in the hope that these results generalize to the testing set. Evaluating the objective function is expensive because it requires training the machine learning model with a specific set of hyperparameters. Ideally, we want a method that can explore the search space while also limiting evaluations of poor hyperparameter choices. Bayesian hyperparameter tuning uses a continually updated probability model to “concentrate” on promising hyperparameters by reasoning from past results. Python Options There are several Bayesian optimization libraries in Python which differ in the algorithm for the surrogate of the objective function. In this article, we will work with Hyperopt, which uses the Tree Parzen Estimator (TPE) Other Python libraries include Spearmint (Gaussian Process surrogate) and SMAC (Random Forest Regression). There is a lot of interesting work going on in this area, so if you aren’t happy with one library, check out the alternatives! The general structure of a problem (which we will walk through here) translates between the libraries with only minor differences in syntax. For a basic introduction to Hyperopt, see this article. There are four parts to a Bayesian Optimization problem: With those four pieces, we can optimize (find the minimum) of any function that returns a real value. This is a powerful abstraction that lets us solve many problems in addition to tuning machine learning hyperparameters. Dataset For this example, we will use the Caravan Insurance dataset where the objective is to predict whether a customer will purchase an insurance policy. This is a supervised classification problem with 5800 training observations and 4000 testing points. The metric we will use to assess performance is the Receiver Operating Characteristic Area Under the Curve (ROC AUC) because this is an imbalanced classification problem. (A higher ROC AUC is better with a score of 1 indicating a perfect model). The dataset is shown below: Because Hyperopt requires a value to minimize, we will return 1-ROC AUC from the objective function, thereby driving up the ROC AUC. Gradient Boosting Model Detailed knowledge of the gradient boosting machine (GBM) is not necessary for this article and here are the basics we need to understand: The GBM is an ensemble boosting method based on using weak learners (almost always decision trees) trained sequentially to form a strong model. There are many hyperparameters in a GBM controlling both the entire ensemble and individual decision trees. One of the most effective methods for choosing the number of trees (called estimators) is early stopping which we will use. LightGBM provides a fast and simple implementation of the GBM in Python. For more details on the GBM, here’s a high level article and a technical paper. With the necessary background out of the way, let’s go through writing the four parts of a Bayesian optimization problem for hyperparameter tuning. The objective function is what we are trying to minimize. It takes in a set of values — in this case hyperparameters for the GBM — and outputs a real value to minimize — the cross validation loss. Hyperopt treats the objective function as a black box because it only considers what goes in and what comes out. The algorithm does not need to know the internals of the objective function in order to find the input values that minimize the loss! At a very high level (in pseudocode), our objective function should be: We need to be careful not to use the loss on the testing set because we can only use the testing set a single time, when we evaluate the final model. Instead, we evaluate the hyperparameters on a validation set. Moreover, rather than separating training data into a distinct validation set, we use KFold cross validation, which, in addition to preserving valuable training data, should give us a less biased estimate of error on the testing set. The basic structure of the objective function for hyperparameter tuning will be the same across models: the function takes in the hyperparameters and returns the cross-validation error using those hyperparameters. Although this example is specific to the GBM, the structure can be applied to other methods. The complete objective function for the Gradient Boosting Machine using 10 fold cross validation with early stopping is shown below. The main line is cv_results = lgb.cv(...) . To implement cross-validation with early stopping , we use the LightGBM function cv which takes in the hyperparameters, a training set, a number of folds to use for cross validation, and several other arguments. We set the number of estimators ( num_boost_round) to 10000, but this number won’t actually be reached because we are using early_stopping_rounds to stop the training when validation scores have not improved for 100 estimators. Early stopping is an effective method for choosing the number of estimators rather than setting this as another hyperparameter that needs to be tuned! Once the cross validation is complete, we get the best score (ROC AUC), and then, because we want a value to minimize, we take 1-best score. This value is then returned as the loss key in the return dictionary. This is objective function is actually a little more complicated than it needs to be because we return a dictionary of values. For the objective function in Hyperopt, we can either return a single value, the loss, or a dict that has at a minimum keys ""loss"" and ""status"" . Returning the hyperparameters will let us inspect the loss resulting from each set of hyperparameters. The domain space represents the range of values we want to evaluate for each hyperparameter. Each iteration of the search, the Bayesian optimization algorithm will choose one value for each hyperparameter from the domain space. When we do random or grid search, the domain space is a grid. In Bayesian optimization the idea is the same except this space has probability distributions for each hyperparameter rather than discrete values. Specifying the domain is the trickiest part of a Bayesian optimization problem. If we have experience with a machine learning method, we can use it to inform our choices of hyperparameter distributions by placing greater probability where we think the best values are. However, the optimal model settings will vary between datasets and with a high-dimensionality problem (many hyperparameters) it can be difficult to figure out the interaction between hyperparameters. In cases where we aren’t sure about the best values, we can use wide distributions and let the Bayesian algorithm do the reasoning for us. First, we should look at all the hyperparameters in a GBM: I’m not sure there’s anyone in the world who knows how all of these interact together! Some of these we don’t have to tune (such as objective and random_state ) and we will use early stopping to find the best n_estimators. However, we still have 10 hyperparameters to optimize! When first tuning a model, I usually create a wide domain space centered around the default values and then refine it in subsequent searches. As an example, let’s define a simple domain in Hyperopt, a discrete uniform distribution for the number of leaves in each tree in the GBM: This is a discrete uniform distribution because the number of leaves must be an integer (discrete) and each value in the domain is equally likely (uniform). Another choice of distribution is the log uniform which distributes values evenly on a logarithmic scale. We will use a log uniform (from 0.005 to 0.2) for the learning rate because it varies across several orders of magnitude: Because this is a log-uniform distribution, the values are drawn between exp(low) and exp(high). The plot on the left below shows the discrete uniform distribution and the plot on the right is the log uniform. These are kernel density estimate plots so the y-axis is density and not a count! Now, let’s define the entire domain: Here we use a number of different domain distribution types: (There are other distributions as well listed in the documentation.) There is one important point to notice when we define the boosting type: Here we are using a conditional domain which means the value of one hyperparameter depends on the value of another. For the boosting type ""goss"", the gbm cannot use subsampling (selecting only a subsample fraction of the training observations to use on each iteration). Therefore, the subsample ratio is set to 1.0 (no subsampling) if the boosting type is ""goss"" but is 0.5–1.0 otherwise. This is implemented using a nested domain. Conditional nesting can be useful when we are using different machine learning models with completely separate parameters. A conditional lets us use different sets of hyperparameters depending on the value of a choice. Now that our domain is defined, we can draw one example from it to see what a typical sample looks like. When we sample, because subsample is initially nested, we need to assign it to a top-level key. This is done using the Python dictionary get method with a default value of 1.0. (This reassigning of nested keys is necessary because the gradient boosting machine cannot deal with nested hyperparameter dictionaries). Although this is the most conceptually difficult part of Bayesian Optimization, creating the optimization algorithm in Hyperopt is a single line. To use the Tree Parzen Estimator the code is: That’s all there is to it! Hyperopt only has the TPE option along with random search, although the GitHub page says other methods may be coming. During optimization, the TPE algorithm constructs the probability model from the past results and decides the next set of hyperparameters to evaluate in the objective function by maximizing the expected improvement. Keeping track of the results is not strictly necessary as Hyperopt will do this internally for the algorithm. However, if we want to find out what is going on behind the scenes, we can use a Trials object which will store basic training information and also the dictionary returned from the objective function (which includes the loss andparams ). Making a trials object is one line: Another option which will allow us to monitor the progress of a long training run is to write a line to a csv file with each search iteration. This also saves all the results to disk in case something catastrophic happens and we lose the trials object (speaking from experience). We can do this using the csv library. Before training we open a new csv file and write the headers: and then within the objective function we can add lines to write to the csv on every iteration (the complete objective function is in the notebook): Writing to a csv means we can check the progress by opening the file while training (although not in Excel because this will cause an error in Python. Use tail out_file.csv from bash to view the last rows of the file). Once we have the four parts in place, optimization is run with fmin : Each iteration, the algorithm chooses new hyperparameter values from the surrogate function which is constructed based on the previous results and evaluates these values in the objective function. This continues for MAX_EVALS evaluations of the objective function with the surrogate function continually updated with each new result. Results The best object that is returned from fmin contains the hyperparameters that yielded the lowest loss on the objective function: Once we have these hyperparameters, we can use them to train a model on the full training data and then evaluate on the testing data (remember we can only use the test set once, when we evaluate the final model). For the number of estimators, we can use the number of estimators that returned the lowest loss in cross validation with early stopping. Final results are below: As a reference, 500 iterations of random search returned a model that scored 0.7232 ROC AUC on the test set and 0.76850 in cross validation. A default model with no optimization scored 0.7143 ROC AUC on the test set. There are a few important notes to keep in mind when we look at the results: Bayesian optimization is effective, but it will not solve all our tuning problems. As the search progresses, the algorithm switches from exploration — trying new hyperparameter values — to exploitation — using hyperparameter values that resulted in the lowest objective function loss. If the algorithm finds a local minimum of the objective function, it might concentrate on hyperparameter values around the local minimum rather than trying different values located far away in the domain space. Random search does not suffer from this issue because it does not concentrate on any values! Another important point is that the benefits of hyperparameter optimization will differ with the dataset. This is a relatively small dataset (~ 6000 training observations) and there is a small payback to tuning the hyperparameters (getting more data would be a better use of time!). With all of those caveats in mind, in this case, with Bayesian optimization we can get: Bayesian methods can (although will not always) yield better tuning results than random search. In the next few sections, we will examine the evolution of the Bayesian hyperparameter search and compare to random search to understand how Bayesian Optimization works. Visualizing Search Results Graphing the results is an intuitive way to understand what happens during the hyperparameter search. Moreover, it’s helpful to compare Bayesian Optimization to random search so we can see how the methods differ. To see how the plots are made and random search is implemented, see the notebook, but here we will go through the results. (As a note, the exact results will change across iterations, so if you run the notebook, don’t be surprised if you get different images. All of these plots are made with 500 iterations). First we can make a kernel density estimate plot of the learning_rate sampled in random search and Bayes Optimization. As a reference, we can also show the sampling distribution. The vertical dashed lines show the best values (according to cross validation) for the learning rate. We defined the learning rate as a log-normal between 0.005 and 0.2, and the Bayesian Optimization results look similar to the sampling distribution. This tells us that the distribution we defined looks to be appropriate for the task, although the optimal value is a little higher than where we placed the greatest probability. This could be used to inform the domain for further searches. Another hyperparameter is the boosting type, with the bar plots of each type evaluated during random search and Bayes optimization shown below. Since random search does not pay attention to past results, we would expect each boosting type to be used roughly the same number of times. According to the Bayesian algorithm, the gdbt boosting type is more promising than dart or goss. Again, this could help inform further searches, either Bayesian methods or grid search. If we wanted to do a more informed grid search, we could use these results to define a smaller grid concentrated around the most promising values of the hyperparameters. Since we have them, let’s look at all of the numeric hyperparameters from the reference distribution, random search, and Bayes Optimization. The vertical lines again indicate the best value of the hyperparameter for each search: In most cases (except for the subsample_for_bin ) the Bayesian optimization search tends to concentrate (place more probability) near the hyperparameter values that yield the lowest loss in cross validation. This shows the fundamental idea of hyperparameter tuning using Bayesian methods: spend more time evaluating promising hyperparameter values. There are also some interesting results here that might help us in the future when it comes time to define a domain space to search over. As just one example, it looks like reg_alpha and reg_lambda should complement one another: if one is high (close to 1.0), the other should be lower. There’s no guarantee this will hold across problems, but by studying the results, we can gain insights that might be applied to future machine learning problems! As the optimization progresses, we expect the Bayes method to focus on the more promising values of the hyperparameters: those that yield the lowest error in cross validation. We can plot the values of the hyperparameters versus the iteration to see if there are noticeable trends. The black star indicates the optimal value. The colsample_bytree and learning_rate decrease over time which could guide us in future searches. Finally, if Bayes Optimization is working, we would expect the average validation score to increase over time (conversely the loss decreases): The validation scores from Bayesian hyperparameter optimization increase over time, indicating the method is trying “better” hyperparameter values (it should be noted that these are only better according to the validation score). Random search does not show an improvement over the iterations. If we are not satisfied with the performance of our model, we can keep searching using Hyperopt from where we left off. We just need to pass in the same trials object and the algorithm will continue searching. As the algorithm progresses, it does more exploitation — picking values that have done well in the past — and less exploration — picking new values. Instead of continuing where the search left off, it might therefore be a good idea to start an entirely different search. If the best hyperparameters from the first search really are “optimal”, we would expect subsequent searches to focus on the same values. Given the high dimensionality of the problem, and the complex interactions between hyperparameters, it’s unlikely that another search would result in a similar set of hyperparameters. After another 500 iterations of training, the final model scores 0.72736 ROC AUCon the test set. (We really should not have evaluated the first model on the test set and instead relied only on validation scores. The test set should ideally be used only once to get a measure of algorithm performance when deployed on new data). Again, this problem may have diminishing returns to further hyperparameter optimization because of the small size of the dataset and there will eventually be a plateau in validation error (there is an inherent limit to the performance of any model on a dataset because of hidden variables that are not measured and noisy data, referred to as Bayes’ Error). Conclusions Automated hyperparameter tuning of machine learning models can be accomplished using Bayesian optimization. In contrast to random search, Bayesian optimization chooses the next hyperparameters in an informed method to spend more time evaluating promising values. The end outcome can be fewer evaluations of the objective function and better generalization performance on the test set compared to random or grid search. In this article, we walked step-by-step through Bayesian hyperparameter optimization in Python using Hyperopt. We were able to improve the test set performance of a Gradient Boosting Machine beyond both the baseline and random search although we need to be cautious of overfitting to the training data. Furthermore, we saw how random search differs from Bayesian Optimization by examining the resulting graphs which showed that the Bayesian method placed greater probability on the hyperparameter values that resulted in lower cross validation loss. Using the four parts of an optimization problem, we can use Hyperopt to solve a wide variety of problems. The basic parts of Bayesian optimization also apply to a number of libraries in Python that implement different algorithms. Making the switch from manual to random or grid search is one small step, but to take your machine learning to the next level requires some automated form of hyperparameter tuning. Bayesian Optimization is one approach that is both easy to use in Python and can return better results than random search. Hopefully you now feel confident to start using this powerful method for your own machine learning problems! As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Four Parts of Optimization Problem Objective Function Domain Space Optimization Algorithm Result History Optimization Results Visualizing Search Results Evolution of Search Continue Searching Domain Space: hyperparameter values to search over Optimization algorithm: method for constructing the surrogate model and choosing the next hyperparameter values to evaluate Result history: stored outcomes from evaluations of the objective function consisting of the hyperparameters and validation loss quniform : discrete uniform (integers spaced evenly) uniform: continuous uniform (floats spaced evenly) loguniform: continuous log uniform (floats spaced evenly on a log scale) Even using 10-fold cross-validation, the hyperparameter tuning overfits to the training data. The best score from cross-validation is significantly higher than that on the testing data. Random search may return better hyperparameters just by sheer luck (re-running the notebook can change the results). Bayesian optimization is not guaranteed to find better hyperparameters and can get stuck in a local minimum of the objective function. Fewer iterations to tune the hyperparameters Objective Function: what we want to minimize, in this case the validation error of a machine learning model with respect to the hyperparameters choice : categorical variables The optimal hyperparameters are those that do best in cross validation and not necessarily those that do best on the testing data. When we use cross validation, we hope that these results generalize to the testing data. Better performance on the testing set",Automated Machine Learning Hyperparameter Tuning in Python,7,published,49339,4298,0.6747324336900884,10,1,1,1,1,0
62,727,311.2814098278009,116,https://towardsdatascience.com/machine-learning-kaggle-competition-part-three-optimization-db04ea415507,3,Towards Data Science,2018-07-20 08:57:00,18.18,15,1016,2018-07-10 19:56:00,"['Machine Learning', 'Data Science', 'Python', 'Education', 'Towards Data Science']","Machine Learning Kaggle Competition: Part Three Optimization Getting the most out of a machine learning model How best to describe a Kaggle contest? It’s a machine learning education disguised as a competition! Although there are valid criticisms of Kaggle, overall, it’s a great community that provides interesting problems, thousands of data scientists willing to share their knowledge, and an ideal environment for exploring new ideas. As evidence of this, I never would have learned about the Gradient Boosting Machine, or, one of the topics of this article, automated model optimization, were it not for the Kaggle Home Credit contest. In this article, part three of a series (Part One: Getting Started and Part Two: Improving) documenting my work for this contest, we will focus on a crucial aspect of the machine learning pipeline: model optimization through hyperparameter tuning. In the second article, we decided on the Gradient Boosting Machine as our model of choice, and now we have to get the most out of it through optimization. We’ll do this primarily with two methods: random search and automated tuning with Bayesian optimization. All the work presented here is available to run on Kaggle in the following notebooks. The article itself will highlight the key ideas but the code details are all in the notebooks (which are free to run with nothing to install!) For this article we will skip the background, so if at any time you feel lost, I encourage you to go to the previous articles and notebooks. All of the notebooks can be run on Kaggle without the need to download anything, so I highly recommend checking them out or competing! In the first part of this series, we got familiar with the dataset, performed exploratory data analysis, tried our hand at feature engineering, and built a few baseline models. Our public leaderboard score from this round was 0.678 (which at the moment would place us lower than 4000 on the leaderboard). Part two involved in-depth manual feature engineering, followed by feature selection and more modeling. Using the expanded (and then contracted) set of features, we ended up with a score of 0.779, quite an improvement from the baseline, but not quite in the top 50% of competitors. In this article, we will once again better our score and move up 1000 places on the leaderboard. Machine Learning Optimization Optimization in the context of machine learning means finding the set of model hyperparameter values that yield the highest cross validation score for a given dataset. Model hyperparameters, in contrast to model parameters that are learned during training, are set by the data scientist before training. The number of layers in a deep neural network is a model hyperparameter while the splits in a decision tree are model parameters. I like to think of model hyperparameters as settings that we need to tune for a dataset: the ideal combination of values is different for every problem! There are a handful of ways to tune a machine learning model: These are presented in order of increasing efficiency, with manual search taking the most time (and often yielding the poorest results) and automated methods converging on the best values the quickest, although, as with many topics in machine learning, this is not always the case! As shown in this great paper, random search does surprisingly well (which we’ll see shortly). (There are also other hyperparameter tuning methods such as evolutionary and gradient-based. There are constantly better methods being developed, so make sure to stay up to date on the current best practice!) In the second part of this series, we decided to use the Gradient Boosting Machine (GBM) model because of its superior performance on structured data with many features. The GBM is extremely powerful (yet easy to implement in Python), but it has dozens of hyperparameters that significantly affect performance and which must be optimized for a problem. If you want to feel overwhelmed, check out the documentation on the LightGBM library: I don’t think there is anyone in the world who can look at all of these and pick the best values! Therefore, we need to implement one of the four methods for choosing the hyperparameters. For this problem, I didn’t even try manual tuning, both because this was my first time working with the GBM and because I didn’t want to waste any time. e’ll skip straight to random search and automated techniques using Bayesian optimization. In the first notebook, we go through an implementation of grid and random search covering the four parts of an optimization problem: These four parts also form the basis of Bayesian optimization, so laying them out here will help when it comes to that implementation. For the code details, refer to the notebooks, but here we’ll briefly touch on each concept. The objective function takes in a set of inputs and returns a score that we want to maximize. In this case, the inputs are the model hyperparameters, and the score is the 5-fold cross validation ROC AUC on the training data. The objective function in pseudo-code is: The goal of hyperparameter optimization is to find the hyperparameters that return the best value when passed into the objective function. That seems pretty simple, but the problem is evaluating the objective function is very expensive in terms of time and computational resources. We cannot try every combination of hyperparameter values because we have a limited amount of time, hence the need for random search and automated methods. The domain is the set of values over which we search. For this problem with a GBM, the domain is as follows: We can visualize two of these distributions, the learning rate, which is logarithmic normal, and the number of leaves which is uniform normal: Algorithm Although we don’t generally think of them as such, both grid and random search are algorithms. In the case of grid search, we input the domain and the algorithm selects the next value for each hyperparameter in an ordered sequence. The only requirement of grid search is that it tries every combination in a grid once (and only once). For random search, we input the domain and each time the algorithm gives us a random combination of hyperparameter values to try. There are no requirements for random search other than that the next values are selected at random. Random search can be implemented as in the following: This is in effect a very simple algorithm! The results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function. When we get to Bayesian Optimization, the model actually uses the past results to decide on the next hyperparmeters to evaluate. Random and grid search are uninformed methods that do not use the past history, but we still need the history so we can find out which hyperparameters worked the best! In this case, the results history is simply a dataframe. Bayesian Hyperparameter Optimization Automated hyperparameter tuning with bayesian optimization sounds complicated, but in fact it uses the same four parts as random search with the only difference the Algorithm used. For this competition, I used the Hyperopt library and the Tree Parzen Estimator (TPE) algorithm with the work shown in this notebook. For a conceptual explanation, refer to this article, and for an implementation in Python, check out the notebook or this article. The basic concept of Bayesian optimization is that it uses the previous evaluation results to reason about which hyperparameters perform better and uses this reasoning to choose the next values. Hence, this method should spend fewer iterations evaluating the objective function with poorer values. Theoretically, Bayesian optimization can converge on the ideal values in fewer iterations than random search (although random search can still get lucky)! The aspirations of Bayesian optimization are: This is a powerful method that promises to deliver great results. The question is, does the evidence in practice show this to be the case? To answer that, we turn to the final notebook, a deep dive into the model tuning results! Hyperparameter Optimization Results After the hard work of implementing random search and Bayesian optimization, the third notebook is a fun and revealing exploration of the results. There are over 35 plots, so if you like visuals, then check it out. Although I was trying to do the whole competition on Kaggle, for these searches, I did 500 iterations of random search and 400 of Bayesian optimization which took about 5 days each on a computer with 64 GB of RAM (thanks to Amazon AWS). All the results are available, but you’ll need some serious hardware to redo the experiments! First off: which method did better? The image below summarizes the results from 501 iterations of random search and 402 of Bayesian optimization (called opt in the dataframe): Going by max score, random search does slightly better but if we measure by average score, bayesian optimization wins. The good news is this is almost exactly what we expect: random search can happen upon a great set of values because it thoroughly explores the search space, but Bayesian optimization will “focus” on the highest scoring hyperparameter values by reasoning from previous results. Let’s take a look at a very revealing plot, the value of the cross validation score versus the number of iterations: We see absolutely no trend for random search, while Bayesian Optimization (again shown by opt ) improves over the trials. If you can understand this graph, then you can see the benefits of both methods: random search explores the search domain but Bayesian optimization gets better over time. We can also see that Bayesian optimization appears to reach a plateau, indicating diminishing returns to further trials. Second major question: what were the best hyperparameter values? Below are the optimal results from Bayesian optimization: We can use these results to build a model and submit predictions to the competition, or they can be used to inform further searches by allowing us to define a concentrated search space around the best values. One interesting aspect to consider is the values tried by each search method for each hyperparameter. The plots below show kernel density estimate (KDE) functions for each of the search methods as well as the sampling distribution (the hyperparameter grid). The dashed vertical lines indicate the optimal value found for each method. First is the learning rate: Even though the learning rate distribution stretched over several orders of magnitude, both methods found optimal values quite low in the domain. We can use this knowledge to inform further hyperparameter searches by concentrating our search in this region. In most cases, a lower learning rate increases the cross validation performance but at the cost of increased run-time, which is a trade-off we have to make. Let’s look at some other graphs. For most of the hyperparameters, the optimal values from both methods are fairly close, but not for colsample_bytree : This refers to the fraction of columns used when building each tree in the GBM, and random search found the optimal value was higher than Bayesian optimization. Again, these results can be used for further searches, as we see that the Bayesian method tended to concentrate on values around 0.65. We’ll show two more plots from this analysis because they are fairly interesting, involving two regularization parameters: What’s noteworthy here is that these two hyperparameters appear to be complements of one another: if one regularization value is high, then we want the other to be low and vice versa. Maybe this helps the model achieve a balance between bias/variance, the most common issue in machine learning. While random search does not change the distribution of values over the search, Bayesian optimization does, by concentrating on where it thinks the best values are in the search domain. We can see this by graphing the hyperparameter values against the iteration: The hyperparameters with the clearest trend are colsample_bytree and learning_rate which both continue downward over the trials. The reg_lambda and reg_alpha are diverging, which confirms our earlier hypothesis that we should decrease one of these while increasing the other. We want to be careful about placing too much value in these results, because the Bayesian optimization might have found a local minimum of the cross validation loss that it is exploiting. The trends here are pretty small, but it’s encouraging that the best value was found close to the end of the search indicating cross validation scores were continuing to improve. These next plots show the value of a single hyperparameter versus the score. We want to avoid placing too much emphasis on these graphs because we are not changing one hyperparameter at a time and there can be complex interactions between multiple hyperparameters. A truly accurate graph would be 10-dimensional and show the values of all hyperparameters and the resulting score. If we could understand a 10-dimensional graph, then we might be able to figure out the optimal combination of hyperparameters! Here the random search is in blue and the Bayesian in green: The only clear distinction is that the score decreases as the learning rate increases. We cannot say whether that is due to the learning rate itself, or some other factor (we will look at the interplay between the learning rate and the number of estimators shortly). There are not any strong trends here. The score versus subsample is a little off because boosting_type = 'goss' cannot use subsample (it must be set equal to 1.0). While we can’t look at all 10 hyperparameters at one time, we can view two at once if we turn to 3D plots! 3D Plots To try and examine the simultaneous effects of hyperparameters, we can make 3D plots with 2 hyperparameters and the score. A truly accurate plot would be 10-D (one for each hyperparameter) but in this case we will stick to 3 dimensions. (See the code for details, 3D plotting in Python is surprisingly straightforward). First we can show the reg_alpha and reg_lambda , the regularization hyperparameters versus the score (for Bayesian opt): It’s a little hard to make sense, but if we remember the best scores occurred around 0.5 for reg_alpha and 0.4 for reg_lambda , we can see generally better scores in this region. Next is the learning_rate and n_estimators (number of decision trees trained in the ensemble). While the learning rate was a hyperparameter in the grid, the number of decision trees (also called the number of boosting rounds) was found by early stopping with 5-fold cross validation: This time there is a clear trend: a lower learning rate and higher number of estimators increases the score. The relationship is expected because a lower learning rate means the contribution of each tree is reduced which necessitates training more trees. The expanded number of trees increases the model’s capacity to fit the training data (while also increasing the time to run). Moreover, as long as we use early stopping with enough folds, we don’t have to be concerned about overfitting with more trees. It’s nice when the results agree with our understanding (although we probably learn more when they don’t agree!) For the final plot, I wanted to show the correlations between each hyperparameter and each other and the score. These plots can’t prove causation, but they can tell us which variables are correlated: We already found most of the information from the graphs, but we can see the negative correlation between the learning rate and the score, and the positive correlation between the number of estimators and the score. Testing Optimal Hyperparameters The final step of the exploration was to implement both the random search best hyperparameters and the Bayesian optimization best hyperparameters on a full dataset (the dataset comes from this kernel and I would like to thank the author for making it public). We train the model, make predictions on the testing set, and finally upload to the competition to see how we do on the public leaderboard. After all the hard work, do the results hold up? If we go by best score on the public leaderboard, Bayesian Optimization wins! However, the public leaderboard is based only on 10% of the test data, so it’s possible this is a result of overfitting to this particular subset of the testing data. Overall, I would say the complete results — including cross validation and public leaderboard — suggest that both methods produce similar outcomes when run for enough iterations. All we can say for sure is that either method is better than hand-tuning! Our final model is enough to move us 1000 places up the leaderboard compared to our previous work. Finally, to end with one more plot, we can take a look at the feature importances from the trained GBM: NEW_CREDIT_TO_ANNUITY_RATIO and NEW_EXT_SOURCES_MEAN were features derived by the data science community on Kaggle and not in the original data. It’s reassuring to see these so high up in the importance because it shows the value of feature engineering. Conclusions and Next Steps The major takeaways from this work are: Where to go from here? Well, there are always plenty of other methods to try such as automated feature engineering or treating the problem as a time-series. I’ve already done a notebook on automated feature engineering so that will probably be where my focus will turn next. We can also try other models or even journey into the realm of deep learning! I’m open to suggestions, so let me know on Kaggle or on Twitter. Thanks for reading, and if you want to check out any of my other work on this problem, here are the complete set of notebooks: There should be enough content there to keep anyone busy for a little bit. Now it’s off to do more learning/exploration for the next post! The best part about data science is that you’re constantly on the move, looking for the next technique to conquer. Whenever I find out what that is, I’ll be sure to share it! As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Automated Hyperparameter tuning Tuning Results Grid Search: set up a hyperparameter grid and for each combination of values, train a model and find the validation score. In this approach, every combination of hyperparameter values is tried which is very inefficient! Random search: set up a hyperparameter grid and select random combinations of values to train the model and find the validation score. The number of search iterations is based on time/resources. Automated Hyperparameter Tuning: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters. These methods use the previous results to choose the next hyperparameter values in an informed search compared to random or grid which are uninformed methods. Domain: the set of hyperparameter values over which we want to search. Algorithm: method for selecting the next set of hyperparameters to evaluate in the objective function. Results history: data structure containing each set of hyperparameters and the resulting score from the objective function. Use fewer iterations for optimization than grid or random search Bayesian optimization results scored 0.791 Bayesian optimization tends to “concentrate” on higher scoring values while random search better explores the possibilities When given enough iterations, both methods produce similar results in terms of cross validation and test scores Optimizing the hyperparameters has a significant effect on performance, comparable to feature engineering for this problem Manual Feature Engineering Part One Manual Feature Engineering Part Two Introduction to Automated Feature Engineering Advanced Automated Feature Engineering Feature Selection Intro to Model Tuning: Grid and Random Search Automated Model Tuning Model Tuning Results Random and Grid Search Manual: select hyperparameters with intuition/experience/guessing, train the model with the values, and find the validation score Repeat process until you run out of patience or are satisfied with the results. Objective function: a function that takes in hyperparameters and returns a score we are trying to minimize or maximize To find better hyperparameter values as measured by performance Random search results scored 0.790 Random search and automated hyperparameter tuning using Bayesian optimization are effective methods for model tuning A Gentle Introduction",Machine Learning Kaggle Competition: Part Three Optimization,7,published,5588,3718,0.19553523399677245,9,1,1,1,1,1
59,738,305.2547300565857,138,https://towardsdatascience.com/how-to-get-the-right-data-why-not-ask-for-it-d26ced1bbd46,7,Towards Data Science,2018-07-26 09:35:00,42.15,6,2027,2018-07-26 07:02:00,"['Data Science', 'Education', 'Data For Good', 'Towards Data Science', 'Skills']","How to get the right data? Trying asking for it. An example of why the most important skills in data science may not be technical While the technical skills of data science — think modeling with a gradient boosting machine — get most of the attention, other equally important, general-purpose problem-solving abilities can be overlooked. Proficiency in asking the right question, being persistent, and taking advantage of multiple resources are critical to the success of a data science project but often take a back seat to coding ability when people ask what it takes to be a data scientist. Recently, I was reminded of the importance of these non-technical skills while working on a data science for good project. The project, currently live on Kaggle involves identifying schools in New York City that would most benefit from programs that encourage disadvantaged students to take the Specialized High Schools Admission Test (SHSAT). This task comes with a small data set including test results from 2016, but the organizers encourage the use of any publicly available data. Knowing that the success of a data science project is proportional to the quality and quantity of the data, I set out to find newer test results. Not surprisingly with the vast resources now at our fingertips, I was ultimately successful, and along the way I learned a few lessons about the “other” proficiencies necessary for data science that I’ve laid out below. The broad availability of resources can be both a blessing and a curse: with so many options, sometimes it can be difficult to find a place to start (a phenomenon I see often when people want to learn data science). The right question or goal can help you narrow down the options. If I asked “Is there any New York City data I can use?” I would have been overwhelmed with the possibilities, much as people who “want to learn Python” are confronted with a dizzying array of resources (a better objective is “I want to learn Python for x” because it will limit the choices). If you don’t succeed in your initial objective, you can always cast a wider net or change the question / goal. Moreover, sometimes you can answer your initial question with a different set of data than you had in mind or you might find out there was a better question to ask. Keeping this in mind, I set out my search with one question: could I find the most recent results for the SHSAT? With my single focused question, the best place to start was the New York City Open Data Portal. NYC, like many large cities, has an extensive collection of data free to download and use in your projects. Open data portals are a great place to start for exploring issues and using data science to make an impact. Unfortunately, although the NYC data was extensive, none of it covered the SHSAT. So I broadened my search — meaning that I went further down the list of Google results — and came across an article by the New York Times analyzing exactly the data I wanted (with some great infographics)! Clearly the data was publicly available somewhere if the NYT could get it! Since I had already checked the open data portal, I decided to try a more direct route and contact the author. I’ve had success with this method before — I once scored a free college textbook that was out of print by emailing the author — and now it’s simple to find a social media or professional contact address. As long as your request is civil (a compliment or two doesn’t hurt) most authors are more than willing to help if they can. However, in this case my direct approach failed as the author didn’t respond on any the channels I used. To be honest, I don’t blame her: it can be tough dealing with all the requests you get as a writer, and I would much rather she concentrate on writing more articles than reply to every single comment! One of the most important parts of being a data scientist is the ability to pay attention to details. Valuable information can hide in unexpected places (such as file names) and in this case, reading the fine print under the infographic revealed the source: the NYC Department of Education, which I had already searched through the open data portal! While I had already tried this source, I went back to the portal and decided to make a request from the contact page. I submitted a ticket with the specific data I wanted and received the slightly discouraging note it could take up to 2 weeks to hear a reply. Fortunately, that seems to be a pessimistic overestimate and within 2 days I had a response back — from an actual human! — that the data I had requested was available. That same day, the full data appeared in the NYC data portal, free for the world to use to the benefit of NYC students. There was no barrier to making the data public, someone just had to ask! Although this project is technically a competition on Kaggle, there was no way I was going to keep the availability of this data a secret. I immediately set up a discussion thread and shared the link to the data source. Within hours, other data scientists were using this data for their own analysis and then sharing their findings. That’s what’s great about the data science community: it’s not about competing, but about learning from each other. One person can only have so much experience but the collective knowledge of a community can be vast. This means that when you find something interesting, don’t keep it to yourself but rather share it so others can learn as well! Having received so much from other data scientists on Kaggle, it felt great to be able to give a little back. This small example illustrates a few key points: first, it never hurts to ask! I’ve written about this before, but when you ask someone for a favor (as long as the request is reasonable) the worst they can say is no. Second, the ability to use multiple resources and stay persistent will take you farther than any particular technical skill in your career. None of the steps I took involved any coding, but without going through with them, I wouldn’t have gotten the data I needed to do an analysis! Finally, don’t be afraid to reach out to people for help or to use any of the great resources now available to us. As always, I welcome comments, constructive criticism, and discussion. I can be reached on Twitter @koehrsen_will Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",How to get the right data? Trying asking for it.,12,published,4809,1247,0.5918203688853247,0,1,1,1,0,0
70,4000,291.2762923639005,538,https://towardsdatascience.com/why-automated-feature-engineering-will-change-the-way-you-do-machine-learning-5c15bf188b96,24,Towards Data Science,2018-08-09 09:04:00,24.17,11,7230,2018-07-27 11:11:00,"['Machine Learning', 'Data Science', 'Towards Data Science', 'Python', 'Education']","Why Automated Feature Engineering Will Change the Way You Do Machine Learning Automated feature engineering will save you time, build better predictive models, create meaningful features, and prevent data leakage There are few certainties in data science — libraries, tools, and algorithms constantly change as better methods are developed. However, one trend that is not going away is the move towards increased levels of automation. Recent years have seen progress in automating model selection and hyperparameter tuning, but the most important aspect of the machine learning pipeline, feature engineering, has largely been neglected. The most capable entry in this critical field is Featuretools, an open-source Python library. In this article, we’ll use this library to see how automated feature engineering will change the way you do machine learning for the better. Automated feature engineering is a relatively new technique, but, after using it to solve a number of data science problems using real-world data sets, I’m convinced it should be a standard part of any machine learning workflow. Here we’ll take a look at the results and conclusions from two of these projects with the full code available as Jupyter Notebooks on GitHub. Each project highlights some of the benefits of automated feature engineering: Feel free to dig into the code and try out Featuretools! (Full disclosure: I work for Feature Labs, the company developing the library. These projects were completed with the free, open-source version of Featuretools). Feature engineering is the process of taking a dataset and constructing explanatory variables — features — that can be used to train a machine learning model for a prediction problem. Often, data is spread across multiple tables and must be gathered into a single table with rows containing the observations and features in the columns. The traditional approach to feature engineering is to build features one at a time using domain knowledge, a tedious, time-consuming, and error-prone process known as manual feature engineering. The code for manual feature engineering is problem-dependent and must be re-written for each new dataset. Automated feature engineering improves upon this standard workflow by automatically extracting useful and meaningful features from a set of related data tables with a framework that can be applied to any problem. It not only cuts down on the time spent feature engineering, but creates interpretable features and prevents data leakage by filtering time-dependent data. Automated feature engineering is more efficient and repeatable than manual feature engineering allowing you to build better predictive models faster. Loan Repayment: Build Better Models Faster The primary difficulty facing a data scientist approaching the Home Credit Loan problem (a machine learning competition currently running on Kaggle where the objective is to predict if a loan will be repaid by a client) is the size and spread of the data. Take a look at the complete dataset and you are confronted with 58 million rows of data spread across seven tables. Machine learning requires a single table for training, so feature engineering means consolidating all the information about each client in one table. My first attempt at the problem used traditional manual feature engineering: I spent a total of 10 hours creating a set of features by hand. First I read other data scientist’s work, explored the data, and researched the problem area in order to acquire the necessary domain knowledge. Then I translated the knowledge into code, building one feature at a time. As an example of a single manual feature, I found the total number of late payments a client had on previous loans, an operation that required using 3 different tables. The final manual engineered features performed quite well, achieving a 65% improvement over the baseline features (relative to the top leaderboard score), indicating the importance of proper feature engineering. However, inefficient does not even begin to describe this process. For manual feature engineering, I ended up spending over 15 minutes per feature because I used the traditional approach of making a single feature at a time. Besides being tedious and time-consuming, manual feature engineering is: Furthermore, the final manual engineered features are limited both by human creativity and patience: there are only so many features we can think to build and only so much time we have to make them. The promise of automated feature engineering is to surpass these limitations by taking a set of related tables and automatically building hundreds of useful features using code that can be applied across all problems. As implemented in Featuretools, automated feature engineering allows even a domain novice such as myself to create thousands of relevant features from a set of related data tables. All we need to know is the basic structure of our tables and the relationships between them which we track in a single data structure called an entity set. Once we have an entity set, using a method called Deep Feature Synthesis (DFS), we’re able to build thousands of features in one function call. DFS works using functions called “primitives” to aggregate and transform our data. These primitives can be as simple as taking a mean or a max of a column, or they can be complex and based on subject expertise because Featuretools allows us to define our own custom primitives. Feature primitives include many operations we already would do manually by hand, but with Featuretools, instead of re-writing the code to apply these operations on different datasets, we can use the same exact syntax across any relational database. Moreover, the power of DFS comes when we stack primitives on each other to create deep features. (For more on DFS, take a look at this blog post by one of the inventors of the technique.) Deep Feature Synthesis is flexible — allowing it to be applied to any data science problem — and powerful — revealing insights in our data by creating deep features. I’ll spare you the few lines of code needed for the set-up, but the action of DFS happens in a single line. Here we make thousands of features for each client using all 7 tables in our dataset ( ft is the imported featuretools library) : Below are some of the 1820 features we automatically get from Featuretools: Each of these features is built using simple aggregations and hence is human-interpretable. Featuretools created many of the same features I did manually, but also thousands I never would have conceived — or had the time to implement. Not every single feature will be relevant to the problem, and some of the features are highly correlated, nonetheless, having too many features is a better problem than having too few! After a little feature selection and model optimization, these features did slightly better in a predictive model compared to the manual features with an overall development time of 1 hour, a 10x reduction compared to the manual process. Featuretools is much faster both because it requires less domain knowledge and because there are considerably fewer lines of code to write. I’ll admit that there is a slight time cost to learning Featuretools but it’s an investment that will pay off. After taking an hour or so to learn Featuretools, you can apply it to any machine learning problem. The following graphs sum up my experience for the loan repayment problem: My takeaway is that automated feature engineering will not replace the data scientist, but rather by significantly increasing efficiency, it will free her to spend more time on other aspects of the machine learning pipeline. Furthermore, the Featuretools code I wrote for this first project could be applied to any dataset while the manual engineering code would have to be thrown away and entirely rewritten for the next dataset! Retail Spending: Build Meaningful Features and Prevent Data Leakage For the second dataset, a record of online time-stamped customer transactions, the prediction problem is to classify customers into two segments, those who will spend more than $500 in the next month and those who won’t. However, instead of using a single month for all the labels, each customer is a label multiple times. We can use their spending in May as a label, then in June, and so on. Using each customer multiple times as an observation brings up difficulties for creating training data: when making features for a customer for a given month, we can’t use any information from months in the future, even though we have access to this data. In a deployment, we’ll never have future data and therefore can’t use it for training a model. Companies routinely struggle with this issue and often deploy a model that does much worse in the real world than in development because it was trained using invalid data. Fortunately, ensuring that our data is valid in a time-series problem is straightforward in Featuretools. In the Deep Feature Synthesis function we pass in a dataframe like that shown above, where the cutoff time represents the point past which we can’t use any data for the label, and Featuretools automatically takes the time into account when building features. The features for a customer in a given month are built using data filtered to before the month. Notice that the call to create our set of features is the same as that for the loan repayment problem with the addition of cutoff_time. The result of running Deep Feature Synthesis is a table of features, one for each customer for each month. We can use these features to train a model with our labels and then make predictions for any month. Moreover, we can rest assured that the features in our model do not use future information, which would result in an unfair advantage and yield misleading training scores. With the automated features, I was able to build a machine learning model that achieved 0.90 ROC AUC compared to an informed baseline (guessing the same level of spending as the previous month) of 0.69 when predicting customer spending categories for one month. In addition to delivering impressive predictive performance, the Featuretools implementation gave me something equally valuable: interpretable features. Take a look at the 15 most important features from a random forest model: The feature importances tell us that the most important predictors of how much the customer will spend in the next month is how much they have spent previously SUM(purchases.total) , and the number of purchases, SUM(purchases.quantity).These are features that we could have built by hand, but then we would have to worry about leaking data and creating a model that does much better in development than in deployment. If the tool already exists for creating meaningful features without any need to worry about the validity of our features, then why do the implementation by hand? Furthermore, the automated features are completely clear in the context of the problem and can inform our real-world reasoning. Automated feature engineering identified the most important signals, achieving the primary goal of data science: reveal insights hidden in mountains of data. Even after spending significantly more time on manual feature engineering than I did with Featuretools, I was not able to develop a set of features with close to the same performance. The graph below shows the ROC curves for classifying one month of future customer sales using a model trained on the two datasets. A curve to the left and top indicates better predictions: I’m not even completely sure if the manual features were made using valid data, but with the Featuretools implementation, I didn’t have to worry about data leakage in time-dependent problems. Maybe this inability to manually engineer a useful set of valid features speaks to my failings as a data scientist, but if the tool exists to safely do this for us, why not use it? We use automatic safety systems in our day-to-day life and automated feature engineering in Featuretools is the secure method to build meaningful machine learning features in a time-series problem while delivering superior predictive performance. Conclusions I came away from these projects convinced that automated feature engineering should be an integral part of the machine learning workflow. The technology is not perfect yet still delivers significant gains in efficiency. The main conclusions are that automated feature engineering: “Work smarter, not harder” may be a cliche, but sometimes there is truth to platitudes: if there is a way to do the same job with the same performance for a smaller time investment, then clearly it’s a method worth learning. Featuretools will always be free to use and open-source (contributions are welcome), and there are several examples — here’s an article I’ve written — to get you started in 10 minutes. Your job as a data scientist is safe, but it can be made significantly easier with automated feature engineering. If building meaningful, high-performance predictive models is something you care about, then get in touch with us at Feature Labs. While this project was completed with the open-source Featuretools, the commercial product offers additional tools and support for creating machine learning solutions. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Retail Spending Prediction: automated feature engineering creates meaningful features and prevents data leakage by internally handling time-series filters, enabling successful model deployment. (Notebooks) Error-prone: each line of code is another opportunity to make a mistake The percentile ranking of a client in terms of average previous credit card debt. This uses a PERCENTILE and MEAN primitive across 2 tables. Whether or not a client turned in two documents during the application process. This uses a AND transform primitive and 1 table. Number of features produced by the method: 30 features manual vs 1820 automated Improvement relative to baseline is the % gain over the baseline compared to the top public leaderboard score using a model trained on the features: 65% manual vs 66% automated Achieved modeling performance at the same level or better Delivered interpretable features with real-world significance Prevented improper data usage that would invalidate a model Fit into existing workflows and machine learning models Loan Repayment Prediction: automated feature engineering can reduce machine learning development time by 10x compared to manual feature engineering while delivering better modeling performance. (Notebooks) Problem-specific: all of the code I wrote over many hours cannot be applied to any other problem The maximum total amount paid on previous loans by a client. This is created using a MAX and a SUM primitive across 3 tables. Reduced implementation time by up to 10x",Why Automated Feature Engineering Will Change the Way You Do Machine Learning,12,published,29918,2634,1.5186028853454823,12,1,1,1,1,1
58,4960,289.40903930348384,733,https://towardsdatascience.com/the-most-important-part-of-a-data-science-project-is-writing-a-blog-post-50715f37833a,25,Towards Data Science,2018-08-11 05:53:00,32.46,8,5982,2018-07-30 20:28:00,"['Data Science', 'Education', 'Towards Data Science', 'Programming', 'Communication']","The most important part of a data science project is writing a blog post Writing creates opportunities, gives you critical communication practice, and makes you a better data scientist through feedback It can be tempting to call a data science project complete after you’ve uploaded the final code to GitHub or handed in your assignment. However, if you stop there, you’re missing out on the most crucial step of the process: writing and sharing an article about your project. Writing a blog post isn’t typically considered part of the data science pipeline, but to get the most from your work, then it should be the standard last step in any of your projects. There are three benefits to writing even a simple blog post about your work: Writing is one of those activities — exercise and education also come to mind — that might have no payout in the short term but almost unlimited potential rewards in the long term. Personally, I make $0 from the 10,000 daily views my blog posts receive, each of which takes 3–15 hours to write. Yet, I also wouldn’t have a full-time data science job were it not for my articles. Moreover, I know the quality of my data science work is much higher, both because I intend to write about it, and because I used the previous feedback I’ve received, making the long term return from writing decidedly positive. Communication: Good Code is not enough I know the feeling: you’ve put up some Jupyter Notebooks or scripts on GitHub and you want to stop and say “I’ve done the work, now I’ll let other people discover it.” While this might happen in an ideal world, in the real world, getting your projects noticed requires communicating your results. It would be nice if the best work on GitHub automatically surfaced to the top, but in reality, it’s the work that is best communicated that has the greatest impact. Think about the last time you found a project code repository on GitHub: if you’re like me, then you read an interesting article about a project and then followed through to the code. People go from an article to the code because first they need a compelling reason to check out the code. That’s not meant to be cynical, it’s just the way things work: people aren’t going to dig into your analysis until they know what you did and why it’s important/interesting. To give a real-world example, my Data Analysis repo is a collection of numerous data science projects, most written with very rough code. Yet, because I wrote a few articles about some of the projects, it has over 600 stars. While stars are not a great way to measure impact, it’s clear that people are using this code and finding value in it. Yet, the other day when I stumbled on this repo for Bayesian Optimization of Combinatorial Structures (BOCS) , which objectively has better code than anything I’ve written, I was shocked to see it had only 2 stars. Much like great ideas die in isolation, the best code will go unnoticed without compelling communication of the results. The value of an analysis is proportional not to using the best algorithm or the most data, but rather to how well you can share the results with a wide audience. In 1854, John Snow helped slow a cholera epidemic in London using 578 data points, a public essay, and a dot map. Rather than hide away his results in a notebook and hope that people stumbled on them, he published his work and made it easily accessible. In the end, he was able to convince the town members to disable a water pump, thereby stopping the spread of cholera and achieving the objective of data science: make better real-world decisions using data. Writing a blog post gives you practice in one of the most critical parts of data science: communicating your work to a wide audience. Well-written code and a thorough analysis is a good start, but to complete your project, you need to tie it into a compelling narrative. An article is the perfect medium to explain your results and make people care about all your hard work. Opportunities: Writing Opens Doors Although data science can be more objective in hiring than other fields, getting a job is still mostly about who you know — or who knows of you — rather than what you know. The whole point of going to college (only a slight exaggeration here) is not to learn things you’ll use in your career, but to get to know people and make connections in your intended career field. Fortunately, in data science at this point, while going to college for something is helpful, it’s not a necessity. With the ability to reach thousands of people online through a blog post, you can form those critical connections and open doors just through the act of writing and sharing— with no tuition required. When you write about your projects in a public forum, you can gain access to opportunities that don’t come just from turning in an assignment. I went to college for mechanical engineering, and didn’t make a single connection (let alone learn any useful skills) in data science at school. However, I did start writing in my last semester, and as a result, was able to form numerous relationships with potential employers, collaborators, and even book editors (the answer is eventually) that have been immensely helpful as I navigate the start of a data science career. Going back to the first point, my code is nowhere near as good as many other data scientists’, but I‘ve been fortunate to get opportunities because I’m able to make my work accessible. I have never been contacted solely from someone who found me on GitHub, but I’ve been contacted hundreds of times from people who read my articles. While my employer — Feature Labs — did find my GitHub work, it wasn’t by searching for “great data science analysis” on GitHub. Rather, it was through an article I’d written that walked through a project and summarized the conclusions. Remember, it’s not code to article, it’s article to code. A blog post is a great medium for building important connections because it shows that 1. You’re doing good data science work and 2. You care about sharing it and teaching it to others. Excessive enthusiasm for data science is not a requisite to a job, but showing that you are interested in the field and learning will help attract employers, especially if you are just starting out and don’t have much experience. Furthermore, well-written blog posts can have a long shelf life, giving you a portfolio for potentially years to come. There isn’t yet an established path to a data science job which means that we all get to forge our own. Writing and sharing with the community can help you form all-important connections and gain a foothold in the field. Feedback: Work, Share, Listen, Improve, Repeat As a new field, there are rarely any standard answers in data science. The best way to learn is to try something out, make a mistake, and learn from that experience. Putting your work out in a public venue means you can get feedback from thousands of data scientists with thousands of years of collective experience. That’s the benefit of being part of a community: together, we know more than any one person ever could, and by being a contributing member of that community, you can take advantage of that knowledge by using feedback to improve your own work. Dealing with feedback on the Internet can be tough, but I’ve found the data science community, and in particular, Towards Data Science on Medium, to be extremely civil. My strategy for dealing with comments is: Unfortunately, we often don’t take the time to review our own work as often as we should, but, fortunately, we can share it with the world and have other people review it. These other people are probably more honest about our work than we would have been, so we get a more objective assessment by sharing. The most valuable part of a class is never the content, it’s the feedback you get from professors on your assignments. Fortunately, you can get that feedback without taking any classes by publicly sharing your projects with the data science community in a blog post. Although school teaches us to be failure-averse, it’s only by repeatedly failing and then improving as a result that we get any better. Unequivocally, I’m a better writer and data scientist because I’ve put my work out for criticism and listened to the feedback. Right now, you probably have one or a dozen Jupyter notebooks that would make great articles! Take an hour or two to write up one of these and put it out into the world. It doesn’t have to be perfect: as long as you have done the data science work, people will respect your article. If you struggle with releasing anything that’s not perfect (one of my largest problems), then set a time limit, say 60 minutes, and whatever you get done in 60 minutes has to be released. I’ve had to do this a couple times, and it’s made my resulting work more to the point and more effective. Right now, take one of your Jupyter notebooks, and write an article. Put it out on Medium and then let the community see your work. Although the rewards are not instantaneous, over time the benefits will accrue: Keep working on your data science projects, but don’t stop at the moment the code goes up on GitHub or is turned in. Take that final step and write an article. Your future self will thank you! As always, I welcome feedback, constructive criticism, and hearing about your data science projects. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Writing Creates Opportunities: by exposing your work to the world, you’ll be able to form connections that can lead to job offers, collaborations, and new project ideas. Feedback: the cycle of getting better is: do work, share, listen to constructive criticism, improve work, repeat Constructive Criticism: write down the comment, fix any parts of the current analysis that can be fixed, and practice implementing the recommendation whenever possible in future projects Non-constructive criticism: ignore Opportunities/connections will open up Your data science and writing will improve as you build on constructive criticism. Communication Practice: good code by itself it not enough. The best analysis will have no impact if you can’t make people care about the work. Positive Comments: acknowledge with a thanks You’ll get better at the crucial task of communication",The most important part of a data science project is writing a blog post,14,published,18428,2041,2.4301812836844685,11,1,1,1,0,0
54,855,284.04123035472225,107,https://towardsdatascience.com/parallelizing-feature-engineering-with-dask-3db88aec33b7,0,Towards Data Science,2018-08-16 14:43:00,27.19,10,1173,2018-08-06 15:18:00,"['Big Data', 'Education', 'Data Science', 'Towards Data Science', 'Python']","Parallelizing Feature Engineering with Dask How to scale Featuretools using parallel processing When a computation is prohibitively slow, the most important question to ask is: “What is the bottleneck?” Once you know the answer, the logical next step is to figure out how to get around that bottleneck. Often, as we’ll see, the bottleneck is that we aren’t taking full advantage of our hardware resources, for example, running a calculation on only one core when our computer has eight. Simply getting a bigger machine — in terms of RAM or cores — will not solve the problem if our code isn’t written to use all our resources. The solution therefore is to rewrite the code to utilize whatever hardware we do have as efficiently as possible. In this article, we’ll see how to refactor our automated feature engineering code to run in parallel on all our laptop’s cores, in the process reducing computation time by over 8x. We’ll make use of two open-source libraries — Featuretools for automated feature engineering and Dask for parallel processing — and solve a problem with a real-world dataset. Our exact solution is specific for this problem, but the general approach we develop can be utilized to scale your own computations to larger datasets. Although here we’ll stick to using one computer and multiple cores, in the future, we’ll use this same method to run computations on multiple machines. The complete code implementation is available in a Jupyter Notebook on GitHub. If you aren’t yet familiar with Featuretools, check out the documentation or this article. Here we’ll focus mainly on using Featuretools with Dask and skip over some of the automated feature engineering details. The Problem: Too Much Data, Not Enough Time The primary issue when applying automated feature engineering with Featuretools for the Home Credit Default Risk problem (a machine learning competition currently running on Kaggle where the objective is to predict whether or not a client will repay a loan) is that we have a lot of data which results in a very long feature calculation time. Using Deep Feature Synthesis, we are able to automatically generate 1820 features from 7 data tables and 58 million rows of client information, but a call to this function with only one core takes 25 hours even on an EC2 instance with 64 GB of RAM! Given that our EC2 instance — and even our laptop — has 8 cores, to speed up the calculation we don’t need more RAM, we need to make use of those cores. Featuretools does allow for parallel processing by setting the n_jobs parameter in a call to Deep Feature Synthesis. However, currently the function must send the entire EntitySet to all workers — cores — on the machine. With a large EntitySet, this can result in problems if the memory for each worker is exhausted. We are currently working on better parallelization at Feature Labs, but for now we solve our problem with Dask. The approach is to break one large problem up into many smaller ones and then use Dask to run multiple small problems at a time — each one on a different core. The important point here is that we make each problem — task — independent of the others so they can run simulataneously. Because we are making features for each client in the dataset, each task is to make a feature matrix for a subset of clients. Our approach is outlined below: At the end, we’ll have a number of smaller feature matrices that we can then join together into a final feature matrix. This same method — breaking one large problem into numerous smaller ones that are run in parallel — can be scaled to any size dataset and implemented in other libraries for distribution computing such as Spark using PySpark. Whatever resources we have, we want to use them as efficiently as possible, and we can take this same framework to scale to larger datasets. Our first step is to create small partitions of the original dataset, each one containing information from all seven tables for a subset of the clients. Each partition can then be used to independently calculate a feature matrix for a group of clients. This operation is done by taking a list of all clients, breaking it into 104 sub-lists, and then iterating through these sub-lists, each time subsetting the data to only include clients from the sub-list and saving the resulting data to disk. The basic pseudo code of this process is: 104 partitions was selected based on trial and error and 3 general guidelines: (As an additional point of optimization, we convert the pandas object data types to category where applicable to reduce memory usage. This gets our entire dataset from 4 GB to about 2 GB. I recommend reading the Pandas documentation for category data types so you are using them effectively. Saving all 104 partitions to disk took about 30 minutes, but this is a process that only must be done once. An Entity Set in Featuretools is a useful data structure because it holds multiple tables and the relationships between them. To create an EntitySet from a partition, we write a function that reads a partition from disk and then generates the EntitySet with the tables and the relations linking them. The pseudo code for this step is: Notice that this function returns the EntitySet rather than saving it as we did with the partitions of data. Saving the raw data is a better option for this problem because we might want to modify the EntitySets — say by adding interesting values or domain knowledge features — while the raw data is never altered. The EntitySets are generated on the fly and then passed to the next stage: calculating the feature matrix. The function feature_matrix_from_entityset does exactly what the name suggests: takes in the EntitySet created previously and generates a feature matrix with thousands of features using Deep Feature Synthesis. The feature matrix is then saved to disk. To ensure we make the same exact features for each partition, we generate the feature definitions once and then use the Featuretools function calculate_feature_matrix. Here’s the entire function (we pass in a dictionary with the EntitySet and the partition number so we can save the feature matrix with a unique name): The chunk_size is the only tricky part of this call: this is used to break the feature matrix calculation into smaller parts, but since we already partitioned the data, this is no longer necessary. As long as the entire EntitySet can fit in memory, then I found it’s more time efficient to calculate all of the rows at once by setting the chunk_size equal to the number of observations. We now have all the individual parts we need to go from a data partition on disk to a feature matrix. These steps comprise the bulk of the work and getting Dask to run the tasks in parallel is surprisingly simple. Dask: Unleash Your Machine(s) Dask is a parallel computing library that allows us to run many computations at the same time, either using processes/threads on one machine (local), or many separate computers (cluster). For a single machine, Dask allows us to run computations in parallel using either threads or processes. Processes do not share memory and run on a single core and are better for compute-intensive tasks that do not need to communicate. Threads share memory, but in Python, due to the Global Interpreter Lock (GIL), two threads cannot operate at the same time in the same program and only some operations can be run in parallel using threads. (For more on threads/processes see this excellent article) Since calculating a feature matrix is compute-intensive and can be done independently for each partition, we want to use processes. The tasks do not need to share memory because each feature matrix does not depend on the others. In computer science terms, by partitioning the data, we have made our problem embarrassingly parallel because there is no need for communication between the workers. If we start Dask using processes — as in the following code — we get 8 workers, one for each core, with each worker allotted 2 GB of memory (16 GB total / 8 workers, this will vary depending on your laptop). To check that everything worked out, we can navigate to localhost:8787 where Dask has set up a Bokeh dashboard for us. On the Workers tab, we see 8 workers each with 2 GB of memory: At the moment, all 8 workers are idle because we haven’t given them anything to do. The next step is to create a “Dask bag” which is basically a list of tasks for Dask to allocate to workers. We make the “bag” using the db.from_sequence method and the list of partition paths. Then, we map computation tasks onto the bag. To map means to take a function and a list of inputs and apply the function to each element in the list. Since we first need to make anEntitySet from each partition, we map the associated function to the “bag”: Next we doing another mapping, this time to make the feature matrixes: This code will take the output of the first map — the EntitySet — and pass it to the second map. These steps don’t actually run the computations, but rather make a list of tasks that Dask will then allocate to workers. To run the tasks and make the feature matrices we call: Dask automatically allocates tasks to workers based on the task graph (a Directed Acyclic Graph) constructed from the mappings. We can view the task graph and status on the Bokeh dashboard as the computation occurs. The set of blocks on the left represent the entity_set_from_partition function calls and the blocks on the right are feature_matrix_from_entityset. From this graph, we can there is a dependency between the two functions but not between feature matrix calculations for each partition. There are a number of other visualizations on the Bokeh dashboard including a task stream (left below) and a profile of operations (right below): From the task stream, we can see that all eight workers are in use at once with a total of 208 tasks to complete. The profile tells us that the longest operation is calculating the feature matrix for each partition. On my MacBook, it took 6200 seconds (1.75 hours) to build and save all 104 feature matrices. That’s quite an improvement, all from rewriting our code to use our available hardware as efficiently as possible. Instead of getting a larger computer, we rewrite our code to make the most efficient use of the resources we have. Then, when we do get a bigger computer, we’ll be able to use the same code to minimize computation time. Once we have the individual feature matrices, we can directly use them for modeling if we are using an algorithm that allows on-line — also called incremental — learning. Another option is to create one feature matrix which can be done in pure Python using Pandas: The single feature matrix has 350,000 rows and 1,820 columns, the same shape as when I first made it using a single core. Conclusions Rather than thinking of how to get more computational power, we should think about how to use the hardware we do have as efficiently as possible. In this article, we walked through how we can parallelize our code using Dask which lets us use a laptop to complete a calculation 8 times faster than on a single core. The solution we engineered makes use of a few key concepts: Now we can not only take advantage of the speed and modeling performance of automated feature engineering with Featuretools, but we can use Dask to carry out our computations in parallel and get results quicker. Moreover, we can use the same methods to scale to larger datasets, leaving us well-situated for whatever machine learning problems come our way. If building meaningful, high-performance predictive models is something you care about, then get in touch with us at Feature Labs. While this project was completed with the open-source Featuretools, the commercial product offers additional tools and support for creating machine learning solutions. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Write functions to make a feature matrix from each partition of data Use Dask to run Step 2 in parallel on all our cores Each partition must be small enough to fit in memory of a single worker More partitions means less variation in time to complete each task Write functions to process one chunk at a time Delegate each chunk to a worker and compute in parallel Make a large problem into many small problems by partitioning data We want at least as many partitions as workers (cores) and the number should be a multiple of number of the workers Break up the problem into smaller, independent chunks",Parallelizing Feature Engineering with Dask,5,published,4314,2343,0.3649167733674776,9,1,1,1,0,1
55,1400,281.82855328452547,272,https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c,8,Towards Data Science,2018-08-18 19:49:00,44.04,3,13755,2018-08-18 19:01:00,"['Machine Learning', 'Education', 'Python', 'Towards Data Science', 'Data Science']","How to Visualize a Decision Tree from a Random Forest in Python using Scikit-Learn A helpful utility for understanding your model Here’s the complete code: just copy and paste into a Jupyter Notebook or Python script, replace with your data and run: The final result is a complete decision tree as an image. Explanation of code 2. Export Tree as .dot File: This makes use of the export_graphviz function in Scikit-Learn. There are many parameters here that control the look and information displayed. Take a look at the documentation for specifics. 3. Convert dot to png using a system command: running system commands in Python can be handy for carrying out simple tasks. This requires installation of graphviz which includes the dot utility. For the complete options for conversion, take a look at the documentation. 4. Visualize: the best visualizations appear in the Jupyter Notebook. (Equivalently you can use matplotlib to show images). With a random forest, every tree will be built differently. I use these images to display the reasoning behind a decision tree (and subsequently a random forest) rather than for specific details. It’s helpful to limit maximum depth in your trees when you have a lot of features. Otherwise, you end up with massive trees, which look impressive, but cannot be interpreted at all! Here’s a full example with 50 features. Conclusions Machine learning still suffers from a black box problem, and one image is not going to solve the issue! Nonetheless, looking at an individual decision tree shows us this model (and a random forest) is not an unexplainable method, but a sequence of logical questions and answers — much as we would form when making predictions. Feel free to use and adapt this code for your data. As always, I welcome feedback, constructive criticism, and hearing about your data science projects. I can be reached on Twitter @koehrsen_will Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",How to Visualize a Decision Tree from a Random Forest in Python using Scikit-Learn,15,published,31231,373,3.753351206434316,0,1,1,1,1,1
52,4100,280.2455565562963,799,https://towardsdatascience.com/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc,13,Towards Data Science,2018-08-20 09:48:00,16.73,17,7471,2018-07-29 12:01:00,"['Machine Learning', 'Data Science', 'Education', 'Python', 'Towards Data Science']","A “Data Science for Good“ Machine Learning Project Walk-Through in Python: Part One Solving a complete machine learning problem for societal benefit Data science is an immensely powerful tool in our data-driven world. Call me idealistic, but I believe this tool should be used for more than getting people to click on ads or spend more time consumed by social media. In this article and the sequel, we’ll walk through a complete machine learning project on a “Data Science for Good” problem: predicting household poverty in Costa Rica. Not only do we get to improve our data science skills in the most effective manner — through practice on real-world data — but we also get the reward of working on a problem with social benefits. It turns out the same skills used by companies to maximize ad views can also be used to help relieve human suffering. The full code is available as a Jupyter Notebook both on Kaggle (where it can be run in the browser with no downloads required) and on GitHub. This is an active Kaggle competition and a great project to get started with machine learning or to work on some new skills. Problem and Approach The Costa Rican Household Poverty Level Prediction challenge is a data science for good machine learning competition currently running on Kaggle. The objective is to use individual and household socio-economic indicators to predict poverty on a household basis. IDB, the Inter-American Development Bank, developed the problem and provided the data with the goal of improving upon traditional methods for identifying families at need of aid. The poverty labels fall into four levels making this a supervised multi-class classification problem: The general approach to a machine learning problem is: While these steps may seem to present a rigid structure, the machine learning process is non-linear, with parts repeated multiple times as we get more familiar with the data and see what works. It’s nice to have an outline to provide a general guide, but we’ll often return to earlier parts of the process if things aren’t working out or as we learn more about the problem. We’ll go through the first four steps at a high-level in this article, taking a look at some examples, with the full details available in the notebooks. This problem is a great one to tackle both for beginners — because the dataset is manageable in size — and for those who already have a firm footing because Kaggle offers an ideal environment for experimenting with new techniques. The last two steps, plus an experimental section, can be found in part two. Understanding the Problem and Data In an ideal situation, we’d all be experts in the problem subject with years of experience to inform our machine learning. In reality, we often work with data from a new field and have to rapidly acquire knowledge both of what the data represents and how it was collected. Fortunately, on Kaggle, we can use the work shared by other data scientists to get up to speed relatively quickly. Moreover, Kaggle provides a discussion platform where you can ask questions of the competition organizers. While not exactly the same as interacting with customers at a real job, this gives us an opportunity to figure out what the data fields represent and any considerations we should keep in mind as we get into the problem. Some good questions to ask at this point are: For example, after engaging in discussions with the organizers, the community found out the text string “yes” actually maps to the value 1.0 and that the maximum value in one of the columns should be 5 which can be used to correct outliers. We would have been hard-pressed to find out this information without someone who knows the data collection process! Part of data understanding also means digging into the data definitions. The most effective way is literally to go through the columns one at a time, reading the description and making sure you know what the data represents. I find this a little dull, so I like to mix this process with data exploration, reading the column description and then exploring the column with stats and figures. For example, we can read that meaneduc is the average amount of education in the family, and then we can plot it distributed by the value of the label to see if it has any noticeable differences between the poverty level . This shows that families the least at risk for poverty — non-vulnerable — tend to have higher average education levels than those most at risk. Later in feature engineering, we can use this information by building features from the education since it seems to show a different between the target labels. There are a total of 143 columns (features), and while for a real application, you want to go through each with an expert, I didn’t exhaustively explore all of these in the notebook. Instead, I read the data definitions and looked at the work of other data scientists to understand most of the columns. Another point to establish from the problem and data understanding stage is how we want to structure our training data. In this problem, we’re given a single table of data where each row represents an individual and the columns are the features. If we read the problem definition, we are told to make predictions for each household which means that our final training dataframe (and also testing) should have one row for each house. This point informs our entire pipeline, so it’s crucial to grasp at the outset. Finally, we want to make sure we understanding the labels and the metric for the problem. The label is what we want to predict, and the metric is how we’ll evaluate those predictions. For this problem, the label is an integer, from 1 to 4, representing the poverty level of a household. The metric is the Macro F1 Score, a measure between 0 and 1 with a higher value indicating a better model. The F1 score is a common metric for binary classification tasks and “Macro” is one of the averaging options for multi-class problems. Once you know the metric, figure out how to calculate it with whatever tool you are using. For Scikit-Learn and the Macro F1 score, the code is: Knowing the metric allows us to assess our predictions in cross validation and using a hold-out testing set, so we know what effect, if any, our choices have on performance. For this competition, we are given the metric to use, but in a real-world situation, we’d have to choose an appropriate measure ourselves. Data Exploration and Data Cleaning Data exploration, also called Exploratory Data Analysis (EDA), is an open-ended process where we figure out what our data can tell us. We start broad and gradually hone in our analysis as we discover interesting trends / patterns that can be used for feature engineering or find anomalies. Data cleaning goes hand in hand with exploration because we need to address missing values or anomalies as we find them before we can do modeling. For an easy first step of data exploration, we can visualize the distribution of the labels for the training data (we are not given the testing labels). Right away this tells us we have an imbalanced classification problem, which can make it difficult for machine learning models to learn the underrepresented classes. Many algorithms have ways to try and deal with this, such as setting class_weight = ""balanced"" in the Scikit-Learn random forest classifier although they don’t work perfectly. We also want to make sure to use stratified sampling with cross validation when we have an imbalanced classification problem to get the same balance of labels in each fold. To get familiar with the data, it’s helpful to go through the different column data types which represent different statistical types of data: I’m using statistical type to mean what the data represents — for example a Boolean that can only be 1 or 0 — and data type to mean the actual way the values are stored in Python such as integers or floats. The statistical type informs how we handle the columns for feature engineering. (I specified usually for each data type / statistical type pairing because you may find that statistical types are saved as the wrong data type.) If we look at the integer columns for this problem, we can see that most of them represent Booleans because there are only two possible values: Going through the object columns, we are presented with a puzzle: 2 of the columns are Id variables (stored as strings), but 3 look to be numeric values. This is where our earlier data understanding comes into play. For these three columns, some entries are “yes” and some are “no” while the rest are floats. We did our background research and thus know that a “yes” means 1 and a “no” means 0. Using this information, we can correct the values and then visualize the variable distributions colored by the label. This is a great example of data exploration and cleaning going hand in hand. We find something incorrect with the data, fix it, and then explore the data to make sure our correction was appropriate. A critical data cleaning operation for this data is handling missing values. To calculate the total and percent of missing values is simple in Pandas: In some cases there are reasons for missing values: the v2a1 column represents monthly rent and many of the missing values are because the household owns the home. To figure this out, we can subset the data to houses missing the rent payment and then plot the tipo_ variables (I’m not sure where these column names come from) which show home ownership. Based on the plot, the solution is to fill in the missing rent payments for households that own their house with 0 and leave the others to be imputed. We also add a boolean column that indicates if the rent payment was missing. The other missing values in the columns are dealt with the same way: using knowledge from other columns or about the problem to fill in the values, or leaving them to be imputed. Adding a boolean column to indicate missing values can also be useful because sometimes the information that a value was missing is important. Another crucial point to note is that for missing values, we often want to think about using information in other columns to fill in missing values such as we did with the rent payment. Once we’ve handled the missing values, anomalies, and incorrect data types, we can move on to feature engineering. I usually view data exploration as an ongoing process rather than one set chunk. For example, as we get into feature engineering, we might want to explore the new variables we create. The data science process is non-linear: while we have a general outline, we often go back and redo previous steps as we get deeper into the problem. Feature Engineering If you follow my work, you’ll know I’m convinced automated feature engineering — with domain expertise — will take the place of traditional manual feature engineering. For this problem, I took both approaches, doing mostly manual work in the main notebook, and then writing another notebook with automated feature engineering. Not surprisingly, the automated feature engineering took one tenth the time and achieved better performance! Here I’ll show the manual version, but keep in mind that automated feature engineering (with Featuretools) is a great tool to learn. In this problem, our primary objective for feature engineering is to aggregate all the individual level data at the household level. That means grouping together the individuals from one house and then calculating statistics such as the maximum age, the average level of education, or the total number of cellphones owned by the family. Fortunately, once we have separated out the individual data (into the ind dataframe), doing these aggregations is literally one line in Pandas (with idhogar the household identifier used for grouping): After renaming the columns, we have a lot of features that look like: The benefit of this method is that it quickly creates many features. One of the drawbacks is that many of these features might not be useful or are highly correlated (called collinear) which is why we need to use feature selection. An alternative method to aggregations is to calculate features one at a time using domain knowledge based on what features might be useful for predicting poverty. For example, in the household data, we create a feature called warning which adds up a number of household “warning signs” ( house is a dataframe of the household variables): We can also calculate “per capita” features by dividing one value by another ( tamviv is the number of household members): When it comes to manual vs automated feature engineering, I think the optimal answer is a blend of both. As humans, we are limited in the features we build both by creativity — there are only so many features we can think to make — and time — there is only so much time for us to write the code. We can make a few informed features like those above by hand, but where automated feature engineering excels is when doing aggregations that can automatically build on top of other features. The best approach is to spend some time creating a few features by hand using domain knowledge, and then hand off the process to automated feature engineering to generate hundreds or thousands more. (Featuretools is the most advanced open-source Python library for automated feature engineering. Here’s an article to get you started in about 10 minutes.) Once we have exhausted our time or patience making features, we apply feature selection to remove some features, trying to keep only those that are useful for the problem. “Useful” has no set definition, but there are some heuristics (rules of thumb) that we use to select features. One method is by determining correlations between features. Two variables that are highly correlated with one another are called collinear. These are a problem in machine learning because they slow down training, create less interpretable models, and can decrease model performance by causing overfitting on the training data. The tricky part about removing correlated features is determining the threshold of correlation for saying that two variables are too correlated. I generally try to stay conservative, using a correlation coefficient in the 0.95 or above range. Once we decide on a threshold, we use the below code to remove one out of every pair of variables with a correlation above this value: We are only removing features that are correlated with one another. We want features that are correlated with the target(although a correlation of greater than 0.95 with the label would be too good to be true)! There are many methods for feature selection (we’ll see another one in the experimental section near the end of the article). These can be univariate — measuring one variable at a time against the target — or multivariate — assessing the effects of multiple features. I also tend to use model-based feature importances for feature selection, such as those from a random forest. After feature selection, we can do some exploration of our final set of variables, including making a correlation heatmap and a pairsplot. One point we get from the exploration is the relationship between education and poverty: as the education of a household increases (both the average and the maximum), the severity of poverty tends to decreases (1 is most severe): On the other hand, as the level of overcrowding — the number of people per room — increases, the severity of the poverty increases: These are two actionable insights from this competition, even before we get to the machine learning: households with greater levels of education tend to have less severe poverty, and households with more people per room tend to have greater levels of poverty. I like to think about the ramifications and larger picture of a data science project in addition to the technical aspects. It can be easy to get overwhelmed with the details and then forget the overall reason you’re working on this problem. The ultimate goal of this project is to figure out how to predict poverty to most effectively get help to those in need. Model Comparison The following graph is one of my favorite results in machine learning: it displays the performance of machine learning models on many datasets, with the percentages showing how many times a particular method beat any others. (This is from a highly readable paper by Randal Olson.) What this shows is that there are some problems where even a simple Logistic Regression will beat a Random Forest or Gradient Boosting Machine. Although the Gradient Tree Boosting model generally works the best, it’s not a given that it will come out on top. Therefore, when we approach a new problem, the best practice is to try out several different algorithms rather than always relying on the same one. I’ve gotten stuck using the same model (random forest) before, but remember that no one model is always the best. Fortunately, with Scikit-Learn, it’s easy to evaluate many machine learning models using the same syntax. While we won’t do hyperparameter tuning for each one, we can compare the models with the default hyperparameters in order to select the most promising model for optimization. In the notebook, we try out six models spanning the range of complexity from simple — Gaussian Naive Bayes — to complex — Random Forest and Gradient Boosting Machine. Although Scikit-Learn does have a GBM implementation, it’s fairly slow and a better option is to use one of the dedicated libraries such as XGBoost or LightGBM. For this notebook, I used Light GBM and choose the hyperparameters based on what have worked well in the past. To compare models, we calculate the cross validation performance on the training data over 5 or 10 folds. We want to use the training data because the testing data is only meant to be used once as an estimate of the performance of our final model on new data. The following plot shows the model comparison. The height of the bar is the average Macro F1 score over the folds recorded by the model and the black bar is the standard deviation: (To see an explanation of the names, refer to the notebook. RF stands for Random Forest and GBM is Gradient Boosting Machine with SEL representing the feature set after feature selection). While this isn’t entirely a level comparison — I did not use the default hyperparameters for the Gradient Boosting Machine — the general results hold: the GBM is the best model by a large margin. This reflects the findings of most other data scientists. Notice that we cross-validated the data before and after feature selection to see its effect on performance. Machine learning is still largely an empirical field, and the only way to know if a method is effective is to try it out and then measure performance. It’s important to test out different choices for the steps in the pipeline — such as the correlation threshold for feature selection — to determine if they help. Keep in mind that we also want to avoid placing too much weight on cross-validation results, because even with many folds, we can still be overfitting to the training data. Finally, even though the GBM was best for this dataset, that will not always be the case! Based on these results, we can choose the gradient boosting machine as our model (remember this is a decision we can go back and revise!). Once we decide on a model, the next step is to get the most out of it, a process known as model hyperparameter optimization. Recognizing that not everyone has time for a 30-minute article (even on data science) in one sitting, I’ve broken this up into two parts. The second part covers model optimization, interpretation, and an experimental section. Conclusions By this point, we can see how all the different parts of machine learning come together to form a solution: we first had to understand the problem, then we dug into the data, cleaning it as necessary, then we made features for a machine learning model, and finally we evaluated several different models. We’ve covered many techniques and have a decent model (although the F1 score is relatively low, it places in the top 50 models submitted to the competition). Nonetheless, we still have a few steps left: through optimization, we can improve our model, and then we have to interpret our results because no analysis is complete until we’ve communicated our work. As a next step, see part two, check out the notebook (also on GitHub), or get started solving the problem for yourself. As always, I welcome feedback, constructive criticism, and hearing about your data science projects. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Multi-Class Classification: labels are discrete with more than 2 values Data cleaning / exploratory data analysis Feature engineering / feature selection Model comparison Model optimization Interpretation of results Are there known issues with the data? Certain values / anomalies to look out for? How was the data collected? If there are outliers, are they likely the result of human error or extreme but still valid data? int : Usually are either Boolean or ordinal (discrete with an ordering) object or category : Usually strings or mixed data types that must be converted in some manner before machine learning Supervised: given the labels for the training data Understand the problem and data descriptions Are there certain variables that are considered most important for this problem according to the domain experts? float : These usually are continuous numeric variables",A “Data Science for Good“ Machine Learning Project Walk-Through in Python: Part One,14,published,44652,3996,1.026026026026026,21,1,1,1,1,1
51,677,280.24359590947915,134,https://towardsdatascience.com/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-two-2773bd52daf0,5,Towards Data Science,2018-08-20 09:51:00,19.63,15,1260,2018-08-20 09:13:00,"['Machine Learning', 'Data Science', 'Education', 'Python', 'Towards Data Science']","A “Data Science for Good” Machine Learning Project Walk-Through in Python: Part Two Getting the most from our model, figuring out what it all means, and experimenting with new techniques Machine learning is a powerful framework that from the outside may look complex and intimidating. However, once we break down a problem into its component steps, we see that machine learning is really only a sequence of understandable processes, each one simple by itself. In the first half of this series, we saw how we could implement a solution to a “data science for good” machine learning problem, leaving off after we had selected the Gradient Boosting Machine as our model of choice. In this article, we’ll continue with our pipeline for predicting poverty in Costa Rica, performing model optimizing, interpreting the model, and trying out some experimental techniques. The full code is available as a Jupyter Notebook both on Kaggle (where it can be run in the browser with no downloads required) and on GitHub. This is an active Kaggle competition and a great project to get started with machine learning or to work on some new skills. Model Optimization Model optimization means searching for the model hyperparameters that yield the best performance — measured in cross-validation — for a given dataset. Because the optimal hyperparameters vary depending on the data, we have to optimize — also known as tuning — the model for our data. I like to think of tuning as finding the best settings for a machine learning model. There are 4 main methods for tuning, ranked from least efficient (manual) to most efficient (automated). Naturally, we’ll skip the first three methods and move right to the most efficient: automated hyperparameter tuning. For this implementation, we can use the Hyperopt library, which does optimization using a version of Bayesian Optimization with the Tree Parzen Estimator. You don’t need to understand these terms to use the model, although I did write a conceptual explanation here. (I also wrote an article for using Hyperopt for model tuning here.) The details are a little protracted (see the notebook), but we need 4 parts for implementing Bayesian Optimization in Hyperopt The basic idea of Bayesian Optimization (BO) is that the algorithm reasons from the past results — how well previous hyperparameters have scored — and then chooses the next combination of values it thinks will do best. Grid or random search are uninformed methods that don’t use past results and the idea is that by reasoning, BO can find better values in fewer search iterations. See the notebook for the complete implementation, but below are the optimization scores plotted over 100 search iterations. Unlike in random search where the scores are, well random over time, in Bayesian Optimization, the scores tend to improve over time as the algorithm learns a probability model of the best hyperparameters. The idea of Bayesian Optimization is that we can optimize our model (or any function) quicker by focusing the search on promising settings. Once the optimization has finished running, we can use the best hyperparameters to cross validate the model. Optimizing the model will not always improve our test score because we are optimizing for the training data. However, sometimes it can deliver a large benefit compared to the default hyperparameters. In this case, the final cross validation results are shown below in dataframe form: The optimized model (denoted by OPT and using 10 cross validation folds with the features after selection) places right in the middle of the non-optimized variations of the Gradient Boosting Machine (which used hyperparameters I had found worked well for previous problems.) This indicates we haven’t found the optimal hyperparameters yet, or there could be multiple sets of hyperparameters that performly roughly the same. We can continue optimization to try and find even better hyperparameters, but usually the return to hyperparameter tuning is much less than the return to feature engineering. At this point we have a relatively high-performing model and we can use this model to make predictions on the test data. Then, since this is a Kaggle competition, we can submit the predictions to the leaderboard. Doing this gets us into the top 50 (at the moment) which is a nice vindication of all our hard work! At this point, we have implemented a complete solution to this machine learning problem. Our model can make reasonably accurate predictions of poverty in Costa Rican households (the F1 score is relatively low, but this is a difficult problem). Now, we can move on to interpreting our predictions and see if our model can teach us anything about the problem. Even though we have a solution, we don’t want to lose sight of why our solution matters. The very nature of machine learning competitions can encourage bad practices, such as the mistake of optimizing for the leaderboard score at the cost of all other considerations. Generally this leads to using ever more complex models to eke out a tiny performance gain. In the real-world, above a certain threshold — which depends on the application — accuracy becomes secondary to explainability, and you’re better off with a slightly less performant model if it is simpler. A simple model that is put in use is better than a complex model which can never be deployed. Moreover, those at the top of the leaderboard are probably overfitting to the testing data and do not have a robust model. A good strategy for getting the most out of Kaggle is to work at a problem until you have a reasonably good solution — say 90% of the top leaderboard scores — and then not stress about getting to the very top. Competing is fun, but learning is the most valuable aspect of taking on these projects. Interpret Model Results In the midst of writing all the machine learning code, it can be easy to lose sight of the important questions: what are we making this model for? What will be the impact of our predictions? Thankfully, our answer this time isn’t “increasing ad revenue” but, instead, effectively predicting which households are most at risk for poverty in Costa Rica so they can receive needed help. To try and get a sense of our model’s output, we can examine the prediction of poverty levels on a household basis for the test data. For the test data, we don’t know the true answers, but we can compare the relative frequency of each predicted class with that in the training labels. The image below shows the training distribution of poverty on the left, and the predicted distribution for the testing data on the right: Intriguingly, even though the label “not vulnerable” is most prevalent in the training data, it is represented less often on a relative basis for the predictions. Our model predicts a higher proportion of the other 3 classes, which means that it thinks there is more severe poverty in the testing data. If we convert these fractions to numbers, we have 3929 households in the “non vulnerable” category and 771 households in the “extreme” category. Another way to look at the predictions is by the confidence of the model. For each prediction on the test data, we can see not only the label, but also the probability given to it by the model. Let’s take a look at the confidence by the value of the label in a boxplot. These results are fairly intuitive — our model is most confident in the most extreme predictions — and less confident in the moderate ones. Theoretically, there should be more separation between the most extreme labels and the targets in the middle should be more difficult to tease apart. Another point to draw from this graph is that overall, our model is not very sure of the predictions. A guess with no data would place 0.25 probability on each class, and we can see that even for the least extreme poverty, our model rarely has more than 40% confidence. What this tells us is this is a tough problem — there is not much to separate the classes in the available data. Ideally, these predictions, or those from the winning model in the competition, will be used to determine which families are most likely to need assistance. However, just the predictions alone do not tell us what may lead to the poverty or how our model “thinks”. While we can’t completely solve this problem yet, we can try to peer into the black box of machine learning. In a tree-based model — such as the Gradient Boosting Machine — the feature importances represent the sum total reduction in gini impurity for nodes split on a feature. I never find the absolute values very helpful, but instead normalize the numbers and look at them on a relative basis. For example, below are the 10 most important features from the optimized GBM model. Here we can see education and ages of family members making up the bulk of the most important features. Looking further into the importances, we also see the size of the family. This echoes findings by poverty researchers: family size is correlated to more extreme poverty, and education level is inversely correlated with poverty. In both cases, we don’t necessarily know which causes which, but we can use this information to highlight which factors should be further studied. Hopefully, this data can then be used to further reduce poverty (which has been decreasing steadily for the last 25 years). In addition to potentially helping researchers, we can use the feature importances for further feature engineering by trying to build more features on top of these. An example using the above results would be taking the meaneduc and dividing by the dependency to create a new feature. While this may not be intuitive, it’s hard to tell ahead of time what will work for a model. An alternative method to using the testing data to examine our model is to split the training data into a smaller training set and a validation set. Because we have the labels for all the training data, we can compare our predictions on the holdout validation data to the true values. For example, using 1000 observations for validation, we get the following confusion matrix: The values on the diagonal are those the model predicted correctly because the predicted label is the same as the true label. Anything off the diagonal the model predicted incorrectly. We can see that our model is the best at identifying the non-vulnerable households, but is not very good at discerning the other labels. As one example, our model incorrectly classifies 18 households as non-vulnerable which are in fact in extreme poverty. Predictions like these have real-world consequences because those might be families that as a result of this model, would not receive help. (For more on the consequences of incorrect algorithms, see Weapons of Math Destruction.) Overall, this mediocre performance — the model accuracy is about 60% which is much better than random guessing but not exceptional — suggests this problem may be difficult. It could be there is not enough information to separate the classes within the available data. One recommendation for the host organization — the Inter-American Development Bank — is that we need more data to better solve this problem. That could come either in the form of more features — so more questions on the survey — or more observations — a greater number of households surveyed. Either of these would require a significant effort, but the best return to time invested in a data science project is generally by gathering greater quantities of high-quality labeled data. There are other methods we can use for model understanding, such as Local Interpretable Model-agnostic Explainer (LIME), which uses a simpler linear model to approximate the model around a prediction. We can also look at individual decision trees in a forest which are typically straightforward to parse because they essentially mimic a human decision making process. Overall, machine learning still suffers from an explainability gap, which hinders its applicability: people want not only accurate predictions, but an understanding of how those predictions were generated. Exploratory Techniques We’ve already solved the machine learning problem with a standard toolbox, so why go further into exploratory techniques? Well, if you’re like me, then you enjoy learning new things just for the sake of learning. What’s more, the exploratory techniques of today will be the standard tools of tomorrow. For this project, I decided to try out two new (to me) techniques: Recursive feature elimination is a method for feature selection that uses a model’s feature importances — a random forest for this application — to select features. The process is a repeated method: at each iteration, the least important features are removed. The optimal number of features to keep is determined by cross validation on the training data. Recursive feature elimination is simple to use with Scikit-Learn’s RFECV method. This method builds on an estimator (a model) and then is fit like any other Scikit-Learn method. The scorer part is required in order to make a custom scoring metric using the Macro F1 score. While I’ve used feature importances for selection before, I’d never implemented the Recursive Feature Elimination method, and as usual, was pleasantly surprised at how easy this was to do in Python. The RFECV method selected 58 out of around 190 features based on the cross validation scores: The selected set of features were then tried out to compare the cross validation performance with the original set of features. (The final results are presented after the next section). Given the ease of using this method, I think it’s a good tool to have in your skill set for modeling. Like any other Scikit-Learn operation, it can fit into a Pipeline, allowing you to quickly execute a complete series of preprocessing and modeling operations. There are a number of unsupervised methods in machine learning for dimension reduction. These fall into two general categories: Typically, PCA (Principal Components Analysis) and ICA (Independent Components Analysis) are used both for visualization and as a preprocessing step for machine learning, while manifold methods like t-SNE (t-Distributed Stochastic Neighbors Embedding) are used only for visualization because they are highly dependent on hyperparameters and do not preserve distances within the data. (In Scikit-Learn, the t-SNE implementation does not have a transform method which means we can’t use it for modeling). A new entry on the dimension reduction scene is UMAP: Uniform Manifold Approximation and Projection. It aims to map the data to a low-dimensional manifold — so it’s an embedding technique, while simultaneously preserving global structure in the data. Although the math behind it is rigorous, it can be used like an Scikit-Learn method with a fit and transform call. I wanted to try these methods for both dimension reduction for visualization, and to add the reduced components as additional features. While this use case might not be typical, there’s no harm in experimenting! Below shows the code for using UMAP to create embeddings of both the train and testing data. The application of the other three methods is exactly the same (except TSNE which cannot be used to transform the testing data). After completing the transformations, we can visualize the reduced training features in 3 dimensions, with the points colored by the value of the target: None of the methods cleanly separates the data based on the label which follows the findings of other data scientists. As we discovered earlier, it may be that this problem is difficult considering the data to which we have access. Although these graphs cannot be used to say whether or not we can solve a problem, if there is a clean separation, then it indicates that there is something in the data that would allow a model to easily discern each class. As a final step, we can add the reduced features to the set of features after applying feature selection to see if they are useful for modeling. (Usually dimension reduction is applied and then the model is trained on just the reduced dimensions). The performance of every single model is shown below: The model using the dimension reduction features has the suffix DR while the number of folds following the GBM refers to the number of cross validation folds. Overall, we can see that the selected set of features (SEL) does slightly better, and adding in the dimension reduction features hurts the model performance! It’s difficult to conclude too much from these results given the large standard deviations, but we can say that the Gradient Boosting Machine significantly outperforms all other models and the feature selection process improves the cross validation performance. The experimental part of this notebook was probably the most enjoyable for me. It’s not only important to always be learning to stay ahead in the data science field, but it’s also enjoyable for the sake of learning something new. The drive to constantly be improving and gaining new knowledge is a critical skill for a data scientist. Next Steps Despite this exhaustive coverage of machine learning tools, we have not yet reached the end of methods to apply to this problem! Some additional steps we could take are: The great part about a Kaggle competition is you can read about many of these cutting-edge techniques in other data scientists’ notebooks. Moreover, these contests give us realistic datasets in a non-mission-critical setting, which is a perfect environment for experimentation. The best contests can lead to new advances by encouraging friendly competition, open sharing of work, and rewarding innovative approaches. As one example of the ability of competitions to better machine learning methods, the ImageNet Large Scale Visual Recognition Challenge led to significant improvements in convolutional neural networks. Conclusions Data science and machine learning are not incomprehensible methods: instead, they are sequences of straightforward steps that combine into a powerful solution. By walking through a problem one step at a time, we can learn how to build the entire framework. How we use this framework is ultimately up to us. We don’t have to dedicate our lives to helping others, but it is rewarding to take on a challenge with a deeper meaning. In this article, we saw how we could apply a complete machine learning solution to a data science for good problem, building a machine learning model to predict poverty levels in Costa Rica. Our approach followed a sequence of processes (1–4 were in part one): Finally, if after all that you still haven’t got your fill of data science, you can move on to exploratory techniques and learn something new! As with any process, you’ll only improve as you practice. Competitions are valuable for the opportunities they provide us to employ and develop skills. Moveover, they encourage discussion, innovation, and collaboration, leading both to more capable individual data scientists and a better community. Through this data science project, we not only improve our skills, but also make an effort to improve outcomes for our fellow humans. As always, I welcome feedback, constructive criticism, and hearing about your data science projects. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Grid Search: set up a hyperparameter grid and for every single combination of values, train a model, and find the validation score. The optimal set of hyperparameters are the ones that score the highest. Random Search: set up a hyperparameter grid and select random combinations of values, train the model, and find the validation score. Search iterations are limited based on time/resources Automated Tuning: Use methods (gradient descent, Bayesian Optimization, evolutionary algorithms) for a guided search for the best hyperparameters. These are informed methods that use past information. Domain space: region over which to search Algorithm for choosing the next hyperparameters: uses the past results to suggest next values to evaluate Results history: saves the past results Uniform Manifold Approximation and Projection for dimension reduction and visualization Embedding techniques that map data onto low-dimension manifolds: IsoMap, t-SNE Oversampling the minority class: a method to account for imbalanced classes by generating synthetic data points Further feature selection: especially after automated feature engineering, we have features that could negatively impact model performance Ensembling or stacking models: sometimes combining weaker — lower performing — models with stronger models can improve performance Perform data cleaning alongside exploratory data analysis Engineer relevant features automatically and manually Compare machine learning models Optimize the best performing model Interpret the model results and explore how it makes predictions Manual Tuning: select hyperparameters with intuition/experience or by guessing, train the models with the values, find the validation score, and repeat until you run out of patience or are satisfied with the results. Objective function: what we want to maximize (or minimize) Recursive Feature Elimination for feature selection Matrix decomposition algorithms: PCA and ICA Automated Feature Engineering: see this notebook for details Understand the problem and data",A “Data Science for Good” Machine Learning Project Walk-Through in Python: Part Two,14,published,6419,3834,0.17657798643714137,0,1,1,1,1,1
49,670,270.8566691947338,117,https://towardsdatascience.com/how-to-put-fully-interactive-runnable-code-in-a-medium-post-3dce6bdf4895,2,Towards Data Science,2018-08-29 19:08:00,74.24,1,1265,2018-08-29 08:40:00,"['Data Science', 'Programming', 'Education', 'Python', 'Writing']","How to Put Fully Interactive, Runnable Code in a Medium Post Code is meant to be interactive. Feel free to play around with the code above. This is a simple example, but you can create complex scripts for readers to run in many languages. Besides embedding code in your articles, repl.it has a great discussion forum and interactive exercises to learn coding. I welcome comments, feedback, or constructive criticism and can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Write some code in your favorite language. Copy and paste the url for your repl into a Medium post. Publish the post to make the code interactive. You can add packages, data files, or include multiple scripts in your repl This does work on mobile although not perfectly","How to Put Fully Interactive, Runnable Code in a Medium Post",12,published,1704,163,4.110429447852761,0,1,1,0,0,1
50,1900,270.1233693279167,215,https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76,6,Towards Data Science,2018-08-30 12:44:00,17.48,16,2598,2018-08-29 20:01:00,"['Machine Learning', 'Python', 'Education', 'Data Science', 'Towards Data Science']","An Implementation and Explanation of the Random Forest in Python A guide for using and understanding the random forest by building up from a single decision tree. Fortunately, with libraries such as Scikit-Learn, it’s now easy to implement hundreds of machine learning algorithms in Python. It’s so easy that we often don’t need any underlying knowledge of how the model works in order to use it. While knowing all the details is not necessary, it’s still helpful to have an idea of how a machine learning model works under the hood. This lets us diagnose the model when it’s underperforming or explain how it makes decisions, which is crucial if we want to convince others to trust our models. In this article, we’ll look at how to build and use the Random Forest in Python. In addition to seeing the code, we’ll try to get an understanding of how this model works. Because a random forest in made of many decision trees, we’ll start by understanding how a single decision tree makes classifications on a simple problem. Then, we’ll work our way to using a random forest on a real-world data science problem. The complete code for this article is available as a Jupyter Notebook on GitHub. Note: this article originally appeared on enlight, a community-driven, open-source platform with tutorials for those looking to study machine learning. Understanding a Decision Tree A decision tree is the building block of a random forest and is an intuitive model. We can think of a decision tree as a series of yes/no questions asked about our data eventually leading to a predicted class (or continuous value in the case of regression). This is an interpretable model because it makes classifications much like we do: we ask a sequence of queries about the available data we have until we arrive at a decision (in an ideal world). The technical details of a decision tree are in how the questions about the data are formed. In the CART algorithm, a decision tree is built by determining the questions (called splits of nodes) that, when answered, lead to the greatest reduction in Gini Impurity. What this means is the decision tree tries to form nodes containing a high proportion of samples (data points) from a single class by finding values in the features that cleanly divide the data into classes. We’ll talk in low-level detail about Gini Impurity later, but first, let’s build a Decision Tree so we can understand it on a high level. Decision Tree on Simple Problem We’ll start with a very simple binary classification problem as shown below: Our data only has two features (predictor variables), x1 and x2 with 6 data points — samples — divided into 2 different labels. Although this problem is simple, it’s not linearly separable, which means that we can’t draw a single straight line through the data to classify the points. We can however draw a series of straight lines that divide the data points into boxes, which we’ll call nodes. In fact, this is what a decision tree does during training. Effectively, a decision tree is a non-linear model built by constructing many linear boundaries. To create a decision tree and train (fit) it on the data, we use Scikit-Learn. During training we give the model both the features and the labels so it can learn to classify points based on the features. (We don’t have a testing set for this simple problem, but when testing, we only give the model the features and have it make predictions about the labels.) We can test the accuracy of our model on the training data: We see that it gets 100% accuracy, which is what we expect because we gave it the answers (y) for training and did not limit the depth of the tree. It turns out this ability to completely learn the training data can be a downside of a decision tree because it may lead to overfitting as we’ll discuss later. Visualizing a Decision Tree So what’s actually going on when we train a decision tree? I find a helpful way to understand the decision tree is by visualizing it, which we can do using a Scikit-Learn function (for details check out the notebook or this article). All the nodes, except the leaf nodes (colored terminal nodes), have 5 parts: The leaf nodes do not have a question because these are where the final predictions are made. To classify a new point, simply move down the tree, using the features of the point to answer the questions until you arrive at a leaf node where the class is the prediction. To make see the tree in a different way, we can draw the splits built by the decision tree on the original data. Each split is a single line that divides data points into nodes based on feature values. For this simple problem and with no limit on the maximum depth, the divisions place each point in a node with only points of the same class. (Again, later we’ll see that this perfect division of the training data might not be what we want because it can lead to overfitting.) Gini Impurity At this point it’ll be helpful to dive into the concept of Gini Impurity (the math is not intimidating!) The Gini Impurity of a node is the probability that a randomly chosen sample in a node would be incorrectly labeled if it was labeled by the distribution of samples in the node. For example, in the top (root) node, there is a 44.4% chance of incorrectly classifying a data point chosen at random based on the sample labels in the node. We arrive at this value using the following equation: The Gini Impurity of a node n is 1 minus the sum over all the classes J (for a binary classification task this is 2) of the fraction of examples in each class p_i squared. That might be a little confusing in words, so let’s work out the Gini impurity of the root node. At each node, the decision tree searches through the features for the value to split on that results in the greatest reduction in Gini Impurity. (An alternative for splitting nodes is using the information gain, a related concept). It then repeats this splitting process in a greedy, recursive procedure until it reaches a maximum depth, or each node contains only samples from one class. The weighted total Gini Impurity at each level of tree must decrease. At the second level of the tree, the total weighted Gini Impurity is 0.333: (The Gini Impurity of each node is weighted by the fraction of points from the parent node in that node.) You can continue to work out the Gini Impurity for each node (check the visual for the answers). Out of some basic math, a powerful model emerges! Eventually, the weighted total Gini Impurity of the last layer goes to 0 meaning each node is completely pure and there is no chance that a point randomly selected from that node would be misclassified. While this may seem like a positive, it means that the model may potentially be overfitting because the nodes are constructed only using training data. Overfitting: Or Why a Forest is better than One Tree You might be tempted to ask why not just use one decision tree? It seems like the perfect classifier since it did not make any mistakes! A critical point to remember is that the tree made no mistakes on the training data. We expect this to be the case since we gave the tree the answers and didn’t limit the max depth (number of levels). The objective of a machine learning model is to generalize well to new data it has never seen before. Overfitting occurs when we have a very flexible model (the model has a high capacity) which essentially memorizes the training data by fitting it closely. The problem is that the model learns not only the actual relationships in the training data, but also any noise that is present. A flexible model is said to have high variance because the learned parameters (such as the structure of the decision tree) will vary considerably with the training data. On the other hand, an inflexible model is said to have high bias because it makes assumptions about the training data (it’s biased towards pre-conceived ideas of the data.) For example, a linear classifier makes the assumption that the data is linear and does not have the flexibility to fit non-linear relationships. An inflexible model may not have the capacity to fit even the training data and in both cases — high variance and high bias — the model is not able to generalize well to new data. The balance between creating a model that is so flexible it memorizes the training data versus an inflexible model that can’t learn the training data is known as the bias-variance tradeoff and is a foundational concept in machine learning. The reason the decision tree is prone to overfitting when we don’t limit the maximum depth is because it has unlimited flexibility, meaning that it can keep growing until it has exactly one leaf node for every single observation, perfectly classifying all of them. If you go back to the image of the decision tree and limit the maximum depth to 2 (making only a single split), the classifications are no longer 100% correct. We have reduced the variance of the decision tree but at the cost of increasing the bias. As an alternative to limiting the depth of the tree, which reduces variance (good) and increases bias (bad), we can combine many decision trees into a single ensemble model known as the random forest. Random Forest The random forest is a model made up of many decision trees. Rather than just simply averaging the prediction of trees (which we could call a “forest”), this model uses two key concepts that gives it the name random: When training, each tree in a random forest learns from a random sample of the data points. The samples are drawn with replacement, known as bootstrapping, which means that some samples will be used multiple times in a single tree. The idea is that by training each tree on different samples, although each tree might have high variance with respect to a particular set of the training data, overall, the entire forest will have lower variance but not at the cost of increasing the bias. At test time, predictions are made by averaging the predictions of each decision tree. This procedure of training each individual learner on different bootstrapped subsets of the data and then averaging the predictions is known as bagging, short for bootstrap aggregating. The other main concept in the random forest is that only a subset of all the features are considered for splitting each node in each decision tree. Generally this is set to sqrt(n_features) for classification meaning that if there are 16 features, at each node in each tree, only 4 random features will be considered for splitting the node. (The random forest can also be trained considering all the features at every node as is common in regression. These options can be controlled in the Scikit-Learn Random Forest implementation). If you can comprehend a single decision tree, the idea of bagging, and random subsets of features, then you have a pretty good understanding of how a random forest works: The random forest combines hundreds or thousands of decision trees, trains each one on a slightly different set of the observations, splitting nodes in each tree considering a limited number of the features. The final predictions of the random forest are made by averaging the predictions of each individual tree. To understand why a random forest is better than a single decision tree imagine the following scenario: you have to decide whether Tesla stock will go up and you have access to a dozen analysts who have no prior knowledge about the company. Each analyst has low bias because they don’t come in with any assumptions, and is allowed to learn from a dataset of news reports. This might seem like an ideal situation, but the problem is that the reports are likely to contain noise in addition to real signals. Because the analysts are basing their predictions entirely on the data — they have high flexibility — they can be swayed by irrelevant information. The analysts might come up with differing predictions from the same dataset. Moreover, each individual analyst has high variance and would come up with drastically different predictions if given a different training set of reports. The solution is to not rely on any one individual, but pool the votes of each analyst. Furthermore, like in a random forest, allow each analyst access to only a section of the reports and hope the effects of the noisy information will be cancelled out by the sampling. In real life, we rely on multiple sources (never trust a solitary Amazon review), and therefore, not only is a decision tree intuitive, but so is the idea of combining them in a random forest. Random Forest in Practice Next, we’ll build a random forest in Python using Scikit-Learn. Instead of learning a simple problem, we’ll use a real-world dataset split into a training and testing set. We use a test set as an estimate of how the model will perform on new data which also lets us determine how much the model is overfitting. The problem we’ll solve is a binary classification task with the goal of predicting an individual’s health. The features are socioeconomic and lifestyle characteristics of individuals and the label is 0 for poor health and 1 for good health. This dataset was collected by the Centers for Disease Control and Prevention and is available here. Generally, 80% of a data science project is spent cleaning, exploring, and making features out of the data. However, for this article, we’ll stick to the modeling. (For details of the other steps, look at this article). This is an imbalanced classification problem, so accuracy is not an appropriate metric. Instead we'll measure the Receiver Operating Characteristic Area Under the Curve (ROC AUC), a measure from 0 (worst) to 1 (best) with a random guess scoring 0.5. We can also plot the ROC curve to assess a model. The notebook contains the implementation for both the decision tree and the random forest, but here we’ll just focus on the random forest. After reading in the data, we can instantiate and train a random forest as follows: After a few minutes to train, the model is ready to make predictions on the testing data as follows: We make class predictions (predict) as well as predicted probabilities (predict_proba) to calculate the ROC AUC. Once we have the testing predictions, we can calculate the ROC AUC. Results The final testing ROC AUC for the random forest was 0.87 compared to 0.67 for the single decision tree with an unlimited max depth. If we look at the training scores, both models achieved 1.0 ROC AUC, which again is as expected because we gave these models the training answers and did not limit the maximum depth of each tree. Although the random forest overfits (doing better on the training data than on the testing data), it is able to generalize much better to the testing data than the single decision tree. The random forest has lower variance (good) while maintaining the same low bias (also good) of a decision tree. We can also plot the ROC curve for the single decision tree (top) and the random forest (bottom). A curve to the top and left is a better model: The random forest significantly outperforms the single decision tree. Another diagnostic measure of the model we can take is to plot the confusion matrix for the testing predictions (see the notebook for details): This shows the predictions the model got correct in the top left and bottom right corners and the predictions missed by the model in the lower left and upper right. We can use plots such as these to diagnose our model and decide whether it’s doing well enough to put into production. The feature importances in a random forest indicate the sum of the reduction in Gini Impurity over all the nodes that are split on that feature. We can use these to try and figure out what predictor variables the random forest considers most important. The feature importances can be extracted from a trained random forest and put into a Pandas dataframe as follows: Feature importances can give us insight into a problem by telling us what variables are the most discerning between classes. For example, here DIFFWALK, indicating whether the patient has difficulty walking, is the most important feature which makes sense in the problem context. Feature importances can be used for feature engineering by building additional features from the most important. We can also use feature importances for feature selection by removing low importance features. Finally, we can visualize a single decision tree in the forest. This time, we have to limit the depth of the tree otherwise it will be too large to be converted into an image. To make the figure below, I limited the maximum depth to 6. This still results in a large tree that we can’t completely parse! However, given our deep dive into the decision tree, we grasp how our model is working. Next Steps A further step is to optimize the random forest which we can do through random search using the RandomizedSearchCV in Scikit-Learn. Optimization refers to finding the best hyperparameters for a model on a given dataset. The best hyperparameters will vary between datasets, so we have to perform optimization (also called model tuning) separately on each datasets. I like to think of model tuning as finding the best settings for a machine learning algorithm. Examples of what we might optimize in a random forest are the number of decision trees, the maximum depth of each decision tree, the maximum number of features considered for splitting each node, and the maximum number of data points required in a leaf node. For an implementation of random search for model optimization of the random forest, refer to the Jupyter Notebook. Complete Running Example The below code is created with repl.it and presents a complete interactive running example of the random forest in Python. Feel free to run and change the code (loading the packages might take a few moments). Conclusions While we can build powerful machine learning models in Python without understanding anything about them, I find it’s more effective to have knowledge about what is occurring behind the scenes. In this article, we not only built and used a random forest in Python, but we also developed an understanding of the model by starting with the basics. We first looked at an individual decision tree, the building block of a random forest, and then saw how we can overcome the high variance of a single decision tree by combining hundreds of them in an ensemble model known as a random forest. The random forest uses the concepts of random sampling of observations, random sampling of features, and averaging predictions. The key concepts to understand from this article are: Hopefully this article has given you the confidence and understanding needed to start using the random forest on your projects. The random forest is a powerful machine learning model, but that should not prevent us from knowing how it works. The more we know about a model, the better equipped we will be to use it effectively and explain how it makes predictions. As always, I welcome comments, feedback, and constructive criticism. I can be reached on Twitter @koehrsen_will. This article was originally published on enlight, an open-source community for studying machine learning. I would like to thank enlight and also repl.it for hosting the code in the article. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. gini: The Gini Impurity of the node. The average weighted Gini Impurity decreases as we move down the tree. samples: The number of observations in the node. value: The number of samples in each class. For example, the top node has 2 samples in class 0 and 4 samples in class 1. class: The majority classification for points in the node. In the case of leaf nodes, this is the prediction for all samples in the node. Random subsets of features considered when splitting nodes Gini Impurity: a measure that the decision tree tries to minimize when splitting each node. Represents the probability that a randomly selected sample from a node will be incorrectly classified according to the distribution of samples in the node. Bootstrapping: sampling random sets of observations with replacement. Random subsets of features: selecting a random set of the features when considering splits for each node in a decision tree. Random Forest: ensemble model made of many decision trees using bootstrapping, random subsets of features, and average voting to make predictions. This is an example of a bagging ensemble. Bias-variance tradeoff: a core issue in machine learning describing the balance between a model with high flexibility (high variance) that learns the training data very well at the cost of not being able to generalize to new data , and an inflexible model (high bias) that cannot learn the training data. A random forest reduces the variance of a single decision tree leading to better predictions on new data. Question asked about the data based on a value of a feature. Each question has either a True or False answer that splits the node. Based on the answer to the question, a data point moves down the tree. Random sampling of training data points when building trees Decision tree: an intuitive model that makes decisions based on a sequence of questions asked about feature values. Has low bias and high variance leading to overfitting the training data.",An Implementation and Explanation of the Random Forest in Python,10,published,14866,4042,0.47006432459178626,0,1,1,1,1,1
48,3100,265.9268483399421,534,https://towardsdatascience.com/practical-advice-for-data-science-writing-cc842795ed52,12,Towards Data Science,2018-09-03 17:27:00,26.9,10,4043,2018-08-26 15:27:00,"['Data Science', 'Education', 'Writing', 'Advice', 'Towards Data Science']","Practical Advice for Data Science Writing Useful tips for writing about your data science projects Writing is something that everyone wants to do more of, yet we often find it difficult to get started. We know that writing about data science projects improves our communication abilities, opens doors, and makes us better data scientists, but we often struggle with thoughts that our writing isn’t good enough or that we don’t have the necessary background or education. I’ve struggled with these feelings myself, and, over the past year, have developed a mindset to get through these barriers as well as general principles about data science writing. While there is no one secret to writing, there are practical tips that make it easier to establish a productive writing habit: In this article, we’ll go through each point these briefly, and I’ll touch on ways in which I’ve implemented them to improve my writing. Over the course of dozens of articles, I’ve made lots of mistakes, and, rather than making these same errors yourself, you can learn from my experiences. Perfection is Overrated: Aim for 90% The biggest mental obstacle I’ve had to overcome and what I commonly hear others struggle with is the idea that “my writing/data science skills aren’t good enough.” This can be debilitating: when considering a project, people will rationalize that since they can’t achieve perfection, they might as well not even start. In other words, they let the perfect become the enemy of the good. The fallacy here is that only an immaculate project is worthy of being shared. However, a rough-around-the-edges project that gets completed is far better than the idealized project that can never be finished. While flawless performance is to be expected is some domains — you want your car brakes to work every single time — blog writing is not one of these areas. Think about the last time you read a data science article. I’m guessing (especially if you read one of my articles) it had at least a few errors. However, you probably finished the article anyway because what matters is the value of the content. We’re willing to overlook a few mistakes as long as the article has compelling content. When I write, I aim to make my articles readable and do several edits, but I have stopped demanding that they be entirely free from errors. In practice, I aim for 90% and anything above that is a bonus. Putting out an article with a few errors is better than putting out none at all (and if you are concerned with grammar / style, I recommend the free tool Grammarly). This attitude extends beyond writing to a data science project itself. There will always be another method you can try or another round of model tuning to carry out. At a certain point, the returns from this work will be less than the time invested. Knowing when to stop optimizing is an important skill. Don’t let this be an excuse for a half-finished project, but don’t stress trying to attain an impossible 100%. If you’ve made a couple mistakes, then you have opportunities to learn by putting your work out for feedback. Be willing to put out imperfect work and respond positively to constructive criticism so you don’t make the same mistakes the next time. You Don’t Get Better by Doing Something Once: Consistency Counts While the 10,000 hour rule has been debunked (it turns out that focusing while you practice, called “deliberate practice”, matters at least as much as how much you practice) there is something to be said for accumulating more experience at a task. Writing is not an activity requiring special abilities, but rather a process that requires repetition to master. Writing may not ever be simple, but it does get easier to do as you practice. Moreover, writing is a positive feedback loop: as you continue to write, it gets easier and your writing gets better, leading you to want to write more. A significant barrier to my writing is getting started, what I like to think of as activation energy. As you write more often, that barrier to beginning is lowered and you reduce the amount of friction needed to start writing. Then once you’ve started, you’re usually past the hardest part. If you write consistently, you can change your mindset from “now I’m going to have to take time from this other activity to write “ to “now that I’ve finished the project, it’s time to write about it as usual.” Even writing about failed projects can be valuable. Writing about every project reinforces the concept that writing isn’t an extra chore but a critical part of the data science pipeline. Writing often doesn’t just mean sharing articles. While you’re working on an analysis, try adding more text cells explaining your thought process to your Jupyter Notebook. This is how I initially got around to writing a blog: I started annotating my notebooks thoroughly and realized to get to an article was only a little more work. Moreover, when you start adding explanations to your code, your future self and co-workers who look at your work will thank you. Writing my first few articles did feel like a chore, but as I got used to the idea that this wasn’t going to be a one-off thing, it became much easier until I reached the point where it was an accepted part of my workflow. Habit is extremely powerful, and writing can be acquired like any other habit. Titles are Meaningless in Data Science: Don’t Worry about Credentials Think about the last time you installed a Python package or forked a repo from GitHub. Did you search by authors who had an advanced degree? Did you only look at code written by professional software engineers? Of course not: you looked at the content of the repository before even checking the credentials of the author (if you bothered to at all). The same concept applies to data science articles: they are judged by the quality of the work and not on the author’s credentials. On the internet — for better and occasionally for worse — there are no barriers to publishing. There are no arbitrary certificates needed, no ivory tower to climb, no examinations to pass, and no gatekeepers preventing you from learning and writing about anything in data science. While a college degree in something is useful (I have a degree in mechanical engineering and don’t regret it despite never using it) it is certainly not necessary to contribute to data science. In this excellent article, Rachel Thomas, a professional machine learning researcher gives her opinion as to why an advanced degree is not necessary even in deep learning. Here is a partial list she made of contributors to deep learning without a PhD: In data science, your ability to acquire new knowledge is more important than your education background. If you don’t feel confident about a subject, then there are a plethora of resources to learn what you need to know. I can personally recommend Udacity, Coursera, and the excellent Hands-on Machine Learning with Scikit-Learn and TensorFlow as my favorite resources, but there are countless others. While MOOCS haven’t fully “democratized education” in all subjects, they have been successful for data science. Don’t stop yourself from taking on a project because you think you don’t have the background. I initially worried about my credentials, but after I thought about it from the reader’s side —people don’t consider the title of someone before reading their article online — it became much easier for me to publish without worrying about my background. Also, once you realize that where your education is from doesn’t matter, you’ll find it much easier to learn because you can stop thinking of formal education as the only reservoir of information. For data science, you can learn everything you need from the internet, often much quicker than you would be able to in a classroom. It’s also important to stay open-minded: I try to admit in my articles when I’m not entirely sure I’m using the right method and I always welcome any corrections. There is no standard method to do data science but you can still learn a lot from others who have experience solving similar problems. Recognize that you can learn anything necessary to take on any data science project on your own, but also remain open to advice. The Best Tool is the One that Gets the Job Done Windows vs MacOS. R vs Python. Sublime vs Atom vs PyCharm. Medium vs your own blog. These arguments are all unproductive. The correct response is to use whatever tool lets you solve the problem (within the confines of your environment). Moreover, the tool with more options is not always better. While more features can sound great, they often get in the way of you doing work. Generally, I try to keep things as simple as possible. When people ask me for recommendations for a writing platform, I say Medium because it has a limited amount of features. When I write, I want to focus on the content instead of spending time trying to format everything exactly as I want. More customization options means more time customizing those options and less time doing what you should be doing — writing or coding. I’ve gotten stuck in the tool optimization loop before: I’ve been persuaded to switch to a new technology and spent time to learn the features only to be told that this technology is obsolete and the next thing will make me even more productive. I stopped switching between IDEs (integrated development environments) a while ago and just settled on Jupyter + Sublime Text because I realized the extras were only getting in the way writing code. I’m not opposed to switching tools when the argument is strong enough, but switching just for novelty is not a recipe for productivity. If you really want to get started, pick a stack and stick with it. If you start a project and notice something missing from your tools, then you can start looking for what you need. Don’t fall for the flashy new tool promising more features until you know you need those features (this also applies to buying a car). In other words, don’t let optimization of a work routine get in the way of doing work. Where to Get Your Ideas: Read Widely and Deeply Great ideas don’t emerge on their own, isolated from all others. Instead, they’re created by applying old concepts to new problems, mixing two existing ideas, or improving upon a proven design. The best way to figure out what to write about is to read what other data scientists are writing. When I’m stuck on a problem or need some new writing ideas, I inevitably start reading. Moreover, if you aren’t confident about your writing style, start by emulating your favorite writers. Look at the structure of their articles, and how they approach problems and try to apply the same framework to your project and article. Everyone has to start somewhere, and there is no shame in building on the techniques of others. Eventually you’ll develop your own writing style which someone else can then adapt and so on. I recommend reading both widely and deeply in order to balance exploration versus exploitation. The explore / exploit problem is a classic in machine learning, particularly in reinforcement learning: we have an agent that needs to balance learning more about the environment, exploring, versus choosing actions based on what it believes will lead to the highest reward, exploiting. By reading widely, we explore many different areas of data science, and by reading deeply, we develop our understanding of a particular area of expertise. You can apply this to your writing and data science by practicing the skills you already have — exploiting — and frequently learning new ones — exploring. I also like to apply the idea of explore / exploit to choosing a data science project. Both extremes can lead to unsatisfactory projects: select a project based only on what you’ve done in the past, and you might find it stale and lose interest. If you choose a project where you can’t apply any prior knowledge, then you can get frustrated and give up. Instead, find something in the middle, where you know you can build up those skills you already have, but also need to learn something new. My final advice for choosing a project is to start small. Projects only grow as you work on them, and no matter how much time you allotted to the project, it will take longer (Hofstadter’s Rule). It might be tempting to take on a complete machine learning project, but if you are still trying to learn Python, then you probably want to tackle one piece at a time. That being said, if you are confident enough to take on an entire project, then go for it! There is no more effective method for learning than practice, especially putting all the pieces together in one problem. Conclusions As with any activity with delayed long-term rewards, writing can be difficult at times. Nonetheless, there are concrete actions which make the process easier and create positive feedback loops. There is no one secret to writing, but rather a sequence of steps that reduce the friction to get started and help you keep going. As you work to start or advance your data science career, keep these tips in mind to establish and maintain a productive writing habit. I welcome discussion on writing advice, comments, and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Consistency helps: the more you write, the easier it gets Don’t worry about credentials: in data science there are no barriers to prevent you from contributing or learning anything you want The best tool is the one that gets the job done: don’t over-optimize your writing software, blogging platform, or development environment Read widely and deeply: borrow, remix, and improve on other’s ideas Aim for 90%: the imperfect project that gets finished is better than the perfect project you never complete",Practical Advice for Data Science Writing,6,published,15027,2658,1.1662904439428141,8,1,1,1,0,0
47,3300,259.26092073702546,594,https://towardsdatascience.com/another-machine-learning-walk-through-and-a-challenge-8fae1e187a64,5,Towards Data Science,2018-09-10 09:26:00,18.89,15,4734,2018-09-07 14:54:00,"['Machine Learning', 'Data Science', 'Python', 'Towards Data Science', 'Education']","Another Machine Learning Walk-Through and a Challenge Don’t just read about machine learning — practice it! After spending considerable time and money on courses, books, and videos, I’ve arrived at one conclusion: the most effective way to learn data science is by doing data science projects. Reading, listening, and taking notes is valuable, but it’s not until you work through a problem that concepts solidify from abstractions into tools you feel confident using. In this article, I’ll present another machine learning walk-through in Python and also leave you with a challenge: try to develop a better solution (some helpful tips are included)! The complete Jupyter Notebook for this project can be run on Kaggle — no download required — or accessed on GitHub. Problem Statement The New York City Taxi Fare prediction challenge, currently running on Kaggle, is a supervised regression machine learning task. Given pickup and dropoff locations, the pickup timestamp, and the passenger count, the objective is to predict the fare of the taxi ride. Like most Kaggle competitions, this problem isn’t 100% reflective of those in industry, but it does present a realistic dataset and task on which we can hone our machine learning skills. To solve this problem, we’ll follow a standard data science pipeline plan of attack: This outline may seem to present a linear path from start to finish, but data science is a highly non-linear process where steps are repeated or completed out of order. As we gain familiarity with the data, we often want to go back and revisit our past decisions or take a new approach. While the final Jupyter Notebook may present a cohesive story, the development process is very messy, involving rewriting code and changing earlier decisions. Throughout the article, I’ll point out a number of areas in which I think an enterprising data scientist — you — could improve on my solution. I have labeled these potential improvements because as a largely empirical field, there are no guarantees in machine learning. The Taxi Fare dataset is relatively large at 55 million training rows, but simple to understand, with only 6 features. The fare_amount is the target, the continuous value we’ll train a model to predict: Throughout the notebook, I used only a sample of 5,000,000 rows to make the calculations quicker. My first recommendation thus is: It’s not assured that larger quantities of data will help, but empirical studies have found that generally, as the amount of data used for training a model increases, performance increases. A model trained on more data can better learn the actual signals, especially in a high-dimensional problem with a large number of features (this is not a high-dimensional dataset so there could be limited returns to using more data). While the sheer size of the dataset may be intimidating, frameworks such as Dask allow you to handle even massive datasets on a personal laptop. Moreover, learning how to set-up and use cloud computing, such as Amazon ECS, is a vital skill once the data exceeds the capability of your machine. Fortunately, it doesn’t take much research to understand this data: most of us have taken taxi rides before and we know that taxis charge based on miles driven. Therefore for feature engineering, we’ll want to find a way to represent the distance traveled based on the information we are given. We can also read notebooks from other data scientists or read through the competition discussion for ideas about how to solve the problem. Although Kaggle data is usually cleaner than real-world data, this dataset still has a few problems, namely anomalies in several of the features. I like to carry out data cleaning as part of the exploration process, correcting anomalies or data errors as I find them. For this problem, we can spot outliers by looking at statistics of the data using df.describe(). The anomalies in passenger_count, the coordinates, and the fare_amount were addressed through a combination of domain knowledge and looking at the distribution of the data. For example, reading about taxi fares in NYC, we see that the minimum fare amount is $2.50 which means we should exclude some of the rides based on the fare. For the coordinates, we can look at the distribution and exclude values that fall well outside the norm. Once we’ve identified outliers, we can remove them using code like the following: Once we clean the data, we can get to the fun part: visualization. Below is a plot of the pickup and dropoff locations on top of NYC colored by the binned fare (binning is a way of turning a continuous variable into a discrete one). We also want to take a look at the target variable. Following is an Empirical Cumulative Distribution Function (ECDF) plot of the Target variable, the fare amount. The ECDF can be a better visualization choice than a histogram for one variable because it doesn’t have artifacts from binning. Besides being interesting to look at, plots can help us identify anomalies, relationships, or ideas for new features. In the maps, the color represents the fare, and we can see that the fares starting or ending at the airport (bottom right) tend to be among the most expensive. Going back to the domain knowledge, we read the standard fare for rides to JFK airport is $45, so if we could find a way to identify airport rides, then we’d know the fare accurately. While I didn’t go that far in this notebook, using domain knowledge for data cleaning and feature engineering is extremely valuable. My second recommendation for improvement is: This can be done with domain knowledge (such as a map), or statistical methods (such as z-scores). One interesting approach to this problem is in this notebook, where the author removed rides that began or ended in the water. The inclusion / exclusion of outliers can have a significant effect on model performance. However, like most problems in machine learning, there’s no standard approach (here’s one statistical method in Python you could try). Feature engineering is the process of creating new features — predictor variables — out of an existing dataset. Because a machine learning model can only learn from the features it is given, this is the most important step of the machine learning pipeline. For datasets with multiple tables and relationships between the tables, we’ll probably want to use automated feature engineering, but because this problem has a relatively small number of columns and only one table, we can hand-build a few high-value features. For example, since we know that the cost of a taxi ride is proportional to the distance, we’ll want to use the start and stop points to try and find the distance traveled. One rough approximation of distance is the absolute value of the difference between the start and end latitudes and longitudes. Features don’t have to be complex to be useful! Below is a plot of these new features colored by the binned fare. What these features give us is a relative measure of distance because they are calculated in terms of latitude and longitude and not an actual metric. These features are useful for comparison, but if we want a measurement in kilometers, we can apply the Haversine formula between the start and end of the trip, which calculates the Great Circle distance. This is still an approximation because it gives distance along a line drawn on the spherical surface of the Earth (I’m told the Earth is a sphere) connecting the two points, and clearly, taxis do not travel along straight lines. (See notebook for details). The other major source of features for this problem are time based. Given a date and time, there are numerous new variables we can extract. Constructing time features is a common task, and in the notebook I’ve included a useful function that builds a dozen features from a single timestamp. Although I built almost 20 features in this project, there are still more to be found. The tough part about feature engineering is you never know when you have fully exhausted all the options. My next recommendation is: Feature engineering also involves problem expertise or applying algorithms that automatically build features for you. After building features, you’ll often have to apply feature selection to find the most relevant ones. Once you have clean data and a set of features, you start testing models. Even though feature engineering comes before modeling on the outline, I often return to this step again and again over the course of a project. A good first choice of model for establishing a baseline on a regression task is a simple linear regression. Moreover, if we look at the Pearson correlation of the features with the fare amount for this problem, we find several very strong linear relationships as shown below. Based on the strength of the linear relationships between some of the features and the target, we can expect a linear model to do reasonably well. While ensemble models and deep neural networks get all the attention, there’s no reason to use an overly complex model if a simple, interpretable model can achieve nearly the same performance. Nonetheless, it still makes sense to try different models, especially because they are easy to build with Scikit-Learn. The starting model, a linear regression trained on only three features (the abs location differences and the passenger_count) achieved a validation root mean squared error (RMSE) of $5.32 and a mean absolute percentage error of 28.6%. The benefit to a simple linear regression is that we can inspect the coefficients and find for example that an increase in one passenger raises the fare by $0.02 according to the model. For Kaggle competitions, we can evaluate a model using both a validation set — here I used 1,000,000 examples — and by submitting test predictions to the competition. This allows us to compare our model to other data scientists — the linear regression places about 600/800. Ideally, we want to use the test set only once to get an estimate of how well our model will do on new data and perform any optimization using a validation set (or cross validation). The problem with Kaggle is that the leaderboard can encourage competitors to build complex models over-optimized to the testing data. We also want to compare our model to a naive baseline that uses no machine learning, which in the case of regression can be guessing the mean value of the target on the training set. This results in an RMSE of $9.35 which gives us confidence machine learning is applicable to the problem. Even training the linear regression on additional features does not result in a great leaderboard score and the next step is to try a more complex model. My next choice is usually the random forest, which is where I turned in this problem. The random forest is a more flexible model than the linear regression which means it has a reduced bias — it can fit the training data better. The random forest also generally has low variance meaning it can generalize to new data. For this problem, the random forest outperforms the linear regression, achieving a $4.20 validation RMSE on the same feature set. The reason a random forest typically outperforms a linear regression is because it has more flexibility — lower bias — and it has reduced variance because it combines together the predictions of many decision trees. A linear regression is a simple method and as such has a high bias — it assumes the data is linear. A linear regression can also be highly influenced by outliers because it solves for the fit with the lowest sum of squared errors. The choice of model (and hyperparameters) represents the bias-variance tradeoff in machine learning: a model with high bias cannot learn even the training data accurately while a model with high variance essentially memorizes the training data and cannot generalize to new examples. Because the goal of machine learning is to generalize to new data, we want a model with both low bias and low variance. The best model on one problem won’t necessarily be the best model on all problems, so it’s important to investigate several models spanning the range of complexity. Every model should be evaluated using the validation data and the best performing model can then be optimized in model tuning. I selected the random forest because of the validation results, and I’d encourage you to try out a few other models (or even combine models in an ensemble). In a machine learning problem, we have a few approaches for improving performance: There are still gains to be made from 1. and 2. (that’s part of the challenge), but I also wanted to provide a framework for optimizing the selected model. Model optimization is the process of finding the best hyperparameters for a model on a given dataset. Because the best values of the hyperparameters depend on the data, this has to be done again for each new problem. I like to think of model optimization — also called model tuning — as finding the ideal settings of a machine learning model. There are a number of methods for optimization, ranging from manual tuning to automated hyperparameter tuning, but in practice, random search works well and is simple to implement. In the notebook, I provide code for running random search for model optimization. To make the computation times reasonable, I again sampled the data and only ran 50 iterations. Even this takes a considerable amount of time because the hyperparameters are evaluated using 3-fold cross validation. This means on each iteration, the model is trained with a selected combination of hyperparameters 3 times! I also tried out a number of different features and found the best model used only 12 of the 27 features. This makes sense because many of the features are highly correlated and hence are not necessary. After running the random search and choosing the features, the final random forest model achieved an RMSE of 3.38 which represents a percentage error of 19.0%. This is a 66% reduction in the error from the naive baseline, and a 30% reduction in error from the first linear model. This performance illustrates a critical point in machine learning: The returns from feature engineering are much greater than those from model optimization. Therefore, it’s crucial to make sure you have a good set of features before you start worrying about having the best hyperparameters. Although I ran 50 iterations of random search, the hyperparameter values have probably not been fully optimized. My next recommendation is: The returns from this will probably be less than from feature engineering, but it’s possible there are still performance gains to be found. If you are feeling up to the task, you can also try out automated model tuning using a tool such as Hyperopt (I’ve written a guide which can be found here.) While the random forest is more complex than the linear regression, it’s not a complete black box. A random forest is made up of an ensemble of decision trees which by themselves are very intuitive flow-chart-like models. We can even inspect individual trees in the forest to get a sense of how they make decisions. Another method for peering into the black box of the random forest is by examining the feature importances. The technical details aren’t that important at the moment, but we can use the relative values to determine which features are considered relevant to the model. The most important feature by far is the Euclidean distance of the taxi ride, followed by the pickup_Elapsed , one of the time variables. Given that we made both of these features, we should be confident that our feature engineering went to good use! We could also feature importances for feature engineering or selection since we do not need to keep all of the variables. Finally, we can take a look at the model predictions both on the validation data and on the test data. Because we have the validation answers, we can compute the error of the predictions, and we can examine the testing predictions for extreme values (there were several in the linear regression). Below is a plot of the validation predictions for the final model. Model interpretation is still a relatively new field, but there are some promising methods for examining a model. While the primary goal of machine learning is making accurate predictions on new data, it’s also equally important to know why the model is accurate and if it can teach us anything about the problem. Although we tried a number of different techniques and implemented a complete solution, there are still steps to take that can improve the model. My next approach would be to try an even more complex model, such as a deep neural network or a gradient boosting machine. I haven’t implemented these in the notebook, but I am working on them. I’ll let you try first (I can’t give away all the answers!). With a machine learning project, there are always more approaches to try, and you can even come up with your own method if you are not satisfied with the existing options! Machine learning is a largely empirical field with no standardized rules, and the only way to know if something works is to test it. At this point, I’ll leave you to your own devices for improving the model. I’ve given you five different recommendations that should allow you to beat the best cross validation score I achieved in the notebook. This is certainly a friendly challenge, so don’t get frustrated, and don’t hesitate to reach out if you need help. All of these recommendations are potential improvements because I can’t guarantee that they’ll improve the score. Nonetheless, I do know it’s possible to build a better model, and I’ll work on my end as well to try and build it! If you need some more inspiration, here’s two other complete machine learning projects I’ve done to give you some ideas: The next steps are up to you! Conclusions In this article and accompanying Jupyter Notebook, I presented a complete machine learning walk-through on a realistic dataset. We implemented the solution in Python code, and touched on many key concepts in machine learning. Machine learning is not a magical art, but rather a craft that can be honed through repetition. There is nothing stopping anyone from learning how to solve real-world problems using machine learning, and the most effective method for becoming adept is to work through projects. In the spirit of learn by doing, my challenge to you is to improve upon my best model in the notebook and I’ve left you with a few recommendations that I believe will improve performance. There’s no one right answer in data science, and I look forward to seeing what everyone can come up with! If you take on the challenge, please leave a link to your notebook in the comments. Hopefully this article and notebook have given you the start necessary to get out there and solve this problem or others. And, when you do need help, don’t hesitate to ask because the data science community is always supportive. As always, I welcome feedback, discussion, and constructive criticism. I can be reached on Twitter @koehrsen_will or by commenting on this article. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Data exploration / data cleaning Feature engineering / feature selection Model evaluation and selection Model optimization Interpretation of results and predictions Construct more features / perform feature selection Optimize selected model Try a more complex model A Complete Machine Learning Walk-Through in Python (three parts) Understand the problem and data Potential improvement 1: use more data for training the model Potential Improvement 2: Experiment with different methods for outlier removal and data cleaning. Potential Improvement 3: Construct more features or apply feature selection to the existing features to find an optimal set. Get more data Potential Improvement 4: run model tuning for more iterations on more data. Potential Improvement 5: try more complex models such as the gradient boosting machine A Data Science for Good Machine Learning Project Walk-Through in Python (two parts)",Another Machine Learning Walk-Through and a Challenge,8,published,25055,3756,0.8785942492012779,2,1,1,1,1,1
53,759,253.21608929931716,125,https://towardsdatascience.com/five-minutes-to-your-own-website-fd0b43cbd886,3,Towards Data Science,2018-09-16 10:31:00,40.99,5,1556,2018-09-15 05:28:00,"['Programming', 'Web Development', 'Blogging', 'Education', 'Towards Data Science']","Five Minutes to Your Own Website How to Use GitHub Pages and Jekyll to get started with your own — entirely free — blog Building your own website is rewarding on several levels. There’s the opportunity to showcase your work to friends, family, and potential employers, the pride in making something, and the freedom to shape a (very small) part of the web to your tastes. While Medium is a great option to start blogging because the limited features let you focus on writing, eventually, like me, you’ll want your own website to serve as a central location for your work. Fortunately, we live in a great age for creativity where you can use free tools to build a website in minutes. In this post, we’ll see how to use the Jekyll site generator and GitHub Pages to build and publish a simple blog with no coding required. If you want an idea of the end product, you can take a look at my (work in progress) site. Jekyll and GitHub Pages Jekyll is a simple static site generator. This means it takes plain text files and renders them as a website that can then be served to the web through a web hosting service. One option for this service is GitHub Pages, free “websites hosted and easily published through GitHub.” With Jekyll + GitHub Pages, the source files for your website live in a GitHub repository that is automatically published to the web. Basically, Jekyll transforms text documents in a repository into a website and then that website is hosted by GitHub. The only thing you have to worry about is writing posts and editing a few of the text files to make things look how you want. As the Jekyll design philosophy states: Jekyll gets out of your way and allows you to concentrate on what truly matters: your content. You can follow this guide on GitHub to set up a new blank website. However, if you’re looking to make a simple blog as quick as possible, a better option is to fork (a GitHub term meaning copy) the Jekyll Now repository which already has a blogging layout built and ready to be published. After forking, go into the repository settings under your account and change the name to username.github.io. Within a few minutes, you’ll be able to navigate to username.github.io in your browser and see your new website published on the web! It’s not really yours yet, but you can fix that in a few steps. You can edit any of the website files locally by cloning the repository and opening them in your favorite text editor. Then, when you commit and push your changes to GitHub, the website will automatically update. However, instead of local editing, I highly recommend the online Prose editor. Prose makes it much easier to see all of your files, quickly make changes, and commit and push them to GitHub as soon as you save. You’ll have to authenticate Prose with GitHub and then you can start improving your blog. For example, to begin personalizing the website, edit the _config.yaml file: After hitting save, these changes can be seen online. Writing Blog Posts All blog posts in Jekyll are written in Markdown, a lightweight language that was made to be converted to HTML or many other formats. It’s extremely easy to learn, is ubiquitous on the web, and provides plenty of opportunities for customizing your posts beyond what you can do on Medium. The posts for your website are located in the _posts/ folder and as you edit and save the files in Prose, you’ll be able to see them appear on your blog. Follow the naming convention: date-title.md and your post will automatically appear correctly named and dated on your site. If you already have posts on Medium, you can convert these to Markdown using this Chrome extension or this command line tool. The conversion isn’t perfect, but with a few edits, you’ll be able to quickly export your Medium work to your blog. Here’s an article I wrote about going from existing Medium articles to Markdown that addresses several problems I found. The benefit of your own website is you can control every aspect of how things look and add any features you want. Start by digging through the files in the repository and looking at the Jekyll documentation. For example, to change fonts and styling, edit the style.scss file, and to enable functions such as comments on posts, edit the _config.yaml. You can even use a custom domain name by purchasing one through a web domain registrar like Hover and following this guide. The nice thing about Jekyll is that if you just want to focus on writing, you can leave the defaults and you’ll have a clean, presentable blog. If you feel like making changes, then you can go as far as you want. For instance, I’ve added a live code editor to my about page to make it more interesting: Conclusions If you’ve been holding off on making a blog because you were concerned about the time or cost, then now is the time to get started. With GitHub Pages and Jekyll, you can have your own blog in five minutes for free! As a reminder, the steps are: For anyone who already has Medium posts, here’s a guide for moving them to Markdown. If you don’t, then here’s some help for writing about data science projects. Finally, get out there and build a website to call your own! As always, I welcome comments and feedback. I can be reached on Twitter @koehrsen_will or at my personal GitHub pages website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Rename the repo to username.github.io . Your website is now published. Make changes to website files in the Prose online editor and save to update the website. Get started by changing _config.yaml and adding your markdown posts in _posts/. Extend / customize the website as much as you want by reading documentation, looking at other websites, or just experimenting Fork the Jekyll Now repository to your GitHub account.",Five Minutes to Your Own Website,6,published,3796,1172,0.6476109215017065,1,1,0,1,0,0
45,282,252.90737442434028,36,https://towardsdatascience.com/converting-medium-posts-to-markdown-for-your-blog-5d6830408467,1,Towards Data Science,2018-09-16 17:55:00,18.17,8,356,2018-09-16 10:51:00,"['Web Development', 'Programming', 'Writing', 'Education']","Converting Medium Posts to Markdown for Your Blog How to quickly export Medium articles to your blog If like me, you got your start blogging on Medium, but also want to build your own website to display your articles, you’ll need a way to move articles from Medium to the Markdown language. Markdown is a lightweight language meant to be converted into HTML for the web, and there are several tools that allow you to go from existing Medium articles to Markdown for a blog. (If you don’t yet have a blog, then follow this guide to build your own website in five minutes using Jekyll and GitHub pages.) There is both a Chrome Extension and a command line tool for taking your Medium posts to Markdown. Unfortunately, I’ve found the Chrome Extension to be unreliable, and if it does work, it makes a number of formatting errors that require correcting. If you can get the chrome extension to work and you aren’t comfortable at the command line, then that is probably the best choice for you. However, I’ve found the command line tool to be better for my use because it works every time, and requires fewer re-adjustments to the text after running. The medium-to-markdown command line package is written in Javascript which means you’ll need node to run and npm to install the package. If you’re on Windows, use this page to install both (it takes about 3 minutes). Then, install the package using npm install medium-to-markdown. The actual script you’ll need to run is available here, and also shown below. To run, save the script as medium-to-markdown.js, change the ""<medium post url""> to the address of your published post, and type node medium-to-markdown.js at the command line (making sure the script is in your directory. This will output the entire post as Markdown in your console. To redirect the output to a file, you can type node medium-to-markdown.js >> file.md If you’re using GitHub Pages + Jekyll for your blog, you’ll want to save the md file in the _posts/ directory of your repository as date-title.md . For example, my latest post file is _posts/2018-09-16-Five-Minutes-to-Your-Own-Website.md . Once the article has been converted to a properly named Markdown file, it can be published by pushing the repository to GitHub. Personally, I found that process of manually renaming the file tedious, so I wrote a Python script that accepts a url of a published Medium post along a date, calls the medium-to-markdown.js conversion script with the url, and saves the resulting markdown with the correct file name. The script can be found here, and the command line usage is below: Overall, it takes 15 seconds to run the script and about 5 minutes for the website to update! Go to your blog to see the article rendered. Solutions to Common Issues Both the Chrome Extension and Command line tool create minor issues in the Markdown that you’ll have to fix. I do all my editing in the Prose online editor and like to pull up the editor and the original Medium article side-by-side.(I also frequently use the Chrome development tools — right click and inspect — on my blog to adjust the css and see the changes immediately. ) Following are a few of the issues I’ve noticed and how to fix them. I’ll assume you’re working with GitHub Pages and Jekyll for your blog (follow this guide to get started) although these tips may work with other frameworks. These solutions are not exhaustive, so if you run into any more problems, let me know in the comments or on Twitter. As you can see above, the default conversion to Markdown renders image captions left-justified below the image. To center the captions, add the following to your styles.scss file (in the repository root directory): Then, in the Markdown itself, change the caption text from the default shown on top to the bottom. Make sure the caption is on a separate line: The caption will now be centered below the image. If you don’t like this styling, change the css listed above. To render images side by side in markdown, you can use a two-column table. I’ve found that you have to include the headers or the table does not show correctly. There are a couple other options, but the table works well. Here’s the blank code for a table so just replace the image_link and header: If you write a lot about programming, then you want your code to look great! Fortunately, unlike on Medium, you can use syntax highlighting to make your code blocks pop out on your blog. Jekyll natively supports the rouge language highlighter which has numerous styles to choose between (view them here). In the Markdown for the article, surround code blocks using backticks and specify the language for highlighting: The default syntax highlighting looks like this: Using a custom defined theme, the highlighting comes out like so: To set your own code theme, refer to this article, or if you like the look of mine, you can copy and paste my code style sheet code-highlighting.scss (link) into your _sass/ directory. Then change the line in style.scss that reads @import “highlights” to @import “code-highlighting"" (this should be near the bottom) to import the new style instead. Another common part of my Medium posts are GitHub gists. To properly show these in your posts, start by going to the original medium article, right click on the Gist, and select inspect frame . This will bring up a page of what looks like incomprehensible code. However, all you need to do is copy what’s between the <script> tags as below: Simply copy and paste this line into the Markdown as shown below and it will be properly rendered on your blog. Of course, like any other element on your website, you can style the block using css however you like! By default, Jekyll will only show the first paragraph of your post on the main page of your blog with a button showing Read More to access the rest. If you have a picture at the top of your post, this is all that will be displayed. To extend the excerpt, add the following line to _config.yaml: Then put the <!--more--> tag in the post markdown wherever you want the excerpt to end. There are still numerous options I haven’t explored. If you want to do something on your blog, chances are there’s a way to get it done. For example, you can add comments to all of your posts by making a Disqus account and adding it to your _config.yaml file (guide here). If you have a lot of posts and want to limit the number that appear on one page, you can specify that as well using pagination in the config file (instructions). Building a website is exciting because you are able to make it look however you want! Although this might not seem amazing to most people, I still appreciate when I make a minor update to my site and I can see the changes online just as I intended. You can’t imagine how enthusiastic I was when I finally got a live code editor running on my about page! At the end of the day, yes, building a website is about putting yourself out there for the world to see, but it’s also about taking pride in something you created. Conclusions Going from an existing Medium article to Markdown can be done quickly using a command line tool. There are a few minor errors that need to be addressed after converting to Markdown, but this also gives you the chance to customize the post exactly how you want. If you are still stuck on any points, take a look through my website repository to see if you can find a solution there, or get in contact with me. Moreover, if you find something cool that you can do with a Jekyll blog, I’d enjoy hearing about it. Now get out there and start writing. As always, I welcome comments and constructive criticism. I can be reached on Twitter @koehrsen_will or through my website willk.online Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Converting Medium Posts to Markdown for Your Blog,8,published,1959,1580,0.17848101265822786,0,1,0,0,0,0
46,3800,246.21974009909724,666,https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c,7,Towards Data Science,2018-09-23 10:26:00,18.21,17,4486,2018-09-20 20:04:00,"['Programming', 'Data Science', 'Education', 'Towards Data Science', 'Python']","Wikipedia Data Science: Working with the World’s Largest Encyclopedia How to programmatically download and parse the Wikipedia Wikipedia is one of modern humanity’s most impressive creations. Who would have thought that in just a few years, anonymous contributors working for free could create the greatest source of online knowledge the world has ever seen? Not only is Wikipedia the best place to get information for writing your college papers, but it’s also an extremely rich source of data that can fuel numerous data science projects from natural language processing to supervised machine learning. The size of Wikipedia makes it both the world’s largest encyclopedia and slightly intimidating to work with. However, size is not an issue with the right tools, and in this article, we’ll walk through how we can programmatically download and parse through all of the English language Wikipedia. Along the way, we’ll cover a number of useful topics in data science: The original impetus for this project was to collect information on every single book on Wikipedia, but I soon realized the solutions involved were more broadly applicable. The techniques covered here and presented in the accompanying Jupyter Notebook will let you efficiently work with any articles on Wikipedia and can be extended to other sources of web data. If you’d like to see more about utilizing the data in this article, I wrote a post using neural network embeddings to build a book recommendation system. The notebook containing the Python code for this article is available on GitHub. This project was inspired by the excellent Deep Learning Cookbook by Douwe Osinga and much of the code is adapted from the book. The book is well worth it and you can access the Jupyter Notebooks at no cost on GitHub. Finding and Downloading Data Programmatically The first step in any data science project is accessing your data! While we could make individual requests to Wikipedia pages and scrape the results, we’d quickly run into rate limits and unnecessarily tax Wikipedia’s servers. Instead, we can access a dump of all of Wikipedia through Wikimedia at dumps.wikimedia.org. (A dump refers to a periodic snapshot of a database). The English version is at dumps.wikimedia.org/enwiki. We view the available versions of the database using the following code. This code makes use of the BeautifulSoup library for parsing HTML. Given that HTML is the standard markup language for web pages, this is an invaluable library for working with web data. For this project, we’ll take the dump on September 1, 2018 (some of the dumps are incomplete so make sure to choose one with the data you need). To find all the available files in the dump, we use the following code: Again, we parse the webpage using BeautifulSoup to find the files. We could go to https://dumps.wikimedia.org/enwiki/20180901/ and look for the files to download manually, but that would be inefficient. Knowing how to parse HTML and interact with websites in a program is an extremely useful skill considering how much data is on the web. Learn a little web scraping and vast new data sources become accessible. (Here’s a tutorial to get you started). The above code finds all of the files in the dump. This includes several options for download: the current version of only the articles, the articles along with the current discussion, or the articles along with all past edits and discussion. If we go with the latter option, we are looking at several terabytes of data! For this project, we’ll stick to the most recent version of only the articles. This page is useful for determining which files to get given your needs. The current version of all the articles is available as a single file. However, if we get the single file, then when we parse it, we’ll be stuck going through all the articles sequentially — one at a time — a very inefficient approach. A better option is to download partitioned files, each of which contains a subset of the articles. Then, as we’ll see, we can parse through multiple files at a time through parallelization, speeding up the process significantly. When I’m dealing with files, I would rather have many small files than one large file because then I can parallelize operations on the files. The partitioned files are available as bz2-compressed XML (eXtended Markup Language). Each partition is around 300–400 MB in size with a total compressed size of 15.4 GB. We won’t need to decompress the files, but if you choose to do so, the entire size is around 58 GB. This actually doesn’t seem too large for all of human knowledge! (Okay, not all knowledge, but still). To actually download the files, the Keras utility get_file is extremely useful. This downloads a file at a link and saves it to disk. The files are saved in ~/.keras/datasets/, the default save location for Keras. Downloading all of the files one at a time takes a little over 2 hours. (You can try to download in parallel, but I ran into rate limits when I tried to make multiple requests at the same time.) Parsing the Data It might seem like the first thing we want to do is decompress the files. However, it turns out we won’t ever actually need to do this to access all the data in the articles! Instead, we can iteratively work with the files by decompressing and processing lines one at a time. Iterating through files is often the only option if we work with large datasets that do not fit in memory. To iterate through a bz2 compressed file we could use the bz2 library. In testing though, I found that a faster option (by a factor of 2) is to call the system utility bzcat with the subprocess Python module. This illustrates a critical point: often, there are multiple solutions to a problem and the only way to find what is most efficient is to benchmark the options. This can be as simple as using the %%timeit Jupyter cell magic to time the methods. For the complete details, see the notebook, but the basic format of iteratively decompressing a file is: If we simply read in the XML data and append it to a list, we get something that looks like this: This shows the XML from a single Wikipedia article. The files we have downloaded contain millions of lines like this, with thousands of articles in each file. If we really wanted to make things difficult, we could go through this using regular expressions and string matching to find each article. Given this is extraordinarily inefficient, we’ll take a better approach using tools custom built for parsing both XML and Wikipedia-style articles. Parsing Approach We need to parse the files on two levels: Fortunately, there are good options for both of these operations in Python. To solve the first problem of locating articles, we’ll use the SAX parser, which is “The Simple API for XML.” BeautifulSoup can also be used for parsing XML, but this requires loading the entire file into memory and building a Document Object Model (DOM). SAX, on the other hand, processes XML one line at a time, which fits our approach perfectly. The basic idea we need to execute is to search through the XML and extract the information between specific tags (If you need an introduction to XML, I’d recommend starting here). For example, given the XML below: We want to select the content between the <title> and <text> tags. (The title is simply the Wikipedia page title and the text is the content of the article). SAX will let us do exactly this using a parser and a ContentHandler which controls how the information passed to the parser is handled. We pass the XML to the parser one line at a time and the Content Handler lets us extract the relevant information. This is a little difficult to follow without trying it out yourself, but the idea is that the Content handler looks for certain start tags, and when it finds one, it adds characters to a buffer until it encounters the same end tag. Then it saves the buffer content to a dictionary with the tag as the key. The result is that we get a dictionary where the keys are the tags and the values are the content between the tags. We can then send this dictionary to another function that will parse the values in the dictionary. The only part of SAX we need to write is the Content Handler. This is shown in its entirety below: In this code, we are looking for the tags title and text . Every time the parser encounters one of these, it will save characters to the buffer until it encounters the same end tag (identified by </tag>). At this point it will save the buffer contents to a dictionary — self._values . Articles are separated by <page> tags, so if the content handler encounters an ending </page> tag, then it should add the self._values to the list of articles, self._pages. If this is a little confusing, then perhaps seeing it in action will help. The code below shows how we use this to search through the XML file to find articles. For now we’re just saving them to the handler._pages attribute, but later we’ll send the articles to another function for parsing. If we inspect handler._pages , we’ll see a list, each element of which is a tuple with the title and text of one article: At this point we have written code that can successfully identify articles within the XML. This gets us halfway through the process of parsing the files and the next step is to process the articles themselves to find specific pages and information. Once again, we’ll turn to a tool purpose built for the task. Wikipedia runs on a software for building wikis known as MediaWiki. This means that articles follow a standard format that makes programmatically accessing the information within them simple. While the text of an article may look like just a string, it encodes far more information due to the formatting. To efficiently get at this information, we bring in the powerful mwparserfromhell , a library built to work with MediaWiki content. If we pass the text of a Wikipedia article to the mwparserfromhell , we get a Wikicode object which comes with many methods for sorting through the data. For example, the following code creates a wikicode object from an article (about KENZ FM) and retrieves the wikilinks() within the article. These are all of the links that point to other Wikipedia articles: There are a number of useful methods that can be applied to the wikicode such as finding comments or searching for a specific keyword. If you want to get a clean version of the article text, then call: Since my ultimate goal was to find all the articles about books, the question arises if there is a way to use this parser to identify articles in a certain category? Fortunately, the answer is yes, using MediaWiki templates. Templates are standard ways of recording information. There are numerous templates for everything on Wikipedia, but the most relevant for our purposes are Infoboxes . These are templates that encode summary information for an article. For instance, the infobox for War and Peace is: Each category of articles on Wikipedia, such as films, books, or radio stations, has its own type of infobox. In the case of books, the infobox template is helpfully named Infobox book. Just as helpful, the wiki object has a method called filter_templates() that allows us to extract a specific template from an article. Therefore, if we want to know whether an article is about a book, we can filter it for the book infobox. This is shown below: If there’s a match, then we’ve found a book! To find the Infobox template for the category of articles you are interested in, refer to the list of infoboxes. How do we combine the mwparserfromhell for parsing articles with the SAX parser we wrote? Well, we modify the endElement method in the Content Handler to send the dictionary of values containing the title and text of an article to a function that searches the article text for specified template. If the function finds an article we want, it extracts information from the article and then returns it to the handler. First, I’ll show the updated endElement : Now, once the parser has hit the end of an article, we send the article on to the function process_article which is shown below: Although I’m looking for books, this function can be used to search for any category of article on Wikipedia. Just replace the template with the template for the category (such as Infobox language to find languages) and it will only return the information from articles within the category. We can test this function and the new ContentHandler on one file. Let’s take a look at the output for one book: For every single book on Wikipedia, we have the information from the Infobox as a dictionary, the internal wikilinks, the external links, and the timestamp of the most recent edit. (I’m concentrating on these pieces of information to build a book recommendation system for my next project). You can modify the process_article function and WikiXmlHandler class to find whatever information and articles you need! If you look at the time to process just one file, 1055 seconds, and multiply that by 55, you get over 15 hours of processing time for all files! Granted, we could just run that overnight, but I’d rather not waste the extra time if I don’t have to. This brings us to our final technique we’ll cover in this project: parallelization using multiprocessing and multithreading. Running Operations in Parallel Instead of parsing through the files one at a time, we want to process several of them at once (which is why we downloaded the partitions). We can do this using parallelization, either through multithreading or multiprocessing. Multithreading and multiprocessing are ways to carry out many tasks on a computer — or multiple computers — simultaneously. We many files on disk, each of which needs to be parsed in the same way. A naive approach would be to parse one file at a time, but that is not taking full advantage of our resources. Instead, we use either multithreading or multiprocessing to parse many files at the same time, significantly speeding up the entire process. Generally, multithreading works better (is faster) for input / output bound tasks, such as reading in files or making requests. Multiprocessing works better (is faster) for cpu-bound tasks (source). For the process of parsing articles, I wasn’t sure which method would be optimal, so again I benchmarked both of them with different parameters. Learning how to set up tests and seek out different ways to solve a problem will get you far in a data science or any technical career. (The code for testing multithreading and multiprocessing appears at the end of the notebook). When I ran the tests, I found multiprocessing was almost 10 times faster indicating this process is probably CPU bound (limited). Learning multithreading / multiprocessing is essential for making your data science workflows more efficient. I’d recommend this article to get started with the concepts. (We’ll stick to the built-in multiprocessing library, but you can also using Dask for parallelization as in this project). After running a number of tests, I found the fastest way to process the files was using 16 processes, one for each core of my computer. This means we can process 16 files at a time instead of 1! I’d encourage anyone to test out a few options for multiprocessing / multithreading and let me know the results! I’m still not sure I did things in the best way, and I’m always willing to learn. To run an operation in parallel, we need a service and a set of tasks . A service is just a function and tasks are in an iterable — such as a list — each of which we send to the function. For the purpose of parsing the XML files, each task is one file, and the function will take in the file, find all the books, and save them to disk. The pseudo-code for the function is below: The end result of running this function is a saved list of books from the file sent to the function. The files are saved as json, a machine readable format for writing nested information such as lists of lists and dictionaries. The tasks that we want to send to this function are all the compressed files. For each file, we want to send it to find_books to be parsed. The final code to search through every article on Wikipedia is below: We map each task to the service, the function that finds the books (map refers to applying a function to each item in an iterable). Running with 16 processes in parallel, we can search all of Wikipedia in under 3 hours! After running the code, the books from each file are saved on disk in separate json files. For practice writing parallelized code, we’ll read the separate files in with multiple processes, this time using threads. The multiprocessing.dummy library provides a wrapper around the threading module. This time the service is read_data and the tasks are the saved files on disk: The multithreaded code works in the exact same way, mapping tasks in an iterable to function. Once we have the list of lists, we flatten it to a single list. Wikipedia has nearly 38,000 articles on books according to our count. The size of the final json file with all the book information is only about 55 MB meaning we searched through over 50 GB (uncompressed) of total files to find 55 MB worth of books! Given that we are only keeping a limited subset of the book information, that makes sense. We now have information on every single book on Wikipedia. You can use the same code to find articles for any category of your choosing, or modify the functions to search for different information. Using some fairly simple Python code, we are able to search through an incredible amount of information. Conclusions In this article, we saw how to download and parse the entire English language version of Wikipedia. Having a ton of data is not useful unless we can make sense of it, and so we developed a set of methods for efficiently processing all of the articles for the information we need for our projects. Throughout this project, we covered a number of important topics: The skills developed in this project are well-suited to Wikipedia data but are also broadly applicable to any information from the web. I’d encourage you to apply these methods for your own projects or try analyzing a different category of articles. There’s plenty of information for everyone to do their own project! (I am working on making a book recommendation system with the Wikipedia articles using entity embeddings from neural networks.) Wikipedia is an incredible source of human-curated information, and we now know how to use this monumental achievement by accessing and processing it programmatically. I look forward to writing about and doing more Wikipedia Data Science. In the meantime, the techniques presented here are broadly applicable so get out there and find a problem to solve! As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or on my personal website at willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Parsing web data (HTML, XML, MediaWiki) using Python libraries Running operations in parallel with multiprocessing/multithreading Benchmarking methods to find the optimal solution to a problem Extract relevant information from the article text Parsing through data in an efficient manner Running operations in parallel to get the most from our hardware Setting up and running benchmarking tests to find efficient solutions Finding and programmatically downloading data from the web Extract the article titles and text from the XML Finding and downloading data programmatically",Wikipedia Data Science: Working with the World’s Largest Encyclopedia,10,published,24639,3756,1.0117145899893503,2,1,1,1,0,1
38,393,243.0837592128704,55,https://medium.com/feature-labs-engineering/featuretools-on-spark-e5aa67eaf807,2,Engineering @ Feature Labs,2018-09-26 13:41:00,28.46,9,880,2018-09-20 14:21:00,"['Apache Spark', 'Python', 'Feature Engineering', 'Data Science', 'Education']","Featuretools on Spark Distributed feature engineering in Featuretools with Spark Apache Spark is one of the most popular technologies on the big data landscape. As a framework for distributed computing, it allows users to scale to massive datasets by running computations in parallel either on a single machine or on clusters of thousands of machines. Spark can be used with Scala, R, Java, SQL, or Python code and its capabilities have led to a rapid adoption as the size of datasets — and the need for methods to work with them — increase. After using Dask to scale automated feature engineering with Featuretools by running calculations in parallel on a single multi-core machine, we wanted to see if we could use a similar approach with Spark to scale to a cluster of multiple machines. While Dask can also be used for cluster computing, we wanted to demonstrate that Featuretools can run on multiple distributed computing frameworks. The same feature engineering code that runs in parallel using Dask requires no modification to also be distributed with Spark. In this article, we’ll see how to use Spark with PySpark to run Featuretools on a computing cluster to scale to even larger datasets. The code for this article is available as a Jupyter Notebook on GitHub. Data and Problem The WSDM customer churn dataset consists of several hundred million rows of transactions, listening records, and background information for customers of a subscription music streaming service. The three tables in the dataset come in just over 30 GB total with a prediction problem of trying to determine when customers will churn. The definition of churn can depend on the business use case and this dataset provides good practice for prediction engineering: defining the problem and then creating labels for a supervised machine learning task. This article will stick to the Spark implementation of feature engineering, but stay tuned for future articles on the process of defining and solving a prediction problem. This article will also skip the Featuretools details, but for a guide on how Featuretools is used for automated feature engineering, check out this article or this project comparing manual to automated feature engineering. The details of how Featuretools works are relatively straightforward, but all we need to know is that it makes hundreds of features from a relational dataset. Distributed Approach We can use the same approach taken with Dask to scale to a cluster with Spark: partition the data into independent subsets, and then distribute feature engineering with these subsets to multiple workers. This follows the general framework of breaking one large problem up into easier sub-problems, each of which can be run by a single worker. To partition the data, we take the customer id represented as a string and The code is shown below: Using the hashing function ensures that a string will always map to the same integer, so the same customer will always end up in the same partition. The assignment of customers to partition is random, but that is not an issue because each customer is independent of the others. The end result of partitioning is that each partition holds all the data necessary to build a feature matrix for a subset of customers. Each partition is independent of all the others: we can calculate the feature matrix for one partition without worrying about the customers in any other partition. This will allow us to run the feature engineering in parallel since workers do not need to communicate with one another. Using the id_to_hash function, we take our three individual large dataframes — representing transactions, user logs, and membership info — and convert all of the customer ids to a partition number. To actually partition the data, the most efficient approach is to use a groupby on the partition number and then iteratively save each partition. For example, in the members dataframe where the msno column is the customer id, then the following code partitions the dataframe into 1000 separate files and saves them. Wrapping this code in a function, we can then partition all of the dataframes. If we have a large file that cannot fit into memory, such as the user_logs, then we can read and partition it in chunks using pandas pd.read_csv. Working with a 30 GB file, this code ran in about 3 minutes. Partitioning data is a common approach when working with large datasets. Dealing with a lot of small files is much easier than dealing with one massive file. When necessary, try to split your data into independent subsets. Architecture Choices All of the partitions are stored in Amazon S3 so they can be accessible to all machines in the Spark cluster. Since this implementation used Amazon EC2 instances, storing the data in Amazon’s cloud makes for efficient read and write operations. The data in S3 is persistent, meaning that it will always be accessible even when we shut down the EC2 instances. The data on the instances, in contrast, are ephemeral which means they don’t maintain state. Storing data in S3 means that it can be accessed it from anywhere — EC2 instances or local machines— and we can shut down the EC2 instances when not in use without worrying about loss of data. Reading and writing can be done directly to S3 using nearly the same syntax as local input / output and if our servers and data are located in the same region, is fairly quick. The distributed architecture is made up of the following parts: Once all the data is partitioned and uploaded to Amazon S3, we need to launch a Spark cluster that will carry out the feature engineering. For the cluster, we choose EC2 instances because of the speed and ease of launching multiple machines. This project only used 3 instances, but the same approach could scale to thousands of machines. Spark Cluster Setting up a Spark cluster can be tough. I recommend following this guide to get up and running. For demonstration purposes, I used a relatively small cluster consisting of one parent node and two worker nodes. (Following the lead of Guido van Rossum and the Python community, I am using the terms parent and worker in place of the outdated master and slave.) Once the instances are running, they can be connected by launching Spark from the parent and then connecting the workers. If everything goes correctly, you can monitor the cluster from localhost:8080 on the parent machine: PySpark Implementation Getting a cluster running is the hardest point, and from here, the code to distribute the computation across all workers is straightforward. Moreover, I was able to run all the code from a Jupyter Notebook, my preferred working environment. It’s pretty neat that a Jupyter Notebook that runs on a single machine can be used to distribute calculations across thousands of machines! (I followed this guide to work with Spark in Jupyter). The first thing to do in the Jupyter Notebook is initialize Spark . For this we use the findspark Python library, passing in the location of the Spark installation on the parent machine. Then, we need to create a SparkContext , the entry point to a Spark application. This sets up the internal services and creates a connection to a Spark execution environment. There are a number of parameters to pass to the SparkContext and we can also use a SparkConf object to specify a certain configuration. The below code creates both a SparkConf and a SparkContext with parameters based on my set up: The number of executors, 3, is set equal to the number of instances, with each executor having access to 4 cores and a maximum of 12 GB of memory. Total, we have 12 workers, each of which can carry out one feature matrix calculation for a partition at a time. The master is the IP address of your spark cluster which can be found at the local dashboard on the parent’s port 8080. These values were set based on trial and error and this guide. The function that we want to distribute works as follows: The pseudo-code for the function is below: The tasks we want to parallelize in this function are just a list of partitions: The final step is to distribute the computation. We tell Spark to take the partitions and divide them between our workers. We then map tasks to the function by sending each partition number to partition_to_feature_matrix. Using our SparkContext , we can do this in a single (long) line. While the calculations are running, we can monitor progress both on the Spark parent dashboard at localhost:8080 and the specific job dashboard at localhost:4040. The job dashboard gives us quite a lot of information starting with a basic overview: The stages tab shows us the directed acyclic graph, which in this case is not very complicated since all of the partitions are a single step run independently. Here we can also see more detailed information as the jobs complete such as summary statistics of the time to run each task. (The job dashboard is only available when the computation is ongoing so wait until you’ve submitted a task before trying to pull it up.) After a few hours, the computation has finished and the feature matrices — with 230 features for each customer — are stored in S3. At this point, we can join the feature matrices together for modeling, or, if we have a model that supports incremental learning, we can train on one partition at a time. While this calculation would have been possible on a single machine given enough time, parallelizing feature engineering is an efficient method to scale to larger datasets. Furthermore, the partition and distribute framework is applicable in many different situations with significant efficiency gains. Conclusions Breaking one large problem up into many smaller problems is a common design pattern when working with massive datasets. Apache Spark is a powerful framework for distributed computing that lets us scale to hundreds of machines and as much data as we can get our hands on. In this article, we saw how an automated feature engineering workflow in Featuretools could scale to a large dataset using a compute cluster managed by Spark. We were able to take the same Featuretools code that runs on a single machine and run it on multiple machines using minimal PySpark code. Although this approach used Pandas dataframes, it’s also possible to write code in PySpark using Spark dataframes for increased efficiency in the Spark ecosystem. We’ll have more updates on this in the future, but for now, we know how to use the partition and distribute approach to efficiently parallelize feature engineering with Spark. If building meaningful, high-performance predictive models is something you care about, then get in touch with us at Feature Labs. While this project was completed with the open-source Featuretools, the commercial product offers additional tools and support for creating machine learning solutions. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Engineering @ Feature Labs Feature Labs Engineering Blog Modulo divide this integer by the number of partitions Computing cluster of EC2 instances Spark used to distribute work between workers Job execution carried out on local machine through SSH Creates an EntitySet with all tables and relationships Calculates a feature_matrix using a predefined set of features Saves the feature_matrix to S3 in the partition directory Hash the id to an integer using the MD5 message-digest algorithm Persistent data storage through S3 with read/write in Python Takes in a partition number and reads partition data from S3",Featuretools on Spark,3,published,3092,2087,0.1883085769046478,5,1,1,0,0,1
39,3300,237.77614348048613,568,https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526,11,Towards Data Science,2018-10-01 21:04:00,27.09,8,8183,2018-09-30 19:52:00,"['Machine Learning', 'Data Science', 'Deep Learning', 'Education', 'Towards Data Science']","Neural Network Embeddings Explained How deep learning can represent War and Peace as a vector Applications of neural networks have expanded significantly in recent years from image segmentation to natural language processing to time-series forecasting. One notably successful use of deep learning is embedding, a method used to represent discrete variables as continuous vectors. This technique has found practical applications with word embeddings for machine translation and entity embeddings for categorical variables. In this article, I’ll explain what neural network embeddings are, why we want to use them, and how they are learned. We’ll go through these concepts in the context of a real problem I’m working on: representing all the books on Wikipedia as vectors to create a book recommendation system. Embeddings An embedding is a mapping of a discrete — categorical — variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed space. Neural network embeddings have 3 primary purposes: This means in terms of the book project, using neural network embeddings, we can take all 37,000 book articles on Wikipedia and represent each one using only 50 numbers in a vector. Moreover, because embeddings are learned, books that are more similar in the context of our learning problem are closer to one another in the embedding space. Neural network embeddings overcome the two limitations of a common method for representing categorical variables: one-hot encoding. The operation of one-hot encoding categorical variables is actually a simple embedding where each category is mapped to a different vector. This process takes discrete entities and maps each observation to a vector of 0s and a single 1 signaling the specific category. The one-hot encoding technique has two main drawbacks: The first problem is well-understood: for each additional category — referred to as an entity — we have to add another number to the one-hot encoded vector. If we have 37,000 books on Wikipedia, then representing these requires a 37,000-dimensional vector for each book, which makes training any machine learning model on this representation infeasible. The second problem is equally limiting: one-hot encoding does not place similar entities closer to one another in vector space. If we measure similarity between vectors using the cosine distance, then after one-hot encoding, the similarity is 0 for every comparison between entities. This means that entities such as War and Peace and Anna Karenina (both classic books by Leo Tolstoy) are no closer to one another than War and Peace is to The Hitchhiker’s Guide to the Galaxy if we use one-hot encoding. Considering these two problems, the ideal solution for representing categorical variables would require fewer numbers than the number of unique categories and would place similar categories closer to one another. To construct a better representation of categorical entities, we can use an embedding neural network and a supervised task to learn embeddings. The main issue with one-hot encoding is that the transformation does not rely on any supervision. We can greatly improve embeddings by learning them using a neural network on a supervised task. The embeddings form the parameters — weights — of the network which are adjusted to minimize loss on the task. The resulting embedded vectors are representations of categories where similar categories — relative to the task — are closer to one another. For example, if we have a vocabulary of 50,000 words used in a collection of movie reviews, we could learn 100-dimensional embeddings for each word using an embedding neural network trained to predict the sentimentality of the reviews. (For exactly this application see this Google Colab Notebook). Words in the vocabulary that are associated with positive reviews such as “brilliant” or “excellent” will come out closer in the embedding space because the network has learned these are both associated with positive reviews. In the book example given above, our supervised task could be “identify whether or not a book was written by Leo Tolstoy” and the resulting embeddings would place books written by Tolstoy closer to each other. Figuring out how to create the supervised task to produce relevant representations is the toughest part of making embeddings. In the Wikipedia book project (complete notebook here), the supervised learning task is set as predicting whether a given link to a Wikipedia page appears in the article for a book. We feed in pairs of (book title, link) training examples with a mix of positive — true — and negative — false — pairs. This set-up is based on the assumption that books which link to similar Wikipedia pages are similar to one another. The resulting embeddings should therefore place alike books closer together in vector space. The network I used has two parallel embedding layers that map the book and wikilink to separate 50-dimensional vectors and a dot product layer that combines the embeddings into a single number for a prediction. The embeddings are the parameters, or weights, of the network that are adjusted during training to minimize the loss on the supervised task. In Keras code, this looks like the following (don’t worry if you don’t completely understand the code, just skip to the images): Although in a supervised machine learning task the goal is usually to train a model to make predictions on new data, in this embedding model, the predictions can be just a means to an end. What we want is the embedding weights, the representation of the books and links as continuous vectors. The embeddings by themselves are not that interesting: they are simply vectors of numbers: However, the embeddings can be used for the 3 purposes listed previously, and for this project, we are primarily interested in recommending books based on the nearest neighbors. To compute similarity, we take a query book and find the dot product between its vector and those of all the other books. (If our embeddings are normalized, this dot product is the cosine distance between vectors that ranges from -1, most dissimilar, to +1, most similar. We could also use the Euclidean distance to measure similarity). This is the output of the book embedding model I built: (The cosine similarity between a vector and itself must be 1.0). After some dimensionality reduction (see below), we can make figures like the following: We can clearly see the value of learning embeddings! We now have a 50-number representation of every single book on Wikipedia, with similar books closer to one another. Embedding Visualizations One of the coolest parts about embeddings are that they can be used to visualize concepts such as novel or non-fiction relative to one another. This requires a further dimension reduction technique to get the dimensions to 2 or 3. The most popular technique for reduction is itself an embedding method: t-Distributed Stochastic Neighbor Embedding (TSNE). We can take the original 37,000 dimensions of all the books on Wikipedia, map them to 50 dimensions using neural network embeddings, and then map them to 2 dimensions using TSNE. The result is below: (TSNE is a manifold learning technique which means that it tries to map high-dimensional data to a lower-dimensional manifold, creating an embedding that attempts to maintain local structure within the data. It’s almost exclusively used for visualization because the output is stochastic and it does not support transforming new data. An up and coming alternative is Uniform Manifold Approximation and Projection, UMAP, which is much faster and does support transform new data into the embedding space). By itself this isn’t very useful, but it can be insightful once we start coloring it based on different book characteristics. We can clearly see groupings of books belonging to the same genre. It’s not perfect, but it’s still impressive that we can represent all books on Wikipedia using just 2 numbers that still capture the variability between genres. The book example (full article coming soon) shows the value of neural network embeddings: we have a vector representation of categorical objects that is both low-dimensional and places similar entities closer to one another in the embedded space. The problem with static graphs is that we can’t really explore the data and investigate groupings or relationships between variables. To solve this problem, TensorFlow developed projector, an online application that lets us visualize and interact with embeddings. I’ll release an article on how to use this tool shortly, but for now, here’s the results: Conclusions Neural network embeddings are learned low-dimensional representations of discrete data as continuous vectors. These embeddings overcome the limitations of traditional encoding methods and can be used for purposes such as finding nearest neighbors, input into another model, and visualizations. Although many deep learning concepts are talked about in academic terms, neural network embeddings are both intuitive and relatively simple to implement. I firmly believe that anyone can learn deep learning and use libraries such as Keras to build deep learning solutions. Embeddings are an effective tool for handling discrete variables and present a useful application of deep learning. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or on my personal website at willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. As input to a machine learning model for a supervised task. For visualization of concepts and relations between categories. The mapping is completely uninformed: “similar” categories are not placed closer to each other in embedding space. TensorFlow Guide to Embeddings Book Recommendation System Using Embeddings Tutorial on Word Embeddings in Keras Finding nearest neighbors in the embedding space. These can be used to make recommendations based on user interests or cluster categories. For high-cardinality variables — those with many unique categories — the dimensionality of the transformed vector becomes unmanageable.",Neural Network Embeddings Explained,4,published,30208,1809,1.824212271973466,1,1,1,1,1,0
43,1100,235.10547076980325,150,https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9,1,Towards Data Science,2018-10-04 13:10:00,20.58,13,1609,2018-09-29 05:03:00,"['Machine Learning', 'Data Science', 'Education', 'Deep Learning', 'Towards Data Science']","Building a Recommendation System Using Neural Network Embeddings How to use deep learning and Wikipedia to create a book recommendation system Deep learning can do some incredible things, but often the uses are obscured in academic papers or require computing resources available only to large corporations. Nonetheless, there are applications of deep learning that can be done on a personal computer with no advanced degree required. In this article, we will see how to use neural network embeddings to create a book recommendation system using all Wikipedia articles on books. Our recommendation system will be built on the idea that books which link to similar Wikipedia pages are similar to one another. We can represent this similarity and hence make recommendations by learning embeddings of books and Wikipedia links using a neural network. The end result is an effective recommendation system and a practical application of deep learning. The complete code for this project is available as a Jupyter Notebook on GitHub. If you don’t have a GPU, you can also find the notebook on Kaggle where you can train your neural network with a GPU for free.This article will focus on the implementation, with the concepts of neural network embeddings covered in an earlier article. (To see how to retrieve the data we’ll use — all book articles on Wikipedia — take a look at this article.) This project was adapted from the Deep Learning Cookbook, an excellent book with hands-on examples for applying deep learning. Neural Network Embeddings Embeddings are a way to represent discrete — categorical — variables as continuous vectors. In contrast to an encoding method like one-hot encoding, neural network embeddings are low-dimensional and learned, which means they place similar entities closer to one another in the embedding space. In order to create embeddings, we need a neural network embedding model and a supervised machine learning task. The end outcome of our network will be a representation of each book as a vector of 50 continuous numbers. While the embeddings themselves are not that interesting — they are just vectors — they can be used for three primary purposes: This project covers primarily the first use case, but we’ll also see how to create visualizations from the embeddings. Practical applications of neural network embeddings include word embeddings for machine translation and entity embeddings for categorical variables. Data: All Books on Wikipedia As usual with a data science project, we need to start with a high-quality dataset. In this article, we saw how to download and process every single article on Wikipedia, searching for any pages about books. We saved the book title, basic information, links on the book’s page that point to other Wikipedia pages (wikilinks), and links to external sites. To create the recommendation system, the only information we need are the title and wikilinks. Even when working with a neural network, it’s important to explore and clean the data, and in the notebook I make several corrections to the raw data. For example, looking at the most linked pages: We see that the top four pages are generic and won’t help in making recommendations. The format of a book tells us nothing about the content: knowing a book is paperback or hardcover does not allow us — or a neural network —to figure out the other books it is similar to. Therefore, we can remove these links to help the neural network distinguish between books. Thinking about the end purpose can help in the data cleaning stage and this action alone significantly improves the recommendations. Out of pure curiosity, I wanted to find the books most linked to by other books on Wikipedia. These are the 10 “most connected” Wikipedia books: This is a mix of reference works and classic books which makes sense. After data cleaning, we have a set of 41758 unique wikilinks and 37020 unique books. Hopefully there is a book in there for everyone! Once we’re confident our data is clean, we need to develop a supervised machine learning task with labeled training examples. To learn meaningful embeddings, our neural network must be trained to accomplish an objective. Working from the guiding assumption of the project — that similar books link to similar Wikipedia pages — we can formulate the problem as follows: given a (book title, wikilink) pair, determine if the wikilink is present in the book’s article. We won’t actually need to give the network the book article. Instead, we’ll feed in hundreds of thousands of training examples consisting of book title, wikilink, and label. We give the network some true examples — actually present in the dataset — and some false examples, and eventually it learns embeddings to distinguish when a wikilink is on a book’s page. Expressing the supervised learning task is the most important part of this project. Embeddings are learned for a specific task and are relevant only to that problem. If our task was to determine which books were written by Jane Austen, then the embeddings would reflect that goal, placing books written by Austen closer together in embedding space. We hope that by training to tell if a book has a certain wikilink on its page, the network learns embeddings that places similar books — in terms of content — closer to one another. Once we’ve outlined the learning task, we need to implement it in code. To get started, because the neural network can only accept integer inputs, we create a mapping from each unique book to an integer: We also do the same thing with the links. After this, to create a training set, we make a list of all (book, wikilink) pairs in the data. This requires iterating through each book and recording an example for each wikilink on its page: This gives us a total of 772798 true examples that we can sample from to train the model. To generate the false examples — done later — we’ll simply pick a link index and book index at random, make sure it’s not in the pairs, and then use it as a negative observation. Note about Training / Testing Sets While using a separate validation and testing set is a must for a normal supervised machine learning task, in this case, our primary objective is not to make the most accurate model, but to generate embeddings. The prediction task is just the means by which we train our network for those embeddings. At the end of training, we are not going to be testing our model on new data, so we don’t need to evaluate the performance or use a validation set to prevent overfitting. To get the best embeddings, we’ll use all examples for training. Embedding Model Although neural network embeddings sound technically complex, they are relatively easy to implement with the Keras deep learning framework. (I recommend starting with Keras if you are new to deep learning. TensorFlow may give you more control, but Keras cannot be beat for development). The embedding model has 5 layers: In an embedding neural network, the embeddings are the parameters — weights — of the neural network that are adjusted during training in order to minimize loss on the objective. The neural network takes in a book and a link as integers and outputs a prediction between 0 and 1 that is compared to the true value. The model is compiled with the Adam optimizer (a variant on Stochastic Gradient Descent) which, during training, alters the embeddings to minimize the binary_crossentropy for this binary classification problem. Below is the code for the complete model: This same framework can be used for many embedding models. The important point to understand is that the embeddings are the model parameters (weights) and also the final result we want. We don’t really care if the model is accurate, what we want is relevant embeddings. We’re used to the weights in a model being a means to make accurate predictions, but in an embedding model, the weights are the objective and the predictions are a means to learn an embedding. There are almost 4 million weights as shown by the model summary: With this approach, we’ll get embeddings not only for books, but also for links which means we can compare all Wikipedia pages that are linked to by books. Neural networks are batch learners because they are trained on a small set of samples — observations — at a time over many rounds called epochs. A common approach for training neural networks is to use a generator. This is a function that yields (not returns) batches of samples so the entire result is not held in memory. Although it’s not an issue in this problem, the benefit of a generator is that large training sets do not need to all be loaded into memory. Our generator takes in the training pairs, the number of positive samples per batch ( n_positive ) , and the ratio of negative:positive samples per batch ( negative_ratio ). The generator yields a new batch of positive and negative samples each time it is called. To get positive examples, we randomly sample true pairs. For the negative examples, we randomly sample a book and link, make sure this pairing is not in the true pairs, and then add it to the batch. The code below shows the generator in its entirety. Each time we call next on the generator, we get a new training batch. With a supervised task, a training generator, and an embedding model, we’re ready to learn book embeddings. There are a few training parameters to select. The first is the number of positive examples in each batch. Generally, I try to start with a small batch size and increase it until performance starts to decline. Also, we need to choose the number of negative examples trained for each positive example. I’d recommend experimenting with a few options to see what works best. Since we’re not using a validation set to implement early stopping, I choose a number of epochs beyond which the training loss does not decrease. (If the training parameters seem arbitrary, in a sense they are, but based on best practices outlined in Deep Learning. Like most aspects of machine learning, training a neural network is largely empirical.) Once the network is done training, we can extract the embeddings. Applying Embeddings: Making Recommendations The embeddings themselves are fairly uninteresting: they are just 50-number vectors for each book and each link: However, we can use these vectors for two different purposes, the first of which is to make our book recommendation system. To find the closest book to a query book in the embedding space, we take the vector for that book and find its dot product with the vectors for all other books. If our embeddings are normalized, then the dot product between the vectors represents the cosine similarity, ranging from -1, most dissimilar, to +1, most similar. Querying the embeddings for the classic War and Peace by Leo Tolstoy yields: The recommendations make sense! These are all classic Russian novels. Sure we could have gone to GoodReads for these same recommendations, but why not build the system ourselves? I encourage you to work with the notebook and explore the embeddings yourself. In addition to embedding the books, we also embedded the links which means we can find the most similar links to a given Wikipedia page: Currently, I’m reading a fantastic collection of essays by Stephen Jay Gould called Bully for Brontosaurus. What should I read next? Visualizations of Embeddings One of the most intriguing aspects of embeddings are that they can be used to visualize concepts such as novel or nonfiction relative to one another. This requires a further dimension reduction technique to get the dimensions to 2 or 3. The most popular technique for reduction is another embedding method: t-Distributed Stochastic Neighbor Embedding (TSNE). Starting with the 37,000-dimensional space of all books on Wikipedia, we map it to 50 dimensions using embeddings, and then to just 2 dimensions with TSNE. This results in the following image: By itself this image is not that illuminating, but once we start coloring it by book characteristics, we start to see clusters emerge: There are some definite clumps (only the top 10 genres are highlighted) with non-fiction and science fiction having distinct sections. The novels seem to be all over the place which makes sense given the diversity in novel content. We can also do embeddings with the country: I was a little surprised at how distinctive the countries are! Evidently Australian books are quite unique. Furthermore, we can highlight certain books in the Wikipedia map: There are a lot more visualizations in the notebook and you make your own. I’ll leave you with one more showing the 10 “most connected” books: One thing to note about TSNE is that it tries to preserve distances between vectors in the original space, but because it reduces the number of dimensions, it may distort the original separation. Therefore, books that are close to one another in the 50-dimensional embedding space may not be closest neighbors in the 2-dimensional TSNE embedding. These visualizations are pretty interesting, but we can make stunning interactive figures with TensorFlow’s projector tool specifically designed for visualizing neural network embeddings. I plan on writing an article on how to use this tool, but for now here are some of the results: To explore a sample of the books interactively, head here. Data science projects aren’t usually invented entirely on their own. A lot of the projects I work on are ideas from other data scientists that I adapt, improve, and build on to make a unique project. (This project was inspired by a similar project for movie recommendations in the Deep Learning Cookbook.) With that attitude in mind, here are a few ways to build on this work: This is by no means a homework assignment, just some project ideas if you want to put what you’ve read into practice. If you do decide to take on a project, I’d enjoy hearing about it! Conclusions Neural network embeddings are a method to represent discrete categorical variables as continuous vectors. As a learned low-dimensional representation, they are useful for finding similar categories, as input into a machine learning model, or to visualize maps of concepts. In this project, we used neural network embeddings to create an effective book recommendation system built on the idea that books which link to similar pages are similar to each other. The steps for creating neural network embeddings are: The details can be found in the notebook and I’d encourage anyone to build on this project. While deep learning may seem overwhelming because of technical complexity or computational resources, this is one of many applications that can be done on a personal computer with a limited amount of studying. Deep learning is a constantly evolving field, and this project is a good way to get started by building a useful system. And, when you’re not studying deep learning, now you know what you should be reading! As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or on my personal website at willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. As input to a machine learning model Visualization in low dimensions Embedding: parallel length 50 embeddings for the book and link Dot: merges embeddings by computing dot product Reshape: needed to shape the dot product to a single number Dense: one output neuron with sigmoid activation Use the embeddings to train a supervised machine learning model to predict the book characteristics which include genre, author, and country. Pick a topic category on Wikipedia and create your own recommendation system. You could use people, landmarks, or even historical events. You can use this notebook to get the data and this notebook for embeddings. Formulate a supervised task to learn embeddings that reflect the problem. Build and train an embedding neural network model. Extract the embeddings for making recommendations and visualizations. Finding nearest neighbors in the embedding space Input: parallel inputs for the book and link Create embeddings using the external links instead of wikilinks. These are to web pages outside Wikipedia and might produce different embeddings. Gather data. Neural networks require many training examples.",Building a Recommendation System Using Neural Network Embeddings,8,published,7820,2999,0.3667889296432144,5,1,1,1,1,0
36,2500,227.1329140586227,365,https://towardsdatascience.com/simpsons-paradox-how-to-prove-two-opposite-arguments-using-one-dataset-1c9c917f5ff9,7,Towards Data Science,2018-10-12 12:31:00,23.6,10,3203,2018-10-10 16:21:00,"['Data Science', 'Statistics', 'Education', 'Towards Data Science', 'Reasoning']","Simpson’s Paradox: How to Prove Opposite Arguments with the Same Data Understanding a statistical phenomenon and the importance of asking why Imagine you and your partner are trying to find the perfect restaurant for a pleasant dinner. Knowing this process can lead to hours of arguments, you seek out the oracle of modern life: online reviews. Doing so, you find your choice, Carlo’s Restaurant is recommended by a higher percentage of both men and women than your partner’s selection, Sophia’s Restaurant. However, just as you are about to declare victory, your partner, using the same data, triumphantly states that since Sophia’s is recommended by a higher percentage of all users, it is the clear winner. What is going on? Who’s lying here? Has the review site got the calculations wrong? In fact, both you and your partner are right and you have unknowingly entered the world of Simpson’s Paradox, where a restaurant can be both better and worse than its competitor, exercise can lower and increase the risk of disease, and the same dataset can be used to prove two opposing arguments. Instead of going out to dinner, perhaps you and your partner should spend the evening discussing this fascinating statistical phenomenon. Simpson’s Paradox occurs when trends that appear when a dataset is separated into groups reverse when the data are aggregated. In the restaurant recommendation example, it really is possible for Carlo’s to be recommended by a higher percentage of both men and women than Sophia’s but to be recommended by a lower percentage of all reviewers. Before you declare this to be lunacy, here is the table to prove it. The data clearly show that Carlo’s is preferred when the data are separated, but Sophia’s is preferred when the data are combined! How is this possible? The problem here is that looking only at the percentages in the separate data ignores the sample size, the number of respondents answering the question. Each fraction shows the number of users who would recommend the restaurant out of the number asked. Carlo’s has far more responses from men than from women while the reverse is true for Sophia’s. Since men tend to approve of restaurants at a lower rate, this results in a lower average rating for Carlo’s when the data are combined and hence a paradox. To answer the question of which restaurant we should go to, we need to decide if the data can be combined or if we should look at separately. Whether or not we should aggregate the data depends on the process generating the data — that is, the causal model of the data. We’ll cover what this means and how to resolve Simpson’s Paradox after we see another example. Another intriguing version of Simpson’s Paradox occurs when a correlation that points in one direction in stratified groups becomes a correlation in the opposite direction when aggregated for the population. Let’s take a look at a simplified example. Say we have data on the number of hours of exercise per week versus the risk of developing a disease for two sets of patients, those below the age of 50 and those over the age of 50. Here are individual plots showing the relationship between exercise and probability of disease. (Example can be re-created in this Jupyter Notebook). We clearly see a negative correlation, indicating that increased levels of exercise per week are correlated with a lower risk of developing the disease for both groups. Now, let’s combine the data together on a single plot: The correlation has completely reversed! If shown only this figure, we would conclude that exercise increases the risk of disease, the opposite of what we would say from the individual plots. How can exercise both decrease and increase the risk of disease? The answer is that it doesn’t and to figure out how to resolve the paradox, we need to look beyond the data we are shown and reason through the data generation process — what caused the results. Resolving the Paradox To avoid Simpson’s Paradox leading us to two opposite conclusions, we need to choose to segregate the data in groups or aggregate it together. That seems simple enough, but how do we decide which to do? The answer is to think causally: how was the data generated and based on this, what factors influence the results that we are not shown? In the exercise vs disease example, we intuitively know that exercise is not the only factor affecting the probability of developing a disease. There are other influences such as diet, environment, heredity and so forth. However, in the plots above, we see only probability versus hours of exercise. In our fictional example, let’s assume disease is caused by both exercise and age. This is represented in the following causal model of disease probability. In the data, there are two different causes of disease yet by aggregating the data and looking at only probability vs exercise, we ignore the second cause — age — completely. If we go ahead and plot probability vs age, we can see that the age of the patient is strongly positively correlated with disease probability. As the patient increases in age, her/his risk of the disease increases which means older patients are more likely to develop the disease than younger patients even with the same amount of exercise. Therefore, to assess the effect of just exercise on disease, we would want to hold the age constant and change the amount of weekly exercise. Separating the data into groups is one way to do this, and doing so, we see that for a given age group, exercise decreases the risk of developing the disease. That is, controlling for the age of the patient, exercise is correlated with a lower risk of disease. Considering the data generating process and applying the causal model, we resolve Simpson’s Paradox by keeping the data stratified to control for an additional cause. Thinking about what question we want to answer can also help us solve the paradox. In the restaurant example, we want to know which restaurant is most likely to satisfy both us and our partner. Even though there may be other factors influencing a review than just the quality of the restaurant, without access to that data, we’d want to combine the reviews together and look at the overall average. In this case, aggregating the data makes the most sense. The relevant query to ask in the exercise vs disease example is should we personally exercise more to reduce our individual risk of developing the disease? Since we are a person either below 50 or above 50 (sorry to those exactly 50) then we need to look at the correct group, and no matter which group we are in, we decide that we should indeed exercise more. Thinking about the data generation process and the question we want to answer requires going beyond just looking at data. This illustrates perhaps the key lesson to learn from Simpson’s Paradox: the data alone are not enough. Data are never purely objective and especially when we only see the final plot, we must consider if we are getting the whole story. We can try to get a more complete picture by asking what caused the data and what factors influencing the data are we not being shown. Often, the answers reveal that we should in fact come away with the opposite conclusion! Simpson’s Paradox in Real Life This phenomenon is not — as seems to be the case for some statistical concepts — a contrived problem that is theoretically possible but never occurs in practice. There are in fact many well-known studied cases of Simpson’s Paradox in the real world. One example occurs with data about the effectiveness of two kidney stone treatments. Viewing the data separated into the treatments, treatment A is shown to work better with both small and large kidney stones, but aggregating the data reveals that treatment B works better for all cases! The table below shows the recovery rates: How can this be? The paradox can be resolved by considering the data generation process — causal model — informed by domain knowledge. It turns out that small stones are considered less serious cases, and treatment A is more invasive than treatment B. Therefore, doctors are more likely to recommend the inferior treatment, B, for small kidney stones, where the patient is more likely to recover successfully in the first place because the case is less severe. For large, serious stones, doctors more often go with the better — but more invasive — treatment A. Even though treatment A performs better on these cases, because it is applied to more serious cases, the overall recovery rate for treatment A is lower than treatment B. In this real-world example, size of kidney stone — seriousness of case — is called a confounding variable because it affects both the independent variable — treatment method — and the dependent variable — recovery. Confounding variables are also something we don’t see in the data table but they can be determined by drawing a causal diagram: The effect in question, recovery, is caused both by the treatment and the size of the stone (seriousness of the case). Moreover, the treatment selected depends on the size of the stone making size a confounding variable. To determine which treatment actually works better, we need to control for the confounding variable by segmenting the two groups and comparing recovery rates within groups rather than aggregated over groups. Doing this we arrive at the conclusion that treatment A is superior. Here’s another way to think about it: if you have a small stone, you prefer treatment A; if you have a large stone you also prefer treatment A. Since you must have either a small or a large stone, you always prefer treatment A and the paradox is resolved. Sometimes looking at aggregated data is useful but in other situations it can obscure the true story. The second real-life example shows how Simpson’s Paradox could be used to prove two opposite political points. The following table shows that during Gerald Ford’s presidency, he not only lowered taxes for every income group, he also raised taxes on a nation-wide level from 1974 to 1978. Take a look at the data: We can clearly see that the tax rate in each tax bracket decreased from 1974 to 1978, yet the overall tax rate increased over the same time period. By now, we know how to resolve the paradox: look for additional factors that influence overall tax rates. The overall tax rate is a function both of the individual bracket tax rates, and also the amount of taxable income in each bracket. Due to inflation (or wage increases), there was more income in the upper tax brackets with higher rates in 1978 and less income in lower brackets with lower rates. Therefore, the overall tax rate increased. Whether or not we should aggregate the data depends on the question we want to answer (and maybe the political argument we are trying to make) in addition to the data generation process. On a personal level, we are just one person, so we only care about the tax rate within our bracket. In order to determine if our taxes rose from 1974 to 1978, we must determine both did the tax rate change in our tax bracket, and did we move to a different tax bracket. There are two causes to account for the tax rate paid by an individual, but only one is captured in this slice of the data. Simpson’s Paradox is important because it reminds us that the data we are shown is not all the data there is. We can’t be satisfied only with the numbers or a figure, we have to consider the data generation process — the causal model — responsible for the data. Once we understand the mechanism producing the data, we can look for other factors influencing a result that are not on the plot. Thinking causally is not a skill most data scientists are taught, but it’s critical to prevent us from drawing faulty conclusions from numbers. We can use our experience and domain knowledge — or those of experts in the field — in addition to data to make better decisions. Moreover, while our intuitions usually serve us pretty well, they can fail in cases where not all the information is immediately available. We tend to fixate on what’s in front of us — all we see is all there is — instead of digging deeper and using our rational, slow mode of thinking. Particularly when someone has a product to sell or an agenda to implement, we have to be extremely skeptical of the numbers by themselves. Data is a powerful weapon, but it can be used by both those who want to help us and nefarious actors. Simpson’s Paradox is an interesting statistical phenomenon but it also demonstrates the best shield against manipulation is the ability to think rationally and ask why. As always, I welcome discussion and constructive criticism. I can be reached through Twitter @koehrsen_will or on my personal website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Stanford Encyclopedia Article on Simpson’s Paradox Simpson’s Paradox: A Cautionary Tale in Advanced Analytics The Book of Why: The New Science of Cause and Effect by Judea Pearl Simpson’s Paradox in Real Life Understanding Simpson’s Paradox",Simpson’s Paradox: How to Prove Opposite Arguments with the Same Data,12,published,13573,2483,1.0068465565847764,1,1,1,1,0,0
40,1000,222.21974938857642,136,https://blog.usejournal.com/the-power-of-i-dont-know-590ab40d1995,3,Noteworthy - The Journal Blog,2018-10-17 10:26:00,25.83,9,559,2018-10-16 06:59:00,"['Growth Mindset', 'Education', 'Learning', 'Self Improvement', 'Science']","The Power of I Don’t Know Intellectual humility is not a weakness but a strength The phrase “I don’t know” has almost disappeared from our discourse. From the job applicant who must claim to have mastered 100 different skills to politicians who need to have a confident opinion on every news event, the modern world does not encourage people to admit when they lack knowledge or skills. However, by refusing to acknowledge our ignorance, we limit our chances for personal improvement. Saying “I don’t know” — practicing intellectual humility — and adopting a growth mindset are powerful means for becoming smarter and more skilled individuals. The Dangers of Certainty When we are young, we refuse to say we don’t know something because we’re ignorant of our own ignorance. We simply have no conception that the world extends beyond our sphere of knowledge, an idea that should be — but often isn’t — disproven in school. Although we are naturally curious — think about the endless “why?” questions asked by children — school teaches us there are a set of certain facts about the world to memorize, focusing on the limited amount of current human knowledge rather than discussing how we continually discover new knowledge that reveals our past beliefs to be incorrect. Moreover, we’re taught to not question these facts, and if we don’t know one of them, the best response is to guess! Once the curiosity has been driven out of us in school and we go into the workforce, we’re even less likely to say “I don’t know”. We perceive a lack of absolute confidence, especially in our leaders, as a weakness. Despite the uncertainty that comes with living in a complex world, we are afraid to admit when we don’t know something for sure and we therefore don’t expect to see uncertainty expressed by others. From economic statistics such as the jobless rate to election forecasts, we like to see a single number in news stories even when the real answer is a wide range of possibilities (this has been dubbed the lure of incredible certitude). It’s far more comfortable for someone to feign certainty than admit to the nuance in a real-world situation. When people refuse to admit what they don’t know, confirmation bias comes into play as people deliberately seek out evidence that confirms their supposed knowledge and avoid that which might show they don’t know enough to make a rational decision. At all ages and level of society, an inability to admit the limits of our expertise is disastrous: personally, I’ve spent way too much time trying to learn new software or techniques on my own rather than admit I was out of my league and seek out help from those with more knowledge. On the national scale, decisions as monumental as that to invade another country (see the Iraq War) are made because individuals have more confidence than is warranted in their beliefs and refuse to admit there is uncertainty in the situation. Examples of the perils of certitude abound: In a Freakonomics Podcast episode, the hosts discuss a case in which a business spent hundreds of millions on an ineffective advertising campaign because they refused to even ask if it was working. Moreover, refusing to admit when we don’t have a skill can have negative consequences. Think of the job applicant with the inflated resume. Once on the job, they’ll have to take the energy to maintain the facade that they know more than they do until it becomes unsustainable and they have to actually learn the skill. It would be much better for everyone involved if the candidate was honest and (if awarded the job) arrived ready to learn on the first day. Considering the potential downsides of failing to say “I don’t know”, it’s pretty clear that this phrase should be a part of our vocabulary. However, given the difficulty of saying those words — what the Freakonomics hosts claim are the 3 hardest words to say in the English language — perhaps “I don’t know” needs a rebranding. The phrase intellectual humility is a better choice as it lacks the negative associations. Furthermore, it encompasses both the idea of not having complete knowledge, and even when you do know something, admitting that there is uncertainty involved. Intellectual humility also fits in nicely with the most crucial part of the “I don’t know” approach: what you do after saying those words. Whatever we call it, asserting your own lack of knowledge or skills by itself is necessary, but it’s only the first step. The true benefits of “I don’t know” come when we start working to address our deficiencies. Consider two different approaches to the following scenario: your boss has asked you to try and automate some routine data entry tasks to save you and your coworkers hours of tedium. The first approach is what we’ll call the defeatist mindset: deterred by your lack of programming knowledge, you tell your boss “I don’t know enough to do that, I think someone else should take it on.” In the defeatist mindset, “I don’t know” is used as an excuse for not completing a task. Much as its name suggests, the defeatist mindset is not a productive form of “I don’t know” — it prevents you from learning new skills keeping you stagnant in your job. Contrast this with what happens when you adopt a growth-driven attitude. This time, when your boss asks the question, your response is “I don’t know but I would enjoy the opportunity to learn.” Within a few days, you have learned enough Python to automate the data entry, saved your co-workers dozens of hours per week, earned yourself a promotion, and opened up many opportunities thanks to your newly acquired skills. The growth-driven version of “I don’t know” turns a lack of knowledge into an opportunity for personal improvement. You currently lack the knowledge or skills but you’re willing to work to acquire them. The definition of the growth driven mindset is that talents can be developed with effort and commitment. This is in contrast to the fixed mindset where one believes that talent is innate and thus there is no reason to try and improve oneself if you don’t already have the skills. Saying I don’t know — using intellectual humility — is the necessary first step of the growth driven mindset. Once you’ve admitted what you don’t know, there is still a lot of work left, but at least you are open to the idea of improving. Acting on the growth driven mindset requires the crucial ability to see where you are now (what you don’t know), where you would like to be in the future (what you want to learn), and then forming a plan to get from here to there. The defeatist mindset is easy and natural: as humans, we are always looking for the easy option, which means staying in our current position. The growth-driven mindset requires not only admitting the limits of our own knowledge, but also committing ourselves to a long-term plan. It also might necessitate going out of our comfort zone. However, the alternative is to stay in the same rut, doing the same job using our current skills or rehashing our same opinions because we haven’t bothered to question them with new evidence. Although people believe that the phrase “I don’t know” symbolizes a character shortcoming, it requires much more self-confidence to take the high road and admit when you don’t know something, especially to others. Once you’ve admitted a lack of knowledge or skills, the next step is correcting that deficiency. The Power of Intellectual Humility One of the best examples of the power of “I don’t know” is the entire scientific enterprise. Science is founded on admitting when we don’t know something and then gathering evidence to figure out the cause of a phenomenon or the effect of a treatment. A great way to be wrong in science is to proclaim that everything is already known. In a famous quote (often misattributed to Lord Kelvin), the Nobel Laureate Albert Michelson stated in 1894: The more important fundamental laws and facts of physical science have all been discovered, and these are so firmly established that the possibility of their ever being supplanted in consequence of new discoveries is exceedingly remote. Within 20 years of this statement, both quantum mechanics and relativity would shake the foundations of physics and lead to such developments as GPS and all modern electronics. As another example, traditional religious beliefs held that the Earth was at the center of the Universe — no question, nothing to debate. Yet, when scientists such as Galileo admitted humanity’s ignorance and started to investigate, they found a much more magnificent and wondrous universe than that offered by any religion. Scientists must be well-practiced in recognizing when they don’t know something and figuring out a way to rectify that shortcoming. The best scientists are able to harness natural human curiosity and use it to direct their research efforts. Perhaps no scientist exemplified this better than Richard Feynman, the charismatic physicist who was driven to answer questions he personally found interesting because he liked “finding things out”. Even for those of us who aren’t scientists, the evidence shows that we would be better off by recognizing our limits and adopting a growth mindset. In a study of students, those with more intellectual humility — the ability to say they didn’t fully understand a topic — had better learning outcomes in part because they were more willing to seek out the extra needed. Intellectual humility and the growth mindset go hand-in-hand: if you believe that you can acquire any new skill, then you’ll be more willing to admit that you’re not quite yet where you want to be which is perfectly fine! Conclusions Saying I don’t know is not an admission of weakness but a power: not only will it mean you’ll be wrong less often, but it will lead you to greater personal improvement in the future. Adopting a practice of intellectual humility and in turn a growth mindset — while not easy — will lead to better long-term performance in school, work, and in your personal life. If saying I don’t know or expressing uncertainty is too difficult, try to rephrase it in terms of intellectual humility. “I don’t know how to program” becomes “learning to program is next on my list.” Or, “I don’t know the exact sales numbers for last month” can be phrased as: “we’re still working to get the numbers correct but here are our upper and lower estimates.” Although it’s a cliche, the best policy in any situation is really full admission of the truth because it prevents you from having to maintain a facade of lies and deal with the fallout when the truth is discovered. “I don’t know” may at first sound defeatist, but in reality, when followed by “but I’m going to find out” it is an extremely optimistic phrase. It implies that you have the ability to change yourself for the better. Even while the rest of the world seems to grow more certain, admitting the limits of your current abilities will put you on a long term path for greater future improvement. Learning is one of the most impressive feats that can be accomplished by humans, and every utterance of “I don’t know” is another chance to learn. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or at my personal website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Figure out what you want to learn Make a plan to learn the skills or knowledge Execute on the plan a little at a time Admit your own ignorance",The Power of I Don’t Know,7,published,2164,2172,0.4604051565377532,1,1,0,0,0,0
30,3700,213.0070225429051,659,https://towardsdatascience.com/my-weaknesses-as-a-data-scientist-1310dab9f566,22,Towards Data Science,2018-10-26 15:32:00,28.35,11,9611,2018-10-15 20:13:00,"['Data Science', 'Education', 'Learning', 'Writing', 'Towards Data Science']","My Weaknesses as a Data Scientist Without recognizing our weak points, we’ll never be able to overcome them If modern job interviews have taught us anything, it’s that the correct answer to the question “What’s your biggest weakness?” is “I work too hard.” Clearly, it’d be ludicrous to actually talk about our weaknesses, right? Why would we want to mention what we can’t yet do? While job applications and LinkedIn profile pages don’t encourage us to disclose our weak points, if we never admit our deficiencies, then we can’t take the steps to address them. The path to getting better in an endeavor is simple: We rarely get past the first step: especially in technical fields, we keep our heads down and continue working, using the skills we already have rather than attaining new ones that would make our jobs easier or open us up to new opportunities. Self-reflection — evaluating ourselves objectively — may seem like a foreign concept, but being able to take a step back and figuring out what we could do better or more efficiently is critical to advancing in any field. With that in mind, I’ve tried to take an objective look at where I am now and identified 3 areas to work on to make me a better data scientist: My purpose in writing this article about my weaknesses in data science is threefold. First, I genuinely care about getting better so I need to admit my weak points. By outlining my deficiencies and how I can address them, my objective is to keep myself motivated to follow through on my learning goals. Second, I hope to encourage others to think about what skills they might not know and how they can work on acquiring them. You don’t have to write your own article disclosing what you don’t know, but taking a few moments to consider the question can pay off if you find a skill to work on. Finally, I want to show you don’t need to know everything to be a successful data scientist. There are an almost unlimited number of data science/ machine learning topics, but a limited amount you can actually know. Despite what unrealistic job applications proclaim, you don’t need complete knowledge of every algorithm (or 5–10 years of experience) to be a practicing data scientist. Often, I hear from beginners who are overwhelmed by the number of topics they think they must learn and my advice is always the same: start with the basics and understand you don’t need to know it all! For each weakness, I’ve outlined the problem and what I’m currently doing to try and get better. Identifying one’s weaknesses is important, but so is forming a plan to address them. Learning a new skill takes time, but planning a series of small, concrete steps considerably increases your chances of success. 1. Software engineering Having received my first real data science experience in an academic environment, I tried to avoid picking up a number of bad habits reflecting an academic way of doing data science. Among these are a tendency to write code that only runs once, a lack of documentation, difficult-to-read code without a consistent style, and hard coding specific values. All of these practices reflect one primary objective: develop a data science solution that works a single time for a specific dataset in order to write a paper. As a prototypical example, our project worked with building energy data that initially came in 15-minute intervals. When we started getting data in 5-minute increments, we discovered our pipelines completely broke down because there were hundreds of places where the interval had been explicitly coded for 15 minutes. We couldn’t do a simple find and replace because this parameter was referred to by multiple names such as electricity_interval, timeBetweenMeasurements, or dataFreq. None of the researchers had given any thought to making the code easy to read or flexible to changing inputs. In contrast, from a software engineering point of view, code must be extensively tested with many different inputs, well-documented, work within an existing framework, and adhere to coding standards so it can be understood by other developers. Despite my best intentions, I still occasionally write code like a data scientist instead of like a software engineer. I’ve started to think what separates the average from the great data scientists is writing code using software engineering best practices — your model won’t be deployed if it’s not robust or doesn’t fit within an architecture— and now I’m trying to train myself to think like a computer scientist. As usual, there’s no better method to learn technical skills than practice. Fortunately, at my current job, I’m able to make contributions both to our internal tooling as well as an open-source library (Featuretools). This has forced me to learn a number of practices including: Even for those data scientists not yet at a company, you can get experience with many of these by working on collaborative open-source projects. Another great way to figure out solid coding practices is to read through source code for popular libraries on GitHub (Scikit-Learn is one of my favorites). Having feedback from others is critical, so find a community and seek out advice from those more experienced than yourself. Thinking like a software engineer requires a change in mindset, but adopting these practices is not difficult if you’re able to slow down and keep them in mind. For example, anytime I find myself copying and pasting code in a Jupyter Notebook and changing a few values, I try to stop and realize I’d be better off using a function which, in the long run, makes me more efficient. While I’m nowhere near perfect on these practices, I’ve found they not only make it easier for others to read my code, they make it easier for me to build on my work. Code is going to be read more than it’s written, and that includes by your future self who will appreciate documentation and a consistent style. When I’m not writing code that is designed to be part of a larger library, I still try to use some of these methods. Writing unit tests for a data analysis may seem strange to a data scientist, but it’s great practice for when you actually need to develop tests to ensure your code works as intended. Also, there are many linting tools that check your code follows a coding style (I still struggle with the no spaces around keyword arguments). There are many other aspects of computer science I’d like to work on such as writing efficient implementations rather than brute force methods (for example using vectorization instead of looping). However, it’s also important to realize you can’t change everything all at once, which is why I’m focusing on a few practices and making them habits built into my workflows. While data science is now its own field, practitioners can benefit by adapting best practices from existing fields such as software engineering. 2. Scaling Data Science Although you can teach yourself everything in data science, there are some limits to what you can put into practice. One is the difficulty in scaling an analysis or a predictive model to large datasets. Most of us don’t have access to a computing cluster and don’t want to put up money for a personal supercomputer. This means that when we learn new methods, we tend to apply them to small, well-behaved datasets. Unfortunately, in the real world, datasets do not adhere to strict size or cleanliness limits and you are going to need different approaches to solve problems. First of all, you probably will need to break out of the safe confines of a personal computer and use a remote instance — such as through AWS EC2 — or even multiple machines. This means learning how to connect to remote machines and mastering the command line — you won’t have access to a mouse and a gui on your EC2 instance. When learning data science, I tried to do work on EC2 machines, either with the free tier or free credits (you can create multiple accounts if you manage all the emails and passwords). This helped get me familiar with the command line, however ,I still didn’t tackle a second issue: datasets that are larger than the memory of the machine. Lately, I’ve realized this is a limitation holding me back, and it’s time to learn how to handle larger datasets. Even without spending thousands of dollars on computing resources, it is possible to practice the methods of working with datasets that don’t fit in memory. Some of these include iterating through a dataset one chunk at a time, breaking one large dataset into many smaller pieces, or with tools like Dask that handle the details of working with large data for you. My current approach, both on internal projects and open-source datasets, is to partition a dataset into subsets, develop a pipeline that can handle one partition, and then use Dask or Spark with PySpark to run the subsets through the pipeline in parallel. This approach doesn’t require a supercomputer or a cluster — you can parallelize operations on a personal machine using multiple cores. Then, when you have access to more resources, you can adapt the same workflow to scale up. Also, thanks to data repositories such as Kaggle, I’ve been able to find some extremely large datasets and read through other data scientist’s approaches to working with them. I’ve picked up a number of useful tips such as reducing memory consumption by changing the data type in a dataframe. These approaches help make me more efficient with datasets of any size. While I haven’t yet had to tackle massive terabyte-scale datasets, these approaches have helped me learn basic strategies of working with large data. For some recent projects, I was able to apply the skills I learned so far to do analysis on a cluster running on AWS. Over the coming months, I hope to gradually increase the size of datasets I’m comfortable analyzing. It’s a pretty safe bet that datasets are not going to decrease in size and I know I’ll need to continue leveling up my skills for handling larger quantities of data. 3. Deep Learning Although artificial intelligence has gone through periods of boom and bust in the past, recent successes in fields such as computer vision, natural language processing, and deep reinforcement learning have convinced me deep learning — using multi-layered neural networks — is not another passing fad. Unlike with software engineering or scaling data science, my current position doesn’t require any deep learning: traditional machine learning techniques (e.g. Random Forest) have been more than capable of solving all our customer’s problems. However, I recognize that not every dataset will be structured in neat rows and columns and neural networks are the best option (at the moment) to take on projects with text or images. I could keep exploiting my current skills on the problems I’ve always solved, but, especially early in my career, exploring topics is an exercise with great potential value. There are many different subfields within deep learning and it’s hard to figure out which methods or libraries will eventually win out. Nonetheless, I think that a familiarity with the field and being confident implementing some of the techniques will allow one to approach a wider range of problems. Given that solving problems is what drove me to data science, adding the tools of deep learning to my toolbox is a worthwhile investment. My plan for studying deep learning is the same as the approach I applied to turning myself into a data scientist: When studying a technical topic, an effective approach is to learn by doing. For me this means starting not with the underlying, fundamental theory, but by finding out how to implement the methods to solve problems. This top-down approach means I place a lot of value on books that have a hands-on style, namely those with many code examples. After I see how the technique works, then I go back to the theory so I can use the methods more effectively. Although I may be on my own because I don’t have the opportunity to learn neural networks from others at work, in data science, you’re never truly on your own because of the abundance of resources and the extensive community. For deep learning I’m relying primarily on three books: The first two emphasis building actual solutions with neural networks while the third covers the theory in depth. When reading about technical topics make it an active experience: whenever possible, get your hands on the keyboard coding along with what you read. Books like the first two that provide code samples are great: often I’ll type an example line-by-line into a Jupyter Notebook to figure out how it works and write detailed notes as I go. Furthermore, I try not just to copy the code examples, but to experiment with them or adapt them to my own project. An application of this is my recent work with building a book recommendation system, a project adapted from a similar code exercise in the Deep Learning Cookbook. It can be intimidating trying to start your own project from scratch, and, when you need a boost, there is nothing wrong with building on what others have done. Finally, one of the most effective ways to learn a topic is by teaching it to others. From experience, I don’t fully comprehend a concept until I try to explain it to someone else in simple terms. With each new topic I cover in deep learning, I’ll keep writing, sharing both the technical implementation details along with a conceptual explanation. Teaching is one of the best ways to learn, and I plan on making it an integral part of my quest to study deep learning. Conclusions It may feel a little strange proclaiming your weaknesses. I know writing this article made me uncomfortable, but I’m putting it out because it will eventually make me a better data scientist. Moreover, I’ve found that many people, employers included, are impressed if you have the self-awareness to admit shortcomings and discuss how you will address them. A lack of skills is not a weakness — the real shortcoming is pretending you know everything and have no need for getting better. By identifying my data science weaknesses — software engineering, scaling analysis/modeling, deep learning — I aim to improve myself, encourage others to think about their weaknesses, and show that you don’t need to learn everything to be a successful data scientist. While reflecting on one’s weak points can be painful, learning is enjoyable: one of the most rewarding experiences is looking back after a sustained period of studying and realizing you know more than you did before you started. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Figure out where you want to be: make a plan to get there Execute on the plan: take one small action at a time Scaling data science Deep learning Following a coding style guide Writing functions that accept changing parameters Documenting code thoroughly Having code reviewed by others Refactoring code to make it simpler and easier to read Practice the techniques and methods on realistic projects Share and explain my projects through writing Deep Learning with Python by Francois Chollet Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville Determine where you are now: identify weaknesses Software engineering Writing unit tests Read books and tutorials that emphasize implementations Deep Learning Cookbook by Douwe Osinga",My Weaknesses as a Data Scientist,6,published,33900,2933,1.261506989430617,10,1,1,1,0,0
37,787,206.79045015163194,125,https://towardsdatascience.com/biases-and-how-to-overcome-them-692c8c35f4a5,10,Towards Data Science,2018-11-01 20:44:00,39.7,6,1336,2018-10-31 21:09:00,"['Psychology', 'Data', 'Towards Data Science', 'Education', 'Thinking']","Overcome Your Biases with Data We’re awful at viewing the world objectively. Data can help. There’s a pervasive myth — perhaps taught to you by an economics course — that humans are rational. The traditional view is we objectively analyze the world, draw accurate conclusions, and make decisions in our best interest. While few people completely buy into this argument anymore, we are still often unaware of our cognitive biases, with the result that we vote, spend money, and form opinions based on a distorted view of the world. A recent personal experience where I badly misjudged reality — due to cognitive illusions— brought home this point and demonstrated the importance of fact-checking our views of the world. While this situation had no negative consequences, it was a great reminder that we are all subject to powerful biases and personal opinions are no substitute for checking the data. Shortly after moving to Boston, I thought I noticed a striking phenomenon: loads of people smoking. After a few days, it seemed to me that every street corner was filled with people lighting up cigarettes. Having come from a small midwestern town where it was exceedingly rare to see anyone smoking, I was dismayed: maybe the big city encouraged negative vices I would eventually pick up, or worse, smoking rates were on the rise nationwide. While a few decades ago I would have had no option but to either persist in this belief or painstakingly look for demographic data in a library, now I was able to find verified data from the Centers for Disease Control and Prevention within seconds. To my surprise, and dealing a large blow to my rational view of myself, I found the following table comparing smoking rates in the metro area nearest my small town (Peoria, IL) to those in Boston: Not only was I wrong, I was significantly wrong as indicated by the non-overlapping 95% confidence intervals. (Although we tend to focus on a single number, considering uncertainty estimates is crucial especially when dealing with real-world demographic data). To show visually how wrong I was, even accounting for the uncertainty, I made the following smoking rate boxplots: Why was I so wrong? I’m a firm believer in analyzing your mistakes so you don’t make them again and, in this process, I came up with three reasons: These are all examples of cognitive biases — mistakes in reasoning and deviations from rational decision making — or heuristics — mental shortcuts (rules of thumb) we use to quickly make judgements. While these served us well in our evolutionary past, they often fail us in today’s world. We are now required to process many streams of information with complex interacting factors and our fast intuitions are not adapted for this purpose. (For the definitive reference on cognitive biases and how to overcome them, read Daniel Kahnemann’s masterwork Thinking, Fast and Slow. A less intimidating format for learning these is the You Are Not So Smart Podcast). One of the simplest ways to correct for our innate shortcomings is to fact-check ourselves. Especially in an age with so much accurate information freely available there is no excuse for persisting in false beliefs. Instead of reasoning from personal experience/anecdotes, look up the actual numbers! Moreover, in addition to just figuring out the right answer, it’s important to think about why we erred. We’re never going to rid ourselves of cognitive biases, but we can learn to recognize when they occur and how to overcome them. For example, I should have noticed the people who weren’t smoking (the denominator), or thought about the total number of people I see in my small town every day compared to the number of people I observed in Boston. Building on that last point, while looking at data by itself is useful, trying to understand what it means in the context of your life can be more helpful. This is where some statistics and basic data manipulation can go a long way. On an average day in my home town, I probably saw about 50 people walking around (okay no one walks in the Midwest but stay with me) compared to Boston with maybe 100 times as many at 5,000. Knowing the smoking rate and the total number of people I expect to see, I simulated 10,000 days to find out how many smokers I would expect to see on a day in Boston versus my hometown. (Jupyter Notebook available on GitHub). Even though my hometown has a statistically greater proportion of smokers, in terms of raw numbers, on an average day, I’d see about 100 times the number of smokers in Boston. These graphs, coupled with my neglect of the denominator, show why I was so susceptible to the availability heuristic. Why This Matters While this small example is innocuous, the general impact of our biases is pervasive and often detrimental. For instance, because of a predominantly negative news cycle (manifesting in the availability heuristic), people generally think the world is getting worse. If fact, by almost all objective measures, we are living at the best time in human history and things are improving. (All graphs from Enlightenment Now by Steven Pinker). This has real-world implications: people vote for leaders who promise a return to better times because they haven’t looked at the data and realized the best time is now! Moving away from politics, think about your personal life: is there a relationship you’ve spent too long in, a negative job you’ve stuck with, or even a book you continue reading despite not enjoying? Then you’ve fallen victim to the Sunk-Cost Fallacy, where we continue squandering time on an endeavor because of the effort we’ve already put in. As another example, if you find yourself worrying about air travel, instead of reading about the miniscule number of plane crashes, look at the data showing flying is the safest way to travel. I’ve tried to adopt two simple rules for myself to mitigate cognitive biases: Following these guidelines won’t make me perfect, but they are helping me gradually become less wrong. I don’t believe there’s always one objective truth, but I do think facts are much better than our subjective judgements. To end this article on a happy note, here’s a graph showing the decline in smoking rates in the United States (despite my misbelief)! As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my website at willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Confirmation bias: once we have a belief, we unconsciously seek out evidence that confirms it and ignore evidence that contradicts it. Denominator neglect: we look at only the numerator — the number of smokers — and ignore the denominator — the total people we see over the course of a day. Seek out disconfirming evidence: when everything you read confirms your beliefs, it’s probably time to read something else. Availability heuristic: we judge how likely something is by the number of occurrences of it we can bring to memory. Look up relevant data: try to find multiple reputable sources, consider uncertainty estimates, and explore the data yourself when available",Overcome Your Biases with Data,5,published,3365,1346,0.5846953937592868,0,1,0,1,0,0
44,3300,203.7582330254398,612,https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470,10,Towards Data Science,2018-11-04 21:30:00,16.9,15,6148,2018-10-14 08:24:00,"['Machine Learning', 'Deep Learning', 'Education', 'Towards Data Science', 'Python']","Recurrent Neural Networks by Example in Python Using a Recurrent Neural Network to Write Patent Abstracts The first time I attempted to study recurrent neural networks, I made the mistake of trying to learn the theory behind things like LSTMs and GRUs first. After several frustrating days looking at linear algebra equations, I happened on the following passage in Deep Learning with Python: In summary, you don’t need to understand everything about the specific architecture of an LSTM cell; as a human, it shouldn’t be your job to understand it. Just keep in mind what the LSTM cell is meant to do: allow past information to be reinjected at a later time. This was the author of the library Keras (Francois Chollet), an expert in deep learning, telling me I didn’t need to understand everything at the foundational level! I realized that my mistake had been starting at the bottom, with the theory, instead of just trying to build a recurrent neural network. Shortly thereafter, I switched tactics and decided to try the most effective way of learning a data science technique: find a problem and solve it! This top-down approach means learning how to implement a method before going back and covering the theory. This way, I’m able to figure out what I need to know along the way, and when I return to study the concepts, I have a framework into which I can fit each idea. In this mindset, I decided to stop worrying about the details and complete a recurrent neural network project. This article walks through how to build and use a recurrent neural network in Keras to write patent abstracts. The article is light on the theory, but as you work through the project, you’ll find you pick up what you need to know along the way. The end result is you can build a useful application and figure out how a deep learning method for natural language processing works. The full code is available as a series of Jupyter Notebooks on GitHub. I’ve also provided all the pre-trained models so you don’t have to train them for several hours yourself! To get started as quickly as possible and investigate the models, see the Quick Start to Recurrent Neural Networks, and for in-depth explanations, refer to Deep Dive into Recurrent Neural Networks. Recurrent Neural Network It’s helpful to understand at least some of the basics before getting to the implementation. At a high level, a recurrent neural network (RNN) processes sequences — whether daily stock prices, sentences, or sensor measurements — one element at a time while retaining a memory (called a state) of what has come previously in the sequence. Recurrent means the output at the current time step becomes the input to the next time step. At each element of the sequence, the model considers not just the current input, but what it remembers about the preceding elements. This memory allows the network to learn long-term dependencies in a sequence which means it can take the entire context into account when making a prediction, whether that be the next word in a sentence, a sentiment classification, or the next temperature measurement. A RNN is designed to mimic the human way of processing sequences: we consider the entire sentence when forming a response instead of words by themselves. For example, consider the following sentence: “The concert was boring for the first 15 minutes while the band warmed up but then was terribly exciting.” A machine learning model that considers the words in isolation — such as a bag of words model — would probably conclude this sentence is negative. An RNN by contrast should be able to see the words “but” and “terribly exciting” and realize that the sentence turns from negative to positive because it has looked at the entire sequence. Reading a whole sequence gives us a context for processing its meaning, a concept encoded in recurrent neural networks. At the heart of an RNN is a layer made of memory cells. The most popular cell at the moment is the Long Short-Term Memory (LSTM) which maintains a cell state as well as a carry for ensuring that the signal (information in the form of a gradient) is not lost as the sequence is processed. At each time step the LSTM considers the current word, the carry, and the cell state. The LSTM has 3 different gates and weight vectors: there is a “forget” gate for discarding irrelevant information; an “input” gate for handling the current input, and an “output” gate for producing predictions at each time step. However, as Chollet points out, it is fruitless trying to assign specific meanings to each of the elements in the cell. The function of each cell element is ultimately decided by the parameters (weights) which are learned during training. Feel free to label each cell part, but it’s not necessary for effective use! Recall, the benefit of a Recurrent Neural Network for sequence learning is it maintains a memory of the entire sequence preventing prior information from being lost. There are several ways we can formulate the task of training an RNN to write text, in this case patent abstracts. However, we will choose to train it as a many-to-one sequence mapper. That is, we input a sequence of words and train the model to predict the very next word. The words will be mapped to integers and then to vectors using an embedding matrix (either pre-trained or trainable) before being passed into an LSTM layer. When we go to write a new patent, we pass in a starting sequence of words, make a prediction for the next word, update the input sequence, make another prediction, add the word to the sequence and continue for however many words we want to generate. The steps of the approach are outlined below: Keep in mind this is only one formulation of the problem: we could also use a character level model or make predictions for each word in the sequence. As with many concepts in machine learning, there is no one correct answer, but this approach works well in practice. Data Preparation Even with a neural network’s powerful representation ability, getting a quality, clean dataset is paramount. The raw data for this project comes from USPTO PatentsView, where you can search for information on any patent applied for in the United States. I searched for the term “neural network” and downloaded the resulting patent abstracts — 3500 in all. I found it best to train on a narrow subject, but feel free to try with a different set of patents. We’ll start out with the patent abstracts as a list of strings. The main data preparation steps for our model are: These two steps can both be done using the Keras Tokenizer class. By default, this removes all punctuation, lowercases words, and then converts words to sequences of integers. A Tokenizer is first fit on a list of strings and then converts this list into a list of lists of integers. This is demonstrated below: The output of the first cell shows the original abstract and the output of the second the tokenized sequence. Each abstract is now represented as integers. We can use the idx_word attribute of the trained tokenizer to figure out what each of these integers means: If you look closely, you’ll notice that the Tokenizer has removed all punctuation and lowercased all the words. If we use these settings, then the neural network will not learn proper English! We can adjust this by changing the filters to the Tokenizer to not remove punctuation. See the notebooks for different implementations, but, when we use pre-trained embeddings, we’ll have to remove the uppercase because there are no lowercase letters in the embeddings. When training our own embeddings, we don’t have to worry about this because the model will learn different representations for lower and upper case. Features and Labels The previous step converts all the abstracts to sequences of integers. The next step is to create a supervised machine learning problem with which to train the network. There are numerous ways you can set up a recurrent neural network task for text generation, but we’ll use the following: Give the network a sequence of words and train it to predict the next word. The number of words is left as a parameter; we’ll use 50 for the examples shown here which means we give our network 50 words and train it to predict the 51st. Other ways of training the network would be to have it predict the next word at each point in the sequence — make a prediction for each input word rather than once for the entire sequence — or train the model using individual characters. The implementation used here is not necessarily optimal — there is no accepted best solution — but it works well! Creating the features and labels is relatively simple and for each abstract (represented as integers) we create multiple sets of features and labels. We use the first 50 words as features with the 51st as the label, then use words 2–51 as features and predict the 52nd and so on. This gives us significantly more training data which is beneficial because the performance of the network is proportional to the amount of data that it sees during training. The implementation of creating features and labels is below: The features end up with shape (296866, 50) which means we have almost 300,000 sequences each with 50 tokens. In the language of recurrent neural networks, each sequence has 50 timesteps each with 1 feature. We could leave the labels as integers, but a neural network is able to train most effectively when the labels are one-hot encoded. We can one-hot encode the labels with numpy very quickly using the following: To find the word corresponding to a row in label_array , we use: After getting all of our features and labels properly formatted, we want to split them into a training and validation set (see notebook for details). One important point here is to shuffle the features and labels simultaneously so the same abstracts do not all end up in one set. Building a Recurrent Neural Network Keras is an incredible library: it allows us to build state-of-the-art models in a few lines of understandable Python code. Although other neural network libraries may be faster or allow more flexibility, nothing can beat Keras for development time and ease-of-use. The code for a simple LSTM is below with an explanation following: We are using the Keras Sequential API which means we build the network up one layer at a time. The layers are as follows: The model is compiled with the Adam optimizer (a variant on Stochastic Gradient Descent) and trained using the categorical_crossentropy loss. During training, the network will try to minimize the log loss by adjusting the trainable parameters (weights). As always, the gradients of the parameters are calculated using back-propagation and updated with the optimizer. Since we are using Keras, we don’t have to worry about how this happens behind the scenes, only about setting up the network correctly. Without updating the embeddings, there are many fewer parameters to train in the network. The input to the LSTM layer is (None, 50, 100) which means that for each batch (the first dimension), each sequence has 50 timesteps (words), each of which has 100 features after embedding. Input to an LSTM layer always has the (batch_size, timesteps, features) shape. There are many ways to structure this network and there are several others covered in the notebook. For example, we can use two LSTM layers stacked on each other, a Bidirectional LSTM layer that processes sequences from both directions, or more Dense layers. I found the set-up above to work well. Once the network is built, we still have to supply it with the pre-trained word embeddings. There are numerous embeddings you can find online trained on different corpuses (large bodies of text). The ones we’ll use are available from Stanford and come in 100, 200, or 300 dimensions (we’ll stick to 100). These embeddings are from the GloVe (Global Vectors for Word Representation) algorithm and were trained on Wikipedia. Even though the pre-trained embeddings contain 400,000 words, there are some words in our vocab that are included. When we represent these words with embeddings, they will have 100-d vectors of all zeros. This problem can be overcome by training our own embeddings or by setting the Embedding layer's trainable parameter to True (and removing the Masking layer). We can quickly load in the pre-trained embeddings from disk and make an embedding matrix with the following code: What this does is assign a 100-dimensional vector to each word in the vocab. If the word has no pre-trained embedding then this vector will be all zeros. To explore the embeddings, we can use the cosine similarity to find the words closest to a given query word in the embedding space: Embeddings are learned which means the representations apply specifically to one task. When using pre-trained embeddings, we hope the task the embeddings were learned on is close enough to our task so the embeddings are meaningful. If these embeddings were trained on tweets, we might not expect them to work well, but since they were trained on Wikipedia data, they should be generally applicable to a range of language processing tasks. If you have a lot of data and the computer time, it’s usually better to learn your own embeddings for a specific task. In the notebook I take both approaches and the learned embeddings perform slightly better. Training the Model With the training and validation data prepared, the network built, and the embeddings loaded, we are almost ready for our model to learn how to write patent abstracts. However, good steps to take when training neural networks are to use ModelCheckpoint and EarlyStopping in the form of Keras callbacks: Using Early Stopping means we won’t overfit to the training data and waste time training for extra epochs that don’t improve performance. The Model Checkpoint means we can access the best model and, if our training is disrupted 1000 epochs in, we won’t have lost all the progress! The model can then be trained with the following code: On an Amazon p2.xlarge instance ($0.90 / hour reserved), this took just over 1 hour to finish. Once the training is done, we can load back in the best saved model and evaluate a final time on the validation data. Overall, the model using pre-trained word embeddings achieved a validation accuracy of 23.9%. This is pretty good considering as a human I find it extremely difficult to predict the next word in these abstracts! A naive guess of the most common word (“the”) yields an accuracy around 8%. The metrics for all the models in the notebook are shown below: The best model used pre-trained embeddings and the same architecture as shown above. I’d encourage anyone to try training with a different model! Patent Abstract Generation Of course, while high metrics are nice, what matters is if the network can produce reasonable patent abstracts. Using the best model we can explore the model generation ability. If you want to run this on your own hardware, you can find the notebook here and the pre-trained models are on GitHub. To produce output, we seed the network with a random sequence chosen from the patent abstracts, have it make a prediction of the next word, add the prediction to the sequence, and continue making predictions for however many words we want. Some results are shown below: One important parameter for the output is the diversity of the predictions. Instead of using the predicted word with the highest probability, we inject diversity into the predictions and then choose the next word with a probability proportional to the more diverse predictions. Too high a diversity and the generated output starts to seem random, but too low and the network can get into recursive loops of output. The output isn’t too bad! Some of the time it’s tough to determine which is computer generated and which is from a machine. Part of this is due to the nature of patent abstracts which, most of the time, don’t sound like they were written by a human. Another use of the network is to seed it with our own starting sequence. We can use any text we want and see where the network takes it: Again, the results are not entirely believable but they do resemble English. As a final test of the recurrent neural network, I created a game to guess whether the model or a human generated the output. Here’s the first example where two of the options are from a computer and one is from a human: What’s your guess? The answer is that the second is the actual abstract written by a person (well, it’s what was actually in the abstract. I’m not sure these abstracts are written by people). Here’s another one: This time the third had a flesh and blood writer. There are additional steps we can use to interpret the model such as finding which neurons light up with different input sequences. We can also look at the learned embeddings (or visualize them with the Projector tool). We’ll leave those topics for another time, and conclude that we know now how to implement a recurrent neural network to effectively mimic human text. Conclusions It’s important to recognize that the recurrent neural network has no concept of language understanding. It is effectively a very sophisticated pattern recognition machine. Nonetheless, unlike methods such as Markov chains or frequency analysis, the rnn makes predictions based on the ordering of elements in the sequence. Getting a little philosophical here, you could argue that humans are simply extreme pattern recognition machines and therefore the recurrent neural network is only acting like a human machine. The uses of recurrent neural networks go far beyond text generation to machine translation, image captioning, and authorship identification. Although this application we covered here will not displace any humans, it’s conceivable that with more training data and a larger model, a neural network would be able to synthesize new, reasonable patent abstracts. It can be easy to get stuck in the details or the theory behind a complex technique, but a more effective method for learning data science tools is to dive in and build applications. You can always go back later and catch up on the theory once you know what a technique is capable of and how it works in practice. Most of us won’t be designing neural networks, but it’s worth learning how to use them effectively. This means putting away the books, breaking out the keyboard, and coding up your very own network. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my website at willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Create feature and labels from sequences Build LSTM model with Embedding, LSTM, and Dense layers Load in pre-trained embeddings Train model to predict next work in sequence Make predictions by passing in starting sequence Convert the individual words into integers A Masking layer to mask any words that do not have a pre-trained embedding which will be represented as all zeros. This layer should not be used when training the embeddings. The heart of the network: a layer of LSTM cells with dropout to prevent overfitting. Since we are only using one LSTM layer, it does not return the sequences, for using two or more layers, make sure to return sequences. A fully-connected Dense layer with relu activation. This adds additional representational capacity to the network. A Dropout layer to prevent overfitting to the training data. A Dense fully-connected output layer. This produces a probability for every word in the vocab using softmax activation. Early Stopping: halts training when validation loss is no longer decreasing Convert abstracts from list of strings into list of lists of integers (sequences) Remove punctuation and split strings into lists of individual words An Embedding which maps each input word to a 100-dimensional vector. The embedding can use pre-trained weights (more in a second) which we supply in the weights parameter. trainable can be set False if we don’t want to update the embeddings. Model Checkpoint: saves the best model (as measured by validation loss) on disk for using best model",Recurrent Neural Networks by Example in Python,7,published,36372,3797,0.8691071898867527,21,1,0,1,1,1
41,874,201.22760224594907,116,https://towardsdatascience.com/prediction-engineering-how-to-set-up-your-machine-learning-problem-b3b8f622683b,0,Towards Data Science,2018-11-07 10:14:00,24.35,9,1324,2018-11-05 15:41:00,"['Machine Learning', 'Data Science', 'Engineering', 'Education', 'Predictive Analytics']","Prediction Engineering: How to Set Up Your Machine Learning Problem An explanation and implementation of the first step in solving problems with machine learning This is the second in a four-part series on how we approach machine learning at Feature Labs. The set of articles is: These articles will cover the concepts and a full implementation as applied to predicting customer churn. The project Jupyter Notebooks are all available on GitHub. (Full disclosure: I work for Feature Labs, a startup developing tooling, including Featuretools, for solving problems with machine learning. All of the work documented here was completed with open-source tools and data.) When working with real-world data on a machine learning task, we define the problem, which means we have to develop our own labels — historical examples of what we want to predict — to train a supervised model. The idea of making our own labels may initially seem foreign to data scientists (myself included) who got started on Kaggle competitions or textbook datasets where the answers are already included. The concept behind prediction engineering — making labels to train a supervised machine learning model — is not new. However, it currently is not a standardized process and is done by data scientists on an as-needed basis. This means that for each new problem — even with the same dataset — a new script must be developed to accomplish this task resulting in solutions that cannot be adapted to different prediction problems. A better solution is to write functions that are flexible to changing business parameters, allowing us to quickly generate labels for many problems. This is one area where data science can learn from software engineering: solutions should be reusable and accept changing inputs. In this article, we’ll see how to implement a reusable approach to the first step in solving problems with machine learning — prediction engineering. Prediction engineering requires guidance both from the business viewpoint to figure out the right problem to solve as well as from the data scientist to determine how to translate the business need into a machine learning problem. The inputs to prediction engineering are the parameters which define the prediction problem for the business requirement , and the historical dataset for finding examples of what we want to predict. The output of prediction engineering is a label times table: a set of labels with negative and positive examples made from past data along with an associated cutoff time indicating when we have to stop using data to make features for that label (more on this shortly). For our use case we’ll work through in this series — customer churn — we defined the business problem as increasing monthly active subscribers by reducing rates of churn. The machine learning problem is building a model to predict which customers will churn using historical data. The first step in this task is making a set of labels of past examples of customer churn. The parameters for what constitutes a churn and how often we want to make predictions will vary depending on the business need, but in this example, let’s say we want to make predictions on the first of each month for which customers will churn one month out from the time of prediction. Churn will be defined as going more than 31 days without an active membership. It’s important to remember this is only one definition of churn corresponding to one business problem. When we write functions to make labels, they should take in parameters so they can be quickly changed to different prediction problems. Our goal for prediction engineering is a label times table as follows: The labels correspond to whether a customer churned or not based on historical data. Each customer is used as a training example multiple times because they have multiple months of data. Even if we didn’t use customers many times, because this is a time-dependent problem, we have to correctly implement the concept of cutoff times. The labels are not complete without the cutoff time which represents when we have to stop using data to make features for a label. Since we are making predictions about customer churn on the first of each month, we can’t use any data after the first to make features for that label. Our cutoff times are therefore all on the first of the month as shown in the label times table above. All the features for each label must use data from before this time to prevent the problem of data leakage. Cutoff times are a crucial part of building successful solutions to time-series problems that many companies do not account for. Using invalid data to make features leads to models that do well in development but fail in deployment. Imagine we did not limit our features to data that occurred before the first of the month for each label. Our model would figure out that customers who had a paid transaction during the month could not have churned in that month and would thus record high metrics. However, when it came time to deploy the model and make predictions for a future month, we do not have access to the future transactions and our model would perform poorly. It’s like a student who does great on homework because she has the answer key but then is lost on the test without the same information. Now that we have the concepts, let’s work go through the details. KKBOX is Asia’s leading music streaming service offering both a free and a pay-per-month subscription option to over 10 million members. KKBOX has made available a dataset for predicting customer churn. There are 3 data tables coming in at just over 30 GB that are represented by the schema below: The three tables consist of: This is a typical dataset for a subscription business and is an example of structured, relational data: observations in the rows, features in the columns, and tables tied together by primary and foreign keys: the customer id. Finding Historical Labels The key to making prediction engineering adaptable to different problems is to follow a repeatable process for extracting training labels from a dataset. At a high-level this is outlined as follows: For customer churn, the parameters are the The following diagram shows each of these concepts while filling in the details with the problem definition we’ll work through. In this case, the customer has churned during the month of January as they went without a subscription for more than 30 days. Because our lead time is one month and the prediction window is also one month, the label of churn is associated with the cutoff time of December 1. For this problem, we are thus teaching our model to predict customer churn one month in advance to give the customer satisfaction team sufficient time to engage with customers. For a given dataset, there are numerous prediction problems we can make from it. We might want to predict churn at different dates or frequencies, such as every two weeks, with a lead time of two months, or define churn as a shorter duration without an active membership. Moreover, there are other problems unrelated to churn we could solve with this dataset: predict how many songs a customer will listen to in the next month; predict the rate of growth of the customer base; or, segment customers into different groups based on listening habits to tailor a more personal experience. When we develop the functions for creating labels, we make our inputs parameters so we can quickly make multiple sets of labels from the dataset. If we develop a pipeline that has parameters instead of hard-coded values, we can rapidly adopt it to different problems. When we want to change the definition of a churn, all we need to do is alter the parameter input to our pipeline and re-run it. To make labels, we develop 2 functions (full code in the notebook): The label_customer function takes in a customer’s transactions and the specified parameters and returns a label times table. This table has a set of prediction times — the cutoff times — and the label during the prediction window for each cutoff time corresponding to a single customer. As an example, our labels for a customer look like the following: The make_labels function then takes in the transactions for all customers along with the parameters and returns a table with the cutoff times and the label for every customer. When implemented correctly, the end outcome of prediction engineering is a function which can create label times for multiple prediction problems by changing input parameters. These labels times — a cutoff time and associated label — are the input to the next stage in which we make features for each label. The next article in the series describes how feature engineering works . Conclusion We haven’t invented the process of prediction engineering, just given it a name and defined a reusable approach for this first part in the pipeline. The process of prediction engineering is captured in three steps: Getting prediction engineering right is crucial and requires input from both the business and data science sides of a business. By writing the code for prediction engineering to accept different parameters, we can rapidly change the prediction problem if the needs of our company change. More generally, our approach to solving problems with machine learning segments the different parts of the pipeline while standardizing each input and output. The end result, as we’ll see, is we can quickly change the prediction problem in the prediction engineering stage without needing to rewrite the subsequent steps. In this article, we’ve developed the first step in a framework that can be used to solve many problems with machine learning. If building meaningful, high-performance predictive models is something you care about, then get in touch with us at Feature Labs. While this project was completed with the open-source Featuretools, the commercial product offers additional tools and support for creating machine learning solutions. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Prediction Engineering (this article) Feature Engineering: What Powers Machine Learning Modeling: Teaching an Algorithm to Make Predictions Search through past data for positive and negative examples Make a table of cutoff times and associate each cutoff time with a label number of days without a subscription before a user is considered a churn lead time: the number of days or months in the future we want to predict prediction window: the period of time we want to make predictions for Translate the business need into a supervised machine learning problem Create label times from historical data Overview: A General-Purpose Framework for Machine Learning customers: Background information such as age and city ( msno is the customer id): Define positive and negative labels in terms of key business parameters prediction date (cutoff time): the point at which we make a prediction and when we stop using data to make features for the label Identify a business need that can be solved with available data",Prediction Engineering: How to Set Up Your Machine Learning Problem,10,published,5438,1979,0.44163719050025263,1,1,1,0,1,0
33,796,201.22589955687502,115,https://towardsdatascience.com/how-to-create-value-with-machine-learning-eb09585b332e,0,Towards Data Science,2018-11-07 10:17:00,38.71,6,2562,2018-11-05 15:17:00,"['Machine Learning', 'Data Science', 'Predictive Analytics', 'Education', 'Data Analytics']","How to Create Value with Machine Learning A General-Purpose Framework for Defining and Solving Meaningful Problems in 3 Steps Imagine the following scenario: your boss asks you to build a machine learning model to predict every month which customers of your subscription service will churn during the month with churn defined as no active membership for more than 31 days. You painstakingly make labels by finding historical examples of churn, brainstorm and engineer features by hand, then train and manually tune a machine learning model to make predictions. Pleased with the metrics on the holdout testing set, you return to your boss with the results, only to be told now you must develop a different solution: one that makes predictions every two weeks with churn defined as 14 days of inactivity. Dismayed, you realize none of your previous work can be reused because it was designed for a single prediction problem. You wrote a labeling function for a narrow definition of churn and the downstream steps in the pipeline — feature engineering and modeling — were also dependent on the initial parameters and will have to be redone. Due to hard-coding a specific set of values, you’ll have to build an entirely new pipeline to address for what is only a small change in problem definition. This situation is indicative of how solving problems with machine learning is currently approached. The process is ad-hoc and requires a custom solution for each parameter set even when using the same data. The result is companies miss out on the full benefits of machine learning because they are limited to solving a small number of problems with a time-intensive approach. A lack of standardized methodology means there is no scaffolding for solving problems with machine learning that can be quickly adapted and deployed as parameters to a problem change. How can we improve this process? Making machine learning more accessible will require a general-purpose framework for setting up and solving problems. This framework should accommodate existing tools, be rapidly adaptable to changing parameters, applicable across different industries, and provide enough structure to give data scientists a clear path for laying out and working through meaningful problems with machine learning. At Feature Labs, we’ve put a lot of thought into this issue and developed what we think is a better way to solve useful problems with machine learning. In the next three parts of this series, I’ll lay out how we approach framing and building machine learning solutions in a structured, repeatable manner built around the steps of prediction engineering, feature engineering, and modeling. We’ll walk through the approach as applied in full to one use case — predicting customer churn — and see how we can adapt the solution if the parameters of the problem change. Moreover, we’ll be able to utilize existing tools — Pandas, Scikit-Learn, Featuretools — commonly used for machine learning. The general machine learning framework is outlined below: We’ll walk through the basics of each step as well as how to implement them in code. The complete project is available as Jupyter Notebooks on GitHub. (Full disclosure: I work for Feature Labs, a startup developing tooling, including Featuretools, for solving problems with machine learning. All of the work documented here was completed with open-source tools and data.) The complete set of articles is: Although this project discusses only one application, the same process can be applied across industries to build useful machine learning solutions. The end deliverable is a framework you can use to solve problems with machine learning in any field, and a specific solution that could be directly applied to your own customer churn dataset. Business Motivation: Make Sure You Solve the Right Problem The most sophisticated machine learning pipeline will have no impact unless it creates value for a company. Therefore, the first step in framing a machine learning task is understanding the business requirement so you can determine the right problem to solve. Throughout this series, we’ll work through the common problem of addressing customer churn. For subscription-based business models, predicting which customers will churn — stop paying for a service for a specified period of time — is crucial. Accurately predicting if and when customers will churn lets businesses engage with those who are at risk for unsubscribing or offer them reduced rates as an incentive to maintain a subscription. An effective churn prediction model allows a company to be proactive in growing the customer base. For the customer churn problem the business need is: increase the number of paying subscribers by reducing customer churn rates. Traditional methods of reducing customer churn require forecasting which customers would churn with survival-analysis techniques, but, given the abundance of historical customer behavior data, this presents an ideal application of supervised machine learning. We can address the business problem with machine learning by building a supervised algorithm that learns from past data to predict customer churn. Stating the business goal and expressing it in terms of a machine learning-solvable task is the critical first step in the pipeline. Once we know what we want to have the model predict, we can move on to using the available data to develop and solve a supervised machine learning problem. Next Steps Over the next three articles, we’ll apply the prediction engineering, feature engineering, and modeling framework to solve the customer churn problem on a dataset from KKBOX, Asia’s largest subscription music streaming service. Look for the following posts (or check out the GitHub repository): We’ll see how to fill in the details with existing data science tools and how to change the prediction problem without rewriting the complete pipeline. By the end, we’ll have an effective model for predicting churn that is tuned to satisfy the business requirement. Through these articles, we’ll see an approach to machine learning that lets us rapidly build solutions for multiple prediction problems. The next time your boss changes the problem parameters, you’ll be able to have a new solution up and running with only a few lines of changes to the code. If building meaningful, high-performance predictive models is something you care about, then get in touch with us at Feature Labs. While this project was completed with the open-source Featuretools, the commercial product offers additional tools and support for creating machine learning solutions. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Feature Engineering: Extract predictor variables — features — from the raw data for each of the labels Modeling: Train a machine learning model on the features, tune for the business need, and validate predictions before deploying to new data Prediction Engineering: How to Set Up Your Machine Learning Problem Feature Engineering: What Powers Machine Learning Modeling: Teaching an Algorithm to Make Predictions Feature Engineering: What Powers Machine Learning Modeling: Training an Algorithm to Make Predictions (coming soon) Prediction Engineering: State the business need, translate into a machine learning problem, and generate labeled examples from a dataset Overview: A General-Purpose Machine Learning Framework (this article) Prediction Engineering: How to Set Up Your Machine Learning Problem",How to Create Value with Machine Learning,7,published,6618,1286,0.6189735614307932,1,1,1,0,1,0
42,795,196.27914314372686,103,https://towardsdatascience.com/feature-engineering-what-powers-machine-learning-93ab191bcc2d,1,Towards Data Science,2018-11-12 09:00:00,21.76,8,1267,2018-11-07 10:23:00,"['Machine Learning', 'Data Science', 'Education', 'Business', 'Predictive Analytics']","Feature Engineering: What Powers Machine Learning How to Extract Features from Raw Data for Machine Learning This is the third in a four-part series on how we approach machine learning at Feature Labs. The complete set of articles is: These articles cover the concepts and a full implementation as applied to predicting customer churn. The project Jupyter Notebooks are all available on GitHub. (Full disclosure: I work for Feature Labs, a startup developing tooling, including Featuretools, for solving problems with machine learning. All of the work documented here was completed with open-source tools and data.) Feature Engineering It’s often said that “data is the fuel of machine learning.” This isn’t quite true: data is like the crude oil of machine learning which means it has to be refined into features — predictor variables — to be useful for training a model. Without relevant features, you can’t train an accurate model, no matter how complex the machine learning algorithm. The process of extracting features from a raw dataset is called feature engineering. Feature engineering, the second step in the machine learning pipeline, takes in the label times from the first step — prediction engineering — and a raw dataset that needs to be refined. Feature engineering means building features for each label while filtering the data used for the feature based on the label’s cutoff time to make valid features. These features and labels are then passed to modeling where they will be used for training a machine learning algorithm. While feature engineering requires label times, in our general-purpose framework, it is not hard-coded for specific labels corresponding to only one prediction problem. If we wrote our feature engineering code for a single problem — as feature engineering is traditionally approached — then we would have to redo this laborious step every time the parameters change. Instead, we use APIs like Featuretools that can build features for any set of labels without requiring changes to the code. This means for the customer churn dataset, we can solve multiple prediction problems — predicting churn every month, every other week, or with a lead time of two rather than one month — using the exact same feature engineering code. This fits with the principles of our machine learning approach: we segment each step of the pipeline while standardizing inputs and outputs. This independence means we can change the problem in prediction engineering without needing to alter the downstream feature engineering and machine learning code. The key to making this step of the machine learning process repeatable across prediction problems is automated feature engineering. Automated Feature Engineering: Build Better Predictive Models Faster Traditionally, feature engineering is done by hand, building features one at a time using domain knowledge. However, this manual process is error-prone, tedious, must be started from scratch for each dataset, and ultimately is limited by constraints on human creativity and time. Furthermore, in time-dependent problems where we have to filter every feature based on a cutoff time, it’s hard to avoid errors that can invalidate an entire machine learning solution. Automated feature engineering overcomes these problems through a reusable approach to automatically building hundreds of relevant features from a relational dataset. Moreover, this method filters the features for each label based on the cutoff time, creating a rich set of valid features. In short, automated feature engineering enables data scientists to build better predictive models in a fraction of the time. After solving a few problems with machine learning, it becomes clear that many of the operations used to build features are repeated across datasets. For instance, we often find the weekday of an event — be it a transaction or a flight— and then find the average transaction amount or flight delay by day of the week for each customer or airline. Once we realize that these operations don’t depend on the underlying data, why not abstract this process into a framework that can build features for any relational dataset? This is the idea behind automated feature engineering. We can apply the same basic building blocks — called feature primitives — to different relational datasets to build predictor variables. As a concrete example, the “max” feature primitive applied to customer transactions can also be applied to flight delays. In the former case, this will find the largest transaction for each customer, and in the latter, the longest flight delay for a given flight number. This is an embodiment of the idea of abstraction: remove the need to deal with the details — writing specific code for each dataset — by building higher level tools that take advantage of operations common to many problems. Ultimately, automated feature engineering makes us more efficient as data scientists by removing the need to repeat tedious operations across problems. Implementation of Feature Engineering Currently, the only open-source Python library for automated feature engineering using multiple tables is Featuretools, developed and maintained by Feature Labs. For the customer churn problem, we can use Featuretools to quickly build features for the label times that we created in prediction engineering. (Full code available in this Jupyter Notebook). We have three tables of data: customer background info, transactions, and user listening logs. If we were using manual feature engineering, we’d brainstorm and build features by hand, such as the average value of a customer’s transactions, or her total spending on weekends in the previous year. For each feature, we’d first have to filter the data to before the cutoff time for the label. In contrast, in our framework, we make use of Featuretools to automatically build hundreds of relevant features in a few lines of code. We won’t go through the details of Featuretools, but the heart of the library is an algorithm called Deep Feature Synthesis which stacks the feature engineering building blocks known as primitives (simple operations like “max” or finding the “weekday” of a transaction) to build “deep features”. The library also automatically filters data for features based on the cutoff time. For more on automated feature engineering in Featuretools see: Featuretools requires some background code to link together the tables through relationships, but then we can automatically make features for customer churn using the following code (see notebook for complete details): This one line of code gives us over 200 features for each label in cutoff_times. Each feature is a combination of feature primitives and is built with only data from before the associated cutoff time. The features built by Featuretools are explainable in natural language because they are built up from basic operations. For example, we see the feature AVG_TIME_BETWEEN(transactions.transaction_date). This represents the average time between transactions for each customer. When we plot this colored by the label we see that customers who churned appear to have a slightly longer average time between transactions. In addition to getting hundreds of valid, relevant features, developing an automated feature engineering pipeline in Featuretools means we can use the same code for different prediction problems with our dataset. We just need to pass in the correct label times to the cutoff_times parameter and we’ll be able to build features for a different prediction problem. Automated feature engineering means we can solve multiple problems in the time it would normally take to complete just one. A change in parameters means tweaking a few lines of code instead of implementing an entirely new solution. To solve a different problem, rather than rewrite the entire pipeline, we: (As a brief note: the feature engineering code can be run in parallel using either Dask or Spark with PySpark. For the latter approach, see this notebook or this article on the Feature Labs engineering blog.) Just as the label times from prediction engineering flowed into feature engineering, the features serve as inputs to the next stage, modeling: training an algorithm to predict the label from the features. In the final article in this series, we’ll look at how to train, tune, validate, and predict with a machine learning model to solve the customer churn problem. As a preview, pictured is the tuned precision-recall curve from machine learning. (Full notebook available on GitHub.) Conclusions Feature engineering has tended to be a tedious aspect of solving problems with machine learning and a source of errors preventing solutions from being successfully implemented. By using automated feature engineering in a general-purpose machine learning framework we: Furthermore, the feature engineering code is not hard-coded for the inputs from prediction engineering which means we can use the same exact code to make features for multiple prediction problems. Applying automated feature engineering in a structured framework we are able to turn feature engineering from a painful process into a quick, reusable procedure allowing us to solve many valuable machine learning problems. If building meaningful, high-performance predictive models is something you care about, then get in touch with us at Feature Labs. While this project was completed with the open-source Featuretools, the commercial product offers additional tools and support for creating machine learning solutions. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Prediction Engineering: How to Set Up Your Machine Learning Problem Feature Engineering (this article) Modeling: Teaching an Algorithm to Make Predictions Input the label times to feature engineering and output features Use the features to train and a supervised machine learning model Create only valid features by filtering data on cutoff times Overview: A General-Purpose Framework for Machine Learning Automated Feature Engineering in Python Tweak the prediction engineering code to create new label times Automatically build hundreds of features for any relational dataset",Feature Engineering: What Powers Machine Learning,6,published,5822,1741,0.456634118322803,4,1,1,0,1,0
34,1000,193.29652526050927,184,https://towardsdatascience.com/modeling-teaching-a-machine-learning-algorithm-to-deliver-business-value-ad0205ca4c86,2,Towards Data Science,2018-11-15 08:35:00,21.67,9,1831,2018-11-07 11:18:00,"['Machine Learning', 'Data Science', 'Predictive Analytics', 'Education', 'Analytics']","Modeling: Teaching a Machine Learning Algorithm to Deliver Business Value How to train, tune, and validate a machine learning model This is the fourth in a four-part series on how we approach machine learning at Feature Labs. The complete set of articles can be found below: These articles cover the concepts and a full implementation as applied to predicting customer churn. The project Jupyter Notebooks are all available on GitHub. (Full disclosure: I work for Feature Labs, a startup developing tooling, including Featuretools, for solving problems with machine learning. All of the work documented here was completed with open-source tools and data.) The Machine Learning Modeling Process The outputs of prediction and feature engineering are a set of label times, historical examples of what we want to predict, and features, predictor variables used to train a model to predict the label. The process of modeling means training a machine learning algorithm to predict the labels from the features, tuning it for the business need, and validating it on holdout data. The output from modeling is a trained model that can be used for inference, making predictions on new data points. The objective of machine learning is not a model that does well on training data, but one that demonstrates it satisfies the business need and can be deployed on live data. Similar to feature engineering, modeling is independent of the previous steps in the machine learning process and has standardized inputs which means we can alter the prediction problem without needing to rewrite all our code. If the business requirements change, we can generate new label times, build corresponding features, and input them into the model. Implementation of Modeling for Customer Churn In this series, we are using machine learning to solve the customer churn problem. There are several ways to formulate the task, but our definition is: Predict on the first of each month which customers will churn during the month. Use a lead time of one month and churn is 31 days with no subscription. With a lead time of 1 month, this means we make predictions 1 month in advance: on January 1, we make predictions of churn during the month of February. Although machine learning algorithms may sound technically complex, implementing them in Python is simple thanks to standard machine learning libraries like Scikit-Learn. As a bit of practical advice, empirical results have shown that the choice of machine learning model and hyperparameters matters, but not as much as feature engineering. Therefore, the rational decision is to put most of the effort into prediction and feature engineering, and insert a pre-built solution for machine learning. In this project, I went with Scikit-Learn to rapidly implement a few models. To get the data ready for machine learning, we have to take some basic steps: missing value imputation, encoding of categorical variables, and optionally feature selection if the input dimension is too large (see notebook for full details). Then, we can create a model with standard modeling syntax: Before applying machine learning, it’s best to establish a naive baseline to determine if machine learning is actually helping. With a classification problem, this can be as simple as guessing the majority label in the training data for all examples in the hold-out testing data. For the customer churn data, guessing every test label is not a churn yields an accuracy of 96.5%. This high accuracy may sound impressive, but for an imbalanced classification problem — where one class is represented more than another — accuracy is not an adequate metric. Instead, we want to use recall, precision, or the F1 score. Recall represents the percentage of actual churns in the data that our model identifies with the naive guess recording 3.5%. Precision measures the percentage of churns predicted by our model that actually were churns, with a naive score of 1.0%. The F1 score is the harmonic mean of these measures. Since this is a classification problem, for a machine learning baseline I tried a logistic regression which did not perform well. This indicates the problem is likely non-linear, so my second attempt used a Random Forest Classifier with better results. The random forest is quick to train, relatively interpretable, highly accurate and is usually a solid model choice. The metrics for no machine learning, logistic regression, and the random forest with default hyperparameters are shown below: Each model was evaluated using about 30% of the data for holdout testing based on a time-series split. (This is crucial when evaluating a model in a time-series problem because it prevents training data leakage and should provide a good estimate of the actual model performance on new data.) Aligning the Model with the Business Requirement Even though the metrics for the ml models are better than with no machine learning, we want to optimize a model for a given metric(s) in line with the business need. In this example, we’ll focus on recall and precision. We will tune the model to achieve a certain recall by adjusting the threshold, the probability above which an observation is classified as positive — a churn. There is a fundamental tradeoff in machine learning between recall and precision, which means we can increase one only at the cost of decreasing the other. For example, if we want to find every instance of churn — a recall of 100% — then we would have to accept a low precision — many false positives. Conversely, if we limit the false positives by increasing the precision, then we will identify fewer of the actual churns lowering the recall. The balance between these two is altered by adjusting the model’s threshold. We can visualize this in the model’s precision-recall curve. This shows the precision versus the recall for different values of the threshold. The default threshold in Scikit-Learn is 0.5, but depending on the business needs, we can adjust this to achieve desired performance. For customer churn we’ll tune the threshold to achieve a recall of 75%. By inspecting the predicted probabilities (the actual values), we determine the threshold should be 0.39 to hit this mark. At a threshold of 0.39, our recall is 75% and our precision is 8.31%. Choosing the recall or precision lies in the business domain. It requires determining which is more costly, false positives — predicting a customer will churn when in fact they will not — or false negatives — predicting a customer will not churn when in fact they will — and adjusting appropriately. A recall of 75% was chosen as an example optimization but this can be changed. At this value, compared to the naive baseline, we have achieved a 20x improvement in recall and an 8x improvement in precision. Model Validation Once we have selected the threshold for classifying a churn, we can plot the confusion matrix from the holdout testing set to examine the predictions. At this threshold, we identify more than half the churns (75%) although with a significant number of false positives (upper right). Depending on the relative cost of false negatives vs false positives, our model might not actually be an improvement! To make sure our model has solved the problem, we need to use the holdout results to calculate the return from implementing the model. Validating Business Value Using the model’s metrics on the hold-out testing set as an estimate of performance on new data, we can calculate the value of deploying this model before deploying it. Using the historical data, we first calculate the typical revenue lost to churn and then the reduced amount of revenue lost to churn with a model that achieves 75% recall and 8% precision. Making a few assumptions about customer conversions (see notebook for details) we arrive at the following conclusion: Machine learning increases the number of active monthly subscribers and recoups 13.5% of the monthly losses from customer churns. Considering a subscription cost, this represents $130,000 (USD) per month. With these numbers, we conclude that machine learning has solved the business need of increasing monthly subscribers and delivered a positive solution. As a final piece of model interpretation, we can look at the most important features to get a sense of the variables most relevant to the problem. The 10 most important variables from the random forest model are shown below: The most important variables agree with our intuition for the problem. For instance, the most important feature is the total spending in the month before the cutoff time. Because we are using a lead time of 1 month, this represents the spending two months prior to the month of prediction. The more customers spent in this period, the less likely they were to churn. We also see top features like the average time between transactions or method of payment id, which could be important to monitor for our business. With our machine learning pipeline complete and the model validated, we are ready to make predictions of future customer churn. We don’t have live data for this project, but if we did, we could make predictions like the following: These predictions and feature importances can go to the customer engagement team where they will do the hard work of retaining members. In addition to making predictions each time we get new data, we’ll want to continue to validate our solution once it has been deployed. This means comparing model predictions to actual outcomes and looking at the data to check for concept drift. If performance decreases below the level of providing value, we can gather and train on more data, change the prediction problem, optimize the model settings, or adjust the tuned threshold. As with prediction and feature engineering, the modeling stage is adaptable to new prediction problems and uses common tools in data science. Each step in the machine learning framework we use is segmented, meaning we are able to implement solutions to numerous problems without needing to rewrite all the code. Moreover, the APIs — Pandas, Featuretools, and Scikit-Learn — are user-friendly, have great documentation, and abstract away the tedious details. Conclusions for the Machine Learning Process The future of machine learning lies not in one-off solutions but in a general-purpose framework allowing data scientists to rapidly develop solutions for all the problems they face. This scaffolding functions in much the same way as website templates: each time we build a website, we don’t start from scratch, we use an existing template and fill in the details. The same methodology should apply to solving problems with machine learning: instead of building a new solution for each problem, adapt an existing scaffolding and fill in the details with user-friendly tooling. In this series of articles, we walked through the concepts and use of a general-purpose framework for solving real-world machine learning problems. The process is summarized in three steps: While machine learning is not a sacred art available only to a select few, it has remained out of the reach of many organizations because of the lack of standardized processes. The objective of this framework is to make machine learning solutions easier to develop and deploy, which will allow more organizations to see the benefits of leveraging this powerful technology. If building meaningful, high-performance predictive models is something you care about, then get in touch with us at Feature Labs. While this project was completed with the open-source Featuretools, the commercial product offers additional tools and support for creating machine learning solutions. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Prediction Engineering: How to Set Up Your Machine Learning Problem Feature Engineering: What Powers Machine Learning Modeling: Teaching an Algorithm (this article) Feature Engineering: Use label times and raw historical data to build predictor variables for each label Modeling: Train, tune for the business need, validate the value of solution, and make predictions with a machine learning algorithm Overview: A General-Purpose Framework for Machine Learning Prediction Engineering: Define a business need, translate the need into a supervised machine learning problem, and create labeled examples",Modeling: Teaching a Machine Learning Algorithm to Deliver Business Value,10,published,8448,2230,0.4484304932735426,7,1,1,0,1,0
32,2800,190.7415605294676,617,https://towardsdatascience.com/deploying-a-keras-deep-learning-model-as-a-web-application-in-p-fc0f2354a7ff,7,Towards Data Science,2018-11-17 21:54:00,30.09,7,8692,2018-11-14 20:12:00,"['Machine Learning', 'Data Science', 'Deep Learning', 'Education', 'Web Development']","Deploying a Keras Deep Learning Model as a Web Application in Python Deep learning, web apps, Flask, HTML, and CSS in one project Building a cool machine learning project is one thing, but at the end of the day, you want other people to be able to see your hard work. Sure, you could put the whole project on GitHub, but how are your grandparents supposed to figure that out? No, what we want is to deploy our deep learning model as a web application accessible to anyone in the world. In this article, we’ll see how to write a web application that takes a trained Keras recurrent neural network and allows users to generate new patent abstracts. This project builds on work from the Recurrent Neural Networks by Example article, but knowing how to create the RNN isn’t necessary. We’ll just treat it as a black box for now: we put in a starting sequence, and it outputs an entirely new patent abstract that we can display in the browser! Traditionally, data scientists develop the models and front end engineers show them to the world. In this project, we’ll have to play both roles, and dive into web development (almost all in Python though). This project requires joining together numerous topics: The final result is a web application that allows users to generate entirely new patent abstracts with a trained recurrent neural network: The complete code for this project is available on GitHub. The goal was to get a web application up and running as quickly as possible. For that, I went with Flask, which allows us to write the app in Python. I don’t like to mess with styling (which clearly shows) so almost all of the CSS is copied and pasted. This article by the Keras team was helpful for the basics and this article is a useful guide as well. Overall, this project adheres to my design principles: get a prototype up and running quickly — copying and pasting as much as required — and then iterate to make a better product. The quickest way to build a web app in Python is with Flask. To make our own app, we can use just the following: If you copy and paste this code and run it, you’ll be able to view your own web app at localhost:50000. Of course, we want to do more than that, so we’ll use a slightly more complicated function which basically does the same thing: handles requests from your browser and serves up some content as HTML. For our main page, we want to present the user with a form to enter some details. When our users arrive at the main page of the application, we’ll show them a form with three parameters to select: To build a form in Python we’ll use wtforms .The code to make the form is: This creates a form shown below (with styling from main.css): The validator in the code make sure the user enters the correct information. For example, we check all boxes are filled and that the diversity is between 0.5 and 5. These conditions must be met for the form to be accepted. The way we actually serve the form is with Flask is using templates. A template is a document with a basic framework that we need to fill in with details. For a Flask web application, we can use the Jinja templating library to pass Python code to an HTML document. For example, in our main function, we’ll send the contents of the form to a template called index.html. When the user arrives on the home page, our app will serve up index.html with the details from form. The template is a simple html scaffolding where we refer to python variables with {{variable}} syntax. For each of the errors in the form (those entries that can’t be validated) an error will flash. Other than that, this file will show the form as above. When the user enters information and hits submit (a POST request) if the information is correct, we want to divert the input to the appropriate function to make predictions with the trained RNN. This means modifying home() . Now, when the user hits submit and the information is correct, the input is sent either to generate_random_start or generate_from_seed depending on the input. These functions use the trained Keras model to generate a novel patent with a diversity and num_words specified by the user. The output of these functions in turn is sent to either of the templates random.html or seeded.html to be served as a web page. Making Predictions with a Pre-Trained Keras Model The model parameter is the trained Keras model which load in as follows: (The tf.get_default_graph() is a workaround based on this gist.) I won’t show the entirety of the two util functions (here is the code), and all you need to understand is they take the trained Keras model along with the parameters and make predictions of a new patent abstract. These functions both return a Python string with formatted HTML. This string is sent to another template to be rendered as a web page. For example, the generate_random_start returns formatted html which goes into random.html: Here we are again using the Jinja template engine to display the formatted HTML. Since the Python string is already formatted as HTML, all we have to do is use {{input|safe}} (where input is the Python variable) to display it. We can then style this page in main.css as with the other html templates. The functiongenerate_random_start picks a random patent abstract as the starting sequence and makes predictions building from it. It then displays the starting sequence, RNN generated output, and the actual output: The functiongenerate_from_seed takes a user-supplied starting sequence and then builds off of it using the trained RNN. The output appears as follows: While the results are not always entirely on-point, they do show the recurrent neural network has learned the basics of English. It was trained to predict the next word from the previous 50 words and has picked up how to write a slightly-convincing patent abstract! Depending on the diversity of the predictions, the output might appear to be completely random or a loop. To run the app for yourself, all you need to do is download the repository, navigate to the deployment directory and type python run_keras_server.py . This will immediately make the web app available at localhost:10000. Depending on how your home WiFi is configured, you should be able to access the application from any computer on the network using your IP address. The web application running on your personal computer is great for sharing with friends and family. I’d definitely not recommend opening this up to everyone on your home network though! For that, we’ll want to set the app up on an AWS EC2 instance and serve it to the world (coming later) . To improve the app, we can alter the styling (through main.css ) and perhaps add more options, such as the ability to choose the pre-trained network. The great thing about personal projects is you can take them as far as you want. If you want to play around with the app, download the code and get started. Conclusions In this article, we saw how to deploy a trained Keras deep learning model as a web application. This requires bringing together a number of different technologies including recurrent neural networks, web applications, templating, HTML, CSS, and of course Python. While this is only a basic application, it shows that you can start building web applications using deep learning with relatively little effort. There aren’t many people who can say they’ve deployed a deep learning model as a web application, but if you follow this article, count yourself among them! As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online. Loading in Trained Model Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Keras: deploying a trained recurrent neural network Templating with the Jinja template library HTML and CSS for writing web pages Choose diversity of RNN predictions Choose the number of words RNN outputs Flask: creating a basic web application in Python Input a starting sequence for RNN or select randomly",Deploying a Keras Deep Learning Model as a Web Application in Python,12,published,28885,1561,1.7937219730941705,3,1,1,0,1,0
29,1800,189.82267165748846,327,https://towardsdatascience.com/deploying-a-python-web-app-on-aws-57ed772b2319,7,Towards Data Science,2018-11-18 19:57:00,34.92,6,5709,2018-11-18 17:07:00,"['Programming', 'Web Development', 'Data Science', 'Education', 'Cloud Computing']","Deploying a Python Web App on AWS How to share your Python project with the world While I enjoy doing data science and programming projects for the personal thrill that comes with building something of my own, there is also a certain joy in sharing your project online with anyone in the world. Fortunately, thanks to Amazon Web Services (AWS), in a few minutes, we can deploy a Python web application to the entire world for free. In this article, we’ll see how to deploy a deep learning web app to AWS on a free EC2 instance. This article will work with the app built in Deploying a Keras Deep Learning Model as a Web Application in Python using the model developed in Recurrent Neural Networks by Example in Python. Neither of these is required, just know that our application generates novel patent abstracts with an RNN. All the code for the project can be found on GitHub. Amazon Web Services is the umbrella term for the range of Amazon’s cloud computing offerings. We be using Amazon Elastic Compute Cloud (EC2), a service where we rent virtual computers in the cloud to run applications. AWS EC2 offers a free tier so we can deploy without spending a cent. To get started, create an AWS account and head to the EC2 console at https://console.aws.amazon.com/ec2. Click on the Launch Instance button which takes you to choose an Amazon Machine Instance (AMI), “ a template that contains the software configuration (operating system) required to launch your instance.” You can use any os you’re familiar with (although some aren’t eligible for the free tier), but I’ll be using Ubuntu Server 18.04: Hit Select, then on the next page choose the free tier eligible t2.micro instance (an instance is the hardware for our AMI). This only has 1 CPU and 1 GB of RAM, but it will actually be enough to run our pre-trained recurrent neural network application! If you’re expecting more traffic or running a cpu-intensive application, you’ll probably have to shell out. Select the instance type you want and then go to tab 6. Configure Security Group at the top of the page. Security groups filter the traffic into and out of our instance — basically, who can access our virtual computer. You (and only you) will need to access the instance via ssh, so add a rule that allows Source “My IP” for SSH. We want others to be able to access our app in a web browser, so add a rule to allow HTTP access for all sources. The final security configuration is: Next, hit Review and Launch and then Launch. This brings up the options for using a key pair. You need this to access the server via ssh, so make sure to create a new key pair and save the private key somewhere you remember it. If you lose this, you will not be able to access your instance again! Finally, hit Launch Instances and Amazon will start up your very own virtual machine which is physically located…somewhere. Wait a few minutes for the instance to boot before heading to the next step: connecting to your instance. Connecting to Server via SSH Once the instance is up and running, select it on the EC2 Instance dashboard (Services > EC2 > Running Instances) and hit Connect. This will give us the exact commands to connect to the instance. Copy the example code, and paste it into Bash or a command prompt running in the folder with your private key (you generate this when launching your instance). Assuming everything goes well, you’ll be logged into your instance and see a familiar terminal command prompt. This AMI comes equipped with Python 3.6, so we just need to clone the repository and install the app dependencies. First, get the repository: Then install pip, move into the repository, and install the requirements. Running the app is simple (you might need sudo for the second command): (If you want to understand what’s going on in the web application, take a look at the previous article for the development process). You should see the following output in the terminal: While it looks like this the app is running on localhost:80/, that’s on the virtual machine. For us to access the web app, we’ll have to use the instance’s Public DNS IPv4 which can be found on the running instance dashboard. Copy and paste the address into your browser, and you’ll see the application! Feel free to play around with the recurrent neural network application. What it’s doing is generating new patent abstracts with a recurrent neural network trained on thousands of abstracts with the keyword “neural network” You can either enter random for a random starting sequence, or your own sequence. (To see the development, check out this article or this notebook). Your application can now be reached by anyone in the world via the IPv4. If you want the app to keep running even after you log out of the instance, run it in a Screen session. (Screen is a handy program that lets you run terminal sessions from a single terminal window using virtual consoles.) My (if I haven’t shut it down or run into errors) application should be running at http://54.173.255.177/. Because I’m using a t2.micro instance, the cost to run this web application in perpetuity is precisely $0.00! If you want a domain name, you can pick up one from a domain name registrar such as Hover. Although this is a decent solution to quickly deploy a personal project, this is not a production-ready deployment! For that, you’ll want to make sure to use proper security (with HTTPS and a certified certificate). You’ll also want to make sure your application can handle expected traffic. Only use this specific solution for small projects without sensitive data. Conclusions We truly live in incredible times: with Flask we can develop a Python web app in a few minutes and then we can deploy it to the world free with AWS. The general process we followed was: develop a web application (in Python preferably), rent commodity hardware from a cloud provider, and deploy a web application to the world. If you were able to follow all the tutorials from the implementation of a recurrent neural network to developing a local web application to deploying on AWS, then you’ll have completed an impressive project! The overall process from a blank file to a running web application may be daunting, but like most technology problems, if you break it down, each step isn’t overwhelming and there are many open-source tools to make the process easy. If you’re a data scientist bored with doing self-contained analysis in Jupyter Notebooks, take the initiative to do a project you can deploy as an application. It’s good to branch out into other disciplines, and building and deploying a web application is a great opportunity to learn some new skills. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Deploying a Python Web App on AWS,7,published,16350,1355,1.3284132841328413,0,1,1,0,0,0
31,1100,182.09546434122686,193,https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce,9,Towards Data Science,2018-11-26 13:25:00,17.94,15,1849,2018-11-23 17:47:00,"['Machine Learning', 'Data Science', 'Deep Learning', 'Pytorch', 'Education']","Transfer Learning with Convolutional Neural Networks in PyTorch How to use a pre-trained convolutional neural network for object recognition with PyTorch Although Keras is a great library with a simple API for building neural networks, the recent excitement about PyTorch finally got me interested in exploring this library. While I’m one to blindly follow the hype, the adoption by researchers and inclusion in the fast.ai library convinced me there must be something behind this new entry in deep learning. Since the best way to learn a new technology is by using it to solve a problem, my efforts to learn PyTorch started out with a simple project: use a pre-trained convolutional neural network for an object recognition task. In this article, we’ll see how to use PyTorch to accomplish this goal, along the way, learning a little about the library and about the important concept of transfer learning. While PyTorch might not be for everyone, at this point it’s impossible to say which deep learning library will come out on top, and being able to quickly learn and use different tools is crucial to succeed as a data scientist. The complete code for this project is available as a Jupyter Notebook on GitHub. This project was born out of my participation in the Udacity PyTorch scholarship challenge. Approach to Transfer Learning Our task will be to train a convolutional neural network (CNN) that can identify objects in images. We’ll be using the Caltech 101 dataset which has images in 101 categories. Most categories only have 50 images which typically isn’t enough for a neural network to learn to high accuracy. Therefore, instead of building and training a CNN from scratch, we’ll use a pre-built and pre-trained model applying transfer learning. The basic premise of transfer learning is simple: take a model trained on a large dataset and transfer its knowledge to a smaller dataset. For object recognition with a CNN, we freeze the early convolutional layers of the network and only train the last few layers which make a prediction. The idea is the convolutional layers extract general, low-level features that are applicable across images — such as edges, patterns, gradients — and the later layers identify specific features within an image such as eyes or wheels. Thus, we can use a network trained on unrelated categories in a massive dataset (usually Imagenet) and apply it to our own problem because there are universal, low-level features shared between images. The images in the Caltech 101 dataset are very similar to those in the Imagenet dataset and the knowledge a model learns on Imagenet should easily transfer to this task. Following is the general outline for transfer learning for object recognition: This approach has proven successful for a wide range of domains. It’s a great tool to have in your arsenal and generally the first approach that should be tried when confronted with a new image recognition problem. With all data science problems, formatting the data correctly will determine the success or failure of the project. Fortunately, the Caltech 101 dataset images are clean and stored in the correct format. If we correctly set up the data directories, PyTorch makes it simple to associate the correct labels with each class. I separated the data into training, validation, and testing sets with a 50%, 25%, 25% split and then structured the directories as follows: The number of training images by classes is below (I use the terms classes and categories interchangeably): We expect the model to do better on classes with more examples because it can better learn to map features to labels. To deal with the limited number of training examples we’ll use data augmentation during training (more later). As another bit of data exploration, we can also look at the size distribution. Imagenet models need an input size of 224 x 224 so one of the preprocessing steps will be to resize the images. Preprocessing is also where we will implement data augmentation for our training data. The idea of data augmentation is to artificially increase the number of training images our model sees by applying random transformations to the images. For example, we can randomly rotate or crop the images or flip them horizontally. We want our model to distinguish the objects regardless of orientation and data augmentation can also make a model invariant to transformations of the input data. An elephant is still an elephant no matter which way it’s facing! Augmentation is generally only done during training (although test time augmentation is possible in the fast.ai library). Each epoch — one iteration through all the training images — a different random transformation is applied to each training image. This means that if we iterate through the data 20 times, our model will see 20 slightly different versions of each image. The overall result should be a model that learns the objects themselves and not how they are presented or artifacts in the image. Image Preprocessing This is the most important step of working with image data. During image preprocessing, we simultaneously prepare the images for our network and apply data augmentation to the training set. Each model will have different input requirements, but if we read through what Imagenet requires, we figure out that our images need to be 224x224 and normalized to a range. To process an image in PyTorch, we use transforms , simple operations applied to arrays. The validation (and testing) transforms are as follows: The end result of passing through these transforms are tensors that can go into our network. The training transformations are similar but with the addition of random augmentations. First up, we define the training and validation transformations: Then, we create datasets and DataLoaders . By using datasets.ImageFolder to make a dataset, PyTorch will automatically associate images with the correct labels provided our directory is set up as above. The datasets are then passed to a DataLoader , an iterator that yield batches of images and labels. We can see the iterative behavior of the DataLoader using the following: The shape of a batch is (batch_size, color_channels, height, width). During training, validation, and eventually testing, we’ll iterate through the DataLoaders, with one pass through the complete dataset comprising one epoch. Every epoch, the training DataLoader will apply a slightly different random transformation to the images for training data augmentation. Pre-Trained Models for Image Recognition With our data in shape, we next turn our attention to the model. For this, we’ll use a pre-trained convolutional neural network. PyTorch has a number of models that have already been trained on millions of images from 1000 classes in Imagenet. The complete list of models can be seen here. The performance of these models on Imagenet is shown below: For this implementation, we’ll be using the VGG-16. Although it didn’t record the lowest error, I found it worked well for the task and was quicker to train than other models. The process to use a pre-trained model is well-established: Loading in a pre-trained model in PyTorch is simple: This model has over 130 million parameters, but we’ll train only the very last few fully-connected layers. Initially, we freeze all of the model’s weights: Then, we add on our own custom classifier with the following layers: When the extra layers are added to the model, they are set to trainable by default ( require_grad=True ). For the VGG-16, we’re only changing the very last original fully-connected layer. All of the weights in the convolutional layers and the the first 5 fully-connected layers are not trainable. The final outputs from the network are log probabilities for each of the 100 classes in our dataset. The model has a total of 135 million parameters, of which just over 1 million will be trained. One of the best aspects of PyTorch is the ease of moving different parts of a model to one or more gpus so you can make full use of your hardware. Since I’m using 2 gpus for training, I first move the model to cuda and then create a DataParallel model distributed over the gpus: (This notebook should be run on a gpu to complete in a reasonable amount of time. The speedup over a cpu can easily by 10x or more.) The training loss (the error or difference between predictions and true values) is the negative log likelihood (NLL). (The NLL loss in PyTorch expects log probabilities, so we pass in the raw output from the model’s final layer.) PyTorch uses automatic differentiation which means that tensors keep track of not only their value, but also every operation (multiply, addition, activation, etc.) which contributes to the value. This means we can compute the gradient for any tensor in the network with respect to any prior tensor. What this means in practice is that the loss tracks not only the error, but also the contribution to the error by each weight and bias in the model. After we calculate the loss, we can then find the gradients of the loss with respect to each model parameter, a process known as backpropagation. Once we have the gradients, we use them to update the parameters with the optimizer. (If this doesn’t sink in at first, don’t worry, it takes a little while to grasp! This powerpoint helps to clarify some points.) The optimizer is Adam, an efficient variant of gradient descent that generally does not require hand-tuning the learning rate. During training, the optimizer uses the gradients of the loss to try and reduce the error (“optimize”) of the model output by adjusting the parameters. Only the parameters we added in the custom classifier will be optimized. The loss and optimizer are initialized as follows: With the pre-trained model, the custom classifier, the loss, the optimizer, and most importantly, the data, we’re ready for training. Training Model training in PyTorch is a little more hands-on than in Keras because we have to do the backpropagation and parameter update step ourselves. The main loop iterates over a number of epochs and on each epoch we iterate through the train DataLoader . The DataLoader yields one batch of data and targets which we pass through the model. After each training batch, we calculate the loss, backpropagate the gradients of the loss with respect to the model parameters, and then update the parameters with the optimizer. I’d encourage you to look at the notebook for the complete training details, but the basic pseudo-code is as follows: We can continue to iterate through the data until we reach a given number of epochs. However, one problem with this approach is that our model will eventually start overfitting to the training data. To prevent this, we use our validation data and early stopping. Early stopping means halting training when the validation loss has not decreased for a number of epochs. As we continue training, the training loss will only decrease, but the validation loss will eventually reach a minimum and plateau or start to increase. We ideally want to stop training when the validation loss is at a minimum in the hope that this model will generalize best to the testing data. When using early stopping, every epoch in which the validation loss decreases, we save the parameters so we can later retrieve those with the best validation performance. We implement early stopping by iterating through the validation DataLoader at the end of each training epoch. We calculate the validation loss and compare this to the lowest validation loss. If the loss is the lowest so far, we save the model. If the loss has not improved for a certain number of epochs, we halt training and return the best model which has been saved to disk. Again, the complete code is in the notebook, but pseudo-code is: To see the benefits of early stopping, we can look at the training curves showing the training and validation losses and accuracy: As expected, the training loss only continues to decrease with further training. The validation loss, on the other hand, reaches a minimum and plateaus. At a certain epoch, there is no return (or even a negative return) to further training. Our model will only start to memorize the training data and will not be able to generalize to testing data. Without early stopping, our model will train for longer than necessary and will overfit to the training data. Another point we can see from the training curves is that our model is not overfitting greatly. There is some overfitting as is always be the case, but the dropout after the first trainable fully connected layer prevents the training and validation losses from diverging too much. Making Predictions: Inference In the notebook I take care of some boring — but necessary — details of saving and loading PyTorch models, but here we’ll move right to the best part: making predictions on new images. We know our model does well on training and even validation data, but the ultimate test is how it performs on a hold-out testing set it has not seen before. We saved 25% of the data for the purpose of determining if our model can generalize to new data. Predicting with a trained model is pretty simple. We use the same syntax as for training and validation: The shape of our probabilities are ( batch_size , n_classes ) because we have a probability for every class. We can find the accuracy by finding the highest probability for each example and compare these to the labels: When diagnosing a network used for object recognition, it can be helpful to look at both overall performance on the test set and individual predictions. Here are two predictions the model nails: These are pretty easy, so I’m glad the model has no trouble! We don’t just want to focus on the correct predictions and we’ll take a look at some wrong outputs shortly. For now let’s evaluate the performance on the entire test set. For this, we want to iterate over the test DataLoader and calculate the loss and accuracy for every example. Convolutional neural networks for object recognition are generally measured in terms of topk accuracy. This refers to the whether or not the real class was in the k most likely predicted classes. For example, top 5 accuracy is the % the right class was in the 5 highest probability predictions. You can get the topk most likely probabilities and classes from a PyTorch tensor as follows: Evaluating the model on the entire testing set, we calculate the metrics: These compare favorably to the near 90% top 1 accuracy on the validation data. Overall, we conclude our pre-trained model was able to successfully transfer its knowledge from Imagenet to our smaller dataset. Although the model does well, there’s likely steps to take which can make it even better. Often, the best way to figure out how to improve a model is to investigate its errors (note: this is also an effective self-improvement method.) Our model isn’t great at identifying crocodiles, so let’s look at some test predictions from this category: Given the subtle distinction between crocodile and crocodile_head , and the difficulty of the second image, I’d say our model is not entirely unreasonable in these predictions. The ultimate goal in image recognition is to exceed human capabilities, and our model is nearly there! Finally, we’d expect the model to perform better on categories with more images, so we can look at a graph of accuracy in a given category versus the number of training images in that category: There does appear to be a positive correlation between the number of training images and the top 1 test accuracy. This indicates that more training data augmentation could be helpful, or, even that we should use test time augmentation. We could also try a different pre-trained model, or build another custom classifier. At the moment, deep learning is still an empirical field meaning experimentation is often required! Conclusions While there are easier deep learning libraries to use, the benefits of PyTorch are speed, control over every aspect of model architecture / training, efficient implementation of backpropagation with tensor auto differentiation, and ease of debugging code due to the dynamic nature of PyTorch graphs. For production code or your own projects, I’m not sure there is yet a compelling argument for using PyTorch instead of a library with a gentler learning curve such as Keras, but it’s helpful to know how to use different options. Through this project, we were able to see the basics of using PyTorch as well as the concept of transfer learning, an effective method for object recognition. Instead of training a model from scratch, we can use existing architectures that have been trained on a large dataset and then tune them for our task. This reduces the time to train and often results in better overall performance. The outcome of this project is some knowledge of transfer learning and PyTorch that we can build on to build more complex applications. We truly live in an incredible age for deep learning, where anyone can build deep learning models with easily available resources! Now get out there and take advantage of these resources by building your own project. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Freeze parameters (weights) in model’s lower convolutional layers Add custom classifier with several layers of trainable parameters to model Train classifier layers on training data available for task Fine-tune hyperparameters and unfreeze more layers as needed Center crop to 224 x 224 Convert to a tensor Normalize with mean and standard deviation Freeze all the weights in the lower (convolutional) layers: the layers to freeze are adjusted depending on similarity of new task to original dataset Replace the upper layers of the network with a custom classifier: the number of outputs must be set equal to the number of classes Train only the custom classifier layers for the task thereby optimizing the model for smaller dataset Dropout with 40% chance of dropping Fully connected with log softmax output, shape = (256, n_classes) Load in a pre-trained CNN model trained on a large dataset Resize Load in pre-trained weights from a network trained on a large dataset Fully connected with ReLU activation, shape = (n_inputs, 256)",Transfer Learning with Convolutional Neural Networks in PyTorch,8,published,10304,3398,0.3237198351971748,2,1,1,0,1,0
28,3500,180.22170976850697,531,https://towardsdatascience.com/estimating-probabilities-with-bayesian-modeling-in-python-7144be007815,14,Towards Data Science,2018-11-28 10:23:00,21.65,12,5188,2018-11-26 15:43:00,"['Data Science', 'Statistics', 'Education', 'Towards Data Science', 'Programming']","Estimating Probabilities with Bayesian Modeling in Python A simple application of Probabilistic Programming with PyMC3 in Python It started, as the best projects always do, with a few tweets: This may seem like a simple problem — the prevalences are simply the same as the observed data (50% lions, 33% tigers and 17% bears) right? If you believe observations we make are a perfect representation of the underlying truth, then yes, this problem could not be easier. However, as a Bayesian, this view of the world and the subsequent reasoning is deeply unsatisfying. First, how can we be sure this single trip to the preserve was indicative of all trips? What if we went during the winter when the bears were hibernating? We need to include uncertainty in our estimate considering the limited data. Second, how can we incorporate prior beliefs about the situation into this estimate? If we have heard from a friend the preserve has an equal number of each animal, then surely this should play some role in our estimate. Fortunately, there is a solution that allows to express uncertainty and incorporate prior information into our estimate: Bayesian Inference. In this article, we’ll explore the problem of estimating probabilities from data in a Bayesian framework, along the way learning about probability distributions, Bayesian Inference, and basic probabilistic programming with PyMC3. The complete code is available as a Jupyter Notebook on GitHub. Background: Concepts Often, especially in statistics, I find the theory behind a solution more confusing than actually solving the problem. (I’m convinced statisticians complicate statistics to justify their existence.) Coding an answer and visualizing the solution usually does more for me than reading endless equations. Therefore, when I approached this problem, I studied just enough of the ideas to code a solution, and only after did I dig back into the concepts. This reflects my general top-down approach to learning new topics. Instead of starting with the fundamentals — which are usually tedious and difficult to grasp — find out how to implement an idea so you know why it’s useful and then go back to the formalisms. So, if you feel yourself getting frustrated with the theory, move on to the solution (starting with the Inference section below), and then come back to the concepts if you’re still interested. (This top-down philosophy is exemplified in the excellent fast.ai courses on deep learning. These courses, besides effectively teaching neural networks, have been influential in my approach to learning new techniques.) Bayesian Model Since we want to solve this problem with Bayesian methods, we need to construct a model of the situation. The basic set-up is we have a series of observations: 3 tigers, 2 lions, and 1 bear, and from this data, we want to estimate the prevalence of each species at the wildlife preserve. That is, we are looking for the posterior probability of seeing each species given the data. Before we begin we want to establish our assumptions: The overall system, where we have 3 discrete choices (species) each with an unknown probability and 6 total observations is a multinomial distribution. The multinomial distribution is the extension of the binomial distribution to the case where there are more than 2 outcomes. A simple application of a multinomial is 5 rolls of a dice each of which has 6 possible outcomes. A probability mass function of a multinomial with 3 discrete outcomes is shown below: A Multinomial distribution is characterized by k, the number of outcomes, n, the number of trials, and p, a vector of probabilities for each of the outcomes. For this problem, p is our ultimate objective: we want to figure out the probability of seeing each species from the observed data. In Bayesian statistics, the parameter vector for a multinomial is drawn from a Dirichlet Distribution, which forms the prior distribution for the parameter. The Dirichlet Distribution, in turn, is characterized by, k, the number of outcomes, and alpha, a vector of positive real values called the concentration parameter. This is called a hyperparameter because it is a parameter of the prior. (This chain can keep going: if alpha comes from another distribution then this is a hyperprior which could have its own parameters called hyperyhyperparameters!). We’ll stop our model at this level by explicitly setting the values of alpha, which has one entry for each outcome. The best way to think of the Dirichlet parameter vector is as pseudocounts, observations of each outcome that occur before the actual data is collected. These pseudocounts capture our prior belief about the situation. For example, because we think the prevalence of each animal is the same before going to the preserve, we set all of the alpha values to be equal, say alpha = [1, 1, 1]. Conversely, if we expected to see more bears, we could use a hyperparameter vector like [1, 1, 2] (where the ordering is [lions, tigers, bears]. The exact value of the pseudocounts reflects the level of confidence we have in our prior beliefs. Larger pseudocounts will have a greater effect on the posterior estimate while smaller values will have a smaller effect and will let the data dominate the posterior. We’ll see this when we get into inference, but for now, remember that the hyperparameter vector is pseudocounts, which in turn, represent our prior belief. A Dirichlet distribution with 3 outcomes is shown below with different values of the hyperparameter vector. Color indicates the concentration weighting. There’s a lot more detail we don’t need to get into here, but if you’re still curious, see some of the sources listed below. Our ultimate goal is to estimate the posterior distribution for the probability of observing each species, p, conditioned on the data and hyperparameters: Our final model, consisting of a multinomial distribution with Dirichlet priors is called a Dirichlet-Multinomial and is visualized below: A summary of the problem specifics is below: If you still want more background details, here are some of the sources I relied on (the first is probably the most valuable): Sources: There are also other ways to approach this problem; see here for Allen Downey’s solution which yields similar results. Inference: Making Estimates from Data Now that we have the model of the problem, we can solve for the posteriors using Bayesian methods. Inference in statistics is the process of estimating (inferring) the unknown parameters of a probability distribution from data. Our unknown parameters are the prevalence of each species while the data is our single set of observations from the wildlife preserve. Our goal is to find the posterior distribution of the probability of seeing each species. Our approach to deriving the posterior will use Bayesian inference. This means we build the model and then use it to sample from the posterior to approximate the posterior with Markov Chain Monte Carlo (MCMC) methods. We use MCMC when exact inference is intractable, and, as the number of samples increases, the estimated posterior converges to the true posterior. The result of MCMC is not just one number for our answer, but rather a range of samples that lets us quantify our uncertainty especially with limited data. We’ll see how to perform Bayesian inference in Python shortly, but if we do want a single estimate, we can use the Expected Value of the distribution. The Expected Value is the mean of the posterior distribution. For a Dirichlet-Multinomial, it can be analytically expressed: Once we start plugging in numbers, this becomes easy to solve. N is the number of trials, 6, c_i is the observed count for each category, and alpha_i is the pseudocount (hyperparameter) for each category. Setting all alphas equal to 1, the expected species probabilities can be calculated: This represents the expected value taking into account the pseudocounts which corporate our initial belief about the situation. We can adjust our level of confidence in this prior belief by increasing the magnitude of the pseudocounts. This forces the expected values closer to our initial belief that the prevalence of each species is equal. The expected values for several different hyperparameters are shown below: Our choice of hyperparameters has a large effect. If we are more confident in our belief, then we increase the weight of the hyperparameters. On the other hand, if we want the data to have more weight, we reduce the pseudocounts. While this result provides a point estimate, it’s misleading because it does not express any uncertainty. We only went to the wildlife preserve once, so there should be a large amount of uncertainty in these estimates. With Bayesian Inference, we can get both point estimates and the uncertainty. Bayesian Inference in Python with PyMC3 To get a range of estimates, we use Bayesian inference by constructing a model of the situation and then sampling from the posterior to approximate the posterior. This is implemented through Markov Chain Monte Carlo (or a more efficient variant called the No-U-Turn Sampler) in PyMC3. Compared to the theory behind the model, setting it up in code is simple: Then, we can sample from the posterior: This code draws 1000 samples from the posterior in 2 different chains (with 500 samples for tuning that are discarded). We are left with a trace which contains all of the samples drawn during the run. We use this trace to estimate the posterior distribution. PyMC3 has many methods for inspecting the trace such as pm.traceplot: On the left we have a kernel density estimate for the sampled parameters — a PDF of the event probabilities. On the right, we have the complete samples drawn for each free parameter in the model. We can see from the KDE that p_bears<p_tigers<p_lions as expected but there is some uncertainty. A better way to view this uncertainty is through pm.posterior_plot: Here are histograms indicating the number of times each probability was sampled from the posterior. We have a point estimate for the probabilities — the mean — as well as the Bayesian equivalent of the confidence interval — the 95% highest probability density (also known as a credible interval). We see an extreme level of uncertainty in these estimates, as befits the limited data. To quantify the level of uncertainty we can get a dataframe of the results: This shows the best estimate (mean) for the prevalence but also that the 95% credible interval is very large. We can only nail down the prevalence of lions to between 16.3% and 73.6% based on our single trip to the preserve! Bayesian Inference is so powerful because of this built-in uncertainty. In the real-world, data is always noisy, and we usually have less than we want. Therefore, anytime we make an estimate from data we have to show this uncertainty. For this problem, no one is going to be hurt if we get the percentage of bears at the wildlife preserve incorrect, but what if we were doing a similar method with medical data and inferring disease probability? Once we have the trace, we can draw samples from the posterior to simulate additional trips to the preserve. For example, let’s consider going 1000 more times. How many of each species can we expect to see on each trip? Based on the evidence, there are times when we go to the preserve and see 5 bears and 1 tiger! Granted, this is not very likely, graphs such as these show the entire range of possible outcomes instead of only one. Our single trip to the preserve was just one outcome: 1000 simulations show that we can’t expect the exact observations every time we go to the preserve. If we want to see the new Dirichlet distribution after sampling, it looks like: What happens when we go 4 times to the preserve and want to incorporate additional observations in our model? In PyMC3, this is simple: The uncertainty in the posterior should be reduced with a greater number of observations, and indeed, that is what we see both quantitatively and visually. Intuitively, this again makes sense: as we gather more data, we become more sure of the state of the world. In the case of infinite data, our estimate will converge on the true values and the priors will play no role. Earlier we discussed how the hyperparameters can be thought of as pseudocounts that represent our prior belief. If we set all the values of alpha equal to 1, we get the results we’ve seen so far. What about if we decrease or increase our confidence in our initial theory that the prevalence is equal? To do so, all we have to do is alter the alpha vector. Then, we sample from the posterior again (using the original observations) and inspect the results. The hyperparameters have a large influence on the outcome! A lower value means the data itself has a greater weighting in the posterior, while a higher value results in greater weight placed on the pseudocounts. As the value is increased, the distributions converge on one another. We’d need a lot of data to overcome our strong hyperparameters in the last case. We can compare the posterior plots with alpha = 0.1 and alpha = 15: Ultimately, our choice of the hyperparameters depends on our confidence in our belief. If we have a good reason to think the prevalence of species is equal, then we should make the hyperparameters have a greater weight. If we want to let the data speak, then we can lower the effect of the hyperparameters. Conclusions Well, what should our final answer be to the question of prevalences? If we are good Bayesians, then we can present a point estimate, but only with attached uncertainty (95% credible intervals): And our estimate that the next observation is a bear? Based on the posterior sampling, about 23%. While these results may not be satisfying to people who want a simple answer, they should remember that the real world is uncertain. The benefits of Bayesian Inference are we can incorporate our prior beliefs and we get uncertainty estimates with our answers. The world is uncertain, and, as responsible data scientists, Bayesian methods provide us with a framework for dealing with uncertainty. Furthermore, as we get more data, our answers become more accurate. As with many aspects of Bayesian Inference, this is in line with our intuitions and how we naturally go about the world, becoming less wrong with additional information. Ultimately, Bayesian statistics is enjoyable and useful because it is statistics that finally makes sense. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Our initial (prior) belief is each species is equally represented. Categorical Data / Multinomial Distribution Dirichlet-Multinomial Wikipedia Article Multinomial Distribution Wikipedia Article Alpha in the Dirichlet Distribution Dirichlet Distribution Wikipedia Article Hyperparameter Wikipedia Article Deriving the MAP estimate for Dirichlet-Multinomials Tigers: 32.7% (6.7% — 60.5%) Bears: 22.7% (1.7% — 50.0%) Treat each observation of one species as an independent trial. Bayesian Inference for Dirichlet-Multinomials Lions: 44.5% (16.9% — 75.8%)",Estimating Probabilities with Bayesian Modeling in Python,7,published,23965,2824,1.2393767705382437,1,1,1,1,0,0
35,2300,177.16097729577547,407,https://towardsdatascience.com/python-and-slack-a-natural-match-60b136883d4d,7,Towards Data Science,2018-12-01 11:50:00,19.6,8,4678,2018-11-30 21:17:00,"['Data Science', 'Programming', 'Education', 'Application', 'Python']","Python and Slack: A Natural Match How to send Slack messages, post plots, and monitor machine learning models programmatically with Python Life opens up when you learn just how much you can do with Python. The first shift in my daily workflows came when I read Automate the Boring Stuff with Python and saw there were routine tasks — such as anything to do with spreadsheets — that I no longer had to waste hours on. Instead, I could spend a few minutes writing a Python program (okay maybe a few hours at first), run the script, and then sit back to watch the tedious work do itself. A second shift occurred when I realized that nearly any service with an API — Application Programming Interface — can be manipulated with Python. Anything you can do in apps like Spotify (Spotipy library), Twitter (python-twitter), Wikipedia (Wikipedia API ), and Slack (Slacker for Python), can be accomplished through Python code. This means you’re not constrained to interacting with these services in an app, but instead, can write code to automate complex workflows or build new tools. In this article, we’ll see how to use the Slacker Python library to programmatically interact with Slack. We’ll be able to retrieve any data from a workspace, alter settings for channels and users, post messages, upload files, create applications, and even monitor machine learning results in real time. The complete code is available as a Jupyter Notebook on GitHub. Introduction to Slacker Slacker is a Python interface to the Slack API. It wraps the Slack API service (fully supported by Slack) and allows us to write Python code instead of formatting requests to the API ourselves. This means far less time to construct queries and easier interaction with Slack. The Slack API methods docs are very helpful when using the Slacker library (which has relatively scant documentation). No matter what you want to do in Slack, you can do it through this library as long as you’re willing to do a little searching! First off, you need to create a Slack workspace. I’ll assume if you’re confident writing Python code, you can handle this step by yourself. I’d recommend making a new workspace where you’re the admin for testing out interactions. The next step is to get a legacy API token. If you’re signed into the workspace, head to this link. Make sure to keep your token in a safe location! Authenticate with Slacker using your API token as follows: Check that you’re connected by printing out the name of the workspace: Interacting with Slack Anything (okay nearly anything) you can do in slack you can do through the Python interface to the API. (Check out the complete list of methods here). For example, here’s everything we can do with channels: Let’s start off with some simple data retrieval. Each time we use a different function in Slacker , we get back a response object where the body holds the results (if the request was successful). There’s quite a lot of different information we can retrieve. For example, here’s the channels and purposes: If we want to check out the users (members), here’s the code and result: It’s pretty lonely in this workspace! We can search through channels for files or messages, see a channel’s history, and even look at do not disturb settings: (The user is this case is referred to by id. We can find the user id for a username with slack.users.get_user_id(username)). Not only can we find any information we want, we can also change attributes. Maybe I want to tell my team to leave me alone: I can also alter my profile settings: On the topic of emoji, you can use emoji codes to specify which icon you want. The easiest way to pick these out is to go to Slack, search for the emoji you want, and copy the code that appears: Want to create a new channel and set the purpose? Two lines of python code: Posting Messages Of course we don’t just want to be passive consumers of information. We want to talk to the world! Fortunately, Slacker lets us easily post to any channel: If we set the username then the messages are posted as bots. We can choose the icon by specifying a url or an emoji. We can also mention specific users with <@user_id> or commit the ultimate sin and notify everyone: Messages can be as complex as we want. Using attachments, we can format messages or even create games! Refer to the Slack API documentation on formatting messages for all the options. You can build complex application workflows with interactive messages. Uploading local files is another one-liner: If we’ve done some cool analysis and want to share the plots with the team, we save the figures and upload: We can also remove files or messages, add comments, create private chats, invite users, set reminders, and more with the API. Monitor Machine Learning Training Alright, time to get serious and user Slack for some real work. Here we’ll send messages to monitor the progress of a convolutional neural network (CNN) as it is training. I’m using a script from Keras to train a CNN to recognize MNIST digits. With custom callbacks in Keras, we can send a message to Slack at the end of every epoch with the current stats. See notebook for full code, but the basic outline is below (this is pseudo-code and won’t quite run): Now we get real -time deep learning updates in Slack! At the end of training, we can create some training curves and post those: Finally, once our model has trained, we can post predictions from the test set: This model does pretty well! For this simple example, the monitoring might not be necessary, but there are many use cases for tracking a model or posting continuous integration results. I’ve run models overnight and when I don’t want to head to the command window to check out the progress, it’s useful to just look at Slack instead. Writing tools that automatically post to Slack or notify you when something needs your attention let you shift focus to more important matters. Ultimately, programmatic workflows result in less time and energy spent on tedious tasks. Conclusions While apps such as Slack can be used for fun, they also improve productivity when used correctly. Furthermore, if we realize these apps have Python access to their APIs, we can programmatically control them for great efficiency. In this article we saw how to interact with Slack through the Slacker library in Python. This library gives us access to the Slack API which means we have control over nearly every aspect of our workspace. Among other things we can If Slack isn’t your favorite application (although it should replace your email), check out the list of Python API interfaces. Even if this particular use case is not compelling to you, the idea that we can use Python to complete routine tasks is a powerful one because it enables us to be more efficient. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Alter and manage user, channel, and team settings Post messages including applications Track the progress of and plot results from machine learning Retrieve channel, message, and user information",Python and Slack: A Natural Match,6,published,23869,1397,1.64638511095204,0,1,1,0,0,1
27,4800,170.97623481930557,860,https://towardsdatascience.com/jupyter-notebook-extensions-517fa69d2231,20,Towards Data Science,2018-12-07 16:16:00,44.82,5,12175,2018-11-22 19:51:00,"['Data Science', 'Programming', 'Towards Data Science', 'Productivity', 'Education']","Jupyter Notebook Extensions How to get more productivity in the notebook environment The Jupyter Notebook is a great teaching, exploring, and literate programming environment, but out-of-the-box notebooks are notoriously lacking in features. Fortunately, there are a number of ways, including Jupyter Notebook extensions, to improve this invaluable tool. Extremely Concise Version Run the following in a command prompt: Start up a Jupyter Notebook and navigate to the new Nbextensions tab: Enable the extensions you want and enjoy the productivity benefits. (If you don’t see a tab, open a notebook and click Edit > nbextensions config) The enabled extensions can be seen in the toolbar in a notebook: Slightly Longer Version If that isn’t satisfying enough for you, below are some details about Jupyter notebook extensions. I’ve also included my top 5 to get you started. Jupyter Notebook extensions are simple add-ons that extend the basic functionality of the notebook environment. Written in JavaScript, they do things like autoformat your code or send a browser notification when a cell has completed. Extensions currently only work in Jupyter Notebooks (not Jupyter Lab). Why use these extensions? Jupyter Notebooks are great tools for teaching, learning, prototyping, exploring, and trying out new methods (or even in production at Netflix). However, vanilla notebooks are limited in features which can make working in them frustrating. While Jupyter Notebook extensions don’t completely solve the problem, they do add a few benefits that will make your work easier. Which to Use Following are the 5 Jupyter Notebook extensions I use most often: Once you start getting dozens of cells in one Jupyter Notebook, it can be difficult to keep track of them all. The Table of Contents solves that problem by adding a linked TOC that can be positioned anywhere on the page: You can also use the extension to add a linked table of contents at the top of the notebook. This even shows which cell is selected and which is running: We should all write pep8 compliant code, but sometimes you get caught up in an analysis and it’s hard to stick to best practices. When you’re done writing that amazing plot, this extension allows you to simply click the gavel and automatically format your messy code. Like the best add-ons, this one accomplishes a time-consuming and tedious task with a simple click, enabling you to focus on thinking through problems. The variable inspector shows the names of all variables you’ve created in the notebook, along with their type, size, shape, and value. This tool is invaluable for data scientists migrating from RStudio or if you don’t want to have to keep printing df.shape or can’t recall the type of x. I often find myself trying to figure out how long a cell took to run or when I last ran a notebook that has been open for days. ExecuteTime takes care of both of those by showing when a cell finished and how long it took. There are better methods for timing (such as the %%timeit cell magic) but this is easy to implement and covers all cells in the notebook. Although some of us like to see the hard work that goes into an analysis, some people just like to see the results. The Hide input all extension allows you to instantly hide all the code in the notebook while keeping outputs. The next time someone says they just want to see the results, you have a single click solution. (Although you should always examine the code). These are just the extensions I find myself using the most often and you’re not limited to five. Check out the whole list by installing the extensions and opening a notebook (the notebook shown here is on GitHub)! (If you are up for it, you can even write your own extension. Documentation is pretty scarce, but here’s the basic structure.) Conclusions Install the Jupyter Notebook extensions, spend some time figuring out which ones are useful to you, and improve your productivity. While none of these are life-changing, they all add just enough benefit to be worthwhile, cumulatively saving you hours of valuable development time. Although you’ll probably want to put some time into learning an IDE if you are writing production code (I’m liking VS Code), Jupyter Notebooks are still an integral part of a data science workflow. If you are going to be using this environment, you might as well get the most from your tools. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Jupyter Notebook Extensions,3,published,27163,877,5.473204104903079,14,1,1,1,0,0
25,843,170.04266644199075,137,https://towardsdatascience.com/how-to-write-a-jupyter-notebook-extension-a63f9578a38c,6,Towards Data Science,2018-12-08 14:41:00,22.0,7,1329,2018-11-22 19:51:00,"['Data Science', 'Programming', 'Jupyter Notebook', 'Towards Data Science', 'Education']","How to Write a Jupyter Notebook Extension Make the Jupyter Notebook your playground Jupyter Notebook Extensions are simple add-ons which can significantly improve your productivity in the notebook environment. They automate tedious tasks such as formatting code or add features like creating a table of contents. While there are numerous existing extensions, we can also write our own extension to do extend the functionality of Jupyter. In this article, we’ll see how to write a Jupyter Notebook extension that adds a default cell to the top of each new notebook which is useful when there are libraries you find yourself importing into every notebook. If you want the background on extensions, then check out this article. The complete code for this extension is available on GitHub. The final outcome of the Default cell extension is shown below: (If you don’t yet have Jupyter Extensions, check out this article or just run the following code in a command prompt: pip install jupyter_contrib_nbextensions && jupyter contrib nbextensions install and then start a new notebook server and navigate to the extensions tab). Unfortunately, there’s not much official documentation on writing your own extension. My tactic was to read the other extensions, copy and paste copiously, and experiment until I figured it out. This Stack Overflow question and answer provided the basic code for this extension. Structure of a Jupyter Notebook Extension There are 3 parts (at minimum) to any Jupyter Notebook extension: (We can also have more functions in other files or css for styling). These 3 files should live in a single directory which we’ll call default_cell . This folder, in turn, needs to be in the nbextensions subdirectory of the jupyter_contrib_extensions library (what you installed with pip. To find the location of a library installed with pip run pip view package. ) My jupyter_contrib_extensions directory is: So the file structure looks like (with nbextensions as shown above): When you run jupyter notebook, Jupyter looks in this location for extensions, and shows them on the extensions tab on the server: When you make a change to the extension files while developing and you want to see the effects in a Jupyter Notebook,you need to run the command jupyter contrib nbextensions install to rewrite the Jupyter config files. Then, restart the notebook server to see your changes. With the details out of the way, let’s go through the three files we need. YAML (YAML Ain’t Markup Language) is a human-readable standard for writing configuration and header files. The YAML file describes the extension to the Jupyter Extensions Configurator to be rendered on the extensions tab. This is a fairly easy file to copy + paste and modify to our needs. This is then rendered nicely on the NBExtensions tab in the notebook: The compatibility indicates the versions of Jupyter for which the extension works. I just added all of them (3.x — 6.x) and it seemed to work fine! This is the heart of the application where the actual logic for the extension lives. Jupyter Notebooks are run in the browser, which means that extensions must be written in Javascript, the language of the web. It can be a little difficult to figure out what commands to use to make the notebook do what you want. One way to experiment is using the Chrome developer tools (cntrl + shift + i on Windows) or right-click > inspect. With the developer tools open, we can use the console tab to run commands. Try opening the developer tools in a Jupyter Notebook and play with options that start with Jupyter.notebook. Any commands you enter will have to be in Javascript. An example of this behavior can be seen in the clip below. I open up the developer tools and then run a few commands to execute cells, insert a new cell, and select the previous cell. Developing the Javascript code required a lot of experimenting like this! It also helps to read the other extensions to figure out what you need to do. The final main.js is below: The most important part of the code is the add_cell function. This adds a code cell above the currently selected cell with the python code written in the parenthesis (in the set_text call). This is where the default code cell should be defined. The function then executes the cell and selects the one below. The load_ipython_extension function first checks the number of cells in the notebook. If there is only one cell — a new notebook — it calls the add_cell function which places the default cell at the top of the notebook. It then runs the defaultCellButton function which places a button on the Jupyter Notebook toolbar. We can use this button to add and run the default cell above the currently selected cell. This could be useful when we have an already started notebook and we want our normal imports. There is an almost unlimited number of tasks we could accomplish with Javascript in the browser. This is only a simple application, but there are many more complex Jupyter extensions (such as the Variable Inspector) that demonstrate more of the capabilities. We can also write applications in Javascript that call on Python scripts for even greater control. A readme markdown file should be familiar to anyone with even a little experience programming. Here is where we explain what our application does and how to use it. This is displayed on the extensions tab: (The actual code is pretty boring but for completeness, here it is) Once you have the three required files, your extension is complete. To see the extension working, make sure the default_cell directory is in the correct location, run jupyter contrib nbextensions install and start up a Jupyter Notebook server. If you navigate to the NBExtensions tab, you will be able to enable the extension (if you don’t have the tab, open a notebook and got to Edit > nbextensions config). Start up a new notebook and see your extension at work: This extension isn’t life-changing, but it might save you a few seconds! Conclusions Part of the joy of gaining computer literacy is realizing that if you want to accomplish something on a computer, chances are that you probably can with the right tools and willingness to learn. This small example of making a Jupyter Notebook extension demonstrates that we are not limited by what we get out of the box. We just need a few lines of code to accomplish our goals. Hopefully, either this extension proves useful to you or inspires you to write your own. I have gotten a lot of use from extensions and am looking forward to seeing what else people can develop. Once you do develop an extension, share it so others can marvel at your code and benefit from your hard work. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. main.js : The Javascript code for the extension itself README.md : A markdown description of extension description.yaml : A configuration file read by Jupyter",How to Write a Jupyter Notebook Extension,7,published,6040,1310,0.6435114503816793,15,1,1,1,0,0
20,931,168.7683884073264,102,https://medium.com/p/please-steal-my-articles-476e8e5d1ff2,13,None,2018-12-09 21:16:00,47.94,6,430,2018-12-09 19:10:00,"['Data Science', 'Education', 'Writing', 'Responsibility', 'Community']","Please Steal My Articles A personal license for improving the data science community When Time Warner’s CEO heard Game of Thrones was the most pirated TV show in the world, he said this was “better than an Emmy”. The show’s director similar comments, saying that the illicit downloads created a “buzz” around the show. Instead of pursuing the perpetrators to the full extent of the law, HBO took a soft stance and let the downloads continue. While this may seem counterintuitive, the thinking went the more people who found the show through any means, the more talk about it there would be, leading to a greater number of paying customers. Rather than spending fortunes trying to stop the inevitable, HBO accepted the piracy as a positive. While there are certainly other factors at play, this decision seems to be wise in light of the record viewership Game of Thrones would attain. When I found out my articles were being “pirated” — posted on websites under different authors (or even fake personas) without my permission — my reaction was nearly identical to that of HBO: go for it. My reasoning is simple: while I don’t write for pay, I write for a more fulfilling reward: helping to teach others and being able to watch them succeed. Anytime someone reads my articles and learns something, regardless of whether they know I wrote the article, the world is a better place. Maybe this view strikes you as naive and you think I sound idealistic. My response is we could use more idealistic people who do things not for personal gain, but to make enrich the lives of their fellow humans. Sure, I could complain about my work being stolen (freebooted), but unauthorized reposting of my articles will happen no matter how zealously I try to stop it. Instead of wasting my time and energy, I’ve decided to embrace this stealing, not because I think you should take things that aren’t yours, but because it only benefits the data science community. Therefore, here it is: go ahead and steal my work if you so desire. Spread it far and wide with or without my name attached! The Google Chrome Extension Sci-Hub links is one of my favorite tools. If you end up on a locked journal article with a Document Identification Number (DOI), click on the extension and magically get transported to the PDF. This access is provided through Sci-Hub, a quasi-legal repository of paywalled journal articles and books that aims “to remove all barriers in the way of science.” Whether or not you think this is legal, you can’t argue with the idea that there should not be arbitrary barriers to advancing human progress. What I love about data science is how accessible it is: there are no insurmountable impediments— such as a need for college — that prevent you from entering and contributing to the field. You can learn everything you need online and take free courses teaching cutting-edge techniques. This is an unalloyed good for data science, and it is part of my responsibility as a practicing data scientist to ensure it becomes even more accessible. While I could have continued on an academic path and written articles read by a dozen people at most, I decided this would not be satisfying because I wouldn’t be able to help the general public get into data science. I’m not denigrating academics, and basic research certainly benefits the public once it filters down into accessible tools (such as Scikit-Learn for machine learning or Keras for deep learning). Nonetheless, I think academics should do more to make their work understandable to the public. Science is most effective when it can change people’s minds and that always comes down to communication. Neil deGrasse Tyson is the best-known physicist in the world, not because he does the most advanced work, but because he has made it his career to communicate physics findings to a wide audience. The general public clearly has a desire to behold the wonders of physic — look at the popularity of Cosmos — but don’t want to spend hours trying to decipher esoteric papers. In much the same way, there is an insatiable hunger to learn data science but people don’t have the time to read through all the journal articles (most of which will be obsolete in a few years anyway) or the resources to attend college. Fortunately, there are many data science communicators working to tear down those barriers. The biggest hurdle to getting starting in most technical fields is access; however, in data science, everyone has access to the most up-to-date methods and technology (such as TPUs through Google Colab). The next hurdle is the vocab, it’s easy to get discouraged when you can’t understand what anyone is talking about. In my opinion, there is no need to use a long word when several shorter ones can better explain the concept. Using longer words doesn’t make you sound smarter, it just means whatever you say will be inaccessible to most people. Sure you may be technically right and feel a brief moment of superiority, but if no one understands you, they will quickly stop listening. Ultimately, I view it as my responsibility to encourage and help as many people as I can learn data science. I believe a field only grows stronger the more different people that can participate. Data science holds an immense amount of power in a data-driven world, but if that power is wielded by a small group of people, the benefits will be limited. Again, this may be my idealism, but this is the foundational belief that gets me out of bed in the morning. The short answer: anything. The slightly longer answer: I would prefer you didn’t run my articles with ads (thanks Medium, the subscription model is much better!) but even if you do, I’m not going to stop you. A credit is nice or a link to the original article, but again, it doesn’t really matter if you forget. You can copy and paste my articles anywhere (if you want to print a book version that would be neat) and translate them into any language you speak! Fundamentally, I don’t believe I really own my articles. Sure, I spend dozens of hours writing the code, making Jupyter Notebooks, writing articles, and going through the painful process of editing, but the ideas I’m using are gathered from many different people and I can’t claim sole responsibility. Maybe physics proceeds by standing on the shoulders of giants, but data science advances on the backs of tens of thousands of anonymous contributors — on GitHub, Stack Overflow, Medium, and everywhere else people are motivated to contribute for the good of the whole. As Thomas Jefferson put it “ Knowledge is like a candle. Even as it lights a new candle, the strength of the original flame is not diminished.” (The original quote was slightly different). In the case of my articles being spread without my consent, I’m no worse off as a result, and the data science community is better. If we are being honest, my articles should read: “written by the data science community for the data science community.” Sure, it’s nice to get recognition, but it’s even better to know that your work is helping people even if they don’t know you wrote it. As always, I welcome criticism and constructive feedback. I can be reached on Twitter @koehrsen_will or through my personal website at willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",Please Steal My Articles,4,published,897,1410,0.6602836879432624,0,1,1,0,0,0
26,1400,163.1270107921991,282,https://towardsdatascience.com/introduction-to-interactive-time-series-visualizations-with-plotly-in-python-d3219eb7a7af,5,Towards Data Science,2018-12-15 12:39:00,29.2,9,3959,2018-12-15 07:52:00,"['Data Visualization', 'Data Science', 'Python', 'Education', 'Towards Data Science']","Introduction to Interactive Time Series Visualizations with Plotly in Python Up and running with the powerful plotly visualization library There comes a time when it’s necessary to move on from even the most beloved tools. Matplotlib has served its purpose of quickly creating simple charts, but I’ve grown frustrated with how much code is required to customize plots or do seemingly easy things like get the x-axis to correctly show dates. For a while, I’ve been looking for an alternative — not a complete replacement as Matplotlib is still useful for exploration — ideally, a library that has interactive elements and lets me focus on what I want to show instead of getting caught in the how to show it details. Enter plotly, a declarative visualization tool with an easy-to-use Python library for interactive graphs. In this article, we’ll get an introduction to the plotly library by walking through making basic time series visualizations. These graphs, though easy to make, will be fully interactive figures ready for presentation. Along the way, we’ll learn the basic ideas of the library which will later allow us to rapidly build stunning visualizations. If you have been looking for an alternative to matplotlib, then as we’ll see, plotly is an effective choice. The full code for this article is available on GitHub. You can also view the notebook with interactive elements on nbviewer. The data used in this article is anonymized building energy time-series data from my job at Cortex Building Intelligence. If you want to use your web dev skills to help buildings save energy, then get in touch because we’re hiring! Introduction to Plotly Plotly is a company that makes visualization tools including a Python API library. (Plotly also makes Dash, a framework for building interactive web-based applications with Python code). For this article, we’ll stick to working with the plotly Python library in a Jupyter Notebook and touching up images in the online plotly editor. When we make a plotly graph, it’s published online by default which makes sharing visualizations easy. You will need to create a free plotly account which gives you 25 public charts and 1 private chart. Once you hit your quota, you’ll have to delete some of the old charts to make new ones (or you can run in an offline mode where images only appear in the notebook). Install plotly ( pip install plotly ) and run the following to authenticate the library, replacing the username and API key: The standard plotly imports along with the settings to run offline are: When we make plots in offline mode, we’ll get a link in the bottom right of the image to export to the plotly online editor to make touch-ups and share. Plotly (the Python library) uses declarative programming which means we write code describing what we want to make rather than how to make it. We provide the basic framework and end goals and let plotly figure out the implementation details. In practice, this means less effort spent building up a figure, allowing us to focus on what to present and how to interpret it. If you don’t believe in the benefits of this method, then go check out the dozens of examples such as the one below made with 50 lines of code. For this project, we’ll be using real-world building data from my job at Cortex Building Intelligence (data has been anonymized). Building energy data presents intriguing challenges for time-series analysis because of seasonal, daily, and weekly patterns and drastic effects from weather conditions. Effectively visualizing this data can help us understand the response of a building and where there are chances for energy savings. (As a note, I use the terms “Power” and “Energy” interchangeably even though energy is the ability to do work while power is the rate of energy consumption. Technically, power is measured in kilowatts (KW) and electrical energy is measured in KiloWatt Hours (KWh). The more you know!) Our data is in a dataframe with a multi-index on the columns to keep track of the sensor type and the sensor number. The index is a datetime: Working with multi-index dataframes is an entirely different article (here are the docs), but we won’t do anything too complicated. To access a single column and plot it, we can do the following. The default plot (provided by matplotlib) looks like: This isn’t terrible, especially for one line of code. However, there is no interactivity, and it’s not visually appealing. Time to get into plotly. Much like Bokeh (articles), making a basic plot requires a little more work in plotly, but in return, we get much more, like built-in interactivity. We build up a graph starting with a data object. Even though we want a line chart, we use go.Scatter() . Plotly is smart enough to automatically give us a line graph if we pass in more than 20 points! For the most basic graph, all we need is the x and y values: Then, we create a layout using the default settings along with some titles: (We use dict(x = 'value') syntax which is the same as {'x': 'value'} ). Finally, we can create our figure and display it interactively in the notebook: Right away, we have a fully interactive graph. We can explore patterns, inspect individual points, and download the plot as an image. Notice that we didn’t even need to specify the axis types or ranges, plotly got that completely right for us. We even get nicely formatted hover messages with no extra work. What’s more, this plot is automatically exported to plotly which means we can share the chart with anyone. We can also click Edit Chart and open it up in the online editor to make any changes we want in an easy-to-use interface: If you edit the chart in the online editor you can then automatically generate the Python code for the exact graph and style you have created! Improving Plots Even a basic time-series plot in Plotly is impressive but we can improve it with a few more lines of code. For example, let’s say we want to compare the steam usage of the building with the energy. These two quantities have vastly different units, so if we show them on the same scale it won’t work out. This is a case where we have to use a secondary y-axis. In matplotlib, this requires a large amount of formatting work, but we can do it quite easily in Plotly. The first step is to add another data source, but this time specify yaxis='y2'. (We also add in a few other parameters to improve the styling which can be seen in the notebook). Then, when we create the layout, we need to add a second y-axis. When we display the graph, we get both steam and energy on the same graph with properly scaled axes. With a little online editing, we get a finished product: Plot Annotations Plot annotations are used to call out aspects of a visualization for attention. As one example, we can highlight the daily high consumption of steam while looking at a week’s worth of data. First, we’ll subset the steam sensor into one week (called steam_series_four) and create a formatted data object: Then, we’ll find the daily max values for this sensor (see notebook for code): To build the annotations, we’ll use a list comprehension adding an annotation for each of the daily maximum values ( four_highs is the above series). Each annotation needs a position ( x , y) and text: (The <br> in the text is html which is read by plotly for displaying). There are other parameters of an annotation that we can modify, but we’ll let plotly take care of the details. Adding the annotations to the plot is as simple as passing them to the layout : After a little post-processing in the online editor, our final plot is: The extra annotations can give us insights into our data by showing when the daily peak in steam usage occurs. In turn, this will allow us to make steam start time recommendations to building engineers. The best part about plotly is we can get a basic plot quickly and extend the capabilities with a little more code. The upfront investment for a basic plot pays off when we want to add increased functionality. We have only scratched the surface of what we can do in plotly. I’ll explore some of these functions in a future article, and refer to the notebook for how to add even more interactivity such as selection menus. Eventually, we can build deployable web applications in Dash with Python code. For now, we know how to create basic — yet effective — time-series visualizations in plotly. These charts give us a lot for the small code investment and by touching up and sharing the plots online, we can build a finished, presentable product. Although I’m not abandoning matplotlib — one-line bar and line charts are hard to beat — it’s clear that using matplotlib for custom charts is not a good time investment. Instead, we can use other libraries, including plotly, to efficiently build full-featured interactive visualizations. Seeing your data in a graph is one of the joys of data science, but writing the code is often painful. Fortunately, with plotly, visualizations in Python are intuitive, even enjoyable to create and achieve the goal of graphs: visually understand our data. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Introduction to Interactive Time Series Visualizations with Plotly in Python,10,published,13558,1806,0.7751937984496124,0,1,1,1,0,1
22,205,161.7767560132176,34,https://medium.com/p/regretting-the-present-3f83c21ff1f0,0,None,2018-12-16 21:03:00,35.42,8,147,2018-12-16 18:48:00,"['Psychology', 'Education', 'Writing', 'Advice and Opinion']","Stop Regretting the Present Don’t let an imagined present be the enemy of enjoying reality Case Western Reserve University, my alma mater, is known by its students primarily for being everyone’s second choice. Case is slightly less prestigious than the Ivies or MIT, and almost all the students were there because they had not got into their first selection. The effects of this were noticeable: claiming we had no school spirit was an understatement. There was no pride in the university and the only unifying force was a desire to be somewhere else. The effects went beyond meager attendance at sporting events: it meant students spent four years — supposedly the best in their lives — wishing they were at a different school and regretting the choices that led them to Case. This phenomenon — wanting to have made different decisions in the past, and, as a consequence, not enjoying where you currently are — I call regretting the present. We hear people express a regret for the present constantly in statements such as “if only I had taken that other job”, “if only I had not married my spouse”, or “I always think of how happy I would be in another city.” These capture the same basic idea: somewhere along the line these individuals made a decision they wish they could correct and they know they would be better off if only they had chosen differently. Regretting the present is one of the most insidious mental exercises we can subject ourselves to. Not only do we criticize our past selves, we also disregard our current selves, and even project into the future by claiming that we will only keep making the same mistakes. The end effect is rather than concentrating on reality in front of us, we lose ourselves in self-deprecating fantasy lives we would have if only we had made different choices. Even at the age of 22, I have a long series of decisions I replay over and over in my head, imagining alternative scenarios. These range from conversations I was too nervous to start, activities I turned down because I had to work on homework, and of course, where I went to college. Recently, I’ve decided this has to stop; if I already have this many regrets, how will I be able to live with myself at 40 with another two decades of decisions behind me? There are no possible benefits to this constant criticism of my past decisions (this is decidedly different than objective reflection) and it means I am not fully invested in the present. What is my plan of attack? I’ve adopted a two-part mindset I remember whenever I start to replay past decisions: These are a little ephemeral, so let me explain my reasoning. From an evolutionary standpoint, I struggle to understand why we have a tendency to play out these fantasies of past decisions. It seems like in terms of survival, by definition, we wouldn’t have made it past the decisions we messed up, and, if we survived, then we made the right choice. Perhaps the problem is that now our bad choices don’t kill us (at least not immediately) leaving us plenty of time to mull over what we could have done differently. Although a bad choice doesn’t mean death (which is decidedly a good thing), it means we are stuck replaying and regretting our choices. Confounding the problem is the proliferation of choices in western society. As Barry Schwartz describes in his book The Paradox of Choice, we have created a society with a nearly unlimited number of options in all situations from the small — which shampoo to purchase — to the life-defining — where to go to college and what to study. However, rather than leading to a utopia, the limitless choices have led to increased rates of depression and anxiety as well as decreased rates of life satisfaction. Schwartz provides copious evidence indicating that the more choices we have, the longer we take to make a decision and the greater the regret when we finally choose. This is particularly pronounced for us maximizers, those who want to get the absolute best in every situation. Compared to satisficers, who can do with the merely good, maximizers will spend an inordinate amount of time making and regretting decisions. While those who maximize may make an objectively better choice, subjectively they are much worse off because of the regret they experience. Having identified my natural tendency to maximize, I have been trying to change into (somewhat of) a satisficer. I’ve been relatively successful in adopting the concept of “don’t let the perfect be the enemy of the good.” This is evidenced by my ability to release articles with only two edits instead of four, and accepting work that is at 95% instead of spending double the time to get it to 100%. The next step is to take the same concept and apply it to making decisions. There will always be better choices leading to objectively superior situations, but the effort required to evaluate all the options means I am better off accepting a slightly less-optimal option and then not rethinking my decision. This is a satisficer’s way of looking at the world: there could be better choices out there, but once a decision has been made, they don’t matter anymore. Moreover, I have been trying to strike the very idea of good or bad choices from my mindset. Classifying a past choice as negative only serves to reinforce the idea that you could be in a better place and gives your brain the opening it needs to start constructing harmful imagined presents. Instead, every decision is simply a decision and we need to stop and appreciate where we are rather than thinking of where we could be. (There is something to be said for reflecting on what one might have done differently or drawing lessons from your past, but this becomes detrimental when we start passing judgment on our past selves. It’s one thing to say “maybe if I went back I would have studied history instead of engineering,” but it has a different connotation when phrased as “I would be so much better off if I hadn’t been so naive and pursued a history degree instead.” The first is relatively neutral, while the second can only be harmful to your current self. ) Just as striving for perfection on an assignment can prevent it from ever being completed, striving to make 100% optimal decisions and rethinking those that you’ve already made means you will never appreciate the present. The Alternatives are Not Always Better The second strategy initially may seem morbid: end any sentence that begins: “If only I had done…” with “I would have been hit by a bus.” Nonetheless, the point is not about the exact outcome, but rather that you don’t really know how things would have turned out had you made different decisions. While we like to think of events as a straightforward narrative, the real world is random, and a different past decision would just as likely lead to disaster as to success. There is a common fallacy, perpetrated by traditional history books, that certain outcomes are destined to happen and successful people were shaped just right by their lives to come out on top. The mistake is that these books are always looking backwards, and in hindsight, it’s very easy to draw a straight line through what was actually a random series of occurrences. This tendency to think in linear stories is known as the narrative fallacy and affects our thinking by convincing us outcomes are destined and cannot be changed. Reality laughs at this viewpoint: nearly everything in life, from who we marry to where we end up living, is ultimately random (see The Drunkard’s Walk: How Randomness Rules Our Lives). There is no coherent thread of events in the objective world until we look back and construct one. Why this matters is that the narrative fallacy leads us to imagine different paths for our lives that are straightforward, with us ending up exactly where we wanted (as if we know where that is). In spite of our strong beliefs, it is ridiculous to think things would play out anything like what we imagine. Any potential fantasy of a different series of life choices you dream up will be better than your current situation; that’s the definition of a fantasy. You’ll imagine a linear path from one good decision to the next, culminating in a dream existence with no worries. In reality, even if you had made all the right choices, there is still the chance you could start your dream job by walking out the door and into the maw of an errant mass transportation vehicle. That is just the randomness of life at work. Telling yourself that you would have been better off had you made different decisions is assuming that reality is linear and proceeds smoothly along a predestined path. The strategy here is simple: combat those tempting fantasies by ending them in disaster. When your mind starts revisiting past decisions and creates a rosier future, put a stop to it with a random event. Going Forward I’m well aware that it’s easier to act yourself into a new way of thinking than to think yourself into a new way of acting. There is only so much (pretty much nothing) that mindlessly repeating a mantra can accomplish. To create lasting change, you need to put the ideas into practice. Part of my solution is right here: writing about my problem (thanks for listening). Writing forces you to think critically, and when I objectively examine my practice of regretting the present, I see that there are no benefits and that I am being irrational by constructing these straightforward stories of imagined success. (I’ve also started ending other people’s “I should have” sentences with “but then I would have been hit with a bus.” This strategy is not advisable in all situations, but if anyone objects, tell them it’s for their own benefit.) What’s more, human memories are — for better and for worse — extremely malleable. Although I use to view my college experience — both where I went to school and what I studied — in a negative light there’s nothing stopping me from flipping the script. If I had gone to U Chicago (my first choice), I would have been a small fish in a very large pond and would not have had the chance to grow to love Cleveland. My degree, while not directly applicable to my current job, taught me how to solve challenges and gave me a background in mechanical properties, a useful skill in my data science role at Cortex Building Intelligence. While you should consider your decisions to be set in stone (the ability to reverse choices actually makes us regret them more), your memories don’t have to be. Instead of weaving a narrative about what you could have done, try taking what you did do and putting it in a positive light. Ultimately, there are always going to be many options, but you can only choose one, and those choices have led to a single present. There’s no use spending effort imaging a better present, and instead, appreciate your reality. I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Any sentence that begins: “If only I had done…” must end with “I would have been hit by a bus.” Don’t let imagined presents stop you from enjoying reality",Stop Regretting the Present,4,unlisted,415,2128,0.09633458646616541,0,1,0,0,0,0
24,880,160.81838948700232,188,https://towardsdatascience.com/docker-without-the-hassle-b98447caedd8,3,Towards Data Science,2018-12-17 20:04:00,31.63,5,2170,2018-12-17 08:43:00,"['Docker', 'Data Science', 'Open Source', 'Education', 'Programming']","Docker for Data Science Without the Hassle How to use repo2docker to automatically build Docker images If you aren’t using docker for data science, then you probably should be. Docker is best thought of as a light virtual machine that runs images — environments — containing all the libraries, data, and code for a project. Docker is great for reproducible data science as it makes sharing code much easier: instead of sending code and requirements, you can make a Docker image and your project will just work on anyone else’s machine when they run your image. This solves the “dependency hell” issues that inevitably arise whenever people try to share code: “it worked on my machine” is now a phrase of the past thanks to Docker. Even with the clear benefits of Docker, many data scientists still haven’t embraced the technology. Making a Docker image requires writing a Dockerfilewith the commands to build an image. While this isn’t difficult, it’s far easier to just pip freeze > requirements.txt a Python environment and be done with it (not the best practice but better than nothing). Fortunately, thanks to the repo2dockertool from Project Jupyter, the process to create a Docker image from a GitHub repository is now just a single line of code. If that’s enough for you, then go get started! Otherwise, read on for a few more details. Repo2Docker works by inspecting a GitHub repository for any number of configuration files such as requirements.txt for Python libraries, environment.yml for Conda, or setup.py for building Python packages. It then translates these into a Dockerfile and builds the image. Finally, it runs the image and starts a Jupyter Server where you can access Jupyter Notebooks (or even RStudio). All of this is done in one command! For more info on the details of repo2docker, check out: Intro post, documentation, or the GitHub repository (contributions are always welcome). (repo2docker is also the technology behind binder, another project from Jupyter worth checking out.) The command and eventual output will look like this: Run a notebook and marvel at how easy it was to use docker with repo2docker Once the docker container is running, open a new shell and type docker ps to see the container process. Get the CONTAINER_ID and then run docker exec -it CONTAINER_ID /bin/bash to open an interactive shell in the running container. From the shell, you can do anything you do at the command line such as listing files, running python scripts, or monitoring processes. When you are done with your session, you can shut down the Jupyter server and the container with ctrl + c or docker stop CONTAINER ID. The good news is when we want to restart the container, we don’t have to re-runrepo2docker repo-link again. The entire environment is saved as a docker image on our machine which we can see with docker image list. To start this container again, select the IMAGE ID and run the command: This will start the container, publish the container’s port 8888 to the host port 12345, and run a jupyter notebook accessible at 127.0.0.1:12345. You can once again access the Jupyter Notebook in your browser with all the requirements ready to go. (Thanks to this issue on GitHub for providing this solution. Also, see the docs on docker for more options) repo2docker is under continuous work, and there is an active pull request for automatically using a pre-built image if the configuration files in the GitHub repository have not changed. The above command will always work though and can also be used for images not created through repo2docker . Once you learn the basics from repo2docker , try working through some of the Docker guides to see how to effectively make use of Docker. Conclusions Much as you don’t have to worry about the details of backpropagation when building a neural network in Keras, you shouldn’t have to master complex commands to practice reproducible data science. Fortunately, the tools of data science continue to get better, making it easier to adopt best practices and hopefully encouraging more diverse people to enter the field. Repo2docker is one of these technologies that will make you more efficient as a data scientist. Sure, you can be the old grouchy man: “I spent all that time learning docker and now the young kids can’t even write a dockerfile” or you can take the high road and celebrate the ever-increasing layers of abstraction. These layers separate you from the tedious details allowing you to concentrate on the best part of data science: enabling better decisions through data. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Install repo2docker: pip install jupyter-repo2docker Run repo2docker repo-linkand to get an image in a few minutes: Make sure docker is running. If docker run hello-world shows the message Hello from Docker! then you’re good to go. Install repo2docker with pip install jupyter-repo2docker . Confirm it works by typingrepo2docker -h to bring up the help options. Find a GitHub repository that has at the minimum a requirements.txt file. For example, I’m using the repo for feature-selector, a tool I made for machine learning feature selection which has a setup.py file. Run the magic linerepo2docker repo-link which automatically creates a new docker image, installs all the required dependencies, and finally, launches a jupyter serves in the environment. This can take 5–10 minutes, but keep in mind it’s saving you hours of work and frustration. When the command has finished, copy the url and navigate to the Jupyter notebook running in the docker container.",Docker for Data Science Without the Hassle,7,published,6861,1075,0.8186046511627907,0,1,1,0,0,0
21,172,153.8048004670949,12,https://medium.com/p/on-blame-71a323a56c8a,0,None,2018-12-24 20:23:00,24.22,8,62,2018-12-24 15:26:00,"['Education', 'Philosophy', 'Reason', 'Thinking', 'Self-awareness']","On Blame Where we should look to explain negative trends and how we can improve the world Recently, this chart has started showing up in the news: Originally appearing in a Mother Jones article (original data for your own analysis), I have seen this chart accompanied by thoughts like “thanks e-cigarette companies for taking us back to the 1950s” or “I thought we already solved this problem but evil cigarette companies are back at it selling poison to our youth.” While there is nothing more appealing to humans than jumping on the moral outrage bandwagon, recently I’ve tried to stop myself from immediately falling in line by thinking rationally about where the blame for “bad” events should really be placed. If we take a minute to trace the actual source of these developments, we arrive at an answer both more mundane and more disturbing than our instinctual reaction has us believe. (I’m not in any way trying to downplay the problem of e-cigarette use. The data are alarming — see for yourself — and I am worried about the potential undoing of decades of progress in public health. My point in this article is to show that blaming an “other” gets us no closer to solving the problem.) Let’s start the blame seeking by considering who started this whole business in the first place: the cigarette companies. Philip Morris International (PMI) is one of the largest members of the worldwide cigarette market, with $67.7 billion in sales, nearly $8 billion in profit, and almost 20% market share. The answer for who to point the finger at is simple right? The ones on top must be held accountable, in this case, Louis Camilleri (Chairman) and André Calantzopoulos (CEO). However, that’s not quite right, because even though the CEO and Chairman are the public faces of the company, they must report to the board of directors, so we need to shift the burden of guilt to its members (the faces of evil). Unfortunately, we can’t stop there. While the board may push for such evil tactics such as deliberately advertising to teens, as a publicly-held company, the board of Philip Morris International is elected by and answers to one group: the shareholders. What’s the problem you may ask? Only rich people hold stocks, and they are to blame for poisoning our youth you are tempted to shout. While I have no doubt some very wealthy individuals do own PMI stock, there is another group closer to home that holds a significant fraction of PMI. As a large, relatively stable company, Philip Morris stock is a safe option for long-term investments with little risk, which just happens to be what retirement account investment managers look for. Here’s the kicker: 3 of the most popular retirement mutual funds (measured by managed assets in 401k retirement plans) have substantial investments in Philip Morris International. We’ve traveled down the Yellow Brick Road of blame, pulled back the curtain on the wizard, and revealed that the ultimate responsibility for big tobacco, hundreds of millions of early deaths, and now, the rise in teen e-cig use is the workers and retirees of America. The next time you see an ad for e-cigarettes, be assured that it was paid for by your parents, your grandparents, and yes, even your hard-earned retirement money. The Vanguard Institutional, Fidelity 500, and American Funds EuroPacific Growth mutual funds hold hundreds of millions of dollars in PMI. Vanguard, the largest provider of mutual funds in the world is the single largest holder of Philip Morris International (yes, in case you are asking, my own retirement and personal investment accounts hold PMI within mutual and index funds). But wait you cry, hoping to at least distance yourself from the best-known purveyor of electronic cigarettes (and one of the worst offenders when it comes to advertising to children), Juul is privately owned! Well, on Thursday, December 19, Altria Group Incorporated (makers of family-friendly products such as Marlboro) spent $13 billion to acquire a 35% stake in Juul. But, you probably already knew that because you own Altria Group as part of your Vanguard or Fidelity mutual fund. Moreover, as a shareholder, you were probably thrilled at the news because it seems like a great investment! (Confusing matters, Altria Group Inc. was renamed from Philip Morris Companies in 2003 to disassociate the company from the negative name of Philip Morris in America. Philip Morris International is now a separate company and Philip Morris USA is part of Altria Group). Why This Matters and the Good News Whenever we are confronted with a moral injustice, our immediate reaction is to search for someone to blame. Thanks to our natural tendency towards xenophobia (fear of outsiders) this is almost always an “other”, someone completely unlike our moral, upstanding selves. Let’s consider a typical example. Americans routinely report they think the world is getting more violent when all of the evidence shows the world is actually more peaceful now than ever before and continues to get less violent. When asked the reason for this discrepancy, most people will blame the news: “it’s the media portraying violence which distorts our view of the world”. It may be true the media (especially local news) focuses undue attention on negative events, but it’s not the media’s job to be objective. The media has one objective, to entertain the public, and apparently, people continue to be drawn to terrible stories despite a drastic decline in the number of bad events happening. The full responsibility for our corrupted worldview lies with us, the consumers of the media. If everyone suddenly demanded the media portray the world as it really is and watch the channels that did the best job of being objective, then the tenor of the news would rapidly change. Outrage and blaming of outsiders may be the natural response to moral trespasses, but just because something is natural does not make it right. Xenophobia, violence, and hatred may be some of our most natural tendencies, but no one (save the president of the United States) openly advocates for increased levels of these. Furthermore, blaming someone else prevents you from having to do anything because you are not responsible. In short, looking for somewhere to place blame — for any situation, large or small — is a temporarily satisfying but long-term disastrous tendency (blaming someone is different than searching for a cause that can be used to prevent the next occurrence of a disaster. The former is instinctual and is about not having to take responsibility while the latter is rational and aims to improve safety). In case you are despairing by this point, there is hope. If we can avoid the blame instinct, change can happen on both an individual level and societal level. Let’s look at the two scenarios outlined above: If you don’t like supporting cigarette companies via your retirement funds, you can manage your own retirement funds, or, as I’ve started to do, explore Socially Responsible Investing (SRI), Environmental, Social, and Governance (ESG) investing, or impact investing, all of which take into account “socially beneficial” policies rather than mere profits (Swell Investing is one option). In a bit of positive news, ESG investing has exploded in popularity and now represents around $20 trillion in managed assets worldwide. Keep in mind that change is hard and requires tradeoffs. You may have to spend more time picking retirement funds and investigating company policies and accept a lower return. Real change doesn’t come without sacrifices. Furthermore, if you aren’t happy with the traditional media, there is nothing forcing you to turn on the TV or open CNN every day. Instead, you can read books such as The Better Angels of Our Nature; Enlightenment Now; and Factfulness, all of which take an objective look at the world and reach the inevitable conclusion that things are very good and continue to improve. Also, once you read these books, don’t hesitate to share the “good news” with others. For some reason, people tend to talk to each other about negative events (which just make people more afraid) rather than positive realities such as the fact that 80% of children around the world have been vaccinated or that 80% of the world has access to electricity. Real face-to-face conversations actually do change minds — just look at the extraordinary shift in public perception around gay marriage in the US if you think that people — and by extension society — can’t change. Conclusions Some lessons you only learn when people are honest with you and so consider this my attempt to be straight: most of the terrible things that happen in the world are due to not malicious hidden actors but rather to the choices we make every single day. However, my point is not to make you feel worse about yourself; my overall message is positive: you don’t have to wait a minute for someone else to come along and improve the world, you can start right now. Not to get too philosophical on you, but when you’re looking for someone to blame, start by looking in a mirror. Once you’ve figured out where the real fault lies, start making small changes. You can only directly affect your own life, but by changing your actions, you change those around you and so on until you live in a country where anyone can marry who they choose or in a world where the global poverty rate has fallen by half in the past 20 years! Once we overcome the blame instinct, we can make the world better through the only power available to us: changing ourselves. This article is based on a similar idea expressed by the late Hans Rosling in his incredible book Factfulness: Ten Reasons We’re Wrong About the World — and Why Things Are Better Than You Think. In his version, he concludes we can convince pharmaceutical companies to produce more malaria drugs for poor nations by punching our grandmothers. The book is an excellent objective look at the world and I highly recommend it as well as the GapMinder Project if you want to be less ignorant about the world). As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my website willk.online. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",On Blame,2,unlisted,256,1918,0.08967674661105318,0,1,0,0,0,0
23,472,153.009825936794,54,https://medium.com/p/what-i-learned-in-2018-30e2b5481c01,4,None,2018-12-25 15:28:00,37.92,8,380,2018-12-23 14:45:00,"['Education', 'Books', 'Learning', 'Rationality', 'Science']","What I learned in 2018 30 ideas that changed my worldview “The unexamined life is not worth living” is one of those cliches people throw out when they want to sound profound. The problem is the people who hear this, wanting to seem educated themselves, will nod and accept it without a question, leading to the spread of what is best described as “pseudo-profound dribble” (dribble being a family-friendly replacement for the correct term). In reality, a life lived without looking back is totally worth living; in fact, those who live in the present are generally happier than those who dwell on the past. As I’ve written, thinking too much about the past can lead to regret as we second-guess our decisions that didn’t go well. There are even entire philosophies — Stoicism — about living in the present and letting go of the past. Those Socrates-quoting (it was probably Socrates who uttered this quote, at least according to Plato) people don’t even think about what they are saying, let alone realize how absurd the statement is. So, now I’ve told you about the uselessness of reflection, it’s time to reflect on what I learned in 2018! I was originally going to use that Socrates quote to start this article and lecture you about the importance of looking backwards, before, in a plot twist, realizing it’s complete bunk. Fortunately, this about-face reinforces the one overarching idea I took away from 2018: we are all wrong about a lot of things a lot of the time, but, we can get less wrong. While some may view this conclusion pessimistically — if we’re just going to be wrong, why learn anything at all? — I see it as a great opportunity. Yes, we are currently wrong about many of our beliefs — including our foundational ideas — but that just means we have the chance to get less wrong as we go through life. As we accumulate experiences and learn from data, we can gain a more accurate worldview and, as we’ll see, it turns out that worldview is almost always more optimistic than we originally thought. To continue in my quest to get less wrong and because the end of the year is a time for lists, I’ve listed 30 things I learned in 2018 from books, podcasts, life, and other people. These aren’t meant to be absolute truths, and, I wouldn’t be surprised if a year from now I come back and correct the majority of these! I don’t claim these are the final beliefs I’ll hold the rest of my life, just what I currently believe. As the evidence and society changes, I’ll re-evaluate my positions to make sure they are still compatible with reality. I’ve listed the sources for these ideas in parenthesis and provided a link the first time a source appears. Books are in italics, “podcasts or short works are in quotes”, and life is something I picked up through everyday experiences or from family, friends, co-workers, etc. As a glimpse into my ever-evolving worldview, this is more personal than what I usually write but as always, if you have criticisms, objections, or your own ideas, don’t hesitate to add a comment or write to me on Twitter! With that said, here’s what I learned in 2018: As always, I welcome feedback, discussion, and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Fulfillment does not from external rewards or material accumulation but is derived internally, by doing work that you can take pride in. We create meaning in life through our work — which does not have to be a traditional job. (Drive; The Road to Character, Mastery; life) Meditation is difficult, requires a formidable amount of concentration, and only improves slowly with deliberate repetition, yet the scientifically (and experimentally) proven benefits make it a worthwhile practice (Waking Up; 10% Happier; “The Waking Up Podcast”; life) We have built a society with almost unlimited options from the smallest choices — which toothpaste to buy — to the largest — what to study — yet, rather than creating a utopia, this has made us miserable and contributed to increased rates of depression. More options mean we spend longer making decisions and more time regretting them (The Paradox of Choice; life) An incredibly valuable — yet exceedingly rare — skill is the ability to change one’s mind in light of new evidence. Changing one’s positions when the facts change is not a sign of weakness but shows mental fortitude. The entire enterprise of science is based on self-criticism and updating theories as better evidence is found. (Hitch-22; Superforecasting; Science Friction) Data are never objective. The very act of collecting information encodes biases in it that reflect the goals of those gathering the information and the measurement tools (Dataclysm; The Signal and the Noise) Morals/ethics are a product of evolution and were not invented by any particular religion. As a natural phenomenon, we can scientifically study morals/ethics to make objective claims about the right actions to take. (The Moral Landscape; The Science of Good and Evil; Behave) To make accurate predictions: constantly make forecasts and use the feedback to improve your next estimates, gather data from as many diverse sources of information as possible, express predictions with uncertainty, and change your predictions when the facts on the ground change (The Signal and the Noise; Superforecasting) The fact that living standards are getting better around the world means that our policies in areas such as poverty reduction, disease eradication, environmental protection, and human rights expansion are working. Viewing the world as it really is — rather than through a distorted lens — not only gives us a more optimistic, realistic worldview, it also shows we collectively have the ability to improve the world and should keep at this endeavor (Enlightenment Now; Factfulness) Writing is thinking: there is no better way to make sure you understand a topic than writing an article about it for a general audience. If you are struggling to understand a concept, try writing an article — even if it’s just to yourself. (Bird by Bird; life) Problems are solvable. Whenever you encounter a problem, know there is an answer as long as you’re willing to work at it. Anything not prohibited by the laws of physics can be accomplished by humans given enough time. (The Beginning of Infinity) There is no single secret to doing hard things like writing, learning a new skill, or making a positive life adjustment. There is instead a sequence of simple, actionable steps that over time add up to a major change (Bird by Bird; Behave; Deep Learning Cookbook; life) No matter your external situation, you can be miserable or in euphoria depending on your mental state and attitude. Happiness is not about what happens to us but about how we respond to it. (Waking Up; How to be Miserable; “Stoicism philosophy”; life) Free will — the idea we make choices at our own discretion — does not exist. (Behave; The Science of Good and Evil; Incognito; The Blank Slate; Predictably Irrational; “The Waking Up Podcast”) If you are by yourself, you should be working; if you aren’t working, spend your time with others. (Samuel Johnson quote, How to be Miserable) Helping others — through volunteering, writing, teaching, a public service job, or being there to listen — is the most rewarding experience a human being can have. (The Road to Character; Behave; life) The advertising model for the Internet is a terrible idea: instead of paying directly for content, we pay with our limited time and attention. A subscription model is more sustainable, more honest for all parties, and can lead to better content that is not constrained by advertiser’s agendas (“The Waking Up Podcast”; life) Capitalism may be the single best anti-poverty device ever invented. It is also effective at increasing health, raising living standards, expanding human rights, protecting the environment and increasing tolerance. While capitalism is not perfect, it’s the best way to organize an economy thus far put in place (The Rational Optimist; Enlightenment Now) There (probably) is not a single universe, but an infinite number of them — a multiverse. We (possibly) are living in a simulation and soon will reach the point where we can create our own simulated worlds. Neither of these should affect how you live your life. (Our Mathematical Universe; The Inevitable; But What if We’re Wrong; “The Waking Up Podcast”) History only appears to follow a straight path in hindsight (the narrative fallacy). History books typically draw a single path to connect events after the fact, when reality is actually a mess of random occurrences that follow no plan. (The Black Swan; But What If We’re Wrong; Predictably Irrational) A certain amount of stress is beneficial for people and systems. We should aim not just to be resilient against stress, but to learn how to gain from it. Moreover, sometimes we are better off eliminating things from our lives (tv, news) rather than adding new things — a concept known as addition by subtraction. (Antifragile; The Black Swan; Behave; life) Writing consistently improves your writing (and thinking) and makes it easier. Even bad writing is better than no writing; get your ideas down in a rough first draft without worrying about how good it is. (Bird by Bird; life) One of the most effective ways to be miserable is to reduce your exercise. Other good ways include spending as much time as possible with screens, focusing only on yourself, and revisiting past decisions (How to be Miserable; CGPGrey video “7 Ways to Maximize Misery”) The 21st century may be defined by an increasing gap between those with technical skills who can control computers (programmers) and those who whose jobs will be taken by (narrowly) intelligent machines. This is problematic because a world without meaningful work for humans would be a complete dystopia. (21 Lessons for the 21st Century) Innovation occurs most often not deep within one field, but at intersections between fields. It is in these fertile boundary grounds that ideas find applications beyond the original narrow use envisaged by the inventor. Make sure to not silo yourself within your field. (The Innovators) Humans are not born with equal abilities or opportunities, and ignoring this leads to policies that reinforce disadvantages rather than working to eliminate them. The theory that people are born the same (“blank slate”) denies all biological evidence to the contrary (The Blank Slate; Behave). Formal education is, unfortunately, necessary for most jobs, not because you will learn anything useful, but because you need a piece of paper certifying you can jump through arbitrary hoops and follow orders. Also, college can be detrimental to physical, mental, and financial health. (life) Minimalism is an enjoyable lifestyle: modern consumer culture has taught us fulfillment comes in the form of new cars, gadgets, and items, but it really comes from the work we do and the experiences we have. Limiting the number of things you own makes it easier to travel, reduces costs, and frees you to focus on the important things in life (The Paradox of Choice; Drive; The Inevitable; “The Waking Up Podcast”; life) The world has never been better and continues to improve despite a general consensus of the opposite. Far from being a naively optimistic idea, this is a conclusion fully supported by the relevant data and statistics: Humans today are less violent, healthier, wealthier, have more rights, and do a better job of protecting the environment than at any point in recorded history. (Factfulness; Enlightenment Now; The Rational Optimist)",What I learned in 2018,5,published,1002,2185,0.2160183066361556,2,1,0,0,0,0
3,31,152.76376651931713,7,https://medium.com/p/data-science-for-virtual-energy-audits-d761477ca564,0,None,2018-12-25 21:22:00,11.06,54,25,2018-01-10 08:15:00,"['Data Science', 'Energy', 'Energy Efficiency', 'Machine Learning', 'Education']","Data Science for Virtual Energy Audits A practical implementation of data science for the cause of improving building energy efficiency. Author’s Note: Following is the final report for my senior project at Case Western Reserve University. The work presented here was part of my ongoing involvement with the EDIFES research project conducted with the Great Lakes Energy Institute and the university. (Numbers in [brackets] refer to references after the report) Abstract A major part of the global attempt to combat climate change is improving energy efficiency. Commercial buildings present an attractive target for these efforts because up to thirty percent of the energy they consume is wasted. These losses are especially egregious because once the issues have been identified, they can be eliminated using existing technology. Identification of energy inefficiencies in commercial buildings has traditionally relied on physical audits in which a team of trained auditors travels to a building to perform a series of tests with specialized equipment. However, these in-person energy audits are resource-intensive in time and cost, and audits performed by different teams can result in varying conclusions, casting doubt on the economic benefits of the procedure. The Energy Diagnostic Investigator for Efficiency Savings (EDIFES) Project will upend the traditional audit process by using data science techniques to conduct virtual energy audits. A virtual energy audit will be cheaper, more reliable, and quicker than its physical counterpart, lowering the barrier for a building owner to find and correct energy wastes. The objective of Project EDIFES is to provide virtual energy audits as a service using a data science platform to reduce the time and cost of an energy audit while increasing the accuracy and economic/environmental benefits. The work documented herein comprises a small part of the EDIFES project. The primary goal of this work was to improve a set of building markers under development by the EDIFES team [1] and apply them to a set of several hundred buildings as a cross-sectional analysis. As part of any complex project, the focus of this work changed over the course of the semester in response to the needs of the team. Therefore, this report encompasses not only work relating to building markers, but also an exploratory data analysis (EDA) of eight office buildings, an attempt at using machine learning/statistical methods for building energy prediction/forecasting, and an extensive effort to improve code documentation and enforce standards to make EDIFES a sustainable project. As of the end of this semester, significant progress has been made on all fronts: the first twenty building markers have been refined and integrated into an automatic report generation R markdown script; the cross-sectional study has validate several markers and shown others require adjustment, the EDA revealed insights into patterns of building energy use and correlations with weather conditions; and the machine learning prediction with a random forest regression model was successful. Overall, progress documented in this report encompasses a wide range of domains, which is indicative of an exploratory data project in which members must take on multiple roles to form a cohesive team. Findings and conclusions of this report may appear disparate, but all work complete this semester is encompassed by the overarching goal of developing a platform for virtual energy audits. Introduction Up to 30% of the energy used by commercial buildings in the United States is wasted [2]. Nearly all waste is a result of preventable causes including inefficient heating, ventilation, and air conditioning (HVAC) scheduling, improper insulation, exterior lights that are uncoordinated with natural lighting cycles, or oversized equipment. The barrier to fixing these inefficiencies is not technological, as solutions already exist, but rather identification of savings opportunities. An energy audit is the accepted solution to this problem and involves inspection and analysis of energy flows into and out of a building [3]. Typical commercial in-person energy audits require sending a team of auditors to the physical building location to perform a series of tests. These tests use a range of techniques and equipment, from blower checks of window and door seals to infrared camera scans of rooms to determine where a building is squandering electricity. Due to the human labor and equipment required, these physical energy audits carry a considerable cost, up to $15,000, and have a turn-around time of weeks or months [4]. Moreover, the recommendations for the building owner can vary considerably depending on the auditing team. These factors combine to reduce the economic value of an energy audit. EDIFES, a joint project between Case Western Reserve University and the Great Lakes Energy Institute [5], aims to address these limitations by performing virtual energy audits that do not require setting foot in a building. To reduce requirements on the building owner, she/he will only need to provide EDIFES with the square footage and location of the building. The electricity usage is obtained directly from the utility company, typically in 15-minute energy consumption (in kWh) for at least one past year. The weather data corresponding to each building is retrieved from solarGIS [6] and merged with the energy information. The objective of the EDIFES project is a software platform that will automatically receive electricity data and metadata, clean and analyze the data, and produce a human-understandable report with recommendations for efficiency improvements. EDIFES will reduce the time and cost burden of conducting energy audits and consequently will increase the economic benefit of this critical procedure. EDIFES began as an exploratory data project to investigate the possibility of deriving insights from only the raw electricity consumption of a building [7]. Progress has continued steadily since the project’s inception and at the start of the fall 2017 semester, the following had been completed. A pipeline was developed for cleaning the electricity data and matching it to the corresponding weather data to create a single structure (an R dataframe) containing all relevant information for each building. This by itself was a significant achievement as data cleaning and structuring makes up the bulk of a data analysis, and creating an automatic process for these steps is critical to the feasibility of performing virtual energy audits at scale. Additionally, twenty building markers, functions that identify characteristics of a building to build an energy profile, were developed, although the majority had not been thoroughly tested and were in need of proper documentation. These building markers form the basis for the auditing procedure and like the cleaning process will need to be automatically applied to a building without manual labor. Finally, a significant amount of research went into developing a method for disaggregating individual building systems (mainly HVAC) from the overall energy consumption [8]. The most common approach surveyed in the literature [9] [10] was the Factorial Hidden Markov Chain (FHMC) algorithm. However, these studies were conducted with high-frequency data and typically had access to sub-metered information for each system, both benefits EDIFES will have to do without. Prior EDIFES team members worked extensively on the FHMC method, and investigations have also been made into thermodynamic approaches. Finally, team members have examined wavelet transformations and diffusion indices as a means for forecasting building energy use and assessing the impact of a building retrofit. The primary objective of the semester project was to improve the building marker functions, automate the process of running the markers on a building, and perform a cross-sectional study of the markers on 750 + buildings. The study required the markers to be in working order, and priority was placed on marker improvement and organization before a larger-scale analysis could occur. A secondary task was to perform a complete exploratory data analysis (EDA) of eight office buildings to gain familiarity with the domain and identify any patterns. An additional exploratory task was an attempt to develop a predictive model that can infer the energy use of a building given the time and the weather. The goal of the predictive model is to achieve an r-squared value of at least 0.85 when predicting electricity consumption for a period of six months. During the course of work on these objectives, an additional concern arose, the sustainability of code developed for the project. Project EDIFES relies solely on undergraduate and graduate students to write code, which means it must deal with inevitable graduations and team members arriving and leaving. The result of turnover is an unorganized code base with uncommented, barely comprehensible scripts. If the original author is not present to explain what she was trying to do, it is near impossible to modify or fix these functions. Therefore, a major focus of this semester was to enforce standards both for writing code and for tracking task assignments. The code and packages used by the team must be streamlined so they can be used as a tool for the data analysis and not be an impediment. The objectives of this project were fluid over the course of the semester, but significant progress was ultimately made in all aspects. Approach/Methodology The procedure for performing a virtual energy audit consists of three distinct steps [7]: 1. Obtain electricity and weather data in a clean and standard format 2. Analyze the data using statistical methods encoded in R functions 3. Report results and develop recommendations for the building owner The process for retrieving, cleaning, and structuring the data was fully developed by the start of this semester. Raw electricity data for a building is obtained through the utility, at a frequency of fifteen minutes or greater, and comprising at least one previous year of continuous data. The raw data goes into the Hadoop Distributed File System (HDFS), a file storage system designed for warehousing massive amounts of data in multiple locations for redundancy [11]. After acquisition, the data passes through a set of cleaning methods. First, anomalous data points are identified using the tsoutliers R package and set to NA for later imputation. Breaks of four or fewer missing/NA points in a row are linearly imputed from the data before and after, while longer gaps are filled in using the diffusion indices method typically used for time-series forecasting. All cleaning is done using the BuildingRClean package developed by the EDIFES team. The cleaned data must pass a set of data quality criteria and receives a grade based on the percentage of anomalies, percentage of missing/zeros, and maximum stretch of days with no data. Datasets receiving too low of a grade cannot be analyzed at an acceptable level of certainty and are discarded. Passing electricity data is matched to weather data obtained from solarGIS based on the latitude and longitude provided by the building owner. The merged time-series data is moved into HBase, which is a subset of HDFS holding only high quality, analysis-ready datasets. From HBase, the data can be accessed using the cradletools package developed in the Solar Degradation and Lifetime Extension (SDLE) laboratory [12]. Eventually, the entire ingestion and cleaning process will be automated, but it currently requires several manual commands. Nonetheless, a dataset can be acquired, cleaned, validated, and registered in HBase within a matter of hours. All steps of the process are performed on the CWRU High Power Computing (HPC) cluster due to the computational resources required to analyze the data. Figure 1 provides an overview of the data ingestion and processing pipeline. Analysis of the data, step two, is the heart of the EDIFES project and the focus of the majority of current work. This step is built on applying building markers, a set of functions that identify building energy characteristics. Through these functions, a building energy “profile” is created which can be used to compare buildings and assess if a building is operating near peak efficiency. Building markers cover a wide range of operating features. Examples of markers in use are baseload to peak energy ratio, energy pattern snapshot, HVAC schedule (on/off times), and building occupancy patterns. Results are presented in both graphical and tabular form allowing for quick intuitive explanations as well as quantification and comparison. Figure 2 presents graphical results from the heating type and weekly clustering markers. In order to validate the accuracy of building markers, EDIFES has collaborated with several companies to submit reports and receive feedback on the accuracy of reports. Based on the results, the functions are then updated in an iterative manner. Many of the functions receive minor modifications on a daily basis and current efforts have focused on improving accuracy and understandability to future developers. An additional aspect of the analysis is disaggregating individual systems from overall energy consumption, allowing building owners to discover precisely which equipment uses the most energy. Finally, the value-added part of the data analysis is examining the effects of efficiency changes to a building in order to quantify effects on energy consumption. At the end of a report, a building owner will want to know exactly how she can improve her building, and how much she can expect to save as a result. The initial two steps of the process comprise the technical aspects of the project. Therefore, although reporting may seem to be the simplest part as it does not involve any calculations, it is the most critical aspect of the project because it is the only part of the audit that will be visible to the customer. The reports EDIFES has provided to commercial partners for validation have been relatively simple explanations of results with a few visuals, but the final reports will need to be polished with production-level graphics and a list of concrete recommendations for a building owner with quantified savings attached to each suggestion. Progress on reporting has been far behind that for the first two steps if only because there is nothing to report until the analysis is complete. Moving forward, reporting, and the presentation format will become a major focus. A number of machine learning and statistical methods are presented in this report. § Linear Regression: The objective of linear regression is to create a linear mapping between the response and the explanatory variables. In the case of EDIFES, the response is the energy consumption and the explanatory variables are the weather and time readings for each interval. The response is a linearly weighted combination of all the explanatory variables: Where y, the response, is a linear combination of the features, xn, multiplied by their respective weights, wn. The benefits of a linear model are that it can be interpreted because the weight represents the change in the response due to a unit change in the input. For example, a weight of three for temperature would indicate that a 1˚ C increase in temperature results in a three kWh increase in energy consumption over the interval of time. Linear models are often the first model used in any analysis because they have no hyperparameters, are quick to train, and the results can be understood in terms of physical quantities [13]. § Support Vector Machine Regression: A support vector machine (SVM) regressor is the most complicated and the least interpretable of the models explored in this report. SVMs can be used for both classification and regression and operate based on finding a hyperplane to separate classes. The concept is that any decision boundary becomes linear in a high enough dimensional space [14]. For example, a linear decision boundary in three-dimensional space is a plane. The SVM projects the features of the observations into a higher dimensional space using a kernel, or a transformation of the data. The model then finds the plane that best separates the data by maximizing the margin, the minimum distance between the nearest member of each class and the decision boundary. The support vectors in the name of the algorithm refer to the points closest to the decision boundary, called the support, which have the greatest influence on the position of the hyperplane. SVM regressors work by fitting a non-parametric regression model to the data and trying to minimize the distance between the model and all training instances. SVM models are more complex than either a linear regressor or a random forest regressor and have almost no interpretability. The transformation of the features into a higher-dimensional space using a kernel removes all physical representation of the features. SVM models are black boxes but have high accuracy on small datasets with limited amounts of noise. The support vector machine also takes much longer to train than either of the other models. That being said, this approach demonstrated successful results in the literature and was therefore investigated [15] [16]. An example of a support vector machine for classification with several different kernels is shown in Figure 3. § Random Forest Regression: To understand the powerful random forest, one must first grasp the concept of a decision tree. The best way to describe a single decision tree is as a flowchart of questions, which leads to a classification/prediction. Each question (known as a node) has a yes/no answer based on the value of a particular explanatory variables. The two answers form branches leading away from the node. Eventually, the tree terminates in a node with a classification or prediction, which is called a leaf node. A single decision tree can be arbitrarily large and deep depending on the number of features and the number of classes. They are adept at both classification and regression and can learn a non-linear decision boundary. However, a single decision tree is prone to overfitting, especially as the depth of the tree increases because the decision tree is flexible leading to a tendency to memorize the training data (that is, it has a high variance). To solve this problem, ensembles of decision trees are combined into a powerful classifier known as a random forest [17]. Each tree in the forest is trained on a randomly chosen subset of the training data (either with replacement, called bootstrapping, or without) and on a subset of the features. This increases variability between trees making the overall forest more robust and less prone to overfitting. In order to make predictions, the random forest passes the explanatory variables (features) of an observation to all trees and takes an average of the votes of each tree (known as bagging). The random forest can also weight the votes of each tree with respect to the confidence the tree has in its prediction. Overall, the random forest is fast, relatively simple, has a moderate level of interpretability, and performs extremely well on both classification and regression tasks. There are a number of hyperparameters (best thought of as settings for the algorithm) that must be specified for the forest before training with the most important the number of trees in the forest, the number of features considered by each tree, the depth of the tree, and the minimum number of observations permitted at each leaf node of the tree. These can be selected by training many models with varying hyperparameters and selecting the combination that performs best on cross-validation or a testing set. A random forest performs implicit feature selection and can return relative feature importances which can be interpreted in the domain of the problem to determine the most useful variables for predicting the response. A model of a random forest that predicts energy consumption based on the time and weather conditions is presented in Figure 4. Additional techniques not covered in this report, but forming part of the EDIFES project include: Ø Diffusion Indices Ø Factorial Hidden Markov Chains Ø Wavelet Transformations Ø General Additive Models These techniques are discussed in papers regarding EFIDES, namely [7] and [8]. The majority of the development work for Project EDIFES is done in the R statistical programming language with some additional functionalities provided in Python. A number of packages are used for EDIFES, with most under active development. The main packages are rcradletools, used to retrieve building weather and energy data from HBase; 17-edifes, which contains a number of utilities and experimental functions built by the team; and BuildingRClean, which is used to clean a raw electricity dataset and currently contains all the building marker scripts. All the actual code is written by undergraduate and graduate students at CWRU which means there are issues when a team member graduates and another student must be brought in to the team. Having firsthand experienced the difficulties of wading through many lines of uncommented code, the author made an integral part of this project cleaning the existing code and enforcing standards so that new code can be understood and contributed to by multiple people. Weekly code reviews have now been put in place during which all members of the team examine an existing script. This process helps with standardization for such issues as variable and function naming and code control structures. Furthermore, to keep all members of the team up-to-date on action items and deadlines, the team has started using a Jira server to track and assign tasks. This is the third leg of the Jira-HipChat-BitBucket working foundation. Git is used for version control, with each team member working on a separate branch of each package. New functions must first be tested, then added to a development branch after review by multiple team members, and finally merged into the stable master branch after testing has confirmed the code works as intended and team members have had the opportunity to understand the function. These are all minor changes that collectively have had a substantial impact on the productivity and sustainability of the project. Members of the team are more willing to contribute to functions when they can understand the scripts, and knowing task responsibilities prevents repetition of work and ensures accountability. All data processing work is done through the High Powered Computing cluster at CWRU which is accessed through X2Go clients running on Raspberry Pi 3s in the center for Solar Degradation and Lifetime Extension (SDLE) laboratory. Intensive computations can be distributed and run in parallel on multiple cores with slurm jobs. For example, when running a building marker on all buildings, it is much more efficient to run each building on a different core rather than running them one building at a time sequentially in a loop. The High-Powered Computing resources and the use of HBase allows for rapid ingestion, reliable storage, and fast analysis of building data. Eventually, the entire ingestion, cleaning, analysis, and report generation process will be automated, allowing for virtual energy audits with a turn-around time of hours. Results Building markers are the foundation of the virtual energy audit. Long term, the EDIFES project will have a library of around forty functions which are able to determine information such as HVAC turn on and turn off times, base load to peak load ratio, synchronization of exterior lights with natural cycles, and type and size of HVAC systems. At the start of this semester, twenty markers were nearly complete but scattered across numerous scripts and R packages with poor documentation. In some cases, different versions of the functions were unknowingly run simultaneously by team members because of a lack of consistent naming standards. Over the course of this semester, all twenty building markers were cleaned, commented, renamed, and placed in the BuildingRClean package with proper documentation. This required going through each function line-by-line, and determining how and why it produced the given results. The original authors of the functions were involved when possible, and when not, the entire team was consulted to determine the purpose of the code. Eventually, all markers were documented, tested, and reviewed to an acceptable level where they could be added to the BuildingRClean package in the master branch. The markers continue to be developed on a daily basis, but this development work is done on separate branches with the working version always available in the master branch. After the building markers were stabilized, the next part of the project was to develop a markdown script that could run all of the functions on a single building. R markdown was chosen as the script type because it offers the ability to automatically output a PDF report with visuals, code, and commentary in one seamless document. Developing the script included writing code to retrieve building energy, weather, and meta data from HBase, calling each function in turn, and writing conditional statements to write different interpretations based on the results. The markdown script was completed over several months and has been used to generate over twenty reports. Creating a comprehensive PDF report involves changing only a single line of code (the building alphanumeric identifier) and renaming the file before selecting the option to “knit” the document. Overall, the entire process to generate a report once the building is in HBase is under 15 minutes, and the EDIFES team recently set a record by taking a raw building dataset to a complete report in under 24 hours, whereas at the beginning of the semester, this process took over a week. Eventually, the turn-around time should be shortened to just a few hours for commercial energy audits. A second aspect of the building marker work was to perform a cross-sectional study of the functions on 750+ buildings. Work in this area was repeatedly delayed by issues with data quality and the constant updating of the markers themselves, but nonetheless, preliminary results have been generated. These results can be grouped into several major areas: · Geographical distribution of buildings using both Koppen Geiger and ASHRAE climate zones · Quality of building datasets · Type of heating, either electrical or gas · Annual Consumption in kWh · Energy Use Intensity and Effective Thermal Resistance · Correlations between weather and energy use · Base load to peak load ratio · HVAC operating schedule One major area of interest for the project is comparing buildings across climate zones. Energy consumption patterns differ significantly depending on the geographical location of a building and comparing building within and between climate zones can reveal trends. Functions were written by the author to determine both the Köppen-Geiger and American Society of Heating, Refrigerating, and Air-Conditioning Engineers (ASHRAE) climate zones of buildings [18] from the latitude and longitude. One limitation that was discovered was the skewed distribution of buildings in a small number of climate zones. This can be observed in Figure 5 and Figure 6 where the majority of buildings are located in Cfa (temperate, without a dry season, hot summer) and Dfa (cold, without a dry season, hot summer) Köppen Geiger climate zones. 75% of buildings were located in just two KG climate zones, although there were four zones with more than 30 buildings. The distribution across ASHRAE climate zones was similarly skewed, with over 80% of the buildings located in just two zones. Likewise, there were also four climate zones with at least thirty buildings. Before beginning any analysis, it is critical to ensure the data is of a high enough quality to allow statistically significant results to be derived. The EDIFES team has thus created a set of quality standards for objectively “grading” a building. Eventually, EDIFES might charge customers a small fee for unacceptable data that requires a substantive amount of cleaning before it can be analyzed. In effect, the better the data, the more quality insights that can be gained from it. Figure 8 presents the quality standards for evaluating a building energy dataset. Figure 9 displays the data quality for all the buildings both before and after cleaning. The majority of buildings are at the highest level (AAAP) both before and after cleaning. A number of buildings improve through the cleaning procedure that involves removing anomalous points and imputing missing values. In order to quantify the changes that occur through cleaning, Figure 10 displays all of the changes that occurred. The most common improvement was from BAAP to AAAP reflecting the removal of outliers in the cleaning process. The heating type building marker is crucial to an energy analysis because EDIFES only has access to electricity data, and not natural gas use, and therefore, if a building has non-electrical heating, many of the functions do not apply year-round to the building. The heating type marker identifies whether a building has gas or electric heating. A map showing the results for all buildings is presented in Figure 11. Figure 11: Heating Type by Location Based on domain knowledge, it has been concluded by the EDIFES team that the heating type identification may not work as intended. The number of buildings with non-electrical heating in Southern California is highly suspect, although these could be coffee shops which typically have no heating. Currently, the building marker does not distinguish between no-heating and gas heating which could be a future improvement. The heating type for a building is determined by calculating the break point in temperature, or the temperature at which the building goes from heating to cooling. A piecewise linear model is then fitted to a graph of energy use versus temperature segmented by the temperature breakpoint. If the slope of the linear model in the heating section of the graph is positive, the conclusion is non-electrical heating, whereas if the slope is negative, this is believed to indicate electrical heating. The reasoning is that buildings with electrical heating will consume greater amounts of energy when the temperature is colder, whereas buildings with gas heating may use more gas with colder temperatures, but will not exhibit a noticeable increase in electricity consumption. It is possible this method is not applicable to all building types and may require an adjustment of the slope threshold for determining electrical versus non-electrical heating. The annual consumption of buildings (measured in kilowatt-hours) is mainly useful when comparing buildings with similar sizes in the same industry. As shown in Figure 12, the energy use of buildings can vary across several orders of magnitude. The annual consumption analysis showed the extreme range in energy use among buildings. Therefore, there is a need for a normalized metrics to be able to compare the energy use of buildings with different sizes. One simple measure is the energy use intensity (EUI) which is simply the annual energy consumption of a building divided by the square footage of the building (measured in kBTU/ft2yr). EUI values have been established for a number of industries by Energy Star that can be seen as the purple asterisks in Figure 13, which displays boxplots of the EUI for buildings by industry. Boxplots show the Interquartile range, median, and outliers for a distribution, which is helpful for assessing both the location of the median value as well as the spread of the values. As seen in the figure, most of the values are reasonable. As long as the square footage data is accurate (traditionally this has been an issue), the calculation of EUI is straightforward and mostly involves a conversion of units. Therefore, it would be expected the results from this marker are accurate for buildings with known areas. The benefit of this marker is that it can be used to make objective comparisons of the efficiency of buildings in a given industry while controlling for the size of the building. These results will prove useful when informing a building owner of the relative performance of her building. An additional marker with a significant potential for application is the effective thermal resistance (r- value). The R value for a building is a measure of the “tightness” of a building, or how well it is insulated. A building with better insulation and seals will have a higher R value which will lead to reduce heating and cooling costs. The results of running the marker on those buildings with square footages (the floor area and the number of floors is required to figure out the effective surface area of the building) are presented in Figure 14. The red vertical lines indicate the typical range as has been reported in the literature [19]. (Although these values were determined for residential buildings, they still provide a useful reference point.) The results are substantially above the typical range which indicates the building marker should be edited. One issue is calculating the effective surface area of a building which should be possible using the ground area and number of floors albeit with limited accuracy. This building marker is based solely on a thermodynamic rather than statistical analysis, and while the underlying principle has been proved from a mechanical engineering perspective, the implementation is not properly designed. A future project for the author will be correcting this building marker to reflect physically confirmed values. The energy use of a building depends to a significant extent of the weather conditions. The largest single uses of electricity for most office buildings is HVAC, which encompasses heating in the winter and cooling in the summer. The extent of HVAC usage thus is intricately related with the outdoor temperature, irradiance, and to a lesser extent, relative humidity and wind speed. A standard measure for assessing correlations is the Pearson correlation coefficient which ranges from -1, for a perfectly linear negative relationship, to +1 for a perfectly linear positive relationship [20]. Analysis of the weather correlations with energy use for different climate zones could be useful when developing a model that aims to predict the energy use from the time and weather. Moreover, knowing the greatest contributors to energy usage would allow the project to develop recommendations that aim to mitigate the effect of these weather conditions. Figure 15 and Figure 16 show the summer and winter correlations respectively for each climate zone. The dendrograms to the left and top of the plot are the results of hierarchical clustering which groups together similar climate zones and weather variables in terms of the correlations. There is a considerable amount of information to gather from these visuals. The most highly correlated variable with energy use during the summer is temperature followed by global horizontal irradiance (GHI). This indicates that as the temperature outdoors increases and the amount of sunlight hitting a building increases, the energy use increases correspondingly. During the summer, relative humidity is negatively correlated with energy use in all climate zones and the rest the weather variables have either a small relationship or no relationship with energy consumption during the summer. In the winter, there are no clear relationships, although it can be seen that some climate zones still have a positive correlation with energy use, while others display a negative relationship. A hypothesis to explain these contrasting relationships might be that the buildings with a positive correlation between temperature and energy use during the winter have gas heating while the buildings with a negative correlation have electrical heating and use more energy as the temperature decreases. Overall, the winter correlations are not as strong as those in the summer and do not display consistent trends across climate zones which is expected because buildings are designed differently based on the geographic location. The base load to peak load ratio measures the base average energy use of a building to the maximum sustained energy use. Typically, the base load is the amount of consumption when the building is unoccupied during the night or on the weekends, and the peak load occurs with the greatest level of HVAC usage. According to studies previously conducted by the EDIFES team, a base to peak ratio of greater than 0.3 indicates opportunities for electricity savings by reducing the base load of the building. The base load needs of buildings can vary significantly across industries however (a data center will basically have a base to peak ratio of 1.0, while a retail store could be much lower) and the threshold for reduction of the base load will need to be industry specific. Figure 17 shows the base to peak load ratio for summer and winter divided out by the sample set. Each sample set is composed of different customers although all the buildings in one sample set are not necessarily within the same industry. The blue vertical line indicates the threshold above which electricity use can be reduced by minimizing the base load. Over 90% of buildings have energy saving opportunities during the winter, and 84% can save energy during the summer. The threshold might need to be adjusted based on additional cross-sectional work and literature surveys. Another insightful graph for this marker is the change in base to peak ratio from one year to the next. If the ratio declines, that indicates the building is becoming more efficient, while an increase could represent the building becoming less efficient. These relationships also might be influenced by the weather conditions which should be an area of further research. Changes in the base to peak ratio from the first year of data to the last are shown in Figure 18. A positive change indicates an improvement. Overall, 45.5% of buildings with more than one year of data improved in terms of base to peak ratio, 43.7% declined, and the rest displayed no change. Again, these changes could be the result of changing weather conditions or they could indicate a concerted effort from a company to improve building efficiency. A highly active area of focus for improving building energy efficiency is smart scheduling of the HVAC system. The system should turn on as late as possible in the morning while still reaching the desired set point in time, and should turn off as early as possible in the evening while maintaining comfortable conditions during operating hours. The turn on and turn off times are calculated by finding large changes in energy (positive for turn on and negative for turn off) and recording the most frequent times for these changes. As HVAC systems typically use the most energy of any system in a building, the largest changes in energy consumption should correspond to changes in HVAC operating conditions. Figure 19 and Figure 20 show the HVAC turn on and turn off times respectively for the sample set. A density plot is not the same as a proportion plot, but instead is a probability density function for the times on the x-axis. The density on the y-axis can be interpreted as the probability of a turn on or turn off at the specified time. Different businesses have different operating hours, as is indicated by the plots. These graphics lead to a level of confidence in the building markers because the results are reasonable and have in some cases been verified with real-world results. For example, by looking at the opening and closing times of Starbucks, it was determined that the function produced results aligned with operating hours. By comparing the HVAC schedule with building occupancy schedules, it is possible for a building owner to devise a more efficient climate-control scheme. An even better solution might be the adoption of smart thermostats that automatically learn the schedule of a building and adjust based on both employee occupancy and the projected weather. The first step in working in a new field is to gain an intuitive understanding of the structure and relationships within the available data. This allows a researcher to learn the expected patterns and to spot anomalies or unexpected trends for further study. Therefore, the initial phase of this work began with an exploratory data analysis (EDA) of a cleaned dataset of eight Progressive office buildings distributed in six different Köppen Geiger climate zones. The EDA quickly revealed that buildings exhibit daily, seasonal, and yearly energy consumption patterns that vary significantly across climate zones. Figure 21 shows the variability in energy consumption for a building location in Nevada. The message from this figure is that any model that attempts to predict or forecast energy consumption for a building must be flexible enough to capture all of the inherent trends within the data. Clearly, there is not a simple linear relationship between the weather consumption and the time of day or the season of the year. A major focus of the EDA was on the relationships between weather conditions and energy consumption. Figure 22 illustrates (A.) the summer correlations between weather and energy use and (B.) the winter correlations in terms of the Pearson correlation coefficient for the eight office buildings. Mirroring the cross-sectional analysis, temperature was found to have the greatest positive correlation between energy consumption and energy use (excluding the anomalous PGE1 building), a fact that was confirmed in later modeling with non-linear methods. A linear relationship offers interpretability, and one conclusion was that an increase of 1˚C in temperature could increase the energy use of a large office building by 300 kWh over the course of a single day. This would result in over $40 in increased electricity costs at the national average electricity price of $0.1319 / kWh [21]. There were limitations in the EDA due to the sample size of eight buildings, and therefore an alternative source of data from the City of Chicago [22] was required to determine the relationship between the size of a building and total annual energy consumption. A BoxCox transformation [23] was applied to find the best power transformation for the relationship, and it was concluded that yearly energy consumption is positively linearly correlated with building squared footage on a log-log scale. Using data from 7500 buildings in Chicago, Illinois, the R-Squared value between the log of annual energy consumption and the log of square footage was 0.8476. This is classified as a strong linear relationship. The R-Squared value represents the fraction of variability in the response (energy consumption) explained by the explanatory variables in a model (in this example only square feet). Therefore, from this analysis, 84.76% of the variability in annual energy consumption among buildings in Chicago is due to the variability in square footages of the buildings. Additional findings from the exploratory data analysis are covered in the Prediction and Forecasting Energy Use with Machine Learning Algorithms section of the report. While certainly not the main public focus of data science projects, proper documentation and coding standards is usually the deciding factor in whether a project succeeds or stumbles. In a project comprised primarily of students, documentation takes on an even greater level of importance. Several issues were identified at the beginning of the semester: messy code with no comments indicating functionality, multiple versions of scripts residing across many local directories and branches, and no method for tracking issues and assigned tasks. All three issues were successfully addressed during the semester, with plans put in place to ensure the issues will be managed on an ongoing basis. The objective of the documentation reform was not a one-time push to fix all uncommented scripts, but rather a shift in thought processes so new functions are developed with sustainability in mind. To that end, numerous ongoing practices have been instituted for the EDIFES team. The most beneficial of these has been the weekly code review that focuses on going line-by-line through one or two existing scripts. At least ten scripts have been completely commented and corrected and the process has been beneficial for all members of the team. There are countless ways to solve programming and statistical problems, and one of the most crucial parts of the code review is listening to others explain their methodology so one can view the problem from a different light. As students, all members of the team are open-minded, which means everyone is willing to abandon an old manner of working if they see there are significant benefits to a different approach. In effect, individuals are able to take advantage of the collective knowledge of the team without having to make all the same mistakes themselves. In addition to changing mindsets towards writing reproducible code and analyses, the code reviews have eliminated many mistakes in existing functions, and in one notable case, increased the runtime of the daylight lighting sensors building marker by a factor of twenty. Additional concrete steps taken to streamline the project were the establishment of a set of coding and package development standards, and the set-up of a Jira server to track tasks. The coding standards try to create consistency without requiring uniformity and address matters from variable names to the ideal length of lines. The package development standards require functions to be developed on personal side branches, tested before being moved to a development branch, and reviewed by multiple team members before being merged into the stable branch. The Jira server integrates with the existing communication (HipChat) and version control (Git) tools used on the project. Team members are able to assign tasks through Jira and are notified of pull requests and commits through the HipChat instant messaging service. While there are still some issues with getting all team members to adopt the workflow, it is already showing promising signs of helping individuals to concentrate on the most important tasks. New practices for committing work have helped to reduce the number of non-working scripts in stable branches of the packages and ensure that the entire team has access to the most current version of all functions. Prediction of future energy use (known as forecasting) is critical for the EDIFES project because it will allow comparisons between the building continuing to operate as normal and the building operating with improvements designed to improve efficiency. In addition to developing a model that can accurately forecast consumption given continued operating conditions, EDIFES also must develop a method that allows one to alter different characteristics of a building in order to assess the effects on energy consumption [7]. The final report to a building owner must include a set of recommendations for reducing energy use, and these suggestions should be quantified in terms of dollars and kilowatt-hours. In order to test the accuracy of a forecasting method, the prediction ability can be assessed by predicting six months of energy consumption. In essence, the training data is all historical data except for the final six months, and the final six months of data is used for testing. ARPE-E has established a project requirement of creating a model that can predict six months of energy consumption with an r-squared value of at least 0.85. Work documented in this report is of an exploratory nature and was intended to serve as a measure of the feasibility of different approaches to the problem. Current and past approaches by EDIFES team members have involved techniques such as diffusion indices and wavelet transformation methods [8]. In this report, three machine-learning methods were experimentally tested: linear regression, support vector machine regression, and random forest regression. The problem was approached as a supervised regression problem. There are a number of explanatory variables (weather and time features) that are used to predict the response variable (energy consumption). This is a regression problem because the output is continuous and it is supervised because the targets (energy consumption) are known ahead of time and given to the model during training. The algorithms learn the relationship between the exploratory and response variables, and during testing, the algorithm is evaluated on data for which is has never seen the targets. The three algorithms selected were designed to cover the range of complexity and interpretability in models, starting with the simplest, Linear Regression, proceeding to the ensemble Random Forest method, and finally moving on to the Support Vector Machine Regressor. The Linear Regression has no hyperparameters to tune, the Random Forest was built with the defaults except the number of decision trees was set to two hundred, and the Support Vector Machine used all default hyperparameters and the radial basis kernel. To compare the algorithms, a random train-test split was used (with the same features and labels used for all models). The objective was to select the best performer on the random split and implement this method for predicting six months of energy consumption. All three algorithms were built using the Scikit-Learn library in Python [24] while the data was preprocessed in R. Transferring the data between R and Python is relatively simple with the feather library that saves and loads dataframes in R and pandas dataframes in Python. The basic steps for evaluating the algorithms on a random training/testing split are as follows: 1. Structure the data into an appropriate format, and separate the features and labels. 2. Subset the data into a training and testing set using a 0.75/0.25 split 3. Train each model on the same training set with the model fed both the features and the labels. 4. Use the trained model to predict the testing labels from the testing features. 5. Compute performance metrics with the known testing labels and predicted values. 6. Make comparisons between algorithms for interpretability and accuracy. Data preparation involved the following four steps: Ø One-hot encoding of categorical variables Ø Transformation of cyclical variables into a sinusoidal representation Ø Normalize variables to have zero mean and unit variance Ø Separate into random training and testing features and labels Results achieved on a random training/testing split for six Progressive office buildings appear in Figure 24 and Figure 25. The mean average percentage error (MAPE) is the mean of the residuals divided by the true values and is a relative error. The R-Squared value is a measure of the fraction of variability explained by the explanatory variables in the model divided by the total variable in the response variables. The Random Forest significantly outperformed the Linear Regression and Support Vector Machine Regressor on the random training/testing split. Moreover, the Random Forest was relatively quick to train compared to the SVM and maintains a level of interpretability. Thus, the Random Forest was selected by optimization and assessment of forecasting ability. Optimization of machine learning algorithms either involves altering the features fed into the model or tuning the hyperparameters of the model. As interpretability is a key factor for this project, the features were left as is (with the few minor transformations covered above) but the random forest was adjusted to fit the task. The hyperparameters of a model are best thought of as settings that can be tuned to achieve better performance for a given problem. In contrast to model parameters, which are learned during training, the model hyperparameters must be explicitly set by the developer before training. There are a number of methods for efficiently evaluating a range of hyperparameters for a problem, and the approach used in this report was grid search cross-validation implemented in Scikit-Learn. The final parameters for the random forest were chosen by using grid search with a specified grid and are as follows: Ø 200 decision tres in the forest Ø No maximum depth for each tree Ø A minimum of 1 sample per leaf node Ø A maximum of 50% of the features evaluated by each tree Ø A minimum of 2 samples to split a node Ø A minimum impurity decrease of 0.0 to split a node The optimized Random Forest achieved better performance metrics compared to the baseline model using the same training/testing split for the eight office buildings as shown in . The optimized Random Forest was then used to predict six months of energy use after learning from historical data. Example predictions of the optimized Random Forest on a six month test set can be seen in Figure 26 and Figure 27. The R-Squared for these predictions was over 0.9, meeting the project requirements. The figures demonstrate that the Random Forest regression model was able to accurately predict six months of electricity consumption. Accuracy was not as high on all datasets, and further work is currently ongoing in order to optimize the model for building energy prediction. A benefit of the Random Forest is its ability to return feature importances. These relative weightings can be used for feature selection and to provide a physical explanation for the predictions of the forest. The feature importances for the Random Forest averaged across the eight Progressive office buildings are presented in Figure 28. In agreement with the correlation values calculated in the cross-sectional study and the EDA, the temperature is the greatest explained of energy consumption. Whether or not the day was a weekday or weekend was the second most important variable which is not unexpected considering these buildings are office building that displays considerably different energy consumption on the weekends than during the week. These features importances can be used to perform feature selection for future algorithms or to determine which weather conditions should be targeted for reducing building energy consumption. Furthermore, it is possible to alter a single variable and then observe the effects on the response. In this case, the effect of increasing the temperature on energy consumption can be investigated to see how changes in the climate will change building energy use. Current predictions indicate the global surface temperature will increase between 1˚C and 4˚ C by 2100 as the result of human-caused climate change [25] [26]. In order to assess these effects on future energy use consumption, a reasonable increase of 2˚C can be modeled using the random forest. This is done by training the model on the entire historical data available, and then creating a testing set by keeping all of the explanatory variables the same except for the temperature, which will be increased in every interval by 2˚ C reflecting a mean increase in line with current forecast for future warming. In this situation, there are no correct values to compare the predictions to, and this is purely an experiment to observe the effects. The economic effects in terms of annual energy use costs are demonstrated in Figure 29. These results were obtained by finding the difference between the unaltered energy consumption and the prediction energy consumption with the temperature increase and then multiplying by the average cost of electricity in the United States. One aspect of global warming that often goes unmentioned is it will unevenly effect different areas of the globe, with some locations enjoying longer growing seasons and others experiencing frequent severe weather events. The unequal distribution of the effects of climate change are reflected in the figure which demonstrates that according to the random forest model, some areas will see a reduction in energy usage (and hence cost) while other areas will see a significant increase in energy use. According to this analysis, an office building of 50,000 ft2 could see an increase of nearly $30,000 in annual energy costs due to increased temperatures. Many arguments have been made for taking steps to reduce carbon emissions and transition to sustainable energy sources, but perhaps none is more persuasive than that of simple economics. Overall, the modeling demonstrated the feasibility of using a trained machine learning model for predicting building energy consumption. In contrast to other techniques, the Random Forest predictions can be interpreted and explained in terms of physical quantities, and this method has experimental uses beyond predicting energy use. Discussion A number of objectives drove work on this semester project. Primarily, the focus was on refinement on building markers and a cross-sectional study of the building markers. Secondary work included an exploratory data analysis of eight office buildings, correcting inadequate documentation, and building an energy prediction model. Work done in this report is a small fraction of the overall effort involved in Project EDIFES but touched on a number of areas that will be key to the team moving forward. Deliverables are as follows: Ø Building marker scripts were corrected, commented, documentation, and assembled into one package for ease of access and proper version control Ø A markdown script was developed to run all existing markers on a building and create a PDF report Ø Preliminary work on the cross-sectional study of building markers has validated some markers and shown significant work remains to improve others to better reflect real-world conditions. Ø An EDA of eight office buildings identified major trends and patterns in energy consumption. Ø Code reviews have been instituted on a weekly basis and a set of documentation/coding standards has been adopted by the team. In addition, a system for tracking tasks and commits is now in use Ø A Random Forest Regression model achieved high accuracy in prediction. A general additive model that is able to capture all of the various trends in the building energy data might prove to be more capable as a forecasting method. Limitations/Obstacles A fundamental limitation of any data science project is the quality and quantity of the data. Both of these proved to be obstacles over the course of this semester. Although there are over 750 buildings in HBase with cleaned energy data, the quality of the data varies significantly. Moreover, the buildings have energy data at, different intervals which can play havoc with some of the functions that were originally hard-coded for data every 15 minutes. This makes running the building markers on all of the buildings a challenge in ensuring the markers are building-agnostic. During the Progressive office building EDA, the quality of the data was not an issue but, instead, it was the quantity that was an obstacle. A general rule of thumb in statistics is that 30 samples are required to make statistically significant findings due to the central limit theorem that states non-skewed distributions of variables with more than 30 samples tend to approach normal. Thus, while each building may have had hundreds of thousands of individual datapoints, it was not possible to make meaningful comparisons between buildings. Additional obstacles arose because of the dynamic nature of the project. Functions are constantly updated, which means that in order to keep the script that runs all building markers working requires constant adjustments. Each time a function is changed so that it accepts different inputs or produces different output, the script must be updated to reflect these changes. Furthermore, the interpretations of the results of the markers are constantly in flux as team members adjust models and acquire new domain knowledge. This is not an unexpected or unwelcome issue because it allows errors to be caught and it is better that the functions produce meaningful output rather than that they work but the results have no physical interpretation. There are too many assumptions made across the project to list them all here, but one example is the definition of seasons. This may seem to be a straightforward problem, but the definition of seasons can change significantly depending on the field one works in. The team has decided to use the meteorological definition of seasons, with summer defined as June, July, and August, and winter as December, January, February. These definitions provide a starting point, but it is clear the season definitions should be fluid and change across climate zones. Some locations may essentially have only two six-month long seasons, while others have four distinct seasons with different energy use patterns in each. There are many other examples where assumptions have been made based on intuitions or limited domain knowledge. If every single decision was debated until it had been verified with no doubt, the project would not be able to move forward. Therefore, assumptions are required and are acceptable as long as they are pointed out and addressed when enough evidence allows for a robust conclusion. This report encompasses four major areas of work. The first, and most important, was the organization and fine-tuning of the building markers along with a cross-sectional study applied to hundreds of buildings. The final deliverable from this procedure was a single R markdown script (see the Code Sample in the Appendices) that can run all existing markers on a building and a presentation of the study results. The markdown document allows the team to generate comprehensive reports with a limited amount of human effort. These reports can then be shown to commercial partners who are able to validate the findings. For instance, the team has learned that the heating and cooling oversized building markers provide almost no information at this point because all Heating, Ventilation, and Air Conditioning (HVAC) systems are designed to be oversized so they are not overwhelmed on days with extreme weather conditions. Other markers, such as Energy Use Intensity, and HVAC schedule, have been shown to be reliable reflections of real-world conditions. This is important because it can be easy to get lost in the complicated statistical analysis and forget that in the end, the systems being modeled and predicted have physical counterparts, and the statistical methods must produce results consistent with reality. A careful review of the twenty building markers has improved them both in terms of accuracy and efficiency. The literature for this area mainly does not exist because EDIFES is taking a new approach to the energy audit procedure. Typical industry rules of thumb do not necessarily apply when one is doing an audit without ever setting foot in the building, but many of the markers are informed at least in part by domain knowledge. Overall, the impact of the work on the building markers has been considerable because of the reduced time to generate a report and the capability of running a single marker on hundreds of buildings. Having all of the markers in a single package with proper development rules ensures all team members are using the most recent version of the marker which improves consistency and eliminates confusion from unexpected contrasting results. Furthermore, the cross-sectional study of all building markers has revealed that while some markers are accurate, others will need to be improved, including effective thermal resistance and heating type. Previously, the markers had been run on small, homogeneous samples of buildings, and running them against hundreds of buildings proved there are biases that can arise from developing markers with similar buildings. Buildings vary considerably across industries and geographical regions and functions should be robust to these changes and applicable to all buildings. Addressing these issues could be as simple as adding if statements to the code to handle different buildings appropriately. An additional conclusion from the analysis was there is a need for high-quality data including accurate building areas as the value of the virtual audit will be directly proportional to the quality of the data. Several functions (including EUI, effective thermal resistance, and perhaps building energy disaggregation) require an accurate square footage in order to obtain meaningful results. The cross-sectional study has been exploratory in nature, and the next steps in this project will involve interpreting the results and implementing findings. The office building EDA provided the ideal opportunity to explore the problem domain. Many of the findings, while not entirely novel [7] [8], were surprising to the author. All of these could have been directly told to the author, but it is a more memorable exercise to have to write the code and then make the discoveries firsthand rather than hearing them passively. Moreover, it is reassuring when the conclusions reached by an independent team member agree with those established by the rest of the team. While disagreements drive progress, similar results confirm the methods in use and allow for the establishment of base truths from which to work. The major finding was the number of different patterns that can be ascertained in the energy data. Each building exhibits different trends depending on the day of the week, the season of the year, and even between years if a retrofit to improve efficiency has been performed. Moreover, once the office buildings had been thoroughly explored, a set of coffee shops was examined and found to have entirely different patterns! Clearly, the process of modeling and eventually forecasting energy consumption will require sophisticated methods that can separate each of the trends [27] [28]. Additional significant findings from the EDA were weather and energy correlations that show temperature and global horizontal irradiance (GHI) are the most highly correlated variables with energy use during the summer. These results suggest several methods for reducing energy consumption including increasing insulation, installing shades over windows in direct sunlight, properly sealing windows and doors, and positioning buildings to use the sun as a natural source of heating in the winter without absorbing excess heat in the summer [29]. The EDA also showed the interpretations that can be drawn from a linear model as well as the limitations linear models have because they are not flexible enough to capture intricate trends. One significant aspect of the EDA that was unsuccessful was the attempt to normalize buildings across climate zones, building types, and building sizes. There were not enough buildings to make these comparisons, but they might be possible to make with the entire set of buildings available in HBase. Overall, the exploratory data analysis was an optimal way to get a start with the EDIFES project and opened up many exciting additional questions for research. The main motivation for the improved documentation push was the author’s experience trying to contribute to the project. It was exceedingly difficult to fix broken building markers due to the nearly complete lack of documentation. More than that, there was no standard style across the code with variable and function names appearing as camelCase in some scripts and separated with under_scores in others. All of this made parsing these files extremely difficult, even when taking a dataframe and passing it through the function one line at a time. Trying to fix these scripts was akin to climbing a mountain in a snowstorm with no signs to guide one’s way. These concerns had been present in other team members but were pushed to the side to focus on developing more statistical methods and implementing them in additional functions. It took repeated suggestions to encourage all the project members to make a concerted effort to improve documentation. The first major success was the establishment of weekly code review sessions where all members of the team sit down together and review one or two scripts line-by-line, adding comments and fixing mistakes along the way. Already these sessions have produced notable victories, including improving the efficiency of one marker by twenty times through the elimination of a redundant loop. The code reviews also highlight the benefits of a cross-disciplinary team. Team members have backgrounds in computer science, statistics, electrical engineering, and mechanical engineering and bring different knowledge and skill sets to each meeting. One example of how this benefits the team occurred during a review of a function in which one team member had tried to remove outliers by taking away the top and bottom 2% of datapoints. He did not necessarily have any rationale for this decision other than that it produced results that seemed to agree with intuition. However, there is a statistical definition for outliers that is more robust than “2% of datapoints at the top and bottom” and the statistician was able to provide the correct definition (more than 1.5 times the IQR above the third quartile, or more than 1.5 times the IQR below the first quartile). There was a robust debate about the value of statistical definitions versus intuitive judgments, and through this discussion, everyone looked at the problem from multiple angles. Moments like this have become commonplace in code review and all team members agree it has been positive not only for what gets accomplished during the meeting but for how individuals then approach writing code by themselves outside of meetings. The overall result is a better-documented code base that any team member can feel comfortable contributing to as required. Additional parts of the documentation reformation include implementation of task tracking software and establishment of a set of coding and development standards. These actions have reduced conflicts between function versions and reduce the friction involved in working on code collaboratively. Together with a strong version control system, these measures allow the project to be robust to individual mistakes which consequently encourages team members to be more experimental in developing additional functions. The building owner (or the Department of Energy reviewers) will never see the documentation itself, but they will see the results that the documentation has made possible to attain. The entire EDIFES project is a push to increase the sustainability of society, and the documentation effort is aimed to increase the sustainability of the project. Similar to most other parts of this project, the modeling and forecasting section was experimental. The end goal is to develop a model that is able to forecast future energy consumption, both under normal operating conditions, and accounting for a retrofit. However, these early efforts were mostly concerned with evaluating a number of different approaches for feasibility as opposed to finishing a fully completed model. From the results of the EDA, it was certain a linear model would not be an accurate model because of the non-linearity of relationships between time and weather versus energy consumption. This conclusion was borne out by the modeling investigations that found Linear Regression to be extremely inept at predicting energy consumption from weather and time variables. Moreover, a review of the literature showed that non-linear methods including support vector machines and artificial neural networks had been the most successful tools for predicting energy consumption in buildings [27] [28]. Overall, investigations performed during this report showed the Random Forest Regression model achieved unmatched prediction accuracy when evaluated on a random training/testing split of the data. When the model was then applied to predicting six months of energy consumption, it was able to achieve above 0.85 r-squared on 4 of the 8 buildings in the Progressive dataset and is currently being run on all buildings in HBase. Besides prediction, a random forest can be used for other experimental procedures. One possible implementation of the Random Forest would be to replay past data but change one or multiple variables and observe the response. For example, it is possible to simulate an increase in temperature by training a model on all available data for a year, then adding 1 degree to all the temperature data points and predicting the resulting energy consumption. Altering the weather variables would be interesting, but the real application would be changing characteristics of the building such as the amount of insulation or HVAC operating schedule and observing the resulting effect on electricity use. One motivation for research into these particular methods was the author’s interest in machine learning, and if nothing else, this investigation revealed there are limits to this exciting field and success takes considerable effort. There is a need for more research in the modeling and forecasting part of the project, and many new techniques, such as general additive modeling should be attempted. Modeling requires experience to select the correct algorithm, feature transformations, and model settings, and this project provided many opportunities for exploring and studying the machine learning field. Conclusions The overall objective of Project EDIFES is to develop a platform for virtual energy audits using data science tools and automated processing /analysis workflows. The work documented in this report touched on many different areas of the project and addressed needs of the team as they arose. The main project objectives of refining the building markers, creating a markdown script to generate a pdf document with no manual input, and performing a cross-sectional exploratory analysis were all accomplished. The building markers are now located in one package for ease of access to the current working version, and the completed markdown script can create a pdf report in minutes. The cross-sectional analysis has determine which markers produce reasonable results and which need more focus. The office building exploratory data analysis revealed the numerous trends in energy data at different timescales and the significant correlations between energy use and weather conditions. Moreover, this EDA provided an introduction to the project domain and the tools commonly used by the team. Experimental work with three machine learning algorithms for predicting energy consumption showed the Random Forest Regression model to be capable of achieving r-squared values greater than 0.9 when prediction six months of energy use. Additional methods should be considered for prediction, as this is a problem that will require multiple approaches. Finally, a major push for improved documentation and understandable code led to the establishment of weekly code reviews, a platform for assigning and monitoring tasks, and a set of development and coding standards. Already, the effects of these changes have been felt across the project and it is on more sustainable footing as a result. The many different areas of focus for this report are an indication of the cross-disciplinary nature of Project EDIFES, which makes the work always engaging. Although there are also numerous challenges that come from such a broad project, the author has enjoyed every minute of work thus far and is eager to see the effects the project will have on improving building energy efficiency. There are three main parts of EDIFES that will need to be addressed in the coming semester: building energy use disaggregation, predicting/forecasting energy use, and developing additional building markers. Disaggregation in the building energy domain has traditionally been approached from a pure statistics point of view using techniques such as Factorial Hidden Markov Models [8]. However, thermodynamic approaches have also been successful, although with residential buildings rather rather than commercial [30]. Previous attempts by EDIFES required prior knowledge of the HVAC systems and sizes, which cannot be accurately determined as of now from the overall building energy consumption. There is thus a need for a method that does not rely on knowledge of the individual system sizes in order to separate out the component energy usages from the whole. This is an active area of research and current attempts have shown the thermodynamic model is feasible although it will require some adjustments for use on commercial buildings. The method will likely require the effective thermal resistance, which the current building marker does not accurately compute when compared to the relevant literature. A further area of work is forecasting future energy use, both under operating conditions as normal and with a building energy retrofit. In addition to the author’s work with machine learning methods, current work focuses on a wavelength transformation method that takes a signal and decomposes it into a number of levels each at a different frequency. These levels could represent different physical characteristics of the building, and after performing a transformation, the next step is to separate out the individual components. Then, these components can be altered to reflect a building retrofit and added back in to the overall signal in order to determine the total effect on energy consumption. General additive models (GAM) have proven adept at forecasting time-series in a number of domains [31]. A GAM separates a time-series signal into a number of time patterns, such as weekly, seasonally, or yearly although with the overall trend (such as increasing with time). The model can then be used to forecast the time series into the future. A picture of a general additive model of building energy data created using the prophet package available in both Python and R [32] is presented in Figure 30. Finally, additional building markers need to be developed to build a more robust energy profile of a building. Ideas currently under development include anomalous activity detection, clustering buildings into different energy use groups, and assessing the uncertainty in data analyses performed given the data characteristics. Overall, it is clear that much works remains to be done in many aspects of the EDIFES project. All of the areas of the project involve exploration and experimentation, and it has been a joy to work with this team on a task with substantial, tangible real-world benefits for sustainability! Acknowledgements The author would like to thank the following members of the EDIFES team for their willingness to patiently explain all aspects of the project and for their tolerance of a considerable number of questions: Professor Alexis Abramson, Professor Roger French, Rojiar Haddadian, Arash Khalilnejad, Jack Mousseau, Shreyas Kamath, and Ahmad Karimi. References 1. A. Abramson, R. French, J. Mousseau, A. Khalilnejad, M. Hossain, R. Haddadian, E. Pickering, W. Koehrsen, S. Kamath and K. Nash, BuildingRClean R Package. Cleveland, OH: Case Western Reserve University, 2017. Available: https://bitbucket.org/cwrusdle/buildingrclean/overview 2. “About the Commercial Buildings Integration Program | Department of Energy”, Energy.gov, 2016. [Online]. Available: https://energy.gov/eere/buildings/about-commercial-buildings-integration-program. 3. University of Hawaii’s Environmental Research and Design Laboratory, 2013, Energy Audit Procedures, US Department of Energy Office of Electricity Delivery and Energy Reliability. Available: https://www.hnei.hawaii.edu/sites/dev.hnei.hawaii.edu/files/Energy%20Audit%20Procedures.pdf 4. EMS Environmental Incorporated, “How Much Does a Commercial Energy Audit Cost”, 2017 [Online]. Available: http://emsenv.com/2016/04/28/commercial-energy-audit-cost/. 5. “ARPA-E | Virtual Building Energy Audits”, Arpa-e.energy.gov, 2015. [Online]. Available: https://arpa-e.energy.gov/?q=slick-sheet-project/virtual-building-energy-audits. 6. “GIS data and maps”, Solargis.com, 2017. [Online]. Available: http://solargis.com/products/maps-and-gis-data/. 7. E. Pickering, “EDIFES 0.4 Scalable Data Analytics for Commercial Building Virtual Energy Audits”, Masters of Science in Mechanical Engineering, Case Western Reserve University, 2017. 8. M. Hossain, “Development of Building Markers and an Unsupervised Non-Intrusive Disaggregation Model for Commercial Building Energy Usage”, Ph.D., Case Western Reserve University, Department of Mechanical and Aerospace Engineering, 2017. 9. Kim, H., Marwah, M., Arlitt, M., Lyon, G., and Han, J., 2011, “Unsupervised Disaggregation of Low Frequency Power Measurements”, Proceedings of the 2011 SIAM International Conference on Data Mining, pp.747–758.Available http://epubs.siam.org/doi/abs/10.1137/1.9781611972818.64 10. A. Zoha, A. Gluhak, M. A. Imran, and S. Rajasegarar, “Non-Intrusive Load Monitoring Approaches for Disaggregated Energy Sensing: A Survey,” Sensors, vol. 12, no. 12, pp. 16838–16866, Dec. 2012. 11. D. Borthakur, “HDFS Architecture Guide”, 2008 [Online]. Available: https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html. 12. Solar Degradation and Lifetime Extension (SDLE) Laboratory, cradletools Package. Cleveland, OH: Case Western Reserve University, 2017. Available: https://bitbucket.org/cwrusdle/cradletools/src 13. G. K. Uyanık and N. Güler, “A Study on Multiple Linear Regression Analysis,” Procedia — Social and Behavioral Sciences, vol. 106, no. Supplement C, pp. 234–240, Dec. 2013. 14. B. Scholkopf and A. J. Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. Cambridge, MA, USA: MIT Press, 2001. 15. Dong, B., Cao, C., and Lee, S., 2005, “Applying support vector machines to predict building energy consumption in tropical region”, Energy and Buildings, 37(5), pp. 545–553.Available: http://www.sciencedirect.com/science/article/pii/S0378778804002981 16. A. S. Ahmad et al., “A review on applications of ANN and SVM for building electrical energy consumption forecasting,” Renewable and Sustainable Energy Reviews, vol. 33, no. Supplement C, pp. 102–109, May 2014. 17. L. Breiman, “Random Forests,” Machine Learning, vol. 45, no. 1, pp. 5–32, Oct. 2001. 18. “World Maps of Koppen Geiger Climate Clssification”, Institute for Veterinary Pulic Health Vienna. 2017. [Online]. Availble: http://koeppen-geiger.vu-wien.ac.at/ 19. Nordström, G. H. (2013). “Using the Energy Signature Method to Estimate the Effective U Value of Buildings.” Sustainability in Energy and Buildings Smart Innovation, Systems and Technologies, 35–44. 20. “Pearson Product-Moment Correlation”, Laerd Statistics 2017. [Online] Available: https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php 21. “Electric Power Monthly: Table 5.6., A Average Price of Electricity to Ultimate Customers by End-Use Sector”, United States Energy Information Administration, October 24, 2017. [Online]. Available: https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=epmt_5_6_a 22. “Chicago Energy Benchmarking Homepage”, City of Chicago, 2017. [Online]. Available: https://www.cityofchicago.org/city/en/progs/env/building-energy-benchmarking---transparency.html 23. R. Sakia, “The Box-Cox Transformation Technique: A Review”, The Statistician, vol. 41, no. 2, p. 169, 1992. Available: http://www.jstor.org/stable/2348250?seq=1#page_scan_tab_contents 24. “scikit-learn: machine learning in Python”, Scikit-learn.org, 2017. [Online]. Available: http:/scikit-learn.org/stable/. 25. J. L. Lean and D. H. Rind, “How will Earth’s surface temperature change in future decades?,” Geophys. Res. Lett., vol. 36, no. 15, p. L15708, Aug. 2009. 26. S. R. Loarie, P. B. Duffy, H. Hamilton, G. P. Asner, C. B. Field, and D. D. Ackerly, “The velocity of climate change,” Nature, vol. 462, no. 7276, p. 1052, Dec. 2009. 27. H. Zhao and F. Magoulès, “A review on the prediction of building energy consumption,” Renewable and Sustainable Energy Reviews, vol. 16, no. 6, pp. 3586–3592, Aug. 2012. 28. Amasyali, K., and El-Gohary, N., 2017, “A review of data-driven building energy consumption prediction studies”, Renewable and Sustainable Energy Reviews, 81, pp. 1192–1205. Available: http://www.sciencedirect.com/science/article/pii/S1364032117306093 29. “Retrofitting Existing Buildings to Improve Sustainability and Energy Performance | WBDG Whole Building Design Guide.” [Online]. Available:https:/www.wbdg.org/resources/retrofitting-existing-buildings-improve-sustainability-and-energy-performance. 30. K. Cetin, M Siemann, C Sloop, “Disaggregation and Future Prediction of Monthly Residential Building Energy Use Data Using Localized Weather Data Network,” ACEEE Summer Study on Energy Efficiency in Buildings, 2016. Available: https://aceee.org/files/proceedings/2016/data/papers/12_410.pdf 31. T. Hastie and R. Tibshirani, “Generalized Additive Models,” Statist. Sci., vol. 1, no. 3, pp. 314–318, Aug. 1986. 32. Quick Start Guide to Prophet, CRAN, [Online]. Available: https://cran.r-project.org/web/packages/prophet/vignettes/quick_start.html. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",Data Science for Virtual Energy Audits,6,unlisted,226,15063,0.002058022970191861,349,1,1,0,1,0
5,0,152.7624743223264,0,https://medium.com/p/virtual-building-energy-audits-preliminary-report-bc15108b6fd4,0,None,2018-12-25 21:24:00,33.33,13,1,2018-01-10 08:22:00,"['Machine Learning', 'Data Science', 'Energy', 'Energy Efficiency', 'Education']","Virtual Building Energy Audits — Preliminary Report Applying Data Science to Improve Building Energy Efficiency Author’s Note: The following is a preliminary report of the research project on which I am currently working at Case Western Reserve University. This report provides a high-level overview of the project and documents my current contributions and plans for the future. So often we hear how “Big Data” and “Analytics” will transform our world and yet concrete examples are difficult to enumerate. This project has captured my attention and effort because it utilizes the promise of data science to create measurable real-world benefits. [Numbers] in brackets refer to references at end of report. EDIFES: Energy Diagnostic Investigator for Efficiency Savings Abstract Increasing building energy efficiency is a major part of the global effort to combat climate change. Estimates suggest commercial building energy use can be reduced by thirty percent utilizing existing methods. However, identification of opportunities for energy savings has traditionally relied on expensive and highly variable physical energy audits. The Energy Diagnostic Investigator for Efficiency Savings project at Case Western Reserve University aims to develop a software package to perform virtual energy audits with a turnaround time of several hours. These analyses will require only the building square footage and location from the building owner. Electricity data for the energy audit will be obtained from the utility companies and weather data from Solargis is matched to each building. The EDIFES approach will significantly reduce the time and cost to perform an energy audit while increasing the accuracy and economic/environmental benefits. Introduction Up to 30% of energy used by commercial buildings in the United States is wasted [1]. The majority of this waste is a result of preventable causes such as inefficient heating, ventilation, and air conditioning (HVAC) scheduling, improper insulation, or exterior lights out of sync with natural lighting cycles. The barrier to fixing these inefficiencies is not technology implementations, but rather the identification of savings opportunities. An energy audit involves inspection and analysis of energy flows into and out of a building [2]. Typical commercial energy audits require sending a team of auditors to the physical building location to perform a series of tests. These tests involve a range of techniques and equipment from blower checks of window and door seals to infrared camera scans of entire rooms. Due to the human labor and equipment required, these physical energy audits carry a considerable cost of up to $15,000 [3]. Moreover, these analyses typically have a turn-around time of days or weeks and the conclusions can vary considerably depending on the auditing team. These factors combine to reduce the economic value of an energy audit. EDIFES will address all of these limitations and utilize only a single stream of data, the electricity usage of a building obtained from the utility company [4]. Typically, this data comes in low-frequency 15-minute intervals with consumption in kilowatt-hours (kWh). After receiving the building location and square footage from the building owner, the weather data corresponding to each timestamp obtained from Solargis can obtain the weather data for the location from solarGIS. The objective of the EDIFES project is a software package which will automatically receive the electricity data and metadata, clean the data, analyze the data, and produce a human-readable report with recommendations for efficiency improvements. EDIFES will reduce the time and cost burden of conducing energy audits and consequently will increase the economic benefit of this critical procedure. Approach The procedure for performing a virtual energy audit consists of three distinct steps [5]: 1. Obtaining electricity and weather data 2. Validating and Cleaning data 3. Analyzing the data and composing a report The data is obtained through the electricity utility, Solargis, and building owners. Before processing, raw data is stored in the Hadoop Distributed File System (HDFS), a distributed file storage service designed for warehousing massive amounts of data. After acquisition, the dataset must pass a set of criteria established by the project for data quality. The quality criteria are as follows: data frequency must be 15 minutes or less; data must contain at least one continuous year with fewer than 5 missing days in a row; at most one hour of data may be missing in the first two days; fewer than 10% of data points can be missing/zeros; and fewer than 5% of data points can be anomalous. After passing the screening analysis, the energy data is cleaned with the R software package BuildingRClean developed by the EDIFES team [6]. The cleaning process involves removing outliers and anomalies and imputing missing values either through linear imputation or using diffusion indices if the gap between values is greater than 4 intervals. Validated electricity data is then matched to weather data for the corresponding location and timestamp. Once the electricity data has been validated and matched with weather information, the complete dataset will be registered in HBase, which is built on top of the Hadoop Distributed File Storage system in the Hadoop ecosystem [7]. From HBase, the data can be accessed using the cradletools package developed in the Solar Degradation and Lifetime Extension (SDLE) laboratory [8]. From HBase, the data is retrieved and analyzed using functions in BuildingRClean and the edifes R package. Eventually, the entire ingestion, cleaning, and analysis will be automated, but this procedure currently requires manual commands. Cleaning and analysis is performed within the CWRU High Power Computing (HPC) network due to the computational resources required to analyze the data. The actual data analysis and report generation is the current focus of the majority of work on the EDIFES project. Analysis has primarily focused on creating building markers, identifiers of building characteristics. Other analysis objectives include developing a prediction model for estimating building energy consumption and a building energy disaggregation model. Disaggregation has recently become an area of intense research in the energy efficiency community and involves identifying individual building subsystem energy use from building-level energy data streams [9]. This technique particular interest for residential homes as it would allow homeowners to ascertain contributions of each appliance to an overall electricity bill [10]. Likewise, building managers would be able to identify systems within a large building that consume the most energy use and adjust accordingly. Low frequency, unsupervised, commercial building energy disaggregation is a relatively unexplored research area. Ongoing Work Exploratory Data Analysis The vital first step in working with a new data field is to gain an intuitive understanding of the structure and relationships within the data. This allows a researcher to learn the expected patterns within the data and to spot anomalies or unexpected trends quickly. Therefore, the initial phase of this report began not with data cleaning, but with an exploratory data analysis (EDA) of a cleaned dataset of buildings. The dataset consisted of eight Progressive Insurance office buildings in six different Köppen Geiger climate zones. The EDA quickly revealed the considerable differences in energy patterns both within buildings and between buildings. Consequently, any general model that attempts to predict energy consumption for a building must include a method for normalizing buildings of different sizes, types, and across climate zones. The analysis also revealed that global horizontal irradiance (GHI), a measure of the amount of sunlight striking a surface horizontal to the surface of the Earth, and temperature are the two most highly linearly correlated weather variables with energy consumption as measured by the Pearson correlation coefficient. In the summer, the relationship between temperature and energy consumption is positive, indicating that an increase in temperature is correlated with an increase in energy use. The r-squared value for this relationship is 0.59 indicating that a linear model with temperature as the independent variable can explain 59% of the variability in electricity consumption. During the winter, temperature is generally negatively correlated with electricity use for a building using electric heating, although the strength of the relationship varies by climate region. The summer correlation coefficient (r) between weather variables and electricity consumption for the eight buildings is presented in below. Additional conclusions drawn from the initial EDA include: · Annual energy use is strongly positively correlated with building area in a log-log relationship · Every 1000 ft2 increase in area raises annual energy consumption by 2310 kWh for buildings in the same climate zone · A 1˚C increase in temperature during the summer can result in 200 more kWh of daily electricity usage for a 50000 ft2 building in the Dfrb climate zone · In terms of prediction models, the Random Forest ensemble method significantly outperforms both a Linear Regression model and a Support Vector Machine (SVM) with a radial basis kernel on mean average percentage error evaluated on a test set. An example of the daily, weekly, seasonal, and yearly patterns can be seen below: Building Markers Building markers form the basis of the virtual energy audit. Eventually, the project will have over 40 identifiers which determine information such as HVAC turn on and turn off times, base load to peak load ratio, if exterior lighting is synced to natural light cycles, and the type and size of HVAC systems. At the start of this report, 20 markers had been developed, and 10 had been run on the Progressive set of data. The author of this report was responsible for adding all 20 existing building markers as separate R scripts to the BuildingRClean package with proper documentation. Additionally, an R markdown script was developed that runs all 20 of the building markers and produces a PDF report. Currently, producing the report requires manually entering the file name and square footage of the building, but eventually the entire process will be automated; data will be ingested, cleaned, processed, and reported on without any manual data entry. The first 10 building markers have been validated for accuracy through a review by Progressive building managers. Initial feedback was generally positive but also indicates there is considerable work to do in many areas understanding building energy consumption including HVAC system identification and sizing. The second set of 10 building markers has not been assessed for accuracy but a report containing the results has been completed by the author and is awaiting review. Example output from the base load and base to peak ratio building marker is shown in the following table: The next step with the building markers is to automate report generation and perform a cross-sectional study on 350+ buildings. This will involve developing an R script which can retrieve the building electricity data and metadata from Hbase, feed the data into the completed R markdown file, knit the PDF report, and write the results (including images) back to Hbase for later analysis and querying. All of these steps have been done in isolation by EDIFES, but they will need to be combined and automated for any realistic business application. Modeling Energy Use with Machine Learning Algorithms An additional aspect of the EDIFES project is the prediction of electricity use as a function of weather conditions and building characteristics. An accurate prediction model would allow a building manager (or automated thermostat) to intelligently adjust an HVAC schedule for a weather forecast, or to calculate the effects of a building retrofit on electricity consumption. In order to fill in missing electricity consumption when more than 4 data points in a row are missing, EDIFES currently uses diffusion indices to forecast energy use. This report focuses instead on the development of a supervised machine learning model which uses weather data and time as features and the electricity consumption as the target. The technique will be supervised because the model will learn based on the past electricity consumption and weather information. A complete, optimal model would not only accurately predict energy use for specified weather conditions, but would also incorporate the ability to change building characters, such as the effective thermal resistance, to determine the effect of energy efficiency implementations. As discovered in the EDA, there are significant intra-building and inter-building variations in electricity consumption across seasons, building type, size, and location. Therefore, the approach taken in this report was not to develop a single model for all buildings and seasons, but to create four models for each building corresponding to the four seasons (with winter defined as Dec, Jan, Feb and summer as Jun, Jul, Aug). Due to the wide availability of cheap computing resources and efficient algorithms, training four models for each building is feasible within a matter of seconds even on a personal laptop. Three algorithms were evaluated using the Scikit-Learn library in Python: a Linear Regression, a Random Forest Ensemble with 100 decision trees, and a Support Vector Machine with a radial basis kernel. These models were selected based on a review of the relevant literature and the author’s machine learning experience [12][13][14][15]. The procedure for evaluating a model on a specified season is as follows: 1. Subset the data for the season and extract the features (variables) and labels (electricity consumption). 2. Format the data as numpy arrays and split the subsets into a training and testing set using a .75/.25 split. 3. Train the models on the training data (features and labels) and make predictions on the testing data (features only). 4. Compare the predictions to the known test targets using the mean average percentage error (MAPE). The results for the models on six Progressive buildings are presented below: Average MAPE across all test sets Linear Regression 39.15% Support Vector Machine with Radial Basis Kernel: 26.62% Random Forest with 100 Decision Trees: 7.11% It must be stressed these results are preliminary and were determined from base models with no feature engineering or hyperparameter tuning. Nonetheless, it was concluded linear models are not able to capture the inherent non-linearity of the relationships between the features and the energy use. The Rnadom Forest Ensemble method demonstrates considerably high accuracy compared to the other two approaches. This result also compares favorably to the mean average percentage error reported by previous EDIFES work [5]. Work is currently underway to optimize the algorithms and try additional techniques including a deep neural network and recurrent neural network. Moreover, there might be more optimal ways of approaching the problem such as creating a modeling for every month, or performing principal or independent component analysis on the features before training the models. Early results, as pictured below, indicate machine learning approaches are applicable to the energy modeling and prediction problem. Future Work Current work on the EDIFES project involves many different avenues of exploration, some of which may produce usable results and some of which may prove infeasible. Therefore it is expected that objectives and areas of concentration will evolve as the project requires. The main areas for current and future focus are as follows: · Validate the second set of 10 building markers · Develop an automated script for fetching building data, cleaning the data, generating a PDF report, and writing results to data storage system · Test the complete building markers on 350+ buildings and perform a cross-sectional analysis · Develop an accurate model for disaggregating building subsystems from whole building energy and validate results building upon existing work [16] · Optimize machine learning models for prediction and determining effects of building retrofits on energy consumption. Conclusions The scope of the EDIFES project encompasses many exciting and rapidly evolving areas of data science and has the ultimate goal of increasing real-world energy efficiency. This report presented an overview of the EDIFES project, outlined the virtual audit approach, documented current work done by the author and other members of the EDIFES team, and detailed future work areas. The main work completed thus far by the author is an exploratory data analysis of eight Progressive Insurance buildings, the addition of 20 developed building marker scripts to the BuildingRClean package with documentation, and the development of baseline machine learning prediction models. All three of these areas naturally lead to more exploration, research, and development, and the author is eager to see where this leads while enjoying every minute of work on this project! Acknowledgements The author would like to thank the following members of the EDIFES team for their support and tolerance of an unreasonable large number of questions: Professor Alexis Abramson, Professor Roger French, Rojiar Haddadian, Arash Khalilnejad, Jack Mousseau, and Shreyas Kamath. References [1] “About the Commercial Buildings Integration Program | Department of Energy”, Energy.gov, 2016. [Online]. Available: https://energy.gov/eere/buildings/about-commercial-buildings-integration-program. [2] University of Hawaii’s Environmental Research and Design Laboratory, 2013, Energy Audit Procedures, US Department of Energy Office of Electricity Delivery and Energy Reliability. Available: https://www.hnei.hawaii.edu/sites/dev.hnei.hawaii.edu/files/Energy%20Audit%20Procedures.pdf [3] EMS Environmental Incorporated, “How Much Does a Commercial Energy Audit Cost”, 2017 [Online]. Available: http://emsenv.com/2016/04/28/commercial-energy-audit-cost/. [4] “ARPA-E | Virtual Building Energy Audits”, Arpa-e.energy.gov, 2015. [Online]. Available: https://arpa-e.energy.gov/?q=slick-sheet-project/virtual-building-energy-audits. [5] E. Pickering, “EDIFES 0.4 Scalable Data Analytics for Commercial Building Virtual Energy Audits”, Masters of Science in Mechanical Engineering, Case Western Reserve University, 2017. [6] J. Mousseau, A. Khalilnejad, M. Hossain, R. Haddadian, E. Pickering, W. Koehrsen, S. Kamath and K. Nash, BuildingRClean R Package. Cleveland, OH: Case Western Reserve University, 2017. Available: https://bitbucket.org/cwrusdle/buildingrclean/overview [7] D. Borthakur, “HDFS Architecture Guide”, 2008 [Online]. Available: https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html. [8] Solar Degradation and Lifetime Extension (SDLE) Laboratory, cradletools Package. Cleveland, OH: Case Western Reserve University, 2017. Available: https://bitbucket.org/cwrusdle/cradletools/src [9] Kim, H., Marwah, M., Arlitt, M., Lyon, G., and Han, J., 2011, “Unsupervised Disaggregation of Low Frequency Power Measurements”, Proceedings of the 2011 SIAM International Conference on Data Mining, pp.747–758.Available http://epubs.siam.org/doi/abs/10.1137/1.9781611972818.64 [10] J. Z. Kolter and M. J. Johnson. REDD: A public data set for energy disaggregation research. In Workshop on Data Mining Applications in Sustainability (SIGKDD), San Diego, CA, volume 25, pages 59–62, 2011. Available: http://redd.csail.mit.edu/kolter-kddsust11.pdf [11] “GIS data and maps”, Solargis.com, 2017. [Online]. Available: http://solargis.com/products/maps-and-gis-data/. [12] Dong, B., Cao, C., and Lee, S., 2005, “Applying support vector machines to predict building energy consumption in tropical region”, Energy and Buildings, 37(5), pp. 545–553.Available: http://www.sciencedirect.com/science/article/pii/S0378778804002981 [13] Amasyali, K., and El-Gohary, N., 2017, “A review of data-driven building energy consumption prediction studies”, Renewable and Sustainable Energy Reviews, 81, pp. 1192–1205. Available: http://www.sciencedirect.com/science/article/pii/S1364032117306093 [14] Zhao, H., and MagoulÃ¨s, F., 2012, “A review on the prediction of building energy consumption”, Renewable and Sustainable Energy Reviews, 16(6), pp. 3586–3592.Available http://www.sciencedirect.com/science/article/pii/S1364032112001438 [15] Gonzalez, P., and Zamarreo, J., 2005, “Prediction of hourly energy consumption in buildings based on a feedback artificial neural network”, Energy and Buildings, 37(6), pp. 595–601.Available: http://www.sciencedirect.com/science/article/pii/S0378778804003032 [16] M. Hossain, “Development of Building Markers and an Unsupervised Non-Intrusive Disaggregation Model for Commericial Building Energy Usage”, Ph.D., Case Western Reserve University, Department of Mechanical and Aerospace Engineering, 2017. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",Virtual Building Energy Audits — Preliminary Report,6,unlisted,3,3659,0.0,349,1,1,0,1,0
4,24,152.761657678125,4,https://medium.com/p/building-energy-data-analysis-part-four-7d87ca34a6ae,0,None,2018-12-25 21:25:00,12.15,35,13,2018-01-10 08:17:00,"['Machine Learning', 'Data Science', 'Energy', 'Energy Efficiency', 'Education']","Building Energy Data Analysis Part Four Final Plots and Takeaways Author’s Note: This is the concluding part to the building energy data analysis series. Part One described the background for the problem and introduced the data; part two identified significant trends and relationships within the data through graphs and quantitative analysis; and part three featured an in-depth modeling exploration. In this post, I will highlight the most relevant findings from the first three parts, show the best graphs, and offer some ideas for future work. As always, I appreciate any feedback and constructive criticism at wjk68@case.edu. Full code can be found on GitHub. This project was conducted as part of my involvement with Project EDIFES (Energy Diagnostic Investigator for Efficiency Savings) at Case Western Reserve University. Introduction The procedure followed in this EDA was as follows: 1. Formulate initial exploratory questions to guide analysis.2. Determine the data that needs to be gathered to answer the questions. Obtain, clean, and structure the data into a tidy format.3. Explore the data using quantitative summary statistics and visualizations.4. Determine trends and relationships within the data both quantitatively and with graphs. 5. Use several different models to extract meaningful conclusions from the relationships between explanatory and response variables.6. Evaluate several models for prediction and select the best one for refinement.7. Challenge the best model and assess the prediction results. 8. Interpret and report the results of the data analysis and modeling. Throughout the report, the following meteorological definitions of summer and winter apply: • summer = June, July, August• winter = December, January, February These “seasons” are not necessarily valid across the entire country but are used by the EDIFES team because no other definition has yet been developed. This is an area of concern as the same definitions for the seasons are not necessarily applicable in all climate zones. Exploratory Questions Based on the goals of the overall project and an initial exploration of the data, the following set of questions was formulated at the outset: It was expected these questions would change over the course of the project and additional queries would be asked based on what the data could and could not answer. In the words of John Tukey (a mathematician who developed the FFT and boxplot): “Data analysis, like experimentation, must be considered as an open-ended, highly interactive, iterative process, whose actual steps are selected segments of a stubbily branching, tree-like pattern of possible actions.” By the end of the project, the focus had narrowed to one main question composed of two parts: Which weather and time conditions are correlated with energy use and is it possible to build a model to predict energy use from these variables? Data Science Methods A number of different data science techniques and concepts were covered during the course of the EDA. These ranged from basic statistical concepts to machine learning algorithms. Statistical Concepts Statistical concepts covered in this project are summarized below: • Pearson Correlation Coefficient: ranges from -1 to +1 and demonstrates the strength and direction of a linear relationship between two variables. -1 indicates a perfectly linear negative relationship and +1 indicates a perfectly positive linear relationship.• R-Squared: The square of the Pearson Correlation Coefficient. Indicates the percentage of variation in the response variable explained by the explanatory variables in the model. This can be used to assess the performance of a model in capturing relationships between the exploratory and response variables.• Box-Cox Power Transformation: A method for determining the best transformation to normalize data. This applies a number of power transformations to the data and returns the power that results in the most normal distribution.• Central Limit Theorem: This theorem states that for a non-skewed sample distribution, the distribution approaches normal as the sample size increases. A general heuristic is that 30 samples is the minimum for satisfying the theorem. This means that it is not possible to make statistically significant comparisons between the eight office buildings. Machine Learning Models The challenge of predicting energy consumption from weather and time data is a supervised regression task. It is supervised because the targets, in this case the energy consumption for each 15-minute interval, are known, and it is a regression task because the labels are continuous values. During training, the labels and features (explanatory variables) are given to the model so it can learn a mapping between the features and the labels. In testing, themodel is only given the features and must predict the value of the labels. The predictions can then be compared to the known values (in this case the known energy consumption) to assess the performance of the model. There are numerous modeling approaches for supervised regression tasks that can be implemented in a number of programming languages. These range in complexity from linear models with simple equations and understandable variable weights, to deep neural networks which are essentially black boxes that accept data as an input, perform a sequence of mathematical operations, and produce an (often correct) answer with no human-understandable interpretation. A major concentration of this project was intrepretability in addition to accuracy, and models were chosen for the best combinationof these two features. The three algorithms selected were designed to cover the range of complexity and interpretability, starting with the simplest, Linear Regression, proceeding to the ensemble Random Forest regression method, and finally moving on to the Support Vector Machine Regressor. To compare the algorithms, a random train-test split was used (with the same features and labels used for all models). The objective was to select the best performer on the random split and implement this method for predicting six months of energy consumption. All three algorithms were built using the Scikit-Learn library in Python while the data was preprocessed in R. Transferring the data between R and Python is relatively simple with the feather library that saves and loads dataframes in R and pandas dataframes in Python. The basic steps for evaluating the algorithms on a random training/testing split are as follows: 1. Structure the data into an appropriate format and data structure; separate the features and labels.2. Subset the data into a training and testing set using a 0.70/0.30 random split.3. Train each model on the same training set with the model given both the features and the labels.4. Use the trained model to make predictions on the test features.5. Compare predictions to the known testing labels and compute performance metrics.6. Make comparisons between algorithms for accuracy and interpretability; select the best model for further development. A brief overview of the three machine learning algorithms covered in this report is presented below: Simple and Multivariate Linear Regression The simplest model we can use is linear regression which aims to explain the variance in the response variable by a weighted linear addition of the explanatory variables. Linear Regression can use a single explanatory (independent) variable, or it can use many. The general equation for a linear model is y = w0 + w1 * x1 + w2 * x2 + … + wn * xnwhere w0 is the intercept, xn represents the explanatory variables, wn is the weight assigned to each explanatory variable, and y is the response variable. In the case of the building energy data, y is the energy consumption, x is the weather or time features, and w is the weight assigned to each feature. The weight in a linear model represents the slope of the relationship, or how much a change in the x variable affects the y variable. A great aspect of linear modeling is that the impact of change in one independent variable on the dependent variable can be directly quantified. Random Forest Regression To understand the powerful random forest, one must first grasp the concept of a decision tree. The best way to describe a single decision tree is as a flowchart of questions which leads to a classification/prediction. Each question (known as a node) has a yes/no answer based on the value of a particular explanatory variable. The two answers form branches leading away from the node. Eventually, the tree terminates in a node with a classification or prediction, which is called a leaf node. A single decision tree can be arbitrarily large and deep depending on the number of features and the number of classes. They are adept at both classification and regression and can learn a non-linear decision boundary. However, a single decision tree isprone to overfitting, especially as the depth of the tree increases because the decision tree is flexible, leading to a tendency to memorize the training data (that is, the model has a high variance). To solve this problem, ensembles of decision trees are combined into a powerful classifier known as a random forest. Each tree in the forest is trained on a randomly chosen subset of the training data (either with replacement, called bootstrapping, or without) and on a subset of the features. This increases variability between trees making the overall forest more robust and less prone to overfitting. In order to make predictions, the random forest passes the explanatory variables (features) of an observation to all trees and takes an average of the votes of each tree (known as bagging). The random forest can also weight the votes of each tree with respect to the confidence the tree has in its prediction. Overall, the random forest is fast, relatively simple, has a moderate level of interpretability, and performs extremely well on both classification and regression tasks. There are a number of hyperparameters (best thought of as settings for the algorithm) that must be specified for theforest before training. The most important settings are the number of trees in the forest, the number of features considered by each tree, the depth of the tree, and the minimum number of observations permitted at each leaf node of the tree. These can be selected by training many models with varying hyperparameters and selecting the combination that performs best on cross-validation or a testing set. A random forest performs implicit feature selectionand can return relative feature importances which can be interpreted in the domain of the problem to determine the most useful variables for predicting the response or for variable selection with other models. A simplified depiction of a single decision tree used to predict energy consumption based on the time and weather conditions is presented below. Support Vector Machine Regression A support vector machine (SVM) regressor is the most complicated and the least interpretable of the models explored in this report. SVMs can be used for both classification and regression and operate based on finding a hyperplane to separate classes. The concept is that any decision boundary becomes linear in a high enough dimensional space. For example, a linear decision boundary in three-dimensional space is a plane. The SVM projects the features of the observations into a higher dimensional space using a kernel, or a transformation of the data. The model then finds the plane that best separates the data by maximizing the margin, the minimum distance between the nearest member of each class and the decision boundary. The support vectors in the name of the algorithm refer to the points closest to thedecision boundary, called the support, which have the greatest influence on the position of the hyperplane. SVM regressors work by fitting a non-parametric regression model to the data and trying to minimize the distance between the model and all training instances. SVM models are more complex than either a linear regressor or a random forest regressor and have almost no interpretability. The transformation of the features into a higher-dimensionalspace using a kernel removes all physical meaning of the features. SVM models are black boxes but have high accuracy on small datasets with limited amounts of noise. The RBF or Radial Basis Kernel, is the most popular kernel in use in the literature and is the default in most implementations of the support vector machine including in Skicit-learn in the Python language. An example of a support vector machine for classification with several different kernels is shown below (from Skicit-learn). Exploratory Data Analysis The driving goals behind the EDA were to determine the trends and patterns within the electricity consumption data and to find the most significant relationships between weather/time conditions and energy use. The EDA quickly revealed building energy consumption exhibits daily, weekly, and seasonal repeating patterns in addition to overall trends. Moreover, these patterns differ significantly between buildings in different climate zones and buildings in different industries (for example, retail buildings that are open 7 days a week compared to office building operating only during the week). The patterns were analyzed at different time scales and buildings were compared across seasons and climate zones. Long-Term Trends To get a feel for the overall structure of the data we can graph the entire time series of energy consumption. The following graphs show three buildings with business days separated from the non-business days. There are a number of noticeable trends in these plots. For all the buildings, energy consumption increases during the the summer which agrees with domain knowledge. The largest source of office building energy consumption tends to be the heating, ventilation, and air conditioning (HVAC) system, which sees more use during the summer months for the southern climates. Kansas City, Las Vegas, and Phoenix experience hot summers and hencewill see an increase in energy consumption due to air conditioning use. The buildings in Las Vegas and Phoenix exhibit a decrease in energy consumption during the winter because these cities typically experience mild winters which do not necessitate energy-intensive heating. Meanwhile, the building in Kansas City does not exhibit the same large decrease during the summer. Energy consumption for this building has two peaks, with one occurring during late summer, and the other occurring during late winter. Monthly Trends A great plot for understanding the location and spread of data is the boxplot. A boxplot shows the median, the interquartile range, and any outliers in a dataset. The following graphs show boxplots with the average energy consumption per day by month for the same three buildings. The boxes are calculated by averaging the days across a month, and hence the y-axis represents the daily energy consumption. These boxplots are created using only business days. The same yearly behavior is observed as before, with an increase in daily energy use during summer for theLas Vegas and Phoenix buildings, and two yearly peaks in consumption for the Kansas building. The Kansas building has significantly more outlying points during the spring months indicating that daily energy consumption can vary significantly during these months. This is not unexpected as some springs may be much colder than others, necessitating heating whereas other springs may be mild with no need for heating or air conditioning. Weekly Trends Finally, the last two patterns in the data are those over the course of a week and throughout the day. The graphs below are colored by season to illustrate the change in patterns that occur throughout the year. The energy consumption is in kWh for each 15 minute interval of the day. These graphs show trends by week, over the course of a day, and in different seasons. It is difficult to draw broad conclusions because all of the buildings display different behavior. However, it is clear that energy consumption is higher during the week as is expected for office buildings. The energy use typically rises in the morning which would correspond to the HVAC starting and workers arriving at the building. The energy consumption remains high during the work day, and then drops significantly overnight. The energy consumption never drops to zero because all buildings still have systems that must be kept running constantly. This is referred to as the baseload of a building and is a target for energy use reduction. Energy use in noticeably higher in the summer for the Phoenix and Las Vegas buildings and also slightly higher during the summer for the Kansas building. Spring and Fall generally exhibit lower energy consumption than winter and summer, except for the Kansas building where energy consumption is second greatest during the fall. The weekend patterns are very interesting as the energy consumption is moderate during the morning and afternoon, in line with that observed during the day, but then drops precipitously during the afternoon. This is intriguing because it suggests certain systems are shut off during the afternoon on the weekends but then are turned on again during the morning and afternoon. Overall, there are numerous conclusions to draw about a single building from these plots, and they will be a useful tool to the EDIFES team when analyzing the energy profile of a building. After analyzing patterns, the main focus of the exploratory data analysis shifted to looking for significant correlations between weather conditions and energy consumption. Based on these relationships, it should be possible to inform a building owner ahead of time how much energy they can expect to use given the weather forecast and how she/he may potentially mitigate energy use by preparing for the weather conditions. The best approach for determining weather correlations is to calculate them separately during the winter and summer. Depending on the climate zone and the type of heating used by the building, the direction and strength of weather correlations vary greatly. The following plots show correlations both among the weather variables and between weather conditions and energy use for the building in Phoenix. The plots are separated into winter and summer to show the differences between seasons. Overall, the highest positive linear correlation with energy consumption during the summer is temperature, followed by global horizontal irradiance (ghi). Both relationships agree with domain knowledge. The winter correlations between energy and weather are not as strong as those during the summer. One possible conclusion is that for buildings in climates that are hot year-round, weather has less of an effect on energy consumption during the winter. However this trend will not hold across different geographic sections of the country, and will also change based on the building industry. A method to compare across climate zones is to look at all the correlations between energy and weather conditions for each building. Looking at the numerical correlations will work for this purpose, but a quicker way to gauge the relative differences in correlations between buildings is with a heatmap. These show stronger correlations as more vivid colors and can express vast amounts of information in a compact form. These heatmaps show the Pearson Correlation Coefficient between the respective weather variable and energy consumption during each 15-minute interval. These visualization contain a significant amount of information. The major conclusion for the summer is that temperature tends to be positively correlated with energy consumption as does ghi, dif, and gti. The exception to this statement is the building in Portland, Oregon, which exhibits the exact opposite correlations as the other buildings. This was considered to be anomalous, and the building owners were contacted for an explanation, but none was provided. It is possible that the time stamp on the electricity meter is systematically wrong, or the measurements could be strange but correct. If the meter was wrong, the timestamp would most likely be off by a fixed period such as 12 hours. It would be interesting to re-run these calculations with the timestamp shifted by 12 hours. This building is also the only building location in a relatively cold climate, which could explain the difference in behavior. The knowledge that temperature and the three irradiance variables are positively correlated with energy consumption (ignoring the Portland building for now) could inform recommendations to a building owner. These might include: • Add additional insulation to keep the cool air in during the summer and hot in air during the winter • Install double-paned windows with adequate sealing • Cover windows in direct sunlight during the day • Schedule employees to work earlier in the morning and leave during the afternoon when the temperature peaks. This last recommendation may appear drastic, but it could be necessary as the efforts to reduce energy consumption become more urgent. However, the other recommendations are simpler to implement and can result in significant reductions in energy use with no disruption to productivity. The majority of the winter correlations are smaller in magnitude and not consistent across the buildings. In particular, the temperature correlation varies significantly as it is positive in some buildings and negative for others. One potential explanation is the typical heating in use in the climate zone. If a building has gas heat, then as the temperature decreases, it would not be expected that the electricity consumption would increase and hence the temperature — energy correlation will be small or positive during the winter. A building with electrical heating will need to use more electricity as the temperature decreases, and hence the temperature — energy correlation will be negative. That is, as the temperature decreases, the energy use increases. The Portland building again stands out, although it is similar to the Kansas City building which could support the hypothesis that these buildings are different as they are located parts of the country that experience cold winters. There is considerable additional analysis that can be performed on these correlation heatmaps, and they hold potential for informing decisions with real-world benefits. After performing this analysis on the eight buildings, a similar analysis of weather correlations with energy consumption was done on all 750+ building datasets that EDIFES currently uses. The results are as follows where the numbers and colors again correspond to the Pearson Correlation Coefficient. The climate zones are the Koppen Geiger Climate Zones. The branching trees on the top and left side of the heatmap are dendrograms created by a hierarchical clustering. The clustering groups together similar climate zones and similar weather conditions by the magnitude of the correlation coefficient. Using these dendrograms could provide a method for comparing buildings in different climate zones. In general, temperature, gti and ghi are positively correlated with energy consumption during the summer and relative humidity is negatively correlated with energy use. However, during the winter, there are significant differences between the temperature correlation by climate zone, with some zones exhibiting a negative correlation and others a positive correlation. This is again explained by the type of heating used. The majority of the winter correlations are small in magnitude as was observed for the eight office buildings. The smaller magnitude of these coefficients might be due to averaging across many buildings in each climate zone. For example, in a climate zone with two buildings, if one building has a positive correlation between temperature and energy during the winter and the other has a negative correlation, averaging these two out may lead to the conclusion that temperature has no correlation with energy use in that climate zone during the winter. That being said, we can compare individual buildings to the climate zone average correlations to identify anomalies which could indicate inefficiencies. Modeling and Prediction The Explanatory Data Analysis revealed the many patterns and relationships within the energy data, which left the second part of the main question still to be answered: what types of models will be able to capture these mappings and use them for successful prediction? A number of differences approaches were investigated as it can be difficult to ascertain ahead of time which methods will prove most capable. Modeling started off with a basic simple linear model. The objective was to predict the average daily energy consumption from the average daily temperature. Due to the differences between buildings and seasons, a different model was created for each building and season. Temperature was selected as the single explanatory variable because it displayed the highest correlation with energy consumption. Following is a summary of all the linear models. Reported in the table is the slope of the temperature with respect to the energy consumption and the r-squared for each model during the summer and winter. While a linear model cannot capture complex relationships, it does have a high level of interpretability because the features are not transformed. The slope of this model represents the daily change in kWh of energy use for a 1 degree Celsius daily increase in temperature. Therefore, for the building in Sacramento, a 1 degree warmer day during the summer results in 308 kWh more electricity use, and at the national average energy price of $0.104/ kWh, that represents an additional $30 per day. During the winter, some buildings will observe a decrease in electricity usage with a decrease in temperature, while other buildings will have an increase in electricity usage with an increase in temperature. The results also show that this model is not adequate for explaining the variability in energy consumption. The summer r-squared is less than 0.2 for all buildings indicating that the model can explain less than 20% of the observed variability in energy consumption based only on the temperature. The winter r-squared values are similarly small. This indicates that a more complex model is needed, or more variables should be included in the linear model. An example of the univariate linear model plotted on the actual summer data is below for the first building in Phoenix with a summer slope of 21.16. The points in red are the high leverage points, or those with the greatest distance from the main cluster of points. These points do not have a considerable influence on the slope of the model as can be seen in the three different lines drawn on the data. The blue line represents that created with non high-leverage points, the black line represents a model with all data points, and the red line indicates a model with only the high-leverage points. This shows that even if points have high leverage, they will not necessarily have a significant impact on the slope. From this analysis, removing these three points would not be justified because of their small effect on the slope, and there is no indication they are the result of incorrect measurements. Before moving on to more complex models, a multivariate linear model was created to predict daily average energy use using all the weather variables. The energy use is not dependent on only temperature, and a model with more variables might be able to account for more of the variability in the average daily energy use. The following table shows the r-squared values for a linear model created between temperature and all of the weather conditions. The calculated r-squared values are not significantly better than those produced by temperature alone. An Analysis of Variance (ANOVA) test showed that the model with all weather variables did not exhibit a statistically significant improvement (with alpha = 0.05) in capturing relationships within the data compared to the model with only temperature. Predicting daily energy use averages accurately is not possible because there are significant patterns that occur throughout the day. Averaging over an entire daily essentially reduces the amount of data available by a factor of 96, the number of daily observations of weather and temperature. Therefore, in order to create a more accurate model, the model should predict the energy consumption for each and every 15-minute interval from the corresponding time and weather conditions. To find a daily total, these predictions can then be summed up over the 96 intervals, or a more accurate approach to assessing the predictions is to compare them to the known energy use for each 15-minute period. By making predictions every 15 minutes, and using all of the time variables in addition to the weather variables, the accuracy of the model and the percentage of the variation explained is expected to be significantly greater. The daily averaging method with a linear model proved inadequate, and there is a need for more complex models operating at a higher granularity to capture all of the relationships within the energy information. Three models were selected for comparison based on results reported in the literature and in order to cover a range of complexity. A linear model, random forest, and support vector machine were all evaluated for regression capabilities. The models were compared against one another using a random 0.70/0.30 training/testing split of the data. In hindsight, comparing the models using the random split of data was not the optimal choice because they were being evaluated for predictive abilities. However, the random split of the data should be adequate for comparing the algorithms because it still allows for relative evaluations. A model that cannot perform well at estimating the electricity consumption on a random split of the data also will not be accurate at predicting months worth of electricity data into the future. The objective of the model comparison phase was to determine the most promising model that would then be refined for the task. Data Preparation In order to compare models, they must be trained and tested on the same dataset. Preparation of the data was completed in R, and the results were then saved as feather files for loading into Python. The algorithms themselves were implemented using the Skicit-learn library in Python because of the speed and increased training and model control. The results were then saved as feather files and analyzed using R because of the graphing capabilities. In short, the languages were used for their respective strengths! Data preparation involved the following four steps: Not only must the models be trained and tested on the same data, they must be evaluated using the same performance metrics. The following set of metrics was used for comparison: 1. rmse: root mean square error; the square root of the sum of the squared deviations divided by the number of observations. This is a useful metric because it has the same units (kWh) as the true value so it can serve as a measure of how many kWh the average estimate is from the known true target. 2. r-squared: the percentage of the variation in the dependent variable explained by the independent variables in the model. 3. MAPE: mean average percentage error: the absolute value of the deviation for each prediction divided by the true value and multiplied by 100 to convert to a percentage. This metric indicates the percentage error in the prediction and is better than the rmse because it takes into account the relative magnitude of the target variable. Model Comparison The results of the model comparison on a random train/test split of the data are shown in the following graphs. The random forest regression model is the clear winner on all metrics. The linear regression is not able to capture the relationship between the inputs, the time and weather conditions, and the outputs, the energy consumption. The support vector machine does considerably better, but it does not match the performance of the random forest. Moreover, the runtime of the random forest was much lower than that for the support vector machine, and the svm does not have interpretability because the features are transformed into a higher dimension using a kernel (radial basis kernel in this implementation). The random forest is relatively simple to understand, has good interpretability because it returns the relative importance of features, is quick to train and make predictions, and has the best performance of all the models. Therefore, the random forest model was chosen for refinement and testing for prediction capability. Random Forest Validation and Prediction Once the random forest had been selected, the next step was to optimize the hyperparameters for the particular problem. In contrast to model parameters which are learned by the model during training, the hyperparameters must be set by the programmer ahead of training. One way to think of these is as model setting knobs that can be turned back and forth until the model performs up to standards. In Skicit-learn, models are built with a sensible set of default hyperparameters, but there is no guarantee these are optimal and only a rigorous evaluation routine can determine the best set. Hyperparameter tuning requires creating many models with different options and then evaluating each using cross-validation to determine which set of configuration options performs optimally. This can be done automatically in both R and Python. The method used for this project was grid search with cross-validation using Skicit-learn in Python. The following set of hyperparameters achieved the highest metrics on a random training/testing split (again, perhaps not the best choice for this problem). • 200 decision trees in the forest • No maximum depth for each tree (trees will be grown until all predictions are made) • A minimum of 1 sample per leaf node • A maximum fraction of 50% of the features evaluated for each tree These hyperparameters resulted in an increase in performance of the algorithm without any noticeable increase in runtime. The following graph shows the root-mean squared error comparison of the baseline to optimized model. Here, rmse is an adequate metric to use because it is only serving as a measure of relative performance. The optimized model performs better than the baseline model in rmse for all buildings. The next step was to challenge the random forest by predicting six months of energy consumption. Six months was selected to correspond to the ARPE-E milestone to develop a predictive model capable of an adjusted r-squared of 0.85 when predicting six months worth of energy use. To implement this test, the dataset was split into training and testing with the final six months used for testing and the rest of the data used for training. The data was again prepared in R, the algorithm implemented in Python, and the results analyzed in R. The prediction metrics are presented below in graphical form. Four of the eight buildings meet the r-squared requirement. The average performance metrics were a mape of %21.24, an r-squared of 0.77, and an rmse of 3.83 kWh. Although these metrics are not outstanding, they represent a decent start towards meeting the project modeling requirement. Predicting six months of energy use is difficult, especially with some datasets not quite one year in length. The random forest model is a promising method for predicting energy consumption, but there is also a need to try additional methods. Another method currently used by the team is the diffusion indices method. One method that deserves more exploration is a general additive model, in which the time-series is modeled as sum of patterns on different time scales (days, weeks, months, years), as well as an overall trend (increasing or decreasing with time). The prophet package in R provides a simple implementation although this is based on daily observations and will require adjustment for more frequent observations. Overall, the prediction testing showed the random forest is capable of achieving required performance when predicting six months of energy consumption. Further optimization of the hyperparameters and feature engineering will improve the random forest and it can be combined with ensemble methods to develop a robust prediction model. Discussion The office building energy use EDA provided the ideal opportunity to explore the problem domain. Many of the findings, while not entirely novel, were surprising to the author. All of these could have been directly told to the author, but it is a more memorable exercise to have to write the code and make the discoveries firsthand rather than hearing them passively. Moreover, it is reassuring when the conclusions reached by an independent team member agree with those established by the rest of the team. While disagreements drive progress, similar results confirm the methods in use and allow for the establishment of base truths from which to work. One major finding was the number of different patterns that can be ascertained in the energy data. Each building exhibits different patterns depending on the time of day, day of the week, the season of the year, and even between years if a retrofit to improve efficiency has been performed. Clearly, the process of modeling and eventually forecasting energy consumption will require sophisticated methods that can separate and model each of the trends. Additional significant findings from the EDA were weather and energy correlations that show temperature and global horizontal irradiance (GHI) are the most highly correlated variables with energy use during the summer. These results suggest several methods for reducing energy consumption including increasing insulation, installing shades over windows in direct sunlight, properly sealing windows and doors, and positioning buildings to use the sun as a natural source of heating in the winter without absorbing excess heat in the summer. The EDA also showed the interpretations that can be drawn from a linear model as well as the limitations linear models have because they are not flexible enough to capture intricate trends. One significant aspect of the EDA that was unsuccessfulwas the attempt to normalize buildings across climate zones, building types, and building sizes. There were not enough buildings to make statistically significant comparisons. The modeling section of the report focused on two parts: an explanatory part in which the reason behind energy consumption was investigated, and a predictive part, in which energy consumption was predicted from a set of features. The final random forest model can be used for both explanation, in terms of feature importances, and prediction. Example predictions from the Random Forest are presented in the following figures: The first image shows the entire dataset and the predicted portion which aligns closely to the actual values. The second image shows a typical week of predictions and how the random forest is able to accurately capture all of the different patterns within the data. For this building, the mean average percentage error was 15.72%, which is acceptable but will need to be improved. To get a sense of why the random forest makes a certain prediction, it can be helpful to look at the feature importances. These are not useful so much in absolute terms as in relative terms to rank features. The actual meaning behind the importances is the reduction in uncertainty due to splitting a node on the given feature. The following graphs show first the feature importances for each individual building, and then averaged over all the buildings. In total, after feature engineering, there are 24 different features used by the random forest. Temperature is the single most important factor for determining the energy consumption at any 15-minute interval. This is in agreement with the correlation coefficients and with the linear modeling. Moreover, dif, gti, and ghi are the next highest importance weather variables, in agreement with the correlation coefficients. The day of the week and the time of the day have high importance, which makes sense given the domain because the weekends have considerably lower consumption than during the week, and the consumption also varies greatly over the course of a typical day. The cosine transformation of the time of day has a higher importance than the raw time itself, which validates the cyclical feature transformation performed during data preparation. There are several variables with very low importance, and these could likely be removed from the model without resulting in a performance decrease. The random forest performs implicit feature selection, and it will learn these are not useful, but removing them could help other methods that would only be “confused” by these unimportant features. Most of the features with low importance have low variance, meaning they fall within a narrow range, or take on the same value for most observations. It is possible to remove features with low variance during data preparation using preprocessing in the caret package. Although the random forest does not return weights associated with the features which means that the effect of a unit change in a feature cannot be assessed as in the linear models, the random forest can still be used to observe the effects of altering a variable. Experiments with altering variables can be performed using the random forest in order to assess effects of various changes in weather conditions and possibly changes made to make a building more efficient. Current predictions indicate the global surface temperature will increase between 1 C and 4 C by 2100 as the result of human-caused climate change. In order to assess these effects on future energy use consumption, a reasonable increase of 2 C can be modeled using the random forest. This is done by training the model on the entire historical data available, and then creating a testing set by keeping all of the explanatory variables the same except for the temperature, which will be increased at every interval by 2 C reflecting a mean increase in line with forecasts for future warming. In this situation, there are no correct values to compare the predictions to, and this is purely an experiment to observe the effects. The economic effects in terms of annual energy use costs are demonstrated below. These results were obtained by finding the difference between the unaltered energy consumption and the predicted energy consumption and then multiplying by the average cost of electricity in the United States. One aspect of global warming that often goes unmentioned is that it will unevenly affect different areas of the globe, with some locations enjoying longer growing seasons and others experiencing frequent severe weather events. The unequal distribution of the effects of climate change are reflected in the figure which demonstrates that according to the random forest model, some areas will see a reduction in energy usage (and hence cost) while other areas will see a significant increase in energy use. According to this analysis, an office building of 50,000 square feet could see an increase of nearly $30,000 in annual energy costs due to increased temperatures. Many arguments have been made for taking steps to reduce carbon emissions and transition to sustainable energy sources, but perhaps none is more persuasive than that of simple economics. Additional experiments are possible, modifying any of the weather or time variables. Eventually, in order to quantify the effects of a building efficiency improvement, a method will be developed to alter a building characteristic and gauge the effect. This is an active area of research and work for the EDIFES team because quantifying the savings from a recommendation is critical to the value of a virtual energy audit. The random forest experiment provides a framework for how to alter one (or several if wanted) variables and observe the effects on energy consumption by using the historical data. This exercise demonstrates possible applications of the random forest by “re-running” past data under different conditions and examining effects on energy consumption. Conclusion The main question driving the EDA was Which weather and time conditions are most correlated with energy use, and is it possible to build a model to predict energy use from the explanatory variables? The query was addressed by a combination of data exploration, data visualization, and modeling. In answer to the first half of the question, temperature, global horizontal irradiance, and diffuse horizontal irradiance are most highly positively correlated with energy consumption during the summer. In the winter, temperature is also highly correlated with energy consumption although the direction of this relationship varies across climate zones. In terms of time conditions, the day of the week (if it is a weekday or weekend) is the most important variable followed by the cosine transformation of the time of day in hours, and then the raw time of day in hours. It is possible to predict energy use from the weather and time variables to a high degree of accuracy using machine learning methods. A random forest regression model was developed to predict six months worth of electricity consumption and achieved near 80% accuracy averaged across all eight buildings. A complex, highly non-linear model was needed in order to capture all of the patterns and relationships within the data. Further work remains to be done with the random forest model to improve performance, but it is a promising start on the prediction problem. Additionally, the random forest can be used to perform experiments involving altering one or more variables and seeing the response in the context of historical data. Further models should be developed for the EDIFES project because relying on a single model is unwise, especially with the ease of implementing many varied models using the R statistical language or Python and the numerous open-source libraries devoted to machine learning. Acknowledgements The author would like to thank the following members of the EDIFES team for their willingness to explain all aspects of the project and for their tolerance of a significant number of questions: Professor Alexis Abramson, Professor Roger French, Rojiar Haddadian, Arash Khalilnejad, Jack Mousseau, and Ahmad Karimi. This project would not have been possible without their continuous support. References Following is a nearly complete list of works used over the course of this project. It is impossible to document all the websites, stack overflow questions, books, and individuals consulted for this project, but I acknowledge the help I received from each and every person contributing to this project. 1. A. Abramson, R. French, J. Mousseau, A. Khalilnejad, M. Hossain, R. Haddadian, E. Pickering, W. Koehrsen, S. Kamath and K. Nash, BuildingRClean R Package. Cleveland, OH: Case Western Reserve University, 2017. Available: https://bitbucket.org/cwrusdle/buildingrclean/overview 2. “About the Commercial Buildings Integration Program | Department of Energy”, Energy.gov, 2016. [Online]. Available: https://energy.gov/eere/buildings/about-commercial-buildings-integration-program. 3. University of Hawaii’s Environmental Research and Design Laboratory, 2013, Energy Audit Procedures, US Department of Energy Office of Electricity Delivery and Energy Reliability. Available: https://www.hnei.hawaii.edu/sites/dev.hnei.hawaii.edu/files/Energy%20Audit%20Procedures.pdf 4. EMS Environmental Incorporated, “How Much Does a Commercial Energy Audit Cost”, 2017 [Online]. Available: http://emsenv.com/2016/04/28/commercial-energy-audit-cost/. 5. “ARPA-E | Virtual Building Energy Audits”, Arpa-e.energy.gov, 2015. [Online]. Available: https://arpa-e.energy.gov/?q=slick-sheet-project/virtual-building-energy-audits. 6. “GIS data and maps”, Solargis.com, 2017. [Online]. Available: http://solargis.com/products/maps-and-gis-data/. 7. E. Pickering, “EDIFES 0.4 Scalable Data Analytics for Commercial Building Virtual Energy Audits”, Masters of Science in Mechanical Engineering, Case Western Reserve University, 2017. 8. M. Hossain, “Development of Building Markers and an Unsupervised Non-Intrusive Disaggregation Model for Commercial Building Energy Usage”, Ph.D., Case Western Reserve University, Department of Mechanical and Aerospace Engineering, 2017. 9. Kim, H., Marwah, M., Arlitt, M., Lyon, G., and Han, J., 2011, “Unsupervised Disaggregation of Low Frequency Power Measurements”, Proceedings of the 2011 SIAM International Conference on Data Mining, pp.747–758.Available http://epubs.siam.org/doi/abs/10.1137/1.9781611972818.64 10. A. Zoha, A. Gluhak, M. A. Imran, and S. Rajasegarar, “Non-Intrusive Load Monitoring Approaches for Disaggregated Energy Sensing: A Survey,” Sensors, vol. 12, no. 12, pp. 16838–16866, Dec. 2012. 11. D. Borthakur, “HDFS Architecture Guide”, 2008 [Online]. Available: https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html. 12. Solar Degradation and Lifetime Extension (SDLE) Laboratory, cradletools Package. Cleveland, OH: Case Western Reserve University, 2017. Available: https://bitbucket.org/cwrusdle/cradletools/src 13. G. K. Uyanik and N. G?ler, “A Study on Multiple Linear Regression Analysis,” Procedia- Social and Behavioral Sciences, vol. 106, no. Supplement C, pp. 234–240, Dec. 2013. 14. B. Scholkopf and A. J. Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. Cambridge, MA, USA: MIT Press, 2001. 15. Dong, B., Cao, C., and Lee, S., 2005, “Applying support vector machines to predict building energy consumption in tropical region”, Energy and Buildings, 37(5), pp. 545–553.Available: http://www.sciencedirect.com/science/article/pii/S0378778804002981 16. A. S. Ahmad et al., “A review on applications of ANN and SVM for building electrical energy consumption forecasting,” Renewable and Sustainable Energy Reviews, vol. 33, no. Supplement C, pp. 102–109, May 2014. 17. L. Breiman, “Random Forests,” Machine Learning, vol. 45, no. 1, pp. 5–32, Oct. 2001. 18. “World Maps of Koppen Geiger Climate Clssification”, Institute for Veterinary Pulic Health Vienna. 2017. [Online]. Availble: http://koeppen-geiger.vu-wien.ac.at/ 19. Nordstrom, G. H. (2013). “Using the Energy Signature Method to Estimate the EffectiveU Value of Buildings.” Sustainability in Energy and Buildings Smart Innovation, Systems and Technologies, 35–44. 20. “Pearson Product-Moment Correlation”, Laerd Statistics 2017. [Online] Available: https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php 21. “Electric Power Monthly: Table 5.6., A Average Price of Electricity to Ultimate Customers by End-Use Sector”, United States Energy Information Administration,October 24, 2017. [Online]. Available: https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=epmt_5_6_a 22. “Chicago Energy Benchmarking Homepage”, City of Chicago, 2017. [Online]. Available: https://www.cityofchicago.org/city/en/progs/env/building-energy-benchmarking---transparency.html 23. R. Sakia, “The Box-Cox Transformation Technique: A Review”, The Statistician, vol. 41, no. 2, p. 169, 1992. Available: http://www.jstor.org/stable/2348250?seq=1#page_scan_tab_contents 24. “scikit-learn: machine learning in Python”, Scikit-learn.org, 2017. [Online]. Available: http:/scikit-learn.org/stable/. 25. J. L. Lean and D. H. Rind, “How will Earth’s surface temperature change in future decades?,” Geophys. Res. Lett., vol. 36, no. 15, p. L15708, Aug. 2009. 26. S. R. Loarie, P. B. Duffy, H. Hamilton, G. P. Asner, C. B. Field, and D. D. Ackerly, “The velocity of climate change,” Nature, vol. 462, no. 7276, p. 1052, Dec. 2009. 27. H. Zhao and F. Magouls, “A review on the prediction of building energy consumption,” Renewable and Sustainable Energy Reviews, vol. 16, no. 6, pp. 3586–3592, Aug. 2012. 28. Amasyali, K., and El-Gohary, N., 2017, “A review of data-driven building energy consumption prediction studies”, Renewable and Sustainable Energy Reviews, 81, pp. 1192–1205. Available: http://www.sciencedirect.com/science/article/pii/S1364032117306093 29. “Retrofitting Existing Buildings to Improve Sustainability and Energy Performance| WBDG Whole Building Design Guide.” [Online]. Available:https:/www.wbdg.org/resources/retrofitting-existing-buildings-improve-sustainability-and-energy-performance. 30. K. Cetin, M Siemann, C Sloop, “Disaggregation and Future Prediction of Monthly Residential Building Energy Use Data Using LocalizedWeather Data Network,” ACEEE Summer Study on Energy Efficiency in Buildings, 2016. Available: https://aceee.org/files/proceedings/2016/data/papers/12_410.pdf 31. T. Hastie and R. Tibshirani, “Generalized Additive Models,” Statist. Sci., vol. 1, no. 3, pp. 314–318, Aug. 1986. 32. Quick Start Guide to Prophet, CRAN, [Online]. Available: https://cran.r-project.org/web/packages/prophet/vignettes/quick_start.html. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Can the current day’s weather data be used to predict tomorrow’s energy consumption? Controlling for building size, which climate zone is the most energy intensive? Are there “good” buildings in terms of energy efficiency? What does “good” mean in this context (how can it be quantified)? Can a building energy profile be developed to characterize these buildings and compare them to others in the same industry? Based on the answers to the previous questions, are there concrete recommendations I can give to building managers to reduce energy use? Transformation of cyclical variables into a sinusoidal representation Normalize variables to have zero mean and unit variance Separate into random training and testing features and label A minimum impurity decrease of 0.0 to split a node Which factors (weather and time) are most correlated with energy consumption? One-hot encoding of categorical variables A minimum number of 2 samples to split a node",Building Energy Data Analysis Part Four,6,unlisted,107,9698,0.002474737059187461,349,1,1,0,1,0
1,0,152.76074580133104,0,https://medium.com/p/building-energy-data-analysis-part-two-7861b0c6a2d6,0,None,2018-12-25 21:26:00,29.63,24,8,2018-01-10 08:20:00,"['Data Science', 'Machine Learning', 'Education', 'Energy Efficiency', 'Energy']","Building Energy Data Analysis Part Two Plots, plots, and more plots! Author’s Note: This is part two of a comprehensive exploratory data analysis (EDA) of eight office buildings as part ofthe EDIFES Project at Case Western Reserve University. Part One, where I explain the background, take a look at the data, and make some exploratory plots, can be found here. For those who need a refresher, EDIFES (or the Energy Diagnostic Investigator for Efficiency Savings) is a US Department of Energy project (hence the convoluted acronym) to develop a virtual energy audit platform. Energy audits identify inefficiencies in a building’s electricity usage, and are typically done at the physical building by a trained team. There are a number of drawbacks to this approach, and a virtual audit that can be done without ever setting foot in a building will reduce the time and cost of the procedure while increasing the accuracy. This will lead to improved economic and environmental benefits, making it easier for companies to do well by doing good. I have included some of the code used to produce the plots and analyses, and the full code can be found on GitHub. As always, I welcome feedback (especially constructive criticism) and can be reached at wjk68@case.edu. Introduction Before we can get started with any plots, we have to know what data we are dealing with. There are two categories of data available for this project: metadata, which is information about the buildings themselves, and individual building data, which lists the electricity usage and weather data for each building in 15-minute intervals for a period of at least one year. The metadata is presented below. Annual consumption is in kWh and the climate zones are Koppen Geiger. The two buildings with no name could not be analyzed because poor data quality. This left us with eight office buildings in six climate zones to explore. Each building has its own individual energy and weather file. The rows of each file are the observations every fifteen minutes with the variables forming the columns. Following are all the variables for each building: Not all of these variables turn out to be useful and the most relevant are: • timestamp: gives the date and time for the energy measurement in fifteen minute intervals • elec_cons: the raw electricity consumption data (kWh) • biz_day: business day or not (accounts for holidays) • day_of_week: relatively self-explanatory • week_day_end: weekday or weekend • num_time: number of hours since the start of day • ghi: global horizontal irradiance, generally positively correlated with energy consumption during the summer • temp: temperature, generally positively correlated with energy consumption during the summer • rh: relative humidity The forecast column (named because the energy use has been “forecasted” using imputation methods) is the electricity consumption we use for analysis. The raw electricity consumption has anomalies and missing points, and these are corrected before any summaries/plots can be made. I won’t go into details here, but that involves setting outliers/anomalies to zero, and then imputing the zero values based on the surrounding values (either using a linear method, or diffusion indices). For example, if we have a 0 value in between a 5 and a 7, then the corrected value would be 6. To make things easier, I have renamed the forecast column to energy in the plots. Data Visualizations Now that the tedious (but extremely necessary) business of outlining our variables and making sure the data is clean has been completed, we can move on to the enjoyable part, making graphs and exploring trends! The main question I wanted to explore in this report was: What weather and time conditions are most correlated with energy consumption, and will it be possible to create a model to predict the energy use based on the conditions? The second part of that question is mostly saved for part three, but we can definitely take care of the first with some plots! In the preliminary charts from part one, I noticed there appeared to be patterns that repeated on a daily, weekly, and seasonal timescale as well as overall trends, and it is worth examining each of these in turn. Overall Trends First, we need to get an overview of an entire dataset. Plots with every single point every 15-minutes are a little overwhelming, so it’s better to take daily averages. Examples of running this function on three buildings (Christmas colors in these plots was not intentional!): There are a number of noticeable trends in these plots. For all the buildings,energy consumption increases during the the summer which agrees with intuition. The largest source of office building energy consumption tends to be the heating, ventilation, and air conditioning (HVAC) system, which will see more use during the summer months for southern climates. Kansas City, Las Vegas, and Phoenix experience hot summers and hence will see an increase in energy consumption in the summer due to air conditioning use. The buildings in Las Vegas and Phoenix have a decrease in energy consumption during the winter because these cities experience mild winters which do not necessitate energy-intensive heating. Meanwhile, the building in Kansas City does not exhibit the same large decrease during the winter. Energy consumption for this building has two peaks, with one occurring during late summer, and the other occurring during late winter, probably because of the need for cooling during the summer and heating during the winter. Monthly Trends Now that we can see what is happening on the long term, how does each month look? The next plots show a boxplot with average daily energy consumption for each month of the year. Boxplots are a great way to visualize data because they show the median (middle value), the range of the values (the box is the Interquartile Range), and outliers. We can see both the location and spread of the data. This code creates the boxplots where each box is computed based on the daily averages for the month. The same yearly behavior is observed as before, with an increase in energy use during the summer for the Las Vegas and Phoenix buildings, and two yearly peaks in consumption for the Kansas building. The Kansas building has significantly more outlying points (shown in red) during the spring months indicating that daily energy consumption varies significantly during these months. This is not unexpected as some springs may be much colder than others, necessitating heating. Weekly and Daily Trends The last major repeating patterns observed in the data occur on a daily and weekly level with the exact pattern dependent on the season. There are many different ways to visualize these trends but I found the best way was to show all of the seasons on the same plot. The following code shows average energy consumption over the course of a day for each day of the week with the lines colored by season. There is a ton of information compressed into these single images! All buildings exhibit the highest energy consumption during the summer and the buildings in Phoenix and Las Vegas have the lowest energy consumption during the winter. However, the Kansas City building’s minimum energy consumption is during the spring. We can also see all these buildings have a typical schedule of high electricity use during the workweek, and a significant drop on the weekends, indicative of the normal office building occupancy schedule. The final aspect to notice is energy consumption generally peaks during the mid-afternoon. There is a noticeable increase in the morning, as building HVAC systems are turned on in preparation for the workday, and then the energy use falls before rising into the afternoon. The systems are typically shut down at the end of the working day, and we can infer the working hours by examining the significant increase (start of work) and significant decrease (end of work) in energy. Finally, I wanted to convey this information in a more dynamic fashion. After searching for a method to do this, I saw that R can be used to create animation swith gganimate. I created an animated version of the same plots as above where each line is a different season and the frames are days of the week. We now have a pretty good understanding of the typical patterns of office building energy use. Office buildings generally experience a peak in consumption during the afternoon, have higher electricity usage during the work week, and depending on the geographic location, will exhibit different trends across seasons. From these patterns, we can determine useful info such as occupancy schedule and operating hours. To determine opportunities to increase efficiency, we could compare the actual occupancy schedule with that observed from the energy usage and see if there is a mismatch. Maybe the HVAC starts two hours earlier than it needs to, or there might be an unexpected increase on the weekends due to a system malfunction. It’s often surprising how little building owners know about the energy use patterns of their buildings and if we point out the inconsistencies, they can be addressed to both save money and reduce the building’s carbon footprint. Correlations Once we have observed the typical patterns of energy use, the next question is to ask what weather conditions might cause consumption to increase or decrease beyond the normal range? To address that query, we will mainly be relying on a simple measure called the Pearson Correlation Coefficient, or r-value. This statistic ranges from -1 to +1 and tells us the direction and strength of a linear relationship between two variables. Two variables that are perfectly positively linearly correlated will have an r-value of +1, while two variables with a perfectly negative linear correlation will have an r-value of -1. As an example, if temperature and energy have an r-value of 0.8, then this means that as temperature increases, the energy also tends to increase in a linear fashion. The r-value does not tell us the slope of this relationship, only whether it is linear and positive/negative. Once we have established a linear relationship, we can use modeling to find out the slope, or how large of an effect changing one variable will have on another. Examples of different r-values are shown below. We can look at actual tables of the correlation coefficients, but I find that rather dull. A visual representation is much better at conveying relative differences and is easier to understand at a glance. A quick method to create correlation plots is using ggpairs, a function in the GGally package in R. These show correlations between all variables for a single building (in Phoenix, AZ) in summer and winter. On the lower left are scatterplots between each pair of variables, and the right shows the numerical value of r. The diagonals are density plots of the variables because the correlation between a variable and itself is always 1! To determine the value of the correlation coefficient between variables, pick one variable, and move down the row to another variable for comparison. For instance, in the summer plot, in the top right corner, we can see that energy has a 0.125 correlation with ghi (global horizontal irradiance, a measure of the intensity of sunlight hitting a surface). This is a slightly positive linear correlation and if we move to the bottom left corner, we observe the relationship in a scatterplot of ghi vs energy. The strongest correlation with energy during the summer is temperature at a value of 0.575. These agrees with our intuition: when it is warmer during the summer, a building has to use more air conditioning and hence more electricity. What if we want to look at the correlation values for all of the weather variables with energy for all of the buildings? That’s a lot of information for one plot, but it is doable with one of my favorite graphs: the correlation heatmap. There are built-in libraries in R for making heatmaps, but I decided to create my own starting with the basic geom_tile() in ggplot2. In these plots, the color corresponds to the value of r, making it very easy to spot the anomalous building in the summer graph! The building in Portland, OR displays nearly the exact opposite correlation from all of the other buildings for every weather variable. Instead of an increase in energy use when the temperature rises during the summer, this building has a decrease in temperature. Likewise, all other buildings have a decrease in energy use when the relative humidity increases except for Portland! I notified the team and we got in contact with the building owners to see what was going on. It turns out they had no explanation for us and could not point out anything that made this building different. Our two hypotheses were one, that the building had solar power, which produced more energy as the temperature rose because the sun would be out (and the irradiance would rise) and thus the building would use less electricity from the grid, or two, that the electricity meter timestamp was off by some fixed amount. Our best guess was the meter was 12 hours out sync with the real time, but we have not had a chance to discuss this with the utility. The building owner has confirmed she is not aware of any solar panels and I went so far as to look on Google Maps at the building to see if there were some renegade solar panels anywhere near the building. Alas, I could not find any and the mystery remains. If you have any additional ideas, feel free to contact us! A point of interest from the winter heatmap is that some of the correlation values are positive for temperature and some are negative. This is actually what the team told me to expect because during the winter, buildings with electrical heating will need to use more electricity as the temperature declines, leading to a negative coefficient. However, buildings with gas heating or not heating at all, will not use more electricity as the temperature declines and may have a positive or near zero r value. This is a fact I could have realized beforehand, but it took actually digging through the data and creating some visuals to make the point clear. Note: If you run the function for the first heatmap (plot_heatmap), you will see an awful plot that is barely comprehensible. I decided this was unacceptable and used the second function (plot_final_heatmap) to improve on the first. I enjoy the iterative process of data visualization and spending the extra time to make presentable plots. You can communicate the same info in a table or basic graph, but people are more likely to pay attention if you give them the message in something they will want to look at! Visualizing Relationships After identifying relationships, the next step is generally to quantify them by creating models that let us see how much a change in one variable affects the value of another. That will be the sole focus of report three, and for now, I want to take a quick look at how these relationships play out. The following code creates a plot of daily average energy use and daily average temperature during the summer for the building in Phoenix. The figure illustrates how temperature and daily energy use generally track one another. The correlation coefficient in this case was 0.77. It appears that the temperature leads the energy consumption, or in other words, the temperature rises and then the energy consumption rises which demonstrates the concept of thermal mass. This is the thermodynamic equivalent of inertia, which is says that for two objects going the same speed, it will be more difficult to slow down the heavier object. Likewise, a building with a greater thermal mass will take longer to change temperature, whether that is an increase or decrease. There are other variables at play here besides just temperature that we should take into account, but overall the trend shows that temperature and energy use are positively correlated for this building during the summer. An animated version of this plot for one week is the title image of the post. To see a negative correlation, we can look at the relationship between daily average relative humidity and energy consumption. The relative humidity has a -0.44 relationship with energy use. This is a bit more difficult to spot in the graph because it is not as strong as the temperature relationship, but we can see that the two lines tend to travel in opposite directions. Machine Learning Modeling What we have learned so far is there are many time patterns and weather correlations in the energy data. Now, the question is, can we create a machine learning model that will be able to predict the energy consumption from the weather and time conditions? Part three will go into detail about the modeling process, but I wanted to show the evaluation results of several different models here. The overall objective is to make a model that can predict six months worth of energy consumption given the weather and time information. The model will be trained on all the data except for the last six months, and then tested on the held out data. During training, the model is able to see both the weather and time variables (known as features in machine learning) and the energy consumption (called the target). The goal of training is for the model to learn the mapping between features and the target. The model tries to figure out how much energy will be used during a 15-minute interval based on the weather, time of day, day of the week, and so one. Then, to assess the accuracy of the model, it will have to predict electricity consumption on the test set from only the features (it is not allowed to ever see the test set targets because that would be cheating!). Creating an accurate model requires lots of engineering, and we first need to choose a type of model before we can delve into optimizing that model. To cover a wide range of model complexity, I choose three algorithms for evaluation: linear regression, support vector machine regression, and random forest regression. If you’re not familiar with these methods, it’s not that important for this part of the report, and they will get a full explanation in the next part. For now, just know that linear regression tries to fit a straight line to the data, a support vector machine transforms the features into a higher dimension (yes, it is pretty much magic) and then fits a line to the data, and the random forest is composed of hundreds of decision trees. A single decision tree is basically a flowchart of questions. Starting at the root, you follow the branches down, answering each question until you arrive at an answer and a random forest works by taking a vote of many of these trees to find the average prediction, which is going to be closer to the true value on average than any one particular answer. A decision tree used to determine if an applicant should be approved for a loan based on their info is presented below. Start at the top, answer each question based on the individual’s background, and eventually you arrive at a decision! Model Evaluation With details to come in part three, here I will just show the results of evaluation. The models were compared to one another by splitting the data into a random training and testing set (using the same set for each model), training the model, testing the model, and comparing models using several metrics. The data preparation was completed in R, and the actual implementation of the algorithms was done in Python. The metrics chosen are presented below: 1. rmse: root mean square error, the square root of the sum of the deviations squared divided by the number of observations. This is a useful metric because it has the same units (kWh) as the true value so it can serve as a measure of how many kWh the average estimate is from the known true target. The rmse is dependent on the magnitude of the target variable and is often normalized for model evaluation. 2. MAPE: mean average percentage error: the absolute value of the error for each prediction divided by the target value and multiplied by 100. The percentage accuracy can be found by subtracting the MAPE from 100%. 3. R-squared: the percentage of the variation in the dependent variable explained by the independent variables in the model. In other words, with an r-squared of 0.6, that means our model can explain 60% of the variability in the energy consumption from the weather and time variables. A lower MAPE is better because it is the percentage error, and a higher r-squared is better because it means the model captures the relationships in the data more completely. The MAPE and R-Squared results are presented below the code used to generate them: These results show that the random forest is without a doubt the winner! Next steps for the random forest are tuning the hyperparameters (adjusting the settings to maximize performance) and then assessing how well it can predict six months of energy use. To give you a little idea of what to expect in the next part, the following graphs show an entire dataset with the predictions of the random forest overlaid, and then a typical weekly prediction. The Random Forest shows much promise for prediction, but there is definitely some work to do! That however can wait until the next part… Conclusions After an enjoyable exploration of the eight building energy datasets we can draw the following conclusions: Thanks for making it to the end. As a special bonus, here is another animated plot: See you in part 3! Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Buildings in different climate zones have different patterns on every timescale and varying correlations between weather and energy use. Temperature and global horizontal irradiance are the most positively linearly correlated weather conditions with the energy use of a building during the summer (except for that anomalous Portland mystery). A linear model is not able to capture all of the non-linear time trends and relationships between weather and electricity consumption. Machine learning techniques such as a Random Forest may be able to accurately predict electricity consumption from weather and time conditions, a possibility which will be fully investigated in further work! forecast: final cleaned energy consumption with anomalies removed and missing data imputed Building energy use exhibits daily, weekly, seasonal, and yearly patterns.",Building Energy Data Analysis Part Two,6,unlisted,27,4145,0.0,349,1,1,0,1,0
7,5,152.76122382857642,1,https://medium.com/p/building-energy-data-analysis-part-three-c21b5da3f3ec,0,None,2018-12-25 21:26:00,9.41,43,8,2018-01-10 08:19:00,"['Machine Learning', 'Education', 'Data Science', 'Energy', 'Energy Efficiency']","Building Energy Data Analysis Part Three Machine Learning and Modeling Author’s Note: Following is Part 3 of a comprehensive exploratory data analysis on a dataset of energy consumption for 8 office buildings. Part 1 laid out the background for the project, introduced the data, and posed a series of questions to guide the analysis and Part 2 examined temporal trends and correlations between weather conditions and energy consumption. Part 3 will focus on quantifying the relationships previously identified and delves into using machine learning methods for predicting future energy consumption. This work is part of my involvement on the EDIFES (Energy Diagnostic Investigator for Energy Efficiency Project) at Case Western Reserve University. Full code can be found on GitHub, and as always, I appreciate feedback (especially constructive criticism) at wjk68@case.edu! Introduction The overarching question for the office building energy EDA is: What variables (weather and time) are most strongly correlated with energy consumption in office buildings, and is it possible to create a model to predict the energy use given the time and weather conditions? The plotting and quantitative analysis completed in Part 2 indicated that temperature and global horizontal irradiance (a measure of the amount of sunlight striking a surface) are strongly positively correlated with energy consumption during the summer. As a reminder, the following plots show the Pearson correlation coefficients between each of the weather variables and energy consumption in all eight buildings by season. A positive value of the coefficient indicates a positive linear correlation (as the weather variable increases, the energy consumption does as well), and a negative value indicates a negative linear correlation. The weather variables are the following: ws = wind speed; pwat = precipitable water; rh = relative humidity; temp = temperature; gti = global tilt irradiance; dif = global diffuse irradiance; ghi = global horizontal irradiance Based on the correlations, a good way to being modeling would be to create a linear model between temperature and energy consumption during the summer. Because of the high value of the correlation coefficient, we would expect that knowing only the temperature would allow us to make decent predictions of the energy use. However, before we can actually get into the modeling, we should take a step back and look at the types of modeling available. Modeling Approaches The task of predicting energy consumption from the time and weather is what is known as supervised learning: we have a set of explanatory variables (called features) and known response variables (called targets) to predict. The features in this case are the weather and time conditions and the target is the energy consumption. This is a regression task because the targets are continuous quantities. There are dozens of algorithms for supervised regression that can be easily implemented in Python and the R statistical language. These range in complexity from linear models with simple equations and understandable parameters, to deep neural networks which are essentially black boxes that accept data as an input, perform a sequence of mathematical operations, and produce an (often correct) answer with no understandable justification. For this project I want to focus on both accuracy and interpretability of results in the problem domain, and I will evaluate three different modeling approaches covering the spectrum of complexity: Linear Regression, Random Forest Regression, and Support Vector Machine Regression. Simple and Multivariate Linear Regression The simplest model and therefore the place to start when using machine learning is linear regression which aims to predict the value of the target variable by a weighted linear addition of the explanatory variables. Linear Regression can use a single explanatory variable (simple), or it can use many (multivariate). The general equation for a linear model is y = w0 + w1 * x1 + w2 * x2 + … + wn * xn where w0 is the intercept, xn represents the explanatory variables, wn is the weight assigned to each explanatory variable, and y is the response variable. In the case of the Building Energy Dataset, y is the energy consumption, x is the weather or time features, and w is the weight (slope) assigned to each feature. The weight in a linear model represents the slope of the relationship, or how much a change in the x variable affects the y variable. One nice aspect of linear modeling is that the impact of a change in one independentvariable can be directly observed by measuring the change in the response. Random Forest Regression To understand the powerful random forest, you first need to grasp the concept of a decision tree. The best way to describe a single decision tree is as a flowchart of questions about the explanatory variable values of a data pointthat leads to a classification/prediction. Each question (known as a node) has a yes/no answer based on the value of a particular variable (see image below if I’ve already lost you). The two answers form branches leading away from the node. Eventually, the tree terminates in the final classification/prediction node called a leaf. A single decision tree can be arbitrarily large and deep depending on the number of features and the number of classes. They are adept at both classification and regression and can learn a non-linear decision boundary (they actually learn many small linear decision boundaries which overall are non-linear). However, a single decision tree is prone to overfitting, especially as the depth increases because the decision tree is flexible leading to a tendency to memorize the training data. To solve this problem, ensembles of decision trees are combined into a powerful classifier known as a random forest. Each tree in the forest is trained on a randomly chosen subset of the training data (either with replacement, called bootstrapping, or without) and on a subset of thefeatures. This increases variability between trees making the overall forest more robust and less prone to overfitting. In order to make a prediction, the random forest passes the variables of an observation to all trees, and takes an average of the votes of each tree (this is known as bagging; the random forest can also weight the votes of each tree with respect to the confidence the tree has in its prediction). Overall, the random forest is fast to train (trees can be grown in parallel), intuitive, has a moderate level of interpretability, and performs well on both classification and regression tasks. The random forest should be one of the first models tried on any machine learning problem and is generally my second approach after a linear model. A model of a single decision tree is presented below: This particular decision tree is used to make predictions about whether or not an individual will default on a loan based on personal features. There are a number of hyperparameters that must be specified for the forest ahead of time with the most important the number of trees in the forest, the number of features considered by each tree, the depth of the tree, and the minimum number of observations permitted at each leaf of the tree. These can be selected by training many different models with varying hyperparameters and selecting the combination that performs best on cross-validation or a testing set. A random forest performs implicit feature selection and can return the relative importances of the features (we will take a look at these later in the report) so it can be used as a method to reduce dimensions for additional algorithms. Much as a single analyst might make a bad judgement that is biased in one direction but averaging the predictions of hundreds of analyst will reduce the individual biases, a forest of decision trees makes better predictions on average than any individual decision tree. Support Vector Machine Regression A support vector machine (SVM) regressor is the most complicated and the least interpretable of the ML models explored in this report (this explanation is not crucial to understand the report, so feel free to skip the rest of this paragraph). SVMs can be used for both classification and regression and operate on the basis of finding a hyperplane to separate classes. The idea is that any decision boundary becomes linear in a high-dimensional space. For example, a linear decision boundary in three-dimensional space is a plane. The SVM projects the features of the observations into a higher dimensional space using a kernel, which is simply a transformation of the data. The model then finds the plane that best separates the data by maximizing the margin, the minimum distance between the nearest member of each class and the decision boundary. The support vectors in the name of the algorithm refer to the points closest to the decision boundary which are referred to as the support and have the greatest influence on the position of the hyperplane. SVM regressors work by fitting a non-parametric regression model to the data and trying to minimize the distance between the model and all training instances. SVM models are more complex than either a linear regressor or a random forest regressor and have almost no interpretability. The transformation of the features into a higher-dimensional space using a kernel removes all physical representation of the features. SVM models are truly black boxes, but have high accuracy on small datasets with limited amounts of noise. The RBF or Radial Basis Kernel, is the most popular kernel in use and is the default in many implementations. Examples of the classification ability of SVM using different kernels is shown below: Linear Regression Modeling Simple Linear Regression To start with, we will try to predict the daily average energy consumption of a building from a single variable. (Although we have data in 15-minute intervals, average daily energy use could be an easier to parse metric). Temperature is the best place to start because this had the greatest correlation with energy consumption. Therefore, the problem here is trying to predict average daily energy consumption during the summer and winter from average daily temperature. The following code creates a linear model between daily average energy use and daily average temperature for each building in both the summer and winter (for comparison purposes). ‘forecast’ in the code refers to the final cleaned energy column in each dataset. It was first named ‘forecast’ not ‘energy’ when the project started (not by me!) and tradition is difficult to break! The above table provides both the slope of the models and the R-squared value. R-squared represents the amount of variation in the response variable (energy) that can be explained by the explanatory variable (temperature) in the model. In more meaningful terms, a summer R-Squared of 0.15 means that variations in temperature can explain 15% of the observed variation in energy use. A higher value means the model is “bettter” in the sense that it will have more accurate predictions because it is capturing the relationships in the data. The slope is also interpretable because it represents the change in energy consumption due to a 1 degree Celsius change in temperature. For example, for the SRP building, located in Phoenix, Arizona, a 1 C increase in daily average temperature during the summer will result in 100.8 kWh of increased energy consumption. At the national average price of $0.104/kWh, this is $10.40 in extra cooling costs for each degree temp increase! We can dig further into the model results for this particular building to see if a linear model is appropriate. Examining the results we can see that temp has a P value of 9.77E-06 in the model (farthest column to right in the temp row of the coefficients). This means that temperature is a significant variable in the model and therefore is related to energy consumption. However, the overall R-Squared is low at 0.19 meaning that there are other factors that contribute to energy consumption in addition to temperature. The intercept in this case represents the daily energy use for a day with an average temperature of 0 C, not a likely occurrence in Phoenix during the summer! In addition to quantitative stats, there are plenty of ways to diagnose a model graphically in R. However, most of these graphs get pretty involved so I will stick to one showing the Cook’s Distance versus observation number. The Cook’s Distance is a measure of how far a data point is away from other data points, and tells us how much of an outlier a data point is. Outliers are of interest because they can drastically affect a model. Sometimes outliers are the effect of bad data, but they may also be legitimate data points that are extreme for one reason or another and should be closely examined to determine why they occurred. The graph of Cook’s Distance is below: This shows us that there are 3 data points with a relatively large Cook’s Distance that could be affecting the model of daily energy versus temperature. To determine how large this effect may be, we can create models with and without the data points. The following code creates three models: one with all the data points, one with only the high leverage points, and one with only the data points without high leverage. The black line indicates the model with all data points, the blue line is only the normal leverage points, and the red line is only the high leverage points. Although the red point model is far off from the other models, the difference between the black and blue line is miniscule, which shows us the three high-leverage points do not greatly affect the model. Combining this with the fact we have no evidence to suggest these points are erroneous means that we should not remove these outliers from the data. Conclusion: From the R-Squared values, temperature alone is not a good predictor of average daily energy consumption in office buildings. The next step is to create a linear model that includes all weather variables to see if combining many features can yield a more accurate model for predicting daily energy use. Multivariate Linear Modeling The next model will use all the available weather variables to explain the variation in temperature. Again, the daily averages for each variable are used here rather than data every 15 minutes. The models are created in much the same way as the simple linear ones, with the model call changed to We can examine the R-Squared values to determine if including all the weather variables achievies significant improvement over only using temperature. Well, it looks like using all the weather variables does not result in a significant improvement in performance. We can run an ANOVA (analysis of variance) test to quantify the difference between models. The test shows the Residual Sum of Squares (RSS) of the expanded model is lower, but it is not significant because the P value is 0.217. Generally, a P-value less than 0.05 is considered significant . A p-value represents the probability of the observed effects occurring at random. The chance our improvement is due to randomness is therefore 21.7% , far above the threshold for significance. As a last step with the linear modeling, we can examine one building model in detail to see the magnitude of the slopes and determine which variables are significant in the model. There are 3 important weather variables: global horizontal irradiance, global tilt irradiance, and temperature. The rest of the variables are not useful in the model and could be removed without a decrease in predictive ability. Overall, the model for this building can only explain 20% of the variation in daily energy consumption based on daily weather conditions. Conclusion: Even a multivariate linear model cannot accurately predict daily energy consumption accurately. Different modeling approaches will be required! Moreover, predicting the average daily energy consumption requires averaging 96 observations. This method essentially reduces the amount of data available by a factor of 96 (the number of observations per day) because it averages out the variables over each day. In order to get more accurate predictions, I can look at each individual 15 minute interval of the day and include the temporal variables. The resulting predictions will be energy consumption for each fifteen minute interval as opposed to the daily average. The energy over each interval can then be summed over the course of a day to derive the daily energy consumption. Machine Learning Model Selection In order to develop an optimal model to predict the energy from the weather and time conditions for each 15-minute interval, it is best to try out several approaches. The three models that will be looked at are Linear Regression, Support Vector Machine Regression, and Random Forest Regression. Previous analysis with this dataset suggests the relationship between energy use and weather and time is non-linear and will not be explained by a linear regression, but there is no way to know for certain ahead of time. The only way to choose the best model is to try them out on the data! The basic procedure is to split the data into training and testing sets, fit the models on the training data, make predictions on the test data, and evaluate the models using appropriate metrics. The metrics selected for evaluation are explained below: Before comparing models for energy prediction, I need to prepare the data for machine learning. This will require several steps: • One-hot encoding of categorical variables • Transformation of cyclical variables • Normalization of variables to zero mean and unit variance • Separation into training and testing features and labels One-Hot Encoding One hot encoding is a tricky concept to understand at first that is best illustrated with an example. The goal is to convert categorical variables into numeric variables without creating an arbitrary ordering. The process takes this: and converts it into this: Each value for each categorical variable is represented as either a 0 or 1.One-hot encoding is required because machine learning models do not know how to handle categorical data, and simply mapping the values to numbers imposes a random valuing of the feature values based on order. Cyclical Variable Transformation This is required because temporal variables, such as months in a year, are not properly represented by a 1–12 encoding. Month 12 is closer to Month 1 in terms of weather than Month 5, but using integer numbers results in Month 5 and 12 placed closer together. Transforming the time variables into cosine and sine components creates the desired representation. The general equation for a sinusoidal is: Where f is the frequency and t is the time. For months, the frequency is 1/12 because the pattern repeats every 12 months (for daily time in hours, the frequency is 1/24). The transformation of months into sinusoidal components is: Month 12 will now be represented as occurring closest to Month 1 and 11. The same procedure is required for days of the year and hours of the day. Normalization This involves either subtracting the mean and dividing by the standard deviation or subtracting the minimum value and dividing by the maximum minus the minimum. In the first approach, each feature column will have 0 mean and unit (1) variance. The second approach scales each feature value to between 0 and 1. This step is necessary to remove any disproportionate representations because of the varying units used in variables. In other words, a variable with units of millimeters might have a larger weight attached toit than a unit of meters simply because of different units. Normalization overcomes the problem of differing unit scales and the data in this report is normalized to have a mean of 0 and a unit variance of 1. Training and Testing Split Author’s Note: After completing this analysis, I realized that evaluating algorithms for predictive capability based on a random training/testing split of the data was not the best implementation. However, in the model selection phase of the report, I was more concerned with relative as opposed to absolute results. Therefore, comparing algorithms based on a random sampling of the data is valid because if a model cannot accurately predict data points drawn at random, it will not be able to predict electricity consumption into the future. When actually evaluating the best model, I used the last six months as the testing set and the rest of the data for training. Finally, the dataset must be split into training and testing sets. During training, the model is allowed to see the answers (known targets) in order to learn the relationships between the explanatory and response variables. When testing, the model is asked to make predictions for a set of features it has not seen before. The targets for these features are known and therefore the performance metrics can be computed based on the discrepancy between the known target values and the predictions. We will be using a 0.7/0.3 random training/testing split. Each algorithm will be evaluated against the same training and testing data for a fair comparison. The following code takes a dataset and prepares it for machine learning by implementing the steps outlined above. The output is a training and testing sets of data. Utilizing Python A critical aspect of data science is knowing the right tools to use for an application. In the case of machine learning implementations, Python handily beats R for ease of use and speed. Scikit-learn is an incredible machine learning library built for Python and contains all three models for this report. While it is not yet possible to directly pass R dataframes to Python,it can be done using only two additional steps with the feather library. Feather uses binary files to save dataframes and reading and writing timesare greatly reduced compared to using csv files. The entire structure of a dataframe including column names is preserved using feather files. The following code creates the training and testing datasets as R dataframes and saves them to feather files that can be read directly into Python as Pandas dataframes. Python Code for Machine Learning Evaluation Following is the Python code used to train the model on the training data and make predictions on the test data for all three algorithms. The prediction results are saved back to feather files for analysis/visualization in R. The results are then read back into R for quantitative analysis and plotting! Visualization of Evaluation Metrics The performance of each model could be viewed in a table, but it is easier to make relative comparisons using charts, and besides, graphs are more pleasing to look at! The first plot is the root mean squared error of the three algorithms on six of the buildings. That’s one point for the random forest! On each building (I am showing 6 of the 8), the random forest significantly outperforms the other two models. The linear regression has considerably more error than the random forest which indicates this problem might not be linear. The next measure is the r squared value. This can be interpreted as the percentage of the variation in the target variable explained by the independent variables in the model. For this situation, it is the variability in the energy consumption for 15-minute interval explained by the corresponding weather and time information. The code is very similar to the rmse code so I will just show the graph. The second metric is another clear win for the random forest. On the test set, the random forest achieves over 0.9 R-Squared. All of the models can explain a majority of the variation in the energy consumption except for the linear model on the Kansas building. However, even with the best models, there is variation in the response variable due to latent (hidden) variables, or inherent randomness (irreducible error) that cannot be captured. The last metric is the mean average percentage error. To get an accuracy metric, we can subtract the mape from 100%. The random forest significantly outperforms the other two models byachieving less than 10% error on all of the buildings. The linear model ismuch less accurate with the support vector machine in the middle in terms of performance. It is interesting that all of the models performed much better on the SRP and APS buildings, which are both located in Phoenix. Perhaps the buildings in warmer climates have clearer relationships between weather and energy usage. Conclusion: based on the model evaluation results, the random forest is the most capable algorithm for prediction. I will select this model and optimize it for our specific problem. Hyperparameter Optimization of Random Forest After model selection, the next step in the machine learning pipeline is model optimization. All models have a number of hyperparameters (best thought of as settings) that can be adjusted by the developed to optimize the model for a problem. Hyperparameters differ from model parameters in that the latter are learned by the model during training while hyperparameters must be explicitly defined by the developer before training. Scikit-learn implements a set of sensible default hyperparameters, but there is no guarantee these will be the best settings for a particular problem. Hyperparameters are best optimized by creating a number of different models with different settings and evaluating each one on the same validation set. Scikit-learn in Python has a number of handy tools for automatic hyperparameter optimization including a grid search which exhaustively tries every model in a specified grid and evaluates each one using cross-validation. The function returns the performance of all models and the best performing model can be selected for application. The following Python code performs this grid search over a pre-defined grid of hyperparameters. I used a single building for the hyperparameter tuning, which in hindsight was not the best choice. Settings that work best with one building might not be optimized for all the buildings. However, as shown in a little bit, the optimized model did outperform the baseline model for all buildings. The best parameters are as follows: * 200 decision trees in the forest* No maximum depth for each tree * A minimum of 1 sample per leaf node* A maximum fraction of 50% of the features evaluated for each tree* A minimum number of 2 samples to split a node* A minimum impurity decrease of 0.0 to split a node Once the optimal hyperparameters have been selected, I can evaluate them against the same training and testing set as the baseline set of hyperparameters to assess the difference in performance. The optimized hyperparameters were selected based on a single buildingand therefore I need to make sure that these settings will work better for all buildings. The following procedure to assess performance relative to the baseline is: read in the optimized predictions and true values, calculate the performance metrics, and graph the baseline and optimized results to compare. If the optimized model performs better on average with the same training and testing set than the baseline, then I can conclude that these hyperparameters are a better fit for the problem. Optimized to Baseline Comparison The following code reads in the optimized results and produces a graph showing the optimized versus baseline performance: The optimized random forest has a lower error on all the buildings. Therefore, we can conclude that the settings selected during hyperparameter tuning improve the performance of the Random Forest model. Moreover, the optimized model does not take significantly longer to run than the baseline model, and its use is justified moving forward in the project. Now that the final model has been selected, we use it to predict six months worth of energy consumption. We can also try to interpret the feature importances from the model and use the model to carry out experiments by altering one or more explanatory variables and observing the change in the response variable as will be shown later in the report. Model Implementation One of the primary objectives of the EDIFES research project is to develop a model that can predict six months of future energy consumption with 85% accuracy. Now that we have a fully developed machine learning model, we can test it out on this task and see if the Random Forest is up to the job! Six Month Energy Prediction In order to test the model, we will try to predict the last six months of energy consumption from the weather and time variables for that time period. The random forest requires the same number of features during testing as during training, and so will always need the full weather and time information. When we need to make forecasts into the future without known weather data, we will have to use average weather from the past few years, which is the best indicator of future weather. The six month prediction using known weather is meant to validate the accuracy of the model when the weather is known. (Prediction is done on historical data where all the features and targets are known so we can make predictions and compare them to the actual values. Forecasting is done into the future where neither the features nor the targets are known, and hence we cannot actually tell how accurate our model is until the future plays out. The project requires us to reach a predictive accuracy which is an estimate of the forecasting capability of a model.) The predictions on the testing data can be compared to the actual answers (the true energy consumption) in order to determine the prediction ability of the random forest. Because EDIFES will always be ableto retrieve historical weather data for a location, this approach could be implemented to forecast into the future if it proves successful. We would need to train a model using the existing weather and electricity consumption, match the weather data from the previous year (or an average of multiple previous years) to the future dates we want to forecast, and then have the model make predictions for those dates. Eventually, we will develop a method that could not only make forecasts under current operating conditions but also with efficiency improvements taken into account. These modifications could include improving insulation, altering an HVAC schedule, or using timed lights, and quantifying economic benefits is crucial to getting building owners to implement these improvements. The first step for assessing prediction abilities is to create training/testing sets with the final 6 months used for testing. The code below removes creates training and testing features and labels for the final six months of each dataset. The variables here are also pre-processed using the caret package by centering, scaling, and removing near-zero variance variables. The prepared data sets are again saved to feather files for transport to Python. The model is trained and tested on all the datasets with a Skicit-learn implementation. The code is nearly exactly the same as that for a random training/testing split of data, except the names of the files have changed (and the optimized hyperparameters are used instead of the baselines). I will leave out the code (find it on my GitHub) and show the results. The moment of truth is visualizing the prediction metrics! The final averages across all buildings are: Four of the eight building pass the accuracy bar of 85% as measured by MAPE accuracy. Clearly, there remains significant work to be done on the predictive model, but the random forest regression is a good start. There are many tools in the machine learning engineer’s toolbox for improving performance including: feature engineering, altering the variables fed into the model to provide it with more information to learn mappings; creating a more complex, deeper model; and using ensemble methods that combine diverse algorithms to make a more robust overall model. Work on an ensemble of different predictive models is ongoing, and it is clear that the random forest developed in this report will play an integral role in the EDIFES project! Interpreting Results In machine learning implementations, emphasis is often placed on accuracy at the expense of understandable models. However, I like to know not only that my model can make predictions that reflect real-world results, but also how a model makes a given classification/prediction. We have already seen that the slopes in a linear regression can be interpreted as the change in the response variable due to a change in the input variable. The random forest is not quite this intuitive, but there are several options for getting under the hood of the algorithm. Two that we will examine here are feature importances: the relative predictive capability of explanatory variables in the model, and visualizing a single decision tree in the random forest. Random Forest Feature Importances The random forest is capable of returning feature importances, which are a quantitative measure of how much including a variable improves the performance of the model. The technical details are a little much for this report, so I like to look at the relative feature importances to make comparisons between variables. The feature importances were calculated during training of the random forest regression model in Python. We can visualize them in R with the following code. The feature importances for all buildings are presented in the graph below: As the linear model demonstrated, the most helpful weather variables are temperature and ghi. In terms of all the variables, the time of day in hours (num_time) as well as the cyclical transformation of time of day in hours were most significant in predicting energy consumption. The weekday indicator also was significant, which is not unexpected because office buildings show a significant difference in energy consumption on the weekends compared to during the week. Finally, we can also see that many features might not be useful and could be excluded from future models, such as month, and sun rise or sun set. These importances could be used for feature selection with other algorithms. Including irrelevant variables can decrease the accuracy of models and increase the run-time, so reducing the set of features to those that are most essential is a major concentration in machine learning. To get a better sense of the average importances, we can use the following code to take the mean feature importances across all buildings: This chart is pretty definitive! The temperature is clearly the most significantvariable, followed by whether or not the day is a weekday or weekend, the time of day in hours, and whether or not the day is a business day (which accounts for holidays as well as weekends). Overall, the most significant weather variables are temperature, dif, gti, and ghi. There are a number of unimportant variables, such as windspeed, month (including all cyclical transformations), year, and sun rise or set. This last one is not surprising because the vast majority of times are not sunrise or sunset,and these have no noticeable connection with energy usage. These are low variance variables that do not have much explanatory power. They should most likely be removed from any model to avoid including irrelevant features. Removing unnecessary features can improve the performance of a model as well as decreasing run time. The optimized random forest only considered 50% of the features for each tree and achieved slightly better results than the base random forest. Clearly, not all temporal and weather variables are necessary for predicting the energy consumption at a 15-minute interval. Visualizing a Single Decision Tree One of the coolest parts of using this model is looking at individual trees in the forest. The actual trees in the forest are an average of 46 layers deep and the image is far to massive to fit on one screen. Therefore, we can limit the depth of trees in the forest to create an image that is interpretable. Following is a single decision tree limited to three layers for clarity. There is a lot of information here, but if you can make sense of it, then you understand a decision tree and the random forest! Starting at the top of the tree (the root), we take a data point and move down the tree to a prediction. Say that our data point is a Tuesday at 8:00 am. Therefore, the answer to the first questions is False because the time is greater than 6.90 hours. We move down the right branch and come to the next question, which we answer False because this is a business day (the value of biz_day is 1). This leads down the branch to the far right and we reach a leaf node with the value of 142.7 kWh. Therefore, for a Tuesday at 8 am, we would predict an energy consumption of 142.7 kWh for the 15-minute interval based on the random forest. The real decision trees in the forest are much more complex with 40+ layers in order to make more accurate predictions (The leaf node we ended up in contains 50% of the data points and makes the same prediction for each one, which does not reflect reality where each data point likely has a distinct value). However, this simplistic tree gives the basic idea of how the model works. Furthermore, we can see that the variables we are splitting on, the time of day (num_time) and business day (biz-day) are among the most important as evidenced both by the feature importances and because they are near the root of the tree. More important variables will be closer to the root because they are more informative for making splits of the data. Moving forward, understanding a model will become more important in machine learning, and the random forest is a great blend of intuitive methods and high accuracy. Quantity of Data versus Performance One interesting graph we can quickly make is the performance of the model versus the number of data points. Generally, we expect the performance of any model to increase with the amount of data, a phenomenon recorded as “The Unexpected Effectiveness of Data” in a paper by Peter Norvig et al at Google. The following graph shows the mape (mean average percentage error) as a function of the number of data points for all buildings: As expected, the error decreases as the model has access to greater amounts of data. This trend might not always hold for building energy data, because recent data could be more useful than information from 2–3 years ago which means we might actually want to limit the data to recent observations. Generally, as long as data is valid, increasing the quantity of data should allow the model to better learn the relationships (mappings) between the explanatory variables and response variable. The best way to improve any machine learning model is simple: more (high-quality) data! Experimenting with Random Forest For the final part of this report, I want to do a simple experiment that shows one valuable use of the random forest: alter one or more explanatory variables and observe the effect on the response variable. In this case, I will look at the ramifications of global warming on building energy consumption by increasing the temperature and observing the change in energy use. Current predictions indicate the global surface temperature will increasebetween 1 C and 4 C by 2100 as the result of human-caused climate change. In order to assess these effects on future energy use consumption, an increase of 2 C can be modeled using the random forest. The training data for this experiment is the full historical record for each building. The testing data is exactly the same as the training data except with the temperature increased 2 degrees Celsius at each interval. In effect, we are “replaying” the past under different conditions. There are no right anwers in this situation because it is hypothetical but it will give us an idea of what the energy consumption could be with global increase in temperatures. Data is again prepared in R, transported to Python for training and prediction, and sent back to R for analysis. The following code reads in the results from the predictions and plots the results: The percentage changes are shown in the figure below (using the Economist ggtheme from the ggthemes package in R): This shows 6 of the 8 buildings will see an increase in energy consumption due to global warming. Of course, what every building owner actually cares about is cost of building energy. Using the national average price of energy now, we can quantify the monetary effects: Economic results are shown below: One aspect of global warming that often goes unmentioned is that it will unevenly affect different areas of the globe, with some locations enjoying longer growing seasons and others experiencing frequent severe weather events. The unequal distribution of the effects of climate change are reflected in the figure which demonstrates that according to the random forest model, some areas will see a reduction in energy usage (and hence cost) while other areas will see a significant increase in energy use. According to this analysis, an office building of $50,000 ft² could see an increase of nearly $30,000 in annual energy costs due to increased temperatures. Many arguments have been made for taking steps to reduce carbon emissions and transition to sustainable energy sources, but perhaps none is more persuasive than that of simple economics! Additional experiments are possible, modifying any of the weather or time variables. Eventually, in order to quantify the effects of a proposed building efficiency improvement, a method will be developed to alter a building characteristic and gauge the effect. This is an active area of research and quantifying the savings from a recommendation is critical to the value of a virtual energy audit. The random forest experiment provides a framework for how to alter one (or several) variables and observe the effects on energy consumption using historical data. This exercise demonstrates possible applications of the random forest by “re-running” past data under different conditions and examining effects on energy consumption. Conclusions In this report, we evaluated a number of different modeling techniques, implemented the most successful method for prediction, interpreted the model, and simulated global warming. Modeling is a crucial part of the data analysis pipeline because it allows one to determine the strength of relationships between response and explanatory variables. This can then be translated into real-world applications by concentrating on those variables that have the largest effect on the response. Experiments can be run wherein these features are slightly altered in order to test the model or try to achieve a desired response. For the building energy problem, an ideal model would allow the EDIFES team to alter factors such as the effective thermal resistance of a building (a function of the amount of insulation and quality of seals) or the HVAC schedule and observe the effects on building energy use. Three models were evaluated in this report for their ability to predict electricity consumption from all weather and time variables: both simple and multivariate linear regression; a support vector machine using a radial basis kernel; and a random forest composed of 200 decision trees. On a random training/testing split, the random forest significantly outperformed the other two models in terms of RMSE, mape, and R-Squared. Therefore, the random forest was selected for further exploration. This exploration included a simulated modeling of a 2 degree Celsius increase in temperature and an attempt to predict energy consumption for six months using known weather data. The explorations revealed both the strengths of the random forest model as well as its limitations. Moving forward in the EDIFES project, I expect a random forest model will play a vital role in helping us to understand and improve building energy consumption. Following are the 10 primary conclusions from this report: Well, I appreciate anyone who stuck it out to the end! A final report will summarize the major conclusions and include only results (no code) for a complete overview of the project. Moreover, I will complete a write up focused solely on machine learning that includes work done since this report (anyone looking for neural networks should check this out). That’s it for this report, and as always, I want to know what I did wrong and can improve for next time and contact me at wjk68@case.edu with any comments! Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator r-squared: the percentage of the variation in the dependent variable explained by the independent variables in the model. Measures how much of the relationships within the data are captured by the model. MAPE: mean average percentage error: the absolute value of the error for each prediction divided by the true value and multiplied by 100 to convert to a percentage. This metric indicates the percentage error in the prediction and is better than the rmse because it takes into account the relative magnitude of the target variable. R-Squared = 0.771 RMSE = 3.83 kWh Including all of the weather variables in a linear model with energy as the response yielded slightly better predictions although the model was notsignificantly better than the simple model with only temperature. When trying to predict energy consumption for each 15-minute intervalfrom all of the time and weather variables, the random forest significantly outperformed a support vector machine regression and a linear regression. The most important variables for predicting energy consumption as ranked by the Random Forest are: temperature, weekday or weekend, time of day in hours, business day or non business day, timestamp, and diffuse horizontal irradiance. Several variables, including month and sun rise or sunset, were not significant for predicting energy consumption. A 2 degree Celsius increase in average temperature due to global warming results in an average increase in energy consumption by 1.568% for the eight buildings studied, with 6 of 8 buildings seeing an increase in energy use. The percentage increase could cost a building owner up to $30,000 every year. The Random Forest model might be capable of meeting the EDIFES project requirement of 0.85 accuracy when predicting six months of energy consumption. Overall, the problem of predicting energy consumption from weather and time variables in 15-minute increments is a highly non-linear task. There are significant differences in energy consumption depending not only on the weather conditions, but the time of day, the day of the week, the season, and the building characteristics. rmse: root mean square error; the square root of the sum of the squared errors divided by the number of observations. This is a useful metric because it has the same units (kWh) as the true value so it can serve as a measure of how many kWh the average estimate is from the known true target. MAPE = 21.24% Temperature is the most highly correlated variable with daily energy consumption. However, a univariate linear model attempting to explain daily energy use from only temperature has an R-Squared value below 0.20 for all buildings during the summer.  * A 1 degree Celsius increase in temperature can increase electricity consumption by up to 300 kWh over the course of a day in an office building.",Building Energy Data Analysis Part Three,6,unlisted,85,8507,0.0005877512636652169,349,1,1,0,1,0
0,6,152.76034258068287,3,https://medium.com/p/building-energy-data-analysis-part-one-8d7ccc601645,0,None,2018-12-25 21:27:00,28.78,14,40,2018-01-10 08:21:00,"['Data Science', 'Machine Learning', 'Education', 'Energy', 'Energy Efficiency']","Building Energy Data Analysis Part One Background, Exploratory Questions, and First Plots! Author’s Note Since August 2017, I have had the privilege of working as a data science researcher with the EDIFES project at Case Western Reserve University. While the name is awkward (as a US Department of Energy project, it has to have an acronym), but the idea is simple: take the traditional time-consuming and expensive in-person building energy audit process and make it virtual using data science techniques. The goal is to greatly reduce the time and cost of e energy audits while increasing their reliability, resulting in lower building carbon footprints and significant savings for companies. We want to make it easy for companies to do well by doing good! As a first step in my involvement with the project, I was given a small subset of building data and told to get familiar with the data by performing a full exploratory data analysis (EDA). An EDA is a great way to gain an intuitive understanding of the data involved in the project. Moving forward, this understanding will be vital as I to know the expected patterns and be able to quickly spot anomalies or interesting trends in the data. Moreover, a full EDA will let me try out some of the functions already developed by the EDIFES team and potentially develop new models. Finally, doing an EDA is enjoyable! There is something almost magical about taking millions of lines of data and crafting a single chart to explain all the trends, or making a model that predicts the future with astonishing accuracy. This will be the first in a three part series (with a bonus fourth article entirely devoted to machine learning!). Although promising multiple parts usually leads to disappointment when the author fails to deliver (looking at you George RR Martin), take heart in the knowledge that part 2 is already complete and part 3 is well under way! If you decide to stick with me for the whole journey, you will be treated to the wonders of the R programming language, Python, Tableau, machine learning with Scikit-learn, and maybe even some deep learning with Tensorflow! All of these tools are vital to data scientists, and I hope this may spark some interest in anyone looking to get into this exciting (and high-paying) field. As always, I welcome any feedback (email wjk68@case.edu) and encourage everyone to check out the whole project on GitHub. Project Overview Problem Statement An energy audit is an inspection and analysis of the electricity consumption patterns of a building by documenting incoming and outgoing energy flows. When done correctly, energy audits can significantly increase the overall efficiency of a building, thereby reducing electricity costs and greenhouse gas emissions. However, traditional physical building energy audits are resource intensive in terms of both time and cost. In-person audits require a team of auditors to travel to a building and perform a series of tests and inspections with specialized equipment to determine the efficiency of a building and identify areas for improvement. In addition to the resource costs of a physical audit, assessments can vary significantly between auditing teams, raising into doubt any economic benefit of an in-person audit. Real concrete gains in building energy use are possible, as the US Department of Energy estimates that building efficiency could increase by 30% by 20301 implementing existing technologies. However, it is clear that physical energy audits are not the optimal tool for identifying efficiency improvement opportunities. EDIFES Approach EDIFES: Energy Diagnostic Investigator for Efficiency Savings is an ARPA-E funded project [1] jointly undertaken by the Great Lakes Energy Institute and CWRU that aims to develop a software tool for performing virtual energy audits. The overall objective of the project, under the direction of Professors Alexis Abramson and Roger French, is to eliminate the need for physical audits by determining areas for efficiency improvements using only overall building electricity consumption data and building metadata (location and square footage). Part of this process is developing a set of building markers from the electricity data, such as building occupancy schedule and HVAC on/off cycle times, that can be used to characterize a building. [2] In the first phase of the project, EDIFES has partnered with several companies to gain access to building data, perform preliminary analyses of those buildings, and receive feedback on the accuracy of the report. [3] Introduction to Dataset I will be exploring a dataset of time-series electricity consumption data from 8 Progressive office buildings. In addition, the building information contains metadata allowing the weather data associated with each building to be obtained from solarGIS. The data has already been cleaned by members of the EDIFES team and the corresponding weather data is attached to the time-series for each building. Each building is contained in a separate csv file. In the initial cleaning process performed by the EDIFES team [4], 2 of the officebuildings did not pass the data quality standards established by the project because of issues with anomalies and missing values. As these buildings have already been partially modeled by the team, I will focus on exploring questions that might have been beyond the scope of the initial EDIFES investigation. Building Metadata The building metadata contains city, square footage, and annual consumption which is primarily useful for obtaining the corresponding weather data from solarGIS. Buildings with Reference number 92 and 95 were not analyzed by EDIFES because they did not pass the data quality standards of the project. A brief explanation of the metadata variables follows: • Ref: reference number• Location: City, State• Square Footage: Size of building in ftˆ2• Climate Zone: Koppen-Geiger climate zone classification• Annual Consumption: Total consumption per year in kWh Energy and Weather Data The energy data for 8 of the buildings has already been cleaned and each building is in a separate csv file with associated weather information. Each row of the csv data files contains one time interval of fifteen minutes. The columns contain the energy consumption and weather data associated with that building and timestamp. The csv file for each building contains the following columns. Not all of the variables were examined in the initial modeling of the data by the EDIFES team. The following variables were of primary interest to the team on the first pass through the data. • timestamp: gives the date and time for the energy measurement in fifteen minute intervals• elec_cons: the raw electricity consumption data (kWh)• elec_cons_imp: 1 or 0 marker if the data was linearly imputed• power_dem: difference in power (kW) between measurements• biz_day: business day or not (accounts for holidays)• day_of_week: relatively self-explanatory• week_day_end: weekday or weekend• num_time: number of hours since the start of day• temp: most correlated weather variable with energy consumption• rh: relative humidity, the second highest positively correlated weather variable with energy consumption• forecast: final cleaned energy consumption with anomalies removed and missing data imputed using acustom function• anom_missed_flag: marker that tells if forecast energy had to be corrected from the raw number. There would be two reasons for the raw data to be corrected:the data was missing, or the data was anomalous (an outlier). Although not all of the variables were used in the modeling by the EDIFES team, I will keep all of the weather data to try and find relationships that may have gone overlooked. Data Cleaning The data has been completely cleaned. Team members used the tsoutliers R package to identify and correct anomalies. Missing data was either linearly imputed (if fewer than 4 missing points in a row) or imputed using a custom function developed for the project. Assembling the dataframes will be as simple as reading in the data from the correct csv file. I expect to have one dataframe for each building rather than collecting them into one massive data structure. The EDIFES team is more concerned with analyzing buildings individually rather than performing comparisons and the best strategy is to perform the EDA one building at a time. Exploratory Questions With all this beautiful tidy data at my hands, what questions should I ask? My primary interest is correlations between weather and energy consumption. As building energy consumption can be strongly dependent on the weather, this research holds potential for identifying opportunities for significant energy savings. In particular, I want to identify what weather conditions result in higher energy usage. This information could then be used to inform building energy managers that their building could be optimized to their climate or could be used to develop a more efficient heating/cooling schedule in anticipation of forecasted weather. Furthermore, if it were possible to develop a model that accurately predicted energy consumption for the next day based on the weather forecast, this could be used to derive the building energy schedule in order to prepare for the weather conditions. I would like to see if a machine learning model could accurately predict the next day’s energy consumption based on current/forecasted weather conditions. With the addition of keras into R, it is now possible to relatively quickly develop a deep or recurrent neural network. The weather neural network would take in the previous day’s weather and electricity consumption as labels and try to predict the next day’s electricity consumption as the target. A recurrent neural network with LSTM cells would be an ideal model for this task because it has the capability to “remember” previous inputs, which is vital in time series (or any sequence) analysis.I am also interested in comparitive analysis. Although I will look at the buildings individually, it might be useful to compare buildings with similar characteristics located in different climate zones. Moreover, I wantto determine how a “good” building performs with regards to energy efficiency. I think these questions could be answered by comparing the buildings and using average building energy use statistics fromother data sources. Finally (at least for now), I would like to apply the next series of 10 building markers currently being developed to these buildings. Studying the feedback provided by Progressive with regards to the first 10 buildings markers was a good learning experience because it showed what the team was able to correctly predict, and what parts of the models need to be adjusted. A similar prediction — feedback cycle will take place with the second set of 10 markers, and it will show what we can and cannot understand abouta building from a single stream of data. This would involve using code currently under development by EDIFES members. I hope to develop a pipeline that will automatically run the 10 building marker for a building. For the first 10 markers, the EDIFES team developed a script that took in a cleaned building dataset and generated a complete report. The script willneed to be applicable to any cleaned data and not only the dataset I have for this project. Based on these three paths of exploration, I formulated the following set of 6 questions: 1. Which weather variables are most correlated with energy consumption and what is the physical explanation behind this? 2. Can the current day’s weather data be used to predict tomorrow’s energy consumption? 3. Controlling for building size, which climate zone is the most energy intensive? 4. Are these buildings “good” in terms of energy efficiency? What does “good” mean in this context (i.e. how can it be quantified)? 5. Can the next 10 markers accurately characterize these buildings?\ 6. Based on the answers to the previous 5 questions, are there concrete recommendations for building managers to reduce energy use? I expect these questions to change and adapt over the course of this exploration. EDA is an iterative process, and trying to answer one question can lead to 5 additional queries. Moreover, all data has fundamental limitations and may not be appropriate to answer the initial questions. This is not an undesirable situation because the heart of EDA is to learn what your data can teach you before employing more sophisticated statistical methods to test hypotheses. In the words of John Tukey (a mathematician who developed the FFT and boxplot): Data analysis, like experimentation, must be considered as an open-ended, highly interactive, iterative process, whose actual steps are selected segments of a stubbily branching, tree-like pattern of possible actions. Exploratory Visualizations Yeah, that’s great and all, but show us some plots! First up, I wanted to show the location and the associated climate zone. This demonstrates the diversity in climate zones. This visual was made using Tableau, a powerful visualization software (and as students, we can get the professional edition for free). Part of data science is using the appropriate tools for the situation, and if you have clean, tidy data and want to make a decent looking map in five minutes, then Tableau is the best tool for the task! (If you have a csv file and are thinking about making a plot in Excel,stop, do yourself a favor, and open Tableau so you can make a professional looking chart.) As can be seen, 6 different climate zones are represented meaning there is considerable variability for comparison purposes. Now we can go to the world of R for a few sample plots of the energy consumption and weather data. These plots look at one of the buildings in Phoenix, AZ. The first plot shows the raw energy consumption data. The next figure shows the cleaned energy consumption for the same building. These graphs are somewhat unrefined and cluttered, but they primarily serve to demonstrate the cleaned data does not contain the anomalies or missing data that are present in the raw data. A better visualization is to plot the mean daily energy consumption over the extent of the data. This smooths out the intraday variability inherent when taking observations every 15 minutes. As expected for Phoenix, AZ, in the American southwest, energy consumption rises during the summer months and falls during the winter months. It will be interesting to see how much of a difference a “cool” summer or a “warm” winter has on energy consumption. The next two plots look at possible correlations between energy use and weather conditions. As a hypothesis, I would expect electricity consumption to increase with an increase in temperature and decrease with a decrease in relative humidity. The first plot shows the relationship between average daily electricity consumption and average daily temperature. The next plot shows the relationship between average daily relative humidity and average daily energy consumption. Finally, to quantify all relationships between weather variables and energy consumption, I can generate the entire correlation matrix. To capture the relationships, this code calculates the Pearson correlation coefficient, a measure of the linear relationship between two variables. It ranges from +1 indicating a positive perfectly linear relationship to -1, indicating a negative perfectly linear relationship. The image below shows illustrative values of the Pearson correlation coefficient. In the correlation plot below, forecast refers to the final cleaned energy value. Looking down the forecast column shows first the distribution of the cleaned energy (as a histogram) and then three scatterplots showing the trends between energy consumption and temperature, relative humidity, and wind speed respectively. Moving rightward across the forecast row are the correlation coefficients of the weather variables with energy. The stars represent the significance of the relationship with three stars indicating a strong linear relationship. There are definitely hints of meaningful relationships between forecast (the final cleaned energy) and temperature and relative humidity. There does not appear to be much of a relationship between wind speed and energy consumption, but the possibility cannot yet be ruled out with these preliminary visualizations! Conclusions This first report identified the importance of the project and approach of the EDIFES team, posed a few questions to guide the analysis, and explored some preliminary relationships quantitatively and visually. My exploration of the data will be guided by a set of six initial questions with more to come as I start a thorough examination of the data. Finally, I have made some preliminary charts of the data and have identified possible relationships for further investigation. See you in Report 2! References [1] “About the Commercial Buildings Integration Program | Department of Energy”, Energy.gov, 2016. [Online]. Available: https://energy.gov/eere/buildings/about-commercial-buildings-integration-program. [2]: E. Pickering, “EDIFES 0.4 Scalable Data Analytics for Commercial Building Virtual Energy Audits”, Masters of Science in Mechanical Engineering, Case Western Reserve University, 2017. [3]: M. Hossain, “Development of Building Markers and an Unsupervised Non-Intrusive Disaggregation Model for Commericial Building Energy Usage”, Ph.D., Case Western Reserve University, Department of Mechanical and Aerospace Engineering, 2017. [4] Rojiar Haddadian, Arash Khalilnejad, Mohammad Hossain,Jack Mousseau, Shreyas Kamath, Ethan Pickering Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator",Building Energy Data Analysis Part One,6,unlisted,139,3068,0.001955671447196871,349,1,1,0,1,0
6,0,151.32079875197917,0,https://medium.com/p/books-of-2018-notes-bb27c56390b8,0,None,2018-12-27 08:00:00,8.11,43,3,2018-06-10 07:20:00,"['Books', 'Reading', 'Education', 'Writing', 'Self-awareness']","Books of 2018 Notes Complete and unedited thoughts Here is the entire collection of notes on the books I read in 2018. These are generally not meant to be read by others because they are completely unedited, but I’ve put them out as a supplement to my Books of 2018 article. I try to write a few sentences about each book I read to process them, fitting the facts into my existing worldview, or updating it as required. My initial thoughts are often incoherent (as you’ll see here) but, over time, my mind processes the books until I can extract the most salient points, or at least those that stood out most to me (which you can find in the other article). The books here are listed in the chronological order I read them. I’ve made no distinction between fiction or non-fiction or between audio or print books. Mrs. Sherlock Holmes: The True Story of New York City’s Greatest Female Detective and the 1917 Missing Girl Case That Captivated a Nation by Brad Ricca A story of a female detective (Grace Humiston) operating in New York who made her name righting injustices. This book focuses on the successfully solved 1915 case of a girl’s disappearance, and also touching on the larger story of Mrs. Humiston’s career. I generally find true crime books to be uninteresting because they get lost in the details and are irrelevant outside of a single case, and this book adhered to that tendency too well. (38) Superfreakonomics: Global Cooling, Patriotic Prostitutes, and Why Suicide Bombers Should Buy Life Insurance by Steven Levitt and Stephen Dubner Levitt and Dubner tackle more potentially controversial topics including the economics of prostitution, how complex problems are often solved with simple solutions, and how we could manipulate the environment (geo-engineering) to prevent hurricanes and combat global warming. My issue with the Freakonomics authors is their presentation of the evidence as settled when — as time has shown with their previous work — the facts that do exist can be interpreted to support many different opinions. In other words, Levitt and Dubner are too sure of their conclusions and unwilling to question themselves (at least in their popular works) which drives me away from their writings and podcast. (44) Junk Raft: An Ocean Voyage and a Rising Tide of Activism to Fight Plastic Pollution by Marcus Eriksen An examination of the powerful chemical lobbying interests in the United States told through the true story of sailing a raft made of plastic trash from California to Hawaii. As just a few examples of how these interests operate, the author explains the plastic companies’ push for end-user (consumer) responsibility instead of extended producer responsibility, and encouragement of recycling campaigns to deflect calls for manufacturing products that degrade quicker or use fewer harmful chemicals. This documentation of the ways in which companies argue against science and health in order to maximize profits is discouraging, but Erikson counters the bad news with several stories of grass roots efforts, such as plastic bag bans and regulation of certain chemicals that succeeded as a result of extended work on the part of many individuals. While the science can be overwhelming at points, Erikson keeps the book moving along by jumping back and forth between technical arguments and personal experiences, with his trash-boat journey playing out over the course of the book. (60) The Signal and the Noise: Why So Many Predictions Fail-But Some Don’t by Nate Silver This could be the most important non-technical book about prediction written (an honor that Superforecasting could also compete for). Silver presents a detailed accounting of not only how he has learned to make accurate predictions (such as those at FiveThirtyEight), but how outstanding forecasters in fields as diverse as meteorology, baseball, poker, and finance are able to use a set of principles to not achieve perfect estimates, but to achieve an excellent (or profitable) level of forecasting. The three rules I gleaned from the book are In addition, this book touches on subjects such as out of sample predictions (for events that are nearly-impossible to predict because they have never appeared in the data), uncertainty estimates, why bolder predictions get you more notice but have reduced accuracy, why domain knowledge is still important in many areas of forecasting, and why we so often mistake noise (such as correlations that occur by chance) as meaningful information. Field Notes from a Catastrophe: Man, Nature, and Climate Change by Elizabeth Kolbert Too often climate change is talked about in highly technical terms or potential consequences 50 years from now which leads to people instantly tuning out from the discussion. Kolbert avoids that tendency by focusing on the human stories of climate change the and direct impacts it is already having. There is still some science, but if you want a short, readable book on climate change then this book is a good choice. Everything is Obvious: How Common Sense Fails Us by Duncan Watts Humans have evolved many biases (heuristics) that served us well in the past but not so much in our information-rich age. Watt’s entertaining book highlights how these failures manifest themselves and provides ways in which we can overcome them. The Black Swan: The Impact of the Highly Improbable by Nassim Nicholas Taleb Reading this book about the consequences of highly unlikeable events in 2018 seemed apt given the events of 2016 and this historical perspective on the Great Recession. While Taleb makes the point that when considered individually, each of these events seems highly unlikely, however, given the vast number of unlikely things that can happen, some of them are bound to. Therefore, it’s an inevitability that extreme events — whether natural or man-made — will occur with regularity. While Taleb doesn’t present many solutions in this book, and it’s longer than it needs to be, I still found it worthwhile and I enjoy Taleb’s anti-authoritarian (some may say immature) attitude. While Taleb’s belief in his own superiority clearly shows, it’s possible to get past that (or treat is as amusing) and get something from the book. Antifragile: Things That Gain from Disorder by Nassim Nicholas Taleb The follow-up to The Black Swan, this book presents ways to prepare for and respond to highly unlikely events. These extreme occurrences can be detrimental, but they also present unique opportunities for those who can sport them coming. Given that this is nearly impossible — we know that extreme events will happen but not which events — the better approach is to adopt a mindset of getting stronger through exposure to stress. Moreover, we need to plan for a scenario — such as an earthquake — worse than the worse we’ve historically experienced (think the earthquake that led to the meltdown of the Japanese nuclear power plant). We can’t be constrained in what we think is possible based on what has happened in the past. As with the previous book, I found the style slightly off-putting, but the content was intriguing. I particularly like the idea that the right amount of stress is beneficial on both the society and individual level because it gives rise to an attitude of embracing rather than shying away from challenges. This is a viewpoint that I’ve tried to adopt in my life — do hard things because you’ll be better after you’ve conquered the task. Fundamentals of Data Visualization by Claus Wilke The choices we make when presenting data in graphs can have a large impact on how easy it is to interpret the data and whether the conclusion reached is the one we intended. In this enjoyable book explaining the principles of effective figuring making, Wilke demonstrates (in the R programming language) how our charts can make the intended meaning of the data clearer rather than obscuring it. Knowing how to make an effective chart also means we are better prepared to determine when someone has deliberately altered a graph to make a point. Creating effective graphs is an important skill in data science, but it does not mean having to know complex software or making the nicest-looking figures. Rather, there are simple steps we can take, such as increasing the font size of axis labels or removing unnecessary ink, that result in better graphics. This book is highly recommended for anyone who needs to present or interpret graphical data. Waking Up: A Guide to Spirituality Without Religion by Sam Harris Harris has previously written about the negatives of religion and this book is his outline for how to live a meaningful life without traditional faith. As an atheist (or at least agnostic) who still longs for a greater purpose in the world, I found this book helpful and it has led me to try and cultivate experiences of the sublime such as feeling connected to other humans through volunteering or through extreme exercise. As Harris writes, all of life is a search for altered states of consciousness, and while some people seek solace in religion, there are other frameworks — like trying to make the world better– in which secular individuals can find purpose. The Rational Optimist: How Prosperity Evolves by Matt Ridley There are few books that profoundly change my opinions on a topic, and this has been one of them with regards to the merits of capitalism. Ideas like “capitalism is the greatest anti-poverty device” and “socialism is a great idea but fails in practice” have been floating around in my head for a few years, but this book took that idea to a new level and provided concrete examples of the benefits of capitalism. Not only is capitalism effective at reducing poverty, it has also led to greener, healthier, wealthier, and happier societies culminating in the current Golden Age in which we find ourselves. Moreover, there is no indication that things are declining, but instead, the pace of progress is accelerating meaning that the only logical (rational) view is that the world will be even better in the future. I’ve read a number of books along the lines of “progress is good for humanity (and the entire planet)” and despite recent political events, I’m now rationally optimistic about our collective prospects. The Beginning of Infinity: Explanations That Transform the World by David Deutsch “Problems are solvable” is the central premise of this optimistic examination of technology and civilization. I’ve decided to adopt this as my official motto and more than once I’ve thought back on this book as I struggled through a difficult challenge. Another takeaway from this work is that anything not prohibited by the laws of physics (such as faster than light speed travel) will eventually be accomplished by humans because of the pace of technological advance. The “Beginning of Infinity” in the title refers to the idea that we are at the brink of an explosion in capabilities that will eventually lead to us solving our earth-bound problems and venturing to the starts. Combined with the other books extolling progress, this book was part of a profound shift in my thinking from despair at our fate to rational optimism that we’ll solve our problems. Calvin and Hobbes: Book One of Eleven by Bill Waterson Sometimes it’s nice to take a break from the heavier fare and read something solely for enjoyment. This collection of comics was a nice break from the challenges of coursework while also speaking to larger themes, a hallmark of Calvin and Hobbes. Between the World and Me by Ta-Nehisi Coates Each of us inhabits such a narrow slice of human existence, a point we often fail to think about when we judge others. Coates’ book, written as a letter to his son explains what it’s like to live as a black man in America with all the attendant disadvantages and prejudice. Works like this are why we should read: to experience different perspectives and come away with a more compassionate view of other humans. Coates’ book is a short required read and shatters the notion that we live in a race-blind society. Kafka on the Shore by Haruki Murakami Murakami writes “slightly supernatural fiction” meaning that the world in his stories eerily resembles ours except for a few elements — talking cats, fictional characters come to life — that just don’t happen. Maybe I missed the subtext (the story behind the story) in this book, but I found it too bizarre to hold my attention and moreover, saw nothing profound in its pages. Godel, Escher, Bach: An Eternal Golden Braid by Douglas Hofstadter This book is hailed as a masterwork weaving together computer science, artificial intelligence, logic, mathematics, art, and music. However, I found it extremely difficult to parse — maybe because I lack the formal training in any of these subjects — and therefore did not get much out of it except a profound sense of my naivety. This is one of those books that people like to claim to have read (because of its discussion of fundamental questions in many fields ) without ever reading (because of its often impenetrable explanations and logic puzzles). While I did make it through the whole thing, I don’t think I’m any better off for having made it through. Perhaps this is a book I’ll need to come back to when I have a better knowledge of the topics, but, it already felt dated when I read it and I don’t know if the themes will hold up (much as Godel’s Incompleteness Theorem, a central part of the book, disproved the idea of a consistent mathematics, knew discoveries may invalidate arguments in this book). Predictably Irrational: The Hidden Forces That Shape Our Decisions by Dan Ariely Another book along the lines of “humans evolved many rules of thumb that served us well in the past but now are irrelevant in the information age.” The decisions we have to make now require processing so much more information than we were evolved to assimilate that our natural heuristics fail us time and again in our modern world. However, while other authors merely point out this fact, Ariely goes a step beyond to claim we can predict the kinds of mistakes we’re likely to make and therefore adjust our views accordingly. An entertaining and potentially useful popular neuroscience book that would have stood out if I hadn’t read about 5 others like it this year. A Gentle Introduction to Effective Computing in Quantitative Research: What Every Research Assistant Should Know by Henry Paarsch and Konstantin Golyaev This book presents a number of different topics important for doing research in computer science or data science at a high level. I generally prefer books that dig into one topic rather than cover many on the surface, but this book provides a good overview and some hands-on coding exercises (a requirement for me to read any technical book). I think it may prove useful to newcomers to the field, but for anyone familiar with the concepts, it won’t add anything. Enlightenment Now: The Cast for Reason, Science, Humanism, and Progress by Steven Pinker Pinker’s previous work, The Better Angels of Our Nature: Why Violence Has Declined topped my 2017 reading list because at the time I considered the central premise — that we currently live in the most peaceful time in human history — to be the most important but least understood idea of the 21st century. Now, thanks to Pinker’s new work and several other books in the same manner have uprooted that as the most important idea and supplanted it with the related, but broader concept that society as a whole has never been better and shows no signs of regressing. If you don’t believe this, then Pinker has hundreds of pages of facts and dozens of graphs to support his argument and while I do believe there are counterarguments, Pinker makes a convincing case. Once you break out of the news cycle and look at the actual statistics instead of the anecdotes, the view of the world gets considerably brighter. Pinker is not saying that we should rest on our achievements but that we should acknowledge progress has been made and can continue to be made if we invest in science and promote the ideas that got us here: liberalism, democracy, capitalism, and a rational view of the world. By the end of the book, and after doing my own research, I’ve embraced the idea that we have made progress and can continue to do so, raising the living standards of everyone on Earth. While advancement is not inevitable, we should acknowledge and study the progress we have made so we can institute policies and elect leaders who will continue to promote the techniques that led to our current prosperous state. Our Mathematical Universe: My Quest for the Ultimate Nature of Reality by Max Tegmark Tegmark’s basic premise is that our universe is fundamentally mathematically. What does that mean? Despite several hundred pages, I’m still not exactly sure except I think it involves more than the 4 experienced dimensions and it definitely means an infinite number of universes. I’ve heard these ideas before, and think they are either profound or completely wrong, a view that Tegmark also shares. The problem with theories about multiverses are that they probably cannot be either confirmed experimentally nor disconfirmed which means they fall outside the traditional boundaries of science. We probably will make profound discoveries about our universe(s) in the future that will make our current understanding look amusing in retrospect. The chance that those discoveries will align with what Tegmark proposes is highly unlikely, but I can’t fault Tegmark for enthusiastically proposing his own theories. For an empirical, practical-minded individual, this mostly theoretical book does not hold much appeal, but I still enjoy seeing what the current thinking is at the forefront of other fields. Salt: A World History by Mark Kurlansky While now an everyday resource, at times salt has been extremely valuable and has an entertaining history. I generally don’t enjoy histories that are an endless recitation of facts and dates, and while this does talk about broader issues, I found it wasn’t relevant enough to hold my attention. Deep Learning by Ian Goodfellow Although I try to study technical topics through hands-on books, it’s helpful to learn the theories behind a tool so you can use it efficiently. This book is a thorough dive into deep learning — multilayer neural networks — and I found it a useful compliment to practice-driven books such as Deep Learning with Python by Francois Chollet. For those just getting started with neural networks, this book will be dense, but for anyone who has built a few in Python and wants to understand how they work, this book will be helpful. My concern is that given the pace of advancement within the field, everything in this book could be outdated within a few years so keep that in mind before devoting too much time to studying the theory. The Invention of Nature: Alexander Von Humboldt’s New World by Andrea Wulf A biography of a mostly forgotten naturalist that reminders us science without arts can do little to capture the public imagination. Humboldt’s career demonstrates that the arts and humanities are an integral part of communicating science. Humboldt had a profound effect on the public consciousness regarding science not because he was doing the best science but because he could communicate the wonders of science on a relatable level to the public. I think the present-day disconnect between what scientists and the public — which manifests in less founding for basic research — is partly due to the lack of approachable explanations. The public wants to be captivated by science — consider the fanfare around new space missions excitement when new elements are synthesized — but to be engrossed, they first need to understand the science! The Obstacle is the Way by Ryan Holliday I hesitate to call this a self-help book because it tries to strike a practical tone rather than the traditional fluff in the genre. I got something from the general premises of the book: whatever is in front of you, it is your job to do it to the best of your ability; do hard things, one action at a time and eventually, the hard things will become natural; don’t shy away from difficult things but treat them as a chance for growth. My biggest takeaway is to do whatever you need to do to the best of your ability. Don’t sit around complaining, but do what you are required to while striving for excellence. I realize this is easy to say when you have a job you enjoy, but I’ve tried to take this message to heart. Moreover, my attitude is now: when things get difficult, narrow your focus to one action at a time, and execute each step. Mastery by Robert Greene The idea behind this book is that the path to mastery in any pursuit is to concentrate entirely on what you are doing. That seems pretty simple, but most of us choose not to do what we should be but indulge in distractions that keep us from becoming experts. Those who master their profession generally love what they do and as a result, are able to devote their full attention to it. At this point in my life, I agree with the sentiment, but I’m not sure it’s healthy in the long term. Dedication to work may help you advance in a career, but if that is at the cost of relationships, is it worth it in the end? Is mastery of a job worth it in exchange for impoverishment in others areas of one’s life? How to Change Your Mind by Michael Pollan An intriguing book about the possible benefits of psychedelics, both for treating mental illnesses and for allowing healthy individuals to access new realms of consciousness. One problem I had with this book is it seems that Pollan had made up his mind before gathering the facts, and no matter what he found, he would make it support his arguments about the benefits of psychedelics. I am receptive to the idea that we should not dismiss these substances, but, like any other treatment, they need to be rigorously studied. Anecdotes, even ones about people finding new meaning, are not grounds for promoting psychedelics as beneficial in general. The Blank Slate by Steven Pinker The premise of this book is the “blank slate” theory of humans — that people are born the same and only are distinguished by their environment — is completely false. The point is brilliantly made and Pinker points out that not only is the idea that everyone has the same abilities upon entering the world untrue, but it also leads to harmful social policies and attitudes. Failing to acknowledge genetic differences results in ideas like the less well-off are completely responsible for their position with the subtext that they deserve what they have. Acknowledging differences can help us address inequalities that are present from before birth but can be remedied with the right upbringing. It’s far past time to listen to the science and rid ourselves of the idea that everyone is born biologically identical. Behave by Robert Sapolisky An insightful look into the factors which influence what we do from the microscopic level — cells, neurotransmitters, genes — to the macro level — society, culture, environment. This book was a thoroughly enjoyable journey through genetics, biology, neuroscience, sociology and related fields that was heavy on both the science and examples of how these effects are manifested. My key takeaways were how many different factors at all scales lead to our actions and how context-dependent all of the findings are: what may hold in one situation can be completely different in another. Ultimately, while we can’t trace back the complete reasons for any one action, the author is convinced that we do control what we do and should concentrate on “doing the right thing when it’s the hard thing to do.” I have taken this message to heart and tried to take the difficult route when it can benefit my fellow humans. When Breath Becomes Air by Paul Kalanithi Just because someone writes a book while dying, does that mean it has to be good? I found this book tiresome and of little substance. Although I commend the author for being willing to write about what he was going through, nothing in this book struck me as profound. Superforecasting: The Art and Science of Prediction by Philip Tetlock An engrossing and useful book about what makes a person a great predictor of future events. This book advocates many of the same strategies as in The Signal and the Noise: make lots of predictions, get feedback about predictions to improve for the next one, evaluate information from as many sources as possible, expresses uncertainty in predictions, and don’t be afraid to change your mind with new evidence. As with becoming an expert at anything, there are no tricks, just a lot of focused effort, repetition, and humility to correct one’s past mistakes. The Joy of Finding Things Out by Richard Feynman This collection of essays by one of the greatest physicists and science communicators has an infectious attitude that makes it hard not to start expressing wonder at everything in our world. Feynman describes — from personal experience — that the greatest scientists are driven not by ego but by a passion to “find things out” — that is, to determine something that no human has known before. This book was a joy to read and reminded me the best projects are the ones we pursue not because we have to, but because we enjoy what we are doing and want to discover the answer. Data and Goliath: The Hidden Battles to Collect Your Data and Control Your World by Bruce Schneier The argument in this book — that the government and large corporations are becoming more powerful not because they are growing larger but because they have more of our data and this is a terrible prospect — is increasingly sounding out-dated to me. Maybe my generation has a different conceptions of privacy, but I don’t expect any of my information to remain mine. Personally, my view is that the more information that is exposed to the public, the better civilization will be. While there is certainly evidence that corporations such as Facebook manipulate users, I think the ultimately responsibility lies in the users themselves to not be fooled or give away data they don’t want out in the public. I’ve accepted all my information will be exposed, and hence don’t have to worry about breaches. The information I give to platforms such as a Google and Microsoft is part of the price I’m willing to pay for services like Google Maps or Windows. I’m willing to admit that my view may be foolish if we have signed over our information only to be completely controlled by malevolent companies or governments. There are certain privileges we must give up to live in a secure society, and complete privacy is one of those luxuries that we cannot maintain. The Information by James Gleick I did not get much from this book other than the obvious general idea that the amount of information is increasing exponentially. There was some interesting history of “information” from writing to digital streams, but I had heard it all before. If you are someone who hasn’t looked at the world in a few decades some of this material may be novel, but for the rest of us, there isn’t much to see and I think this book will be outdated shortly. Neuromancer by William Gibson This science fiction book about a computer “hacker” drug addict who enters into virtual reality worlds is supposed to be a classic in the “cyberpunk” genre, but it did not hold my interest. Maybe the idea of virtual reality was novel at the time, but now it just feels typical (blame The Matrix). It was written in 1984, so maybe I shouldn’t judge it by contemporary standards. Either way, I wouldn’t recommend this to science fiction fans who grew up with the likes of Inception or Total Recall. The Inevitable: Understanding the 12 Technological Forces that Will Shape Our Future by Kevin Kelly While I remember enjoying this book and thinking the 12 forces were intriguing ideas, I can’t remember any of them so they must not have made that great of an impression. While I might have enjoyed the book at the time, I think that any predictions about the future that aren’t extremely broad are destined to appear completely foolish. Perhaps that is why Kelly outlined 12 different forces — each of which wasn’t very specific — so that he could be right on at least one. The Road to Character by David Brooks This wins my admiration as the most inspiring book of the year. The basic idea is simple: people are not born willing to dedicate themselves to humanity, but over the course of their lives, develop an overwhelming compulsion to help others. After reading this book, I made a couple decisions: always try to help other people because that is the only path that brings lasting fulfillment and choose my next job based on the amount of good it would do for humanity. Maybe that sounds idealistic, but perhaps we need more people willing to think grandly. In the Heart of the Sea: The Tragedy of the Whaleship Essex by Nathaniel Philbrick An interesting true story about a whaling ship destroyed by a whale and the ordeal endured by the men who survived. I enjoyed this book more than most historical narratives because it didn’t get too caught up in the details (or for that matter some fiction books; looking at you Moby Dick). This book was highly readable and it also made me wonder if I would be tough enough to withstand the suffering endured by the few survivors and if there is anything to learn from stories such as this (probably that people can survive far more than they think they can). Science Friction: Where the Known Meets the Unknown by Michael Shermer An intriguing collection of essays on the philosophy and importance of science. I found this a great reminder of why we need to keep working to make science the foundation of our society. While lately my stance towards religion has softened (I now consider it a neutral idea that at least brings people a sense of community) there clearly are still reasons to argue for a rational basis for civilization instead of tradition and belief. Science is the only self-correcting endeavor that lets us expand our objective knowledge of the universe. Regeneration by Pat Barker An engaging fiction story of a war-hero (World War I) turned pacifist sent to a mental hospital to be re-programmed to return to combat. This book was quite moving and provider constant reminders of the complexities (both negative and positive) of human behavior. It is a shame that it requires a war to bring out the humanity in humans and to create feelings of comradeship stronger than any other human bond. Hope, Human and Wild: True Stories of Living Lightly on the Earth by Bill McKibben McKibben’s previous book The End of Nature, was depressing (in the words of the author) and so he wrote this book about societies that are living responsibly to cheer himself up. This book is not all cheerful, but it does show it’s possible for human societies to live happily and without causing undue strain on the environment. Most of these societies are places were people enjoy their lives while getting by with many fewer materials goods. Given my new-found stance that things don’t make us happy, I’m more than willing to get behind this idea. Instead of finding ways to produce the same amount of goods and energy in a sustainable manner, maybe we would be better off figuring out how to convince people they don’t need so much in the first place. The Ego Tunnel: The Science of the Mind and the Myth of the Self by Thomas Metzinger We like to think of our lives as following a narrative from one moment to the next, but in fact, we only experience temporary sensations, linked together in our mind into a coherent story. The idea of the self as a cohesive entity is a fabrication we tell ourselves to make sense of the world. Realizing there is no actual self within us is disconcerting but also means we are free to radically change ourselves at any moment — a useful lesson for those who want a new start. You may think of yourself as a single character with fixed traits, but in reality that’s a myth you constructed for yourself and there’s nothing preventing you from making a positive change. The Science of Good and Evil: Why People Cheat, Gossip, Care, Share, and Follow the Golden Rule by Michael Shermer There’s a pervasive idea that ethics are outside the realm of science, a concept that Shermer completely takes down. From studying vastly different cultures, we have found there are universal morals which prove an evolutionary origin for our morals. Rather than being defined by traditions, ethics are rooted in our genetics, because human groups that followed moral codes were more likely to survive (the idea of group selection). Science can not only study how these morals evolved, but it can also lay out a groundwork for how we should behave to bring the greatest happiness to the most members of our species. Shermer argues for moral provincialism, a middle ground between moral objectivism and moral relativism, in which actions should be taken depending on the situation and considering the relevant evidence. 10% Happier: How I Tamed the Voice in My Head, Reduced Stress Without Losing My Edge, and Found Self-Help that Actually Works — A True Story by Dan Harris One topic that kept popping up in the books and podcasts I listened to was meditation. Mostly due to the insistence of Sam Harris (no relation) I was willing to give it a try, but, before I could do so, I needed to find out what the science said. Rather than dive right in, I started out with this lighted book relating the personal experience of a stressed-out news anchor who found a little relief through meditation. While I’m normally repelled by books in the self-help genre, this book provided enough evidence to back up the claims and also didn’t make outrageous proclamations of the power of meditation. I wasn’t entirely convinced by the end of this book (it would take Waking Up by Sam Harris to make me commit to meditation) it did show that meditation does not take away your edge and can be adopted as a daily practice by highly successful individuals. Bully for Brontosaurus: Reflections in Natural History by Stephen Jay Gould Stephen Jay Gould is an incredible science communicator and this book — a diverse collection of essays on topics such as evolution, planets, and statistics — was a joy to read. The impact of a scientist comes down to not only how important her ideas are, but how well she can communicate them to the general public. This was the first Gould book I had read, and I can say that I’m looking forward to digging in to more of his works. Meditation for Fidgety Skeptics: A 10% Happier How-to Book by Dan Harris, Jeffrey Warren, and Carlye Adler This book didn’t add much beyond the first one from Dan Harris because at the end of the day, you can read about a technique all you want, but it won’t have any effect until you put it in to practice. Most of the ideas in this book simply went over my head because I was not yet committed to starting a meditation practice. The most beneficial idea from this book to me was that meditation does not require extreme behavior, only a consistent practice. The Book of Why by Judea Pearl This is a book dedicated to the cause of casual thinking: ascertaining the reason why something happened instead of merely stated that it was correlated with another event. There are intriguing claims in this book, such as that casaul thinking may be the answer to our quest to develop artificial general intelligence, because a system that can understand why things happen will be better able to manipulative the world to its will. There is much important ground covered in this book — some of it technical — but I enjoyed the content and came away with a renewed interest in Bayesian methods. Traditional statistics does not even have the vocabulary to talk about the idea of “why”, but Bayesian inference, along with methods developed by Pearl and others (such as the “do calculus”), could finally let us start talking about causation instead of just correlation. Overcomplicated: Technology at the Limits of Comprehension by Samuel Arbesman The central premise of this book is that systems have become too complicated for any one individual to understand them. Our technologies require interaction between many different fields in order to work properly. This means generalists may play a increasingly important role in connecting disparate fields and explaining how complex systems work as we continually advance. It’s an interesting concept, and the general idea is surely true: there is nearly no technology nowadays that could be entirely mastered or even explained by one individual. Where Men Win Glory by Jon Krakeur The story of Pat Tillman, an NFL player who gave up his career to fight in the army in Iraq and then Afghanistan where he was killed by friendly fire. I still cannot understand what drove Tillman but I admire his commitment to a cause, even if that cause was not justified. The most frustrating parts of the story are the criminal cover-up by those close to President Bush and the use of Tillman (and others) for political gain to increase support for wars that should never have been fought. I am now a pacifist (I was before this book) and ultimately was left conflicted by this book because on the one hand I can’t see how anyone with Tillman’s intelligence would support the war, but on the other hand, I still believe, like Tillman, in honor and standing up for what you believe in. Drive: The Surprising Truth about What Motivates Us by Daniel Pink The central premise behind this book has had a profound influence on me: we are motivated not by external rewards, but by the internal reward from doing our work as well as possible. Moreover, the three keys to fulfilling work are autonomy — ability to choose what we do — mastery — learning how to do something well — and purpose — feeling like we are doing something worthwhile with our life. Overall, this was another profound book (following works that Mastery, The Road to Character, and Enlightenment Now) that has made me think about what I truly want to do for my career with the conclusion: try to benefit all of humanity. The Innovators: How a Group of Hackers, Geniuses, and Geeks Created the Digital Revolution by Walter Isaacson The story of the digital revolution differs from other invention stories in that none of the technologies can be said to come from the mind of a single individuals. Rather, as Steve Johnson pointed out in Where Good Ideas Come From, the best innovations occur at the boundary between fields, such as that between information theory and electrical engineering that led to the first implementations of the universal Turing machine. In order to have great ideas, we should be surrounded by other specialists in our field, but also try to interact with diverse research areas. Even the smartest inventors working in isolation will not have an impact without someone to show the value of the idea to the world : every brilliant engineer needs a golden-tongued salesperson to sell their invention to the world. Deep Learning Cookbook: Practical Recipes to Get Started Quickly by Douwe Osinga The best technical books teach my example, and just from the title of this one, it’s clear that it fits the bill. While I haven’t made it through all of the examples, I have thoroughly enjoyed the ones I did do, on convolutional neural networks, recurrent neural networks, plain deep neural networks, and just as importantly, plenty of exercises for data gathering and formatting. In deep learning, even with powerful algorithms, the quality of the data and preprocessing steps are still paramount. Also as with all technical books, it’s much more helpful to do the exercises as you read rather than just read. Moreover, make sure to put your own take on the sample code so you can figure out what is going on rather than just copying the exact code (although you can start with that). I have gotten a lot of use from this book and find it much easier to stay engaged than with those textbooks covering only the theory. Learning by doing is the most effective method, and this practical deep learning workbook provides numerous engaging exercises to get you coding and building actual useful systems. The Mismeasurement of Man by Stephen Jay Gould The idea of this book is that differences exist in IQ between groups but not because groups are inherently smarter than others: this difference is because of the nature of the tests purporting to measure intelligence and unequal living conditions. Although intelligence — however it can be defined — is somewhat genetic, it is also possible to increase intelligence significantly with the right environment which means providing this environment is of paramount importance. This book, combined with the ideas in The Blank Slate suggest that while individuals are born with different levels of intelligence, they don’t necessarily have to stay that way and we should not blame less well-off individuals for their station but work to remove environmental inequities. 21 Lessons for the 21st Century by Yuval Noah Harari This book is more like 21 speculations of a possible future than actionable lessons, but I still enjoyed listening to Harari’s ideas of humans upgrading themselves with biotechnology and fracturing into multiple classes (those who are able to afford upgrades and those who cannot) I’m not sure that this work (and Homo Deus, Harari’s other future-facing work) are anything more than science fiction fantasies in the tradition of Ray Kurzweil (The Singularity is Near) or Freeman Dyson, but they make for great discussion nonetheless. One aspect that must be included in any proposed future to accept as plausible is a growing level of inequality due to technology rendering many jobs obsolete and wealth accumulating to those who can make (innovators) or manipulate technology (computer programmers) for their benefit. The fears of a complete fracturing of society sound a little overblown to me because technology always ends up becoming cheap enough to benefit everyone. Ultimately, Harari is not afraid to think of the big picture and that is why his books inspire such great discussion. The Moral Landscape by Sam Harris Morals and principals derive not from religious traditions but rather evolved based on what was most likely to lead to human survival. This explains why there are universal morals shared across human societies and means that morals are within the realm of science. Despite common opinion to the contrary, science can speak about what should be considered right and wrong. In other words, it is possible to use evidence-based reasoning to go from an is to an ought (how things are to how we should act). Furthermore, morality is not relative to one culture but rather is defined by those actions that increase the well-being of conscious beings. America’s Obsessives by Jonah Lehrer An interesting idea — many notable people considered successful such as Charles Lindbergh and Estee Lauder have succeeded not in spite of but rather due to their compulsive tendencies. While the idea was interesting, the implementation in terms of the writing was less than stellar. The biographies the author uses to make his point were dull and unconvincing. I see little point in diagnosing historical figures with specific maladies long after they have passed especially when the aim seems only to sell a book. The Paradox of Choice by Barry Schwartz The central premise is one I have tried to take to hear: there are too many choices in modern life which makes us less happy. The greater number of choices, the more time we spend making decisions, and the more we regret them afterwards. To be satisfied, we need to limit our choices, and rid ourselves of the idea of optimizing our choices completely. A more realistic approach, called satisficing (to contrast with maximizing) is to accept choices that are good enough and then move on. This book has had a significant impact on my daily life in terms of how I go about trying to minimize my choices, make decisions quicker, and accept the decisions once they are made. The Science of Liberty by Timothy Ferris Two forces have contributed to the lasting success of nations in the 20th and 21st centuries: liberal democracy and science. Liberal democracy, the idea that governments should be elected by the populace and individuals should be free to pursue their own choices, and science, the self-correcting system of learning objective truths about the world, exist together in successful societies (measured by GDP and the welfare of the citizens) because each reinforces the other. The countries that have flourished (US, China, Germany, Britain), have been those that have embraced elements both ideas and nations have historically failed by supporting pseudoscience or enforcing barriers to the questioning of ideas. An integral part of science is self-questioning, which simply cannot take place in a society where the ruling party has declared they are never wrong. Overall, this book is a fascinating account that demonstrates the relevance of science in society and serves as a reminder that humans are capable of greatly improving themselves when given the opportunity and tools to do so. The Three-Body Problem by Ciuxin Liu An engrossing science fiction tale — told from a Chinese perspective — of an alien civilization summoned to Earth by a woman who believes that the only salvation for Earth will come from beyond. This was the first fiction book I’ve been able to enjoy for years, and I think at least some of that must be due to the different perspective (Chinese and with a feminine protagonist) and the intriguing story line. The idea at the heart of the book is interesting: I’ve repeatedly thought we need an external enemy to unite humanity (given the power of the world wars to bring together people) and have thought global warming could fill that need. I would classify this book as an intellectual adventure because problems are solved by clever answers rather than violence. I find most fiction books have either too much pointless action or too much flowery language and this book had neither. Overall, a very solid science fiction tale, one that achieves what to me are the dual purposes of fiction: to entertain and make one question the world. Fermat’s Last Theorem by Simon Singh An ultimately disappointing account of Andrew Wiles’ (and the others who came before him) efforts to solve Fermat’s famous last theorem. While I can see this problem consumed hundreds of years of effort from mathematicians, I fail to see the significance of proving the theorem either on a personal or a society level. This story is one that could be better told through a shorter article without the need for a full book. The math in the book isn’t the problem, the issue is there is no talk about why we even needed to solve this problem other than that mathematicians found it intriguing. But What if We’re Wrong?: Thinking about the Present as if it Were the Past by Chuck Klosterman A fascinating and entertaining investigation of what the present will look like from the future. Klosterman digs into such questions as what are we most wrong about now (we’re definitely living in a simulation), what books will be remembered hundreds of years hence (it will probably be an unheralded book with neutral contemporary ratings), and, are there conspiracy theories which will turn out true? These questions may be impossible to answer (and Klosterman admits as much) but they are nonetheless fun to ponder. I think it useful to reflect on how insignificant most of what we consider large events (elections, natural disasters, changing social mores) will be even a few decades from now because it forces us to think about the larger ideas and get outside of our time. It’s ludicrous to believe our generation is the one to have figured everything out but as Klosterman discusses, maybe there are objective truths onto which we have finally stumbled. While I don’t truly believe this and think future generations will look back on us as intellectually and morally deprived just as we consider those who came before, it is intriguing to consider whether we have finally arrived at ground truths. Hitch-22: A Memoir by Christopher Hitchens A rich memoir of a life well-lived. It might be odd to start with an autobiography of Hitchens as I haven’t read any of his other works, but this gave me an appreciation for his intellect and deep experiences of the world. More than anything else, I was impressed by his ability to critically examine his life and point out times when he was wrong. This ability is lacking among almost all newsworthy figures, but everyone will doubtless be wrong many times over and there is no harm in admitting it. This is more than just a memoir because Hitchens provides concise and insightful commentary about each epoch in his life, often with a critical lens. What’s clear from this work is there’s nothing shameful in changing one’s opinions when the evidence changes. Hitchens repeatedly re-evaluated his political views throughout his life, an act that would be unthinkable to most people, but which shows great maturity and self-confidence. Starting with an author’s memoir may be odd, but it showed me (at least in Hitchen’s words), that what he wanted was the best for humanity and he lived his life in accordance with that hope. The Dark Forest by Liu Cixin The sequel to The Three Body Problem was just as engrossing a piece of science fiction. At times dark and at times extremely hopeful, this book is a great mystery, adventure, and examination of humanity. The best science fiction not only entertains, but makes one think about the larger questions that never come up or are asked in daily life. The ending was rather abrupt, but did not spoil this excellent tale (and was a nice starting point for a sequel). The general idea is interesting: the universe is a dark forest and each civilization (which are innumerable) is forced to be a hunter of all others because they do not know if the others pose a threat. I don’t think this depressing view of the universe (which explains why we haven’t encountered other civilizations) is true. Rather, I think we haven’t found other intelligent societies because of the vast time differences present on a universe scale. Any other life we find (or that finds us) is likely to be far ahead or far behind us, so much so, that communication could be impossible. Like the best stories of non-fiction, the protagonists in this work solve their problems not through force, but by thinking up new solutions, or adapting old ideas to solve new problems. I find this method of resolving conflicts is able to keep me engaged to a much higher degree because it forces me to think as well. Incognito: The Secret Lives of the Brain by David Eagleman We don’t realize how much of our daily thoughts and behaviors our brain handles without our conscious awareness. In fact, the idea of a single self in control of one’s own thoughts in a myth because our brain is a dense tangle of subsystems, most of which we don’t control, and only some of which we can steer in a consistent direction. Moreover, we have to realize that what we know now about the brain and behavior is not the ultimate truth, just as we view the knowledge held by previous generations to be rudimentary. That is no reason that we should give up studying the brain because we already have made many wonderful advances in medicine and treatment that has enriched the lives of many people. The brain will always be the next frontier because there is always something else to discover. Ultimately, we are in control of ourselves, but we are limited by the hardware on which our software is forced to operate. The Shallows: What the Internet is Doing to Our Brains by Nicholas Carr The constant availability of information enabled by the internet means that we don’t engage in long stretches of conscious thought anymore. Whether or not this is bad for society remains to be seen and it may not necessarily be negative or positive, but just a different way of thinking. Personally, although I’ve been fighting to keep my attention span long, I think the internet is a positive influence on human civilization. I think it’s not so much about pushing back against the encroach of technology, but figuring out how to use it responsibly so it adds to instead of subtracts from our lives and results in time well spent. The medium of the internet does influence how we use it, but we can still make its overall effect beneficial with the right policies. Spring Forward: The Annual Madness of Daylight Saving Time by Michael Downing The story of daylight saving time is entirely chaotic. Unlike most history books, which tend to present one cohesive narrative, this book does not fall victim to that fallacy and instead presents all the details of how daylight saving time came into being, most of which make no sense. The invention and institution of daylight saving (no s) time cannot be attributed to one person or industry, but instead, was a patchwork effort with revisions taking place at every level of government. Moreover, there were lobbying efforts from cities, companies, individuals, and governments, some of which switched sides as needs changed! I did not really enjoy this book, but I liked that it was honest and did not try to name one person as the founder of daylight saving time or present a clean narrative. History is never linear, which might not be the point of this book, but it was my main takeaway. Hooked: How to Build Habit-Forming Products by Nir Eyal By now it should be clear that services such as Facebook and technologies like the smartphone have managed to hijack our biological rewards systems. The “Hooked model” for building addictive products consists of four steps that users cycle through as they adopt a new product: 1. Triggers (internal and external: what gets people to the product); 2. Action (what is the behavior that triggers the reward); 3. Variable Reward (the reward must leave the user wanting more); 4. Investment (once a user has committed energy/time to the product, they will feel compelled to keep using it). Though these ideas have been primarily used to addict users, just as any technology (or science) can be used to both negative and positive ends, we can take the same ideas that make apps addictive and make tools that enrich peoples’ lives. For example, we can build apps that form healthy habits such as exercise or daily reading. Understanding how our brains are hijacked means we are better able to control our technology usage instead of conforming to what outside entities (technology companies) want us to do. It’s tough to take a step back and see that technology has managed to hijack our biological systems, but if we can take a step back and realize how we get caught, we can use these principles to make technology into a positive influence on our daily experience. On the larger scale, technology is still in its infancy, and we can still largely shape whether it ends up hurting or helping all of us. World’s End by Cixin Liu The final part in the science fiction trilogy beginning with The Three-Body Problem. While I enjoyed the other books in the series and found them engrossing — a phenomenon that hasn’t occurred with fiction for me in several years — this book was tedious. Maybe I read the series too close together and so the ideas no longer felt novel, but this book dragged on for too long. There were many solutions that felt like they came out of thin air rather than being plausible options for the characters. By the end, I really didn’t care about the protagonists or the outcome which is never a good sign and I could have done without this third book. Overall, thinking about the entire series, the first two books were worthwhile but the third was unsatisfying. Nevertheless, the series provided me with both entertainment and some intriguing ideas for thinking about and discussing. Hacker, Hoaxer, Whistleblower, Spy: The Many Faces of Anonymous by Gabriella Coleman The hacktivist group Anonymous presents an intriguing case story in this history of the group and their major actions. Although I don’t agree with all of the group’s actions, one of the points is that there is no single Anonymous, so it’s hard to say that I don’t like the group. I’m not sure about the lasting effects of Anonymous — they have been able to get significant media attention, but have they actually changed real-world situations — and haven’t even heard anything about the group in recent years. The group could ultimately be one of those internet phenomenon that exists for a few years, makes a lot of noise but doesn’t achieve lasting change, completely disappears, and becomes a footnote for future Internet historians. I found the book to be somewhat tedious because it tends to provide too many details about small events — the dates and names problem of history books — and I think the author got too engrossed in the day-to-day instead of thinking about the longer term. I wouldn’t recommend this book except for those who are already into the hacktivist culture because I found it to have little relevance. Impossible Owls by Brian Phillips An engrossing collection of essays on all sorts of topics. Phillips has long been one of my favorite authors and I tried not to miss his articles (really long essays). This brings many of them together into a book that is wide-ranging and definitely interesting. I don’t like short articles like those typically found online, but I do enjoy the longer essay style, and each one of these works is seriously impressive. Phillips knows how to go deep into the psyche of humans through interesting topics; if you told me I would want to read a story about an animator, I would probably say that doesn’t sound likely, but Phillips is able to weave fascinating tales about diverse topics. Overall, I don’t think any of these essays were that relevant, but I enjoyed reading many different stories in one book. Bird by Bird: Some Instructions on Writing and Life by Anne Lamott A practical guide for what it means to actually be a writer. Lamott does not romanticize the profession, but instead lays all the bad and the exultant parts bare. I found I unknowingly adopted many of her strategies (after much experimenting) and I enjoyed hearing that other writers go through many of the same struggles and experience the same joy. This is a how-to book, but instead of overpromising, it takes an honest approach to what it means to write as a profession (it’s still applicable to those of us who write on the side) and the tips are actionable. The best advice of all is to constantly write, even if it’s just little bits a time. Writing is hard work, but at the best of times, it’s also enjoyable, and this book is a useful guide for those looking to make writing into a habit or a career. How to Be Miserable: 40 Strategies You Already Use by Randy Paterson The idea behind this book is straightforward: instead of telling us how we can enjoy life, Paterson lays out a plan for being as dejected as possible. To improve our well-being, we merely have to do the opposite of everything he suggests! This book was a joy to read and provided many practical pieces of advice backed up by research and the author’s years of experience working with depressed patients. I definitely follow too many of the strategies for misery — mostly to do with not getting enough human contact and spending too much time working — and there are a few takeaways that I will try to implement. One thing is clear from this book: there is more to life than work, and forgetting that is a no-fail method for misery. Instead, we need to focus on the human relationships in our life to be fulfilled. Overall, a worthwhile short read with some small yet important lessons that anyone can implement. Dataclysm: Who We Are (When We Think No One’s Looking) by Christian Rudder Imagine if you were given access not to survey data with its problems of truthfulness, but actual behavior data from decades of a major online dating service. Rudder, one of the co-founders of OkCupid, had precisely that opportunity and he makes the most of it in this book exploring the data behind not how we say we act, but how we actually do act. It turns out that we are more racist than we admit and men of all ages find younger women more attractive (this is not true for what women think of men). I like Rudder’s attitude towards data: he is positive about what we can find out from large scale studies, especially studies in which the data is from what people do rather than responses to a survey. While there aren’t any actionable conclusions that Rudder draws (no policy recommendations or design choices; maybe that will be the follow-up), overall, I found this book entertaining and a great example of the primary goal of data science: extracting meaning and insights from data. Lying by Sam Harris The premise of this book is lying is always wrong and Harris goes out of his way to justify this point in all situations. Harris is persuasive, but I couldn’t help thinking that his position is privileged: he does not have to lie in order to keep himself or his family alive.It’s possible that for some people, in some circumstances, lying is the correct moral choice. Personally, I cannot justify lying in my life any longer (I know that I have done it far too much) and I have resolved to make this a priority going forward. This book, although extremely short, has also been influential in my worldview and the next steps is putting an honest-only policy into practice. Factfulness: Ten Reasons Why We’re Wrong about the World- and Why Things are Better than You Think by Hans Rosling An exceptional entry in the category of books that explain why the world is better than people think and continues to get better. This book is filled with both facts and crucially stories of improvements in living standards, health, wealth, and rights of individuals all around the world and takes down (gently) the myth of a massive divide between the wealthiest and poorest countries. As Rosling points out, the so-called “gap” does not really exist as the majority of the world’s population actually lives in middle-income countries. Moreover, there is not reason to worry about long-term overpopulation so long as the current trends of fewer children per family, enabled by better health and more wealth, continues. Rosling’s work at GapMinder has been an inspiration to me as I try to understand the world, and this book is one of the most critical for not getting everything completely wrong about our current world. Furthermore, the central message is optimistic — yes, there are problems and conditions are poor in some countries, but on the whole, and on the long-term scale, living standards are improving remarkably quickly. The fact that the world is getting better is no reason to stop trying, but rather proof that our current approaches to poverty reduction, disease eradication, education, and human rights expansion are working and we would do well to support them. Overall, I think this was one of the most inspiring books I’ve read, although not an unrealistic inspiration, but one informed by reality. The Aleph and Other Stories by Jorge Luis Borges I’d often heard Borges mentioned as one of the great writers of short stories, but ultimately, I was disappointed by this collection. Perhaps the deeper meaning went over my head (I’m not much one for hiding stories within stories but would prefer authors to say what they mean) but I did not find any of these works particularly profound. Several were interesting, but the largest problem was that they were simply too short. Despite our collective decline in attention span, I still enjoy long stories where I can get to know the characters and gain a sense of why their quest is important. I don’t care about being able to finish a story in a single sitting because I would rather mule it over as I go about my day. The best fiction should give you just as many topics and questions to ponder as non-fiction, but these stories left me more dissatisfied than intrigued. All in all, a rather disappointing year for fiction, but a great year for non-fiction. I just don’t think there is a place on my reading shelf for made-up works anymore given the absolutely captivating nature of our actual reality. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Think Like a Bayesian (form an initial hypothesis and update as more data is collected) Think Like a Basketball shooter (make many predictions and assess why they went wrong to adjust for the next one) Think Like a Hedgehog (hold many small ideas and consult numerous sources)",Books of 2018 Notes,4,unlisted,37,12083,0.0,200,1,0,0,0,0
2,575,151.13274218050927,83,https://medium.com/p/books-of-2018-f51600aa922a,7,None,2018-12-27 12:31:00,19.21,27,267,2018-12-25 15:29:00,"['Books', 'Reading', 'Education', 'Writing', 'Self Improvement']","Books of 2018 1 year, 75 books, innumerable ideas There’s one aspect of reading to make clear up front: I don’t think reading is an inherently better way to spend one’s time than any other activity. People who brag about how many books they read or say “I never watch tv” as if it makes them superior to people who do frustrate me because there is no reason to judge other people for how they choose to live their life. Reading books is just another way to pass the time and you can waste time reading (see the books at the bottom of this rankings) just as you can watching television. For me, reading is how I spend my commute, get through exercise (listening to audiobooks), and the time right before I fall asleep, but, I don’t think this makes me better than people who watch movies or go out with friends instead. As a good rule, don’t judge people on how they spend their free time as long as they aren’t harming others. I say this up front because I don’t want this article to sound like “here’s all the books I read which makes me really smart.” I try to write a few sentences about the books I read to process them, fitting them into my existing worldview, or updating my positions as needed (when the facts change, I change my opinions). While these are originally for myself, I’ve cleaned up my notes (you can access the raw version here) and made them public for anyone curious. This is more personal than my usual data science writing, but, comments, criticisms, and discussion is still encouraged here or on Twitter. Grand Conclusions The one grand sweeping generalization from the books I read in 2018 is this: the world has never been a better place to live and it continues to get better despite what people believe. This was echoed through books such as Enlightenment Now by Steven Pinker, The Rational Optimist by Matt Ridley, and Factfulness by Hans Rosling. While may at first appear to be an overly optimistic view, it’s not naive, but, in fact, the inevitable conclusion once one escapes the negative media cycle and starts reading about the objective data. The world really is improving, it’s only our perception of it that is getting darker. This is is due to a fear-based media that covers fewer and fewer bad events with louder pleas for attention while neglecting the incredible progress of humanity. Another idea that resonated with me is: fulfillment is not derived from external sources — money, clothes, cars, useless items, praise from others — but internally, from pride in the work one does. We create meaning in life from our work, and therefore, we should focus not on the accumulation of material wealth, but on doing what we love — whether that be a job or a side project — to the utmost of our ability. This concept is expressed in the books Drive by Daniel Pink, The Road to Character by David Brooks, and Mastery by Robert Greene. There are other important concepts I’ve learned (see here), but these are the two that have stuck with me. These ideas have changed the way I live and think for the better, showing, at least in my life, the positive value of books. With the deep thoughts out of the way, let’s get to the 75 books. I’ve ranked the nonfiction books I read based on three criteria (ranking spreadsheet) For each book, I’ve written a 2-bullet-point summary. I do this to limit my thoughts (you can see the extended notes — my immediate reactions with no structure — here) and to try to capture the overarching takeaway. Books are hundreds of pages, but without exception, can be summarized in two short sentences. If these sentences are not enough to make you want to read the book, then 200 pages of the same ideas won’t be useful to you! I’ve grouped the books into tens because that’s what you do when making a list. Fiction follows at the end (scroll down) because I don’t think it should be measured by the same criteria — fiction serves a different purpose. 2. Factfulness: Ten Reasons We’re Wrong about the World and Why Things Are Better Than You Think by Hans Rosling 3. The Moral Landscape: How Science Can Determine Human Values by Sam Harris 4. Drive: The Surprising Truth about What Motivates Us by Daniel Pink 5. The Signal and the Noise: Why So Many Predictions Fail — But Some Don’t by Nate Silver 6. The Paradox of Choice: Why More is Less by Barry Schwartz 7. The Beginning of Infinity: Explanations That Transform the World by David Deutsch 8. Behave: The Biology of Humans at Our Best and Worst by Robert Sapolinski 9. The Rational Optimist: How Prosperity Evolves by Matt Ridley 10. The Blank Slate: The Modern Denial of Human Nature by Steven Pinker 11. The Road to Character by David Brooks 12. Superforecasting: The Art and Science of Prediction by Philip Tetlock 13. The Science of Liberty: Democracy, Reason, and the Laws of Nature by Timothy Ferris 14. How to Be Miserable: 40 Strategies You Already Use by Randy Paterson 15. The Science of Good and Evil: Why People Cheat, Gossip, Care, Share, and Follow the Golden Rule by Michael Shermer 16. Dataclysm: Who We Are (When We Think No One’s Watching) by Christian Rudder 17. Lying by Sam Harris 18. How to Change Your Mind: What the New Science of Psychedelics Teaches Us About Consciousness, Dying, Addiction, Depression, and Transcendence by Michael Pollan 19. Field Notes from a Catastrophe: Man, Nature, and Climate Change by Elizabeth Kolbert 20. The Book of Why: The New Science of Cause and Effect by Judea Pearl 21. Waking Up: A Guide to Spirituality Without Religion by Sam Harris 22. Bully for Brontosaurus: Reflections in Natural History by Stephen Jay Gould 23. Fundamentals of Data Visualization by Claus Wilke 24. The Inevitable: Understanding the 12 Technological Forces that Will Shape Our Future by Kevin Kelly 25. Deep Learning Cookbook: Practical Recipes to Get Started Quickly by Douwe Osinga 26. Overcomplicated: Technology at the Limits of Comprehension by Samuel Arbesman 27. Science Friction: Where the Known Meets the Unknown by Michael Shermer 28. Bird by Bird: Some Instructions on Writing and Life by Anne Lamott 29. Mastery by Robert Greene 30. 10% Happier: How I Tamed the Voice in My Head, Reduced Stress Without Losing My Edge, and Found Self-Help that Actually Works — A True Story by Dan Harris 31. Hitch-22: A Memoir by Christopher Hitchens 32. The Shallows: What the Internet is Doing to Our Brains by Nicholas Carr 33. 21 Lessons for the 21st Century by Yuval Noah Harari 34. The Black Swan: The Impact of the Highly Improbable by Nassim Taleb 35. The Obstacle is the Way: The Timeless Art of Turning Trials into Triumph by Ryan Holliday 36. Predictably Irrational: The Hidden Forces That Shape Our Decisions by Dan Ariely 37. The Mismeasurement of Man by Stephen Jay Gould 38. But What if We’re Wrong?: Thinking about the Present as if it Were the Past by Chuck Klosterman 39. The Innovators: How a Group of Hackers, Geniuses, and Geeks Created the Digital Revolution by Walter Isaacson 40. Antifragile: Things That Gain from Disorder by Nassim Taleb 41. The Ego Tunnel: The Science of the Mind and the Myth of the Self by Thomas Metzinger 42. Meditation for Fidgety Skeptics: A 10% Happier How-to Book by Dan Harris 43. The Invention of Nature: Alexander Von Humboldt’s New World by Andrea Wulf 44. Hope, Human and Wild: True Stories of Living Lightly on the Earth by Bill McKibben 45. Hacker, Hoaxer, Whistleblower, Spy: The Many Faces of Anonymous by Gabriella Coleman 46. Everything is Obvious: How Common Sense Fails Us by Duncan Watts 47. Deep Learning by Ian Goodfellow 48. Hooked: How to Build Habit-Forming Products by Nir Eyal 49. Incognito: The Secret Lives of the Brain by David Eagleman 50. A Gentle Introduction to Effective Computing in Quantitative Research: What Every Research Assistant Should Know by Henry Paarsch and Konstantin Golyaev 51. Where Men Win Glory: The Odyssey of Pat Tillman by Jon Krakeur 52. Data and Goliath: The Hidden Battles to Collect Your Data and Control Your World by Bruce Scheier 53. The Pleasure of Finding Things Out: The Best Short Works of Richard P. Feynman by Richard Feynman 54. Impossible Owls by Brian Phillips 55. Junk Raft: An Ocean Voyage and a Rising Tide of Activism to Fight Plastic Pollution by Marcus Eriksen 56. The Information: A History, A Theory, A Flood by James Gleick 57. Between the World and Me by Ta-Nehisi Coates 58. In the Heart of the Sea: The Tragedy of the Whaleship Essex by Nathaniel Philbrick 59. Spring Forward: The Annual Madness of Daylight Saving Time by Michael Downing 60. Superfreakonomics: Global Cooling, Patriotic Prostitutes, and Why Suicide Bombers Should Buy Life Insurance by Steven Levitt and Stephen Dubner 61. Godel, Escher, Bach: An Eternal Golden Braid by Douglas Hofstadter 62. Salt: A World History by Mark Kurlansky 63. Our Mathematical Universe: My Quest for the Ultimate Nature of Reality by Max Tegmark 64. When Breath Becomes Air by Paul Kalanithi 65. America’s Obsessives by Jonah Lehrer 66. Fermat’s Last Theorem by Simon Singh 67. Mrs. Sherlock Holmes: The True Story of New York City’s Greatest Female Detective and the 1917 Missing Girl Case That Captivated a Nation by Brad Ricca In my mind, it’s not fair to judge fiction against non-fiction because they have different purposes: non-fiction is meant to inform and fiction to entertain. Fiction can also teach us a lot — just about ourselves and relationships, not objective knowledge. That being said, I’ve fallen away from reading fiction because I find it uninteresting compared to the real world. While I used to be an avid reader of science fiction and fantasy (Asimov, Heinlein, GRR Martin, Tolkien, Sanderson, Pauloni), I haven’t gotten into a fiction series for a couple years. That changed this year with the Three-Body Problem series (technically called Remembrance of Earth’s Past) from Liu Cixin although I was tired of it by the final book. All in all, I was again disappointed by the fiction I read this year and don’t see it making a major comeback into my life. Reality is simply too interesting to dwell in the make-believe. Fiction was rated based on two criteria (ranking spreadsheet): I’ve tried to make these bullet points not just summaries of the book, but be forewarned there may be spoilers. It’s hard to capture the ideas of fiction books without discussing the plot. 2. The Dark Forest (2 / 3) by Liu Cixin 3. Neuromancer by William Gibson 4. Regeneration by Pat Barker 5. World’s End by Liu Cixin 6. Calvin and Hobbes: Book One of Eleven by Bill Waterson 7. The Aleph and Other Stories by Jorge Luis Borges 8. Kafka on the Shore by Haruki Murakami As always, I welcome feedback, constructive criticism, and reading about what books you’ve read. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Importance (40%): how much do the ideas in this book matter on a society-wide scale? (because books can change a society — gradually) Engrossing (20%): how entertaining and well-written was the book? (because even nonfiction books should be interesting) In terms of health, wealth, crime, living standards, human rights, and environmental protection, we have never lived at a better time and evidence suggests we will continue to gradually improve This is not only optimistic, but it’s also based on objective facts, and, by understanding the drivers — science, liberal democracy, commerce, rationality, communication — we can ensure the trend continues Progress is important to acknowledge because it means our efforts in poverty reduction, disease eradication, expansion of rights, and environmental protection are working and should be supported As a natural phenomenon, we can study ethics/morals to make objective choices about what we should do to achieve the best outcomes Instead of seeking to maximize fulfillment through external factors, we should concentrate on doing meaningful work — either a career or personal project — to the utmost of our abilities We are better at making estimates in situations that commonly occur with an opportunity for rapid feedback and not so great in novel circumstances where the evidence isn’t clear — this is when we need to gather more data People who maximize, that is, try to make the optimal choice, end up spending more time making decisions and more time regretting decisions than satisficers, who are satisfied with a less-than-perfect option We stand poised on the beginning of infinity which will be a tremendous increase in our capabilities to shape the environment enabled by technological progress — humans may be meaningless on a cosmological scale at the moment, but eventually, we can alter the galaxy Even with an imperfect understanding of why people act, we can use the findings of sociology, neuroscience, psychology, and biology to enact better policies and make decisions in our own lives — such as making irreversible plans ahead of time instead of deciding in the moment; morality consists of doing the right thing when it’s the harder thing to do All things considered, limited capitalism (with proper government controls in areas such as the environment and worker rights) is the optimal way we’ve tested for how to run an economy and, surprisingly, has led to better health, more wealth, and more rights when it is adopted (see China) The ultimate good in life is being able to help one’s fellow humans, a pursuit that is also extremely rewarding Individuals who exceed at making estimates of the future — superforecasters — spend a lot of time making predictions, adjusting predictions when facts change, and assimilating information from different sources; moreover, their estimates tend to be less extreme meaning they don’t get news coverage but are correct more often than those who appear as “pundits” in the news Where science and liberal democracy are allowed to flourish, humanity also flourishes meaning we should continue to promote rationality over orthodoxy and elections over dictators Both external — our relationships to others, health, community membership — and internal — our attitude, reactions to events, sense of meaning — factors affect how much we enjoy our lives Both absolute moral subjectivity and moral objectivity are not logical and, instead, we should adopt moral provincialism according to which we need to make decisions based on the evidence in each situation There is much we can learn from data generated through our online interactions, but we must realize behind every data point is a person and therefore data should be used to help people build better relationships Telling the truth is painful in the moment, but over time will result in better relationships and a more fulfilling life Pollan seems to have drawn his conclusion — psychedelics are positive — before writing the book (he does have to earn a living), but nonetheless, he makes a compelling case for, at the least, collecting more information through rigorous research before dismissing these substances as harmful The picture is pretty bleak: there are already people suffering from climate change, which should show us the necessity of international action Meditation is one of the primary means humans have sought meaning for millennia (it’s older than contemporary religions) and the practice is supported by evidence that meditators are better able to deal with the stresses of everyday life and have a greater sense of life fulfillment The field of science advances through self-criticism and self-correction as Gould demonstrates in essays on topics from probability to evolution Simple choices, such as making your chart labels large enough and minimizing the amount of unnecessary ink, vastly improve the usefulness of a graph by ensuring focus is on the data It’s both instructive and entertaining to read knowledgeable predictions for the future because it gives you an idea of what’s possible Deep learning is not an art available to a select few, but a basic system of operations that anyone with a little coding experience can quickly learn to create “smart” machines — or at least ones that can classify pictures and generate semi-sensible text Generalists — people who can quickly learn the basics of technology — may become increasingly relevant as a go-between for different fields to help them understand how disparate technologies can be complementary While science is how we understand the universe, it’s also used to support pseudoscientific ideas such as extrasensory perception or homeopathy and skepticism is crucial to avoid falling for harmful fictions Among the more relevant ideas are you should write every single day, even bad writing is better than no writing, don’t aim for perfection, show your writing to many people for feedback, and set realistic expectations Self-help books are generally bunk, but this work takes a no-nonsense approach: you need to suck it up and work as hard as possible Hitchens emphasizes how it’s important to be willing to change your mind when the facts on the ground shift and provides plenty of examples where his ability to question his beliefs led him to self-correct We should not adopt new technology without first asking whether it will actually improve our life or just complicate it Harari is an intellectually engaging writer, and discusses many grand ideas such as “what will give our lives meaning in the future?” and “how can we create community in an increasingly connected world?” We underestimate what can happen based on what has happened in the past and although we can’t predict Black Swan Events, we can build robustness into our lives and systems to respond positively to unexpected events We can take supposed negatives and turn them into positives, using the Stoic philosophy, a worldview based on accepting the present moment Our deviance from rational choice theory is inevitable, but we can try to understand our unlogical behavior to correct it Trying to develop a single test for intelligence is a fool’s errand and only helps social Darwinists and eugenicists who divide races, instead, we should work to enact policies — such as mandatory pre-kindergarten — that reverse the environments leading to different scores Klosterman thinks we are most wrong in our conception of reality; we are living in a simulation and eventually will create our own simulated worlds Contrary to most historians, Isaacson stresses the importance not of individuals, but collectives — like IBM — in developing technology There is no such thing as a constant “I” from one moment to the next, which is both disorienting and freeing because it means we can completely change ourselves at any moment despite what we’ve been telling ourselves As a practical application of the ideas in 10% Happier, there is not much new content here and it feels repetitive if one read the former work A useful reminder that if you want to get the public to care about science, you have to make it full of life and not just a collection of facts — people are inherently curious and science is about answering questions Primarily, these are societies that are content with less material wealth than in the West — they have stronger communities rather than more items Anonymous can’t be characterized under a single idea because of the diversity of its subgroups, but it demonstrates the power a few determined people connected by technology can now wield for bad or good Our tendency to draw straight lines connecting past events is the narrative fallacy and it means we see intentional causes instead of random occurrences, and leads to circular reasoning such as the Mona Lisa is the most well-known painting because it is famous for being a good painting The field of deep learning — neural networks — is moving so fast this book may be out of date in a few years despite the abundance of useful advice On the plus side, we can use the same techniques that get people addicted to mobile games to adopt good habits — such as reading or exercising; once we understand how we are taken advantage of, it becomes easier to avoid these products or at least use them in moderation Despite this lack of free choice, we are still responsible for our actions and over time can alter our default tendencies; we don’t have mental freedom but can change our thinking patterns with practice Another reminder (we really don’t need any more) of the utter stupidity of war and a look at the falsities used to sell war to the American public Despite the dire warnings, there were no examples of a government or corporation actually harming individuals through the collection of personal data and therefore the message currently lacks validity Feynman makes his passion for science clear in this charming collection that illustrates the importance of encouraging our natural human curiosity Although there are some interesting stories — concerning the Iditarod sled dog race or the strangeness of the American southwest — the lack of a unifying theme means there is no relevant conclusion The boat journey serves as the backdrop for reporting on the nefarious practices of corporations and the influence of lobbyists that prevent even the most common-sense environmental protection laws from being passed The overall idea is clearly observable in our modern society: the amount and velocity of information created by humans has been increasing since the beginning of written history and is accelerating A critical read for anyone who believes the myth that America has achieved race-blindness or that conditions are not that bad for minorities The ends to which the men who survived were driven is exceptional but proof that at their core, humans are survival machines willing to give up their own humanity for a few more days of existence The issue with the Freakonomics authors is they are too sure of their own conclusions leading them to dismiss contrary evidence (the confirmation bias), never a good idea when claiming to be scientific and objective While there may be profound conclusions about consciousness (which is self-recursive loops) contained in these pages, more likely readers will be confused by the endless stories within stories and repetition There is often a lot of history to tell by fully exploring the story of a single object because the world is so interconnected and history is basically a story of people first expanding and then coming back together; even a basic commodity touches almost every aspect of societal development Reading about large, interesting, potentially true ideas is usually fun, but I found this a too technical to be anything more than a slog The basic idea, that what matters in the end is family and a desire to fulfill one’s duties did not seem like anything new and I was left wondering if what people found inspiring was the premise more than the content I vehemently dislike books that post-diagnose historical figures with any ailment and this book was overly frustrating because of this practice Despite the supposed import of this proof, the book made no argument as to why this is actually important for anyone outside the math community Thoughtfulness (50%): How much did this book make me think? (because even when I read fiction, I want to be given something to think about) Humanity, if left to its own devices, is doomed and we require outside interference — either friendly or hostile — to unite us so we can survive and eventually expand to the stars Outside interference, when it does arrive, will not be benevolent and humanity is such an insignificant species that a more advanced civilization would have no use for us; furthermore, the earth, as a habitable planet in a hospitable solar system, is an attractive second home for a civilization trying to escape their own stellar system Ultimately, the universe is a dark forest with civilizations as the silent hunters: no civilization dares to shine a light to contact others (thus solving the Fermi Paradox) because there is no guarantee they will be friendly; this leads to a silent universe where only those who haven’t learned the rules — such as humans — expose themselves at great peril Often I found myself thinking I had already seen the devices used by this book, only to realize it’s because I’d read science-fiction that came after and heavily borrowed from elements established by Gibson in this story A searing portrait of the insanity of war, the horrors young men are forced to endure, the extent to which the government will go to hide the reality of battle and the lies told to the public to win war support A slightly confusing and ambiguous conclusion to the series, but, this was the first fiction series in years where I’ve wanted to read the sequels and it was thought-provoking at times While I don’t anticipate reading many more cartoons, they can be valuable and if the author is clever enough, they often do a great job of expressing ideas about what it means to be human or problems with society Interestingly, part of why I did not enjoy this work is I wanted the stories to be longer — they were too short to get me into the story or care about any of the characters (so much for shortened attention spans) Usefulness (40%): how much does this book matter to my life right now? (because a great book can change my life for the better) We are systematically wrong in our perception of the world and this misjudgment skews in one direction: towards the negative; our misconceptions can be corrected by examining both large-scale data and individual stories of human progress Ethics/morals are a product of evolution and were not invented by religion The most powerful motivators aren’t external — wealth, items, praise from others— but internal feelings of autonomy, mastery, and purpose which come from our work The people who are best at making predictions evaluate evidence from many varied sources (each of which has a different bias) and are open to changing their minds when the evidence changes: foxes, not hedgehogs We have managed to create a society with unlimited choices, from the big (where to go to college) to the small (what cereal to buy), yet, instead of a utopia, this expansion of options has made us miserable Problems are solvable: given enough time, anything not prohibited by the laws of physics will be achieved by humans To discover why someone carried out a particular action, we need to look at both macro — environment — and micro — genes, hormones, neurotransmitters — factors; the complex web of causes makes ascribing a single reason to any action impossible Capitalism — fueled by the division of labor and commerce between nations — is the single greatest anti-poverty device ever invented and has raised the living standards of billions of people over the last few decades Humans are not born equal (the “blank slate” idea) but rather with a host of advantages and disadvantages conferred on them by their genes People are not born being great humans (great meaning devoting one’s life to helping others) but rather are made through their experiences There is no secret to making accurate predictions, just lots of practice, constant feedback, a willingness to self-criticize, openmindedness, examining data from many sources, and an ability to quantify uncertainty Two intertwined forces have helped make western countries world power: science and liberal democracy, neither of which succeeds without the other There is indisputable evidence about what makes us miserable — not exercising, isolating ourselves with screens, sleep deprivation, focusing on ourselves— and so to be as miserable as possible (this is a parody self-help book) we should just keep doing these practices Ethics, as a product of evolution (not one developed by religion), can be researched objectively, so we can figure out why people act for good or bad With access to OkCupid data (Rudder was one of the founders), we can see how people actually act, not how they respond to surveys; we discover people are more racist and hostile to outsiders than they admit, but these results are limited to the narrow domain of online dating Lying is indefensible under any circumstances and always leads to long-term negative outcomes Once a topic of fervent research (in the 1960–1970s) but subsequently banned, psychedelics are again making a comeback due to promising — but preliminary — results in treating mental illnesses such as PTSD and depression, as well as enriching lives of people with good mental health Traditional reporting on climate change focuses on the long-term potential doom and gloom scenarios while Kolbert’s work looks at the effects of climate change on individuals right now; stories of real people are more convincing than reams of statistics Statistics currently does not even have words for expressing the idea of causation — why did something occur — despite the question’s paramount importance to fields such as medicine and sociology For those without a belief in formal religion, it can feel as if we have no sense of meaning — a quintessential part of being human; however, all is not lost because there are methods, backed by science and data, for even secular humanists to find a deeper purpose to our existence Science is our best tool for understanding the universe, but we don’t need to treat it as lifeless and bland, as the late science communicator and evolutionist Gould makes clear through stories that bring important scientific ideas to life in a way no school science class ever could Accurately portraying data is crucial for achieving the goal of data science — better decisions through data — and there are basic rules for achieving this as demonstrated in this useful textbook describing and showing the principles of making effective figures (using the R programming language) Predicting the future is tough, but Kelly has intriguing ideas about how our relationship to and use of technology will change, from the obvious such as increased sharing and more tracking of behaviors to the utopian (or dystopian) such as building a system spanning the planet connecting all people and machines or widely available artificial intelligence The best technical books teach by example, and this book follows that pattern with numerous exercises to build actual deep learning systems Systems built by humans — such as the electrical grid — are now too complicated for any single person to entirely understand Even scientists can occasionally be fooled — or fool themselves — and science is at its best when practitioners are willing to criticize themselves and question the work of even top “experts” in a field Writing is one of the most rewarding activities a human can engage in but it also can be exceedingly difficult at times, which led Lamott to write this short book with practical tips for building a writing habit No matter what job you are currently doing (even if it’s not what you want), you need to strive for mastery — doing the best possible work —because this will affect how you approach every aspect of your life Meditation is difficult, requires intense focus/concentration, and can only be improved through deliberate practice, nonetheless, it’s a worthwhile exercise because of the proven benefits for practitioners A wonderfully engrossing (and self-critical) autobiography of the late journalist and skeptic Christopher Hitchens that takes the reader through many captivating life experiences The Internet is not inherently detrimental to our brains yet it is changing daily life by compressing our thoughts, writing, and personal interactions The world in the 21st century will divide into two classes: those who control the machines and artificial intelligence that take human jobs and those who are left with no meaningful work to do; the ultimate result is unprecedented inequality Due to the sheer number of events that happen every day, some exceedingly unlikely situations (natural disasters, unexpected election results) will inevitably occur with disastrous consequences There are three core ideas in our lives, perception: perceiving what happens in the world, action: how we respond to these events, and will: how we can make the right decision even in tough circumstances Humans don’t think rationally (see Thinking, Fast and Slow), but this deviance from rational behavior occurs in predictable ways Although there are significant differences in IQ scores between ethnicities, this is not an indication of innate racial intelligence differences but rather is the product of unequal environments and the structure of the tests As with every generation, ours will be seen as utterly wrong 50 (or even 10) years from now and it’s an interesting exercise to think where we are incorrect and how the future will view us The best ideas — such as those leading to computers — are born not deep within one field, but rather at the intersection between fields where ideas can mix and inventions find novel applications The sequel to The Black Swan puts forth the idea of antifragility: instead of surviving surprise events, use them to our advantage; some systems, such as human beings, respond positively to the right amount of stress Our conscious experience is not a continuous stream, but a sequence of three second long experiences tied together by our brain into a single cohesive narrative — an internal story of ourselves Meditation is difficult, but as with any practice, one can improve at it through deliberate consistent practice; maintaining an everyday meditation exercise is easier with some basic advice Humboldt — an explorer, biologist, naturalist, and science communicator — captivated the public with his tales of travel and science, bringing science to the masses more effectively than any dry textbook could have McKibben’s previous work (The End of Nature) was very depressing and he wrote this as an exploration of societies where humans have high levels of happiness without destroying the environment — sustainable living The group Anonymous underwent a remarkable turn from pranksters (internet tomfoolery) to hacktivists that accomplished real-world change such as in the Arab Spring and efforts to thwart terrorists Once we have seen the outcome of an event, the storyline seems obvious in retrospect — such as with the success of Facebook as a social network — even though the real cause is a combination of good ideas and a lot of luck A highly theory-driven look at the entire field of deep learning, including history, current breakthroughs, best practices, and future directions Thanks to psychology research, products — such as smartphones and social networks — have hacked into people’s rewards systems to keep them addicted and wasting time on meaningless applications Almost all of what happens in our brain goes on without our conscious awareness meaning the notion that we choose our actions is false It will soon be impossible to do any meaningful research without at least basic data analysis and this work covers many topics in computing for research at a shallow level with a few practical exercises Pat Tillman’s story is a tragedy, not because he gave up a pro football career to join the US Army — this was his choice — but because he was killed by friendly fire and then the United States — all the way to the president — took advantage of his death by lying to win war support Large corporations and governments are now harnessing all our data and subsequently will be able to know even the most personal things about us The best scientists are driven not be a desire for wealth or fame but for the more powerful want to figure out how nature works It’s hard to draw any takeaways from this work of collected essays, other than that different cultures — even within a single country (the US) — can vary significantly and we should try to experience many cultures Major plastics and chemical corporations are ruthless in their lobbying for the right to continue to pollute the ocean and pass responsibility on to consumers as highlighted by Eriksen as he builds a raft out of plastic bottles to sail to Hawaii to highlight these offenses New modes of communication always increase in speed and availability and generally are beneficial for civilization because they reduce barriers It’s impossible to know what it’s like to grow up as anybody other than yourself, a point is driven home with great effect in this letter to the author’s son of what it’s like to grow up a black man in America Moby Dick was written about an actual event, a whaling ship sunk by a whale, as is told in this entertaining historical work There is no single reason we have Dayling Saving Time (no, it was not invented by Benjamin Franklin or advocated by the farmers) but rather a patchwork of causes including multiple lobbying groups, cities, and individuals some of whom took both sides of the debate at different times Another collection (in addition to Freakonomics) of seemingly contradictory stories supported by — in my opinion — too little evidence A bewildering tale of mathematics, art, music, artificial intelligence, recursion, and the human brain that seems as if was written just to prove the author is smarter than you Salt, the simple condiment, has played an outsized role in shaping our modern world from driving trade to allowing meat to be kept longer The universe, or rather the infinitude of universes known as the multiverse, is fundamentally mathematical which means reality is a mathematical structure; this has consequences for research in physics and mathematics, although probably not for everyday life This memoir, written by a doctor as he was dying of cancer, is considered as profound, yet, I found it mundane, leading to the question “If someone writes a book while dying, does that mean it has to be profound?” Many of America’s most successful individuals, from Estee Lauder to Thomas Jefferson, likely suffered from compulsive personality disorders which counterintuitively contributed to their success Andrew Wiles was able to prove Fermat’s Last Theorem (no number n satisfies a^n + b^n = c^n for n > 2) after hundreds of years of effort by the math community and decades of personal work from Wiles in what is considered a stunning mathematical result of great importance A story of a female detective working one hundred years ago who solved crimes and helped those who couldn’t speak for themselves Captivating (50%): How engrossing was the book? (because fiction is designed primarily to entertain) If given a dire enough threat, humans can be convinced to give up their differences and work together towards a common cause, moreover, when it’s necessary for survival, we will make extremely rapid progress A washed-up computer hacker who was prevented from hacking after stealing from his employer is given another job for a shadowy entity in this novel that spawned the cyper-punk genre A surprisingly captivating World War One story about a hero sent to a mental hospital to be “reprogrammed” for battle after becoming a pacifist and publicly renouncing the war The culmination of the Remembrance of Earth’s Past trilogy, ending in increasingly implausible escalations and travel through both time and space (it wouldn’t be a science fiction novel without these elements) A book of cartoons with a boy and his pet tiger which is surprisingly accurate in its portrayal of life; overall, a nice book to use as an escape from technical work and classes A collection of short stories I found to be uninteresting and incomprehensible; this was disappointing after hearing of Borges reputation for being a master of the art An utterly baffling mystery story involving talking cats, animated cartoon characters, and too many ephemeral details",Books of 2018,3,published,1390,7125,0.08070175438596491,1,1,0,0,0,0
12,875,149.1706070963079,159,https://towardsdatascience.com/the-copernican-principle-and-how-to-use-statistics-to-figure-out-how-long-anything-will-last-9cceb7aba20a,4,Towards Data Science,2018-12-29 11:36:00,30.01,8,1626,2018-12-28 11:41:00,"['Science', 'Towards Data Science', 'Education', 'Statistics', 'Universe']","The Copernican Principle and How to Use Statistics to Figure Out How Long Anything Will Last Statistics, the lifetime equation, and when data science will end The pursuit of astronomy has been a gradual process of uncovering the insignificance of humanity. We started out in the center of the universe with the cosmos literally revolving around us. Then we were rudely relegated to one of 8 planets orbiting the sun, a sun which subsequently was revealed to be just one of billions of stars (and not even a large one) in our galaxy. This galaxy, the majestic Milky Way, seemed pretty impressive until Hubble discovered that all those fuzzy objects in the sky are billions of other galaxies, each of which has billions of stars (potentially with their own intelligent life). The demotion has only continued in the 21st century, as mathematicians and physicists have concluded the universe is one of an infinity of universes collectively called the multiverse. On top of the relegation to a smaller and smaller part of the cosmos, now some thinkers are claiming we live in a simulation, and soon will create our own simulated worlds. All of this is a long way to say we are not special. The idea that the Earth, and by extension humanity, does not occupy a privileged place in the universe is known as the Copernican Principle. While the Copernican Principle was first used with respect to our physical location — x, y, and z coordinates — in 1993, J Richard Gott applied the concept that we aren’t special observers to our universe’s fourth dimension, time. In “Implications of the Copernican principle for our future prospects” ($200 here or free through the questionably legal SciHub here), Gott explained that if we assume we don’t occupy a unique moment in history, we can use a basic equation to predict the lifetime of any phenomenon. The equation, in its simple brilliance (derivation at the end of article) is: Where t_current is the amount of time something has already been around, t_future is the expected amount of time it will last from now, and confidence interval expresses how certain we are in the estimate. This equation is based on a simple idea: we don’t exist at a unique moment in time and therefore, when we observe an event, we are most likely watching the middle and not the beginning or the conclusion. As with any equation, the best way to figure out how it works is to input some numbers. Let's apply this to something simple, say the lifetime of the human species. We’ll use a 95% confidence interval and assume modern humans have been around for 200,000 years. Plugging in the numbers, we get: The answer to the classic dinner-party question (okay, only the dinner parties I go to) of how long humans will be around is 5130 to 7.8 million years with 95% confidence. This is in close agreement with actual evidence that shows the mean duration of a mammal species is about 2 million years with the Neanderthals making it 300,000 years and Homo erectus 1.6 million years. The neat part about this equation is it can be applied to anything while relying only on statistics instead of trying to untangle a complex underlying web of causes. How long a television show runs for, the lifetime of a technology, or the length of time a company exists are all subject to numerous factors that are impossible to tease apart. Rather than digging through all the causes, we can take advantage of the temporal (a fancy word for time) Copernican Principle and arrive at a decent estimate for the lifetime of any phenomenon. To apply the equation to something closer to home, data science, we first need to find the current lifetime of the field, which we’ll put at 6 years based on when the Harvard Business Review released the article “Data Scientist: The Sexiest Job of the 21st Century”. Then, we use the equation to find we can expect, with 95% confidence, data science will be around for at least another 8 weeks, and at most, 234 years. If we want a narrower estimate, we reduce our confidence interval: at 50%, we get from 2 to 18 years. This illustrates an important point in statistics: if we want to increase the precision, we have to sacrifice accuracy. A smaller confidence interval is less likely to be correct, but it gives us a narrower range for our answer. If you want to play around with the numbers, here’s a Jupyter Notebook. You might object the answers from this equation are ridiculously wide, a point I’ll concede. However, the objective is not to get a single number — there are almost no situations, even when using the best algorithm, that we can find the one number guaranteed to be spot on — but to find a plausible range. I like to think of the Copernican Lifetime Equation as a Fermi estimate, a back of the envelope style calculation named for the physicist Enrico Fermi. In 1945, with nothing more than some scraps of paper, Fermi estimated the yield of the Trinity atomic bomb test to within a factor of 2! Likewise, we can use the equation to get a reasonable estimate for the lifetime of a phenomenon. There are two important lessons, one technical, one philosophical, from using the Copernican Principle to find out how long something will be around: With regards to the first point, if you want to figure out how long a Broadway show will run, where do you even start gathering data? You could look at reviews, actors’ reputations, or even the dialogue in the script to determine the appeal and figure out how much longer the show will go on. Or, you could do as Gott did, apply his simple equation, and correctly predict the runtimes of 42 out of 44 shows on Broadway. When we think about the individual data points, it’s easy to get lost in the details and mis-interpret some aspect of human behavior. Sometimes, we need to take a step back, abstract away all the details, and apply basic statistics instead of trying to figure out human psychology. On the latter point, as Nassim Taleb points out in his book Antifragile, the easiest way to figure out how long a non-perishable item — such as an idea or a work of art — will be around is to look at its current lifetime. In other words, the future lifetime of a technology is proportional to its past lifetime. This is known as the Lindy Effect and makes sense with a little thought: a concept that has been around for a long time — books as a medium for exchanging information — must have a reason for surviving so long, and we can expect it to persist far into the future. On the other hand, a new idea— Google Glass — is statistically unlikely to survive because of the vast number of new concepts arising every single day. Moreover, companies that have been around for 100 years — Caterpillar — must be doing something right and we can expect them to be around longer than startups — Theranos — which have not demonstrated they fulfill a need. For one more telling example of the Copernican Lifetime Equation, consider the brilliant tweet you sent an hour ago. Statistics tell us this will be relevant for between 90 more seconds to a little less than 2 days. On the other hand, the oldest English story, Beowulf, will still be read by bored students at least 26 years from now and up to 39,000 years in the future. What’s more, this story won’t be experienced in virtual reality — consumer virtual reality having between 73 days and 311 more years — but on the most durable form of media, books, which have 29.5 to 45000 years of dominance left. Some people may view the Copernican Principle — both temporal and spatial — as a tragedy, but I find it exciting. Much as we realized the stunning grandeur of the universe only after we discarded the geocentric model, once we let go of the myth that our time is special and that we exist at the pinnacle of humanity, the possibilities are vast. Yes, we may be insignificant on a cosmic scale now, but 5000 years from now our ancestors — or possibly us — will expand throughout and even fundamentally alter the Milky Way. As David Deutsch points out in his book The Fabric of Reality, anything not prohibited by the laws of physics will be achieved by humans given enough time. Instead of worrying that the job you’re supposed to be doing now is meaningless, think of it as contributing to the great endeavor upon which humanity has embarked. We are currently subject to the Copernican Principle, but maybe humans really are different: after all, we are starstuff that has evolved the ability to contemplate our place in the universe. The derivation of the Copernican Lifetime Equation is as follows. The total lifetime of anything is the current lifetime plus the future lifetime: If we don’t believe our temporal location is privileged, then our observation of a phenomenon occurs neither at the beginning nor the end: Make the following substitution for z: Insert the definition of the total lifetime to get: Then solve for the future lifetime of any phenomenon: With a confidence interval of 95%, we get the multiplicative factors 1/39 and 39; with a confidence interval of 50%, the factors are 1/3 and 3; for 99% confidence, our factors become 1/199 and 199. You can play around with the equations in this Jupyter Notebook. Also, take a look at the original paper by Gott for more details. As always, I welcome constructive criticism and feedback. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. A good first approximation of how long something will last is how long it has already lasted How to Predict Everything We Have a Pretty Good Idea of When Humans Will Go Extinct The Copernican Principle: How to Predict Anything We can use statistics to quickly obtain objective estimates which are not subject to human factors. (Also, statistics can be enjoyable!)",The Copernican Principle and How to Use Statistics to Figure Out How Long Anything Will Last,16,published,5419,1898,0.4610115911485774,0,1,0,1,0,0
13,712,146.9090269775926,112,https://towardsdatascience.com/analyzing-medium-story-stats-with-python-24c6491a8ff0,4,Towards Data Science,2018-12-31 17:53:00,19.87,12,777,2018-12-30 16:56:00,"['Data Science', 'Towards Data Science', 'Python', 'Education', 'Data Analysis']","Data Science with Medium Story Stats in Python A Python toolkit for data science with Medium article statistics Medium is a great place to write: no distracting features, a large — yet civil — readership, and, best of all, no advertisements. However, one aspect where it falls short is in the statistics you can see for your articles. Sure, you can go to the stats page, but all you get to see is some plain numbers and a bar chart in an awful shade of green. There’s no in-depth analysis of any kind and no way to make sense of the data generated by your articles. It’s as if Medium said: “let’s build a great blogging platform, but make it as difficult as possible for writers to get insights from their stats.” Although I don’t care about using stats to maximize views (if I wanted to get the most views, all my articles would be 3-minute lists), as a data scientist, I can’t bear the thought of data going unexamined. Instead of just complaining about the poor state of Medium’s stats, I decided to do something about it and wrote a Python toolkit to allow anyone to quickly retrieve, analyze, interpret, and make beautiful, interactive plots of their Medium statistics. In this article, I’ll show how to use the tools, discuss how they work, and we’ll explore some insights from my Medium story stats. The full toolkit for you to use is on GitHub. You can see a usage Jupyter Notebook on GitHub here (unfortunately interactive plots don’t work on GitHub’s notebook viewer) or in full interactive glory on NBviewer here. Contributions to this toolkit are welcome! Getting Started in 1 Minute First, we need to retrieve some stats. When writing the toolkit, I spent 2 hours trying to figure out how to auto login to Medium in Python before deciding on the 15-second solution listed below. If you want to use my data, it’s already included in the toolkit, otherwise, follow the steps to use your data: This is demonstrated in the following clip: Next, open a Jupyter Notebook or Python terminal in the toolkit’s medium/ directory and run (again, you can use my included data): This will not only parse stats.html file and extracts all the information, it also goes online to every article, retrieves the entire article and metadata, and stores the results in a dataframe. For my 121 articles, this process took about 5 seconds! Now, we have a dataframe with complete info about our articles: (I’ve cut off the dataframe for display so there is even more data than shown.) Once we have this information, we can analyze it using any data science methods we know or we can use the tools in the code. Included in the Python toolkit are methods for making interactive plots, fitting the data with machine learning, interpreting relationships and generating future predictions. As a quick example, we can make a heatmap of the correlations in the data: The <tag> columns indicate whether the story has a specific tag. We can see the tag “Towards Data Science” has a 0.54 correlation with the number of “fans” indicating that attaching this tag to a story is positively correlated with the number of fans (as well as the number of claps). Most of these relationships are obvious (claps is positively correlated with fans) but if you want to maximize story views, you may be able to find some hints here. Another plot we can make in a single line of code is a scatterplot matrix (also affectionately called a “splom”) colored by the publication: (These plots are interactive which can be seen in NBviewer here). How it Works Before we get back to the analysis (there are a lot more plots to look forward to), it’s worth briefly discussing how these Python tools get and display all the data. The workhorses of the code are BeautifulSoup, requests, and plotly, which in my opinion, are as important for data science as the well-known pandas + numpy + matplotlib trio (as we’ll see, it’s time to retire matplotlib). From a first look at the Medium stats page, it doesn’t seem very structured. However, hidden beneath every page on the internet is HyperText Markup Language (HTML), a structured language for rendering web pages. Without Python, we might be forced to open up excel and start typing in those numbers (when I was at the Air Force, no joke, this would have been the accepted method) but, thanks to the BeautifulSoup library, we can make use of the structure to extract data. For example, to find the above table within the downloaded stats.html we use: Once we have a soup object, we step through it, at each point getting the data we need (HTML has a hierarchical tree structure referred to as a Document Object Model — DOM). From the table, we take out an individual row — representing one article — and extract a few pieces of data as follows: This might appear tedious, and it is when you have to do it by hand. It involves a lot of using the developer tools in Google Chrome to find the information you want within the HTML. Fortunately, the Python Medium stats toolkit does all this for you behind the scenes. You just need to type two lines of code! From the stats page, the code extracts metadata for each article, as well as the article link. Then, it grabs the article itself (not just the stats) using the requests library and it parses the article for any relevant data, also with BeautifulSoup. All of this is automated in the toolkit, but it’s worth taking a look at the code. Once you get familiar with these Python libraries, you start to realize how much data there is on the web just waiting for you to grab it. As a side note, the entire code takes about 2 minutes to run sequentially, but since waiting is unproductive, I wrote it to use multiprocessing and reduced the run time to about 10 seconds. The source code for data retrieval is here. This is a highly unscientific chart of my enjoyment of Python plots over time: The plotly library (with the cufflinks wrapper) has made plotting in Python enjoyable once again! It enables single-line-of-code fully-interactive charts that are so easy to make I have vowed to never write another line of matplotlib again. Contrast the two plots below both made with one line of code: On the left is matplotlib's effort— a static, boring chart — while on the right is plotly's work — a nice interactive chart which, critically, lets you make sense of your data quickly. All of the plotting in the toolkit is done with plotly which means much better charts in much less code. What’s more, plots in the notebook can be opened in the online plotly chart editor so you can add your own touches such as notes and final edits for publication: The analysis code implements univariate linear regressions, univariate polynomial regressions, multivariate linear regressions, and forecasting. This is done with standard data science tooling: numpy, statsmodels, scipy, and sklearn. For the full visualization and analysis code, see this script. Analyzing Medium Articles Back to the analysis! I usually like to start off by looking at univariate — single variables — distributions. For this, we can use the following code: Clearly, I should keep publications in “Towards Data Science”! Most of my articles that are not in any publication are unlisted meaning they can only be viewed if you have the link (for that you need to follow me on Twitter). Since all of the data is time-based, there is also a method for making cumulative graphs showing your stats piling up over time: Recently, I’ve had a massive spike in word count, because I released a bunch of articles I’ve been working on for a while. My views started to take off when I published my first articles on Towards Data Science. (As a note, the views aren’t quite correct because this assumes that all the views for a given article occur at one point in time, when the article is published. However, this is fine as a first approximation). The scatterplot is a simple yet effective method for visualizing relationships between two variables. A basic question we might want to ask is: does the percentage of people who read an article decrease with article length? The straightforward answer is yes: As the length of the article — reading time — increases, the number of people who make it through the article clearly decreases and then levels out. With the scatterplot, we can make either axis a log scale and include a third variable on the plot by sizing or coloring the points according to a number or category. This is also done in one line of code: The “Random Forest in Python” article is in many ways an outlier. It has the most views of any of my articles, yet takes 21 minutes to read! Although the reading ratio decreases with the length of the article, does the number of people reading or viewing the article as well? While our immediate answer would be yes, on closer analysis, it seems that the number of views may not decrease with reading time. To determine this, we can use the fitting capabilities of the tools. In this analysis, I limited the data to my articles published in Towards Data Science that are shorter than 5000 words and performed a linear regression of views (dependent variable) onto word count (independent variable). Because views can never be negative, the intercept is set to 0: Contrary to what one might think, as the number of words increases (up to 5000) the number of views also increases! The summary for this fit shows the positive linear relationship and that the slope is statistically significant: There was once a private note left on one of my articles by a very nice lady which said essentially: “You write good articles, but they are too long. You should write shorter articles with bullet points instead of complete sentences.” Now, as a rule of thumb, I assume my readers are smart and can handle complete sentences. Therefore, I politely replied to this women (in bullet points) that I would continue to write articles that are exceedingly long. Based on this analysis, there is no reason to shorten articles (even if my goal were to maximize views), especially for the type of readers who pay attention to Towards Data Science. In fact, every word I add results in 14 more views! We are not limited to regressing one variable onto another in a linear manner. Another method we can use is polynomial regression where we allow higher degrees of the independent variable in our fit. However, we want to be careful as the increased flexibility can lead to overfitting especially with limited data. As a good point to keep in mind: when we have a flexible model, a closer fit to the data does not mean an accurate representation of reality! Using any of the higher-degree fits to extrapolate beyond the data seen here would not be advisable because the predictions can be non-sensical (negative or extremely large). If we look at the statistics for the fits, we can see that the root mean squared error tends to decrease as the degree of the polynomial increases: A lower error means we fit the existing data better, but it does not mean we will be able to accurately generalize to new observations (a point we’ll see in a little bit). In data science, we want the parsimonious model, that is, the simplest model that is able to explain the data. We can also include more than one variable in our linear fits. This is known as multivariate regression since there are multiple independent variables. There are some independent variables, such as the tags Python and Towards Data Science, that contribute to more fans, while others, such as the number of days spent editing, lead to a lower number of fans (at least according to the model). If you wanted to figure out how to get the most fans, you could use this fit and try to maximize it with the free parameters. Future Extrapolations The final tools in our toolkit are also my favorite: extrapolations of the number of views, fans, reads, or word counts far into the future. This might be complete nonsense, but that doesn’t mean it’s not enjoyable! It also serves to highlight the point that a more flexible fit — a higher degree of polynomial — does not lead to more accurate generalizations for new data. Looks like I have a lot of work set out ahead of me in order to meet the expected prediction! (The slider on the bottom allows you to zoom in to different places on the graph. You can play around with this in the fully interactive notebook). Getting a reasonable estimate requires adjusting the degree of the polynomial fit. However, because of the limited data, any estimate is likely to break down far into the future. Let’s do one more extrapolation to see how many reads I can expect: You, my reader, also have your work set out for you! I don’t think these extrapolations are all that useful but they illustrate important points in data science: making a model more flexible does not mean it will be better able to predict the future, and, all models are approximations based on existing data. Conclusions The Medium stats Python toolkit is a set of tools developed to allow anyone to quickly analyze their own medium article statistics. Although Medium itself does not provide great insights into your stats, that doesn’t prevent you from carrying out your own analysis with the right tools! There are few things more satisfying to me than making sense out of data — which is why I’m a data scientist— especially when that data is personal and/or useful. I’m not sure there are any major takeaways from this work — besides keep writing for Towards Data Science — but using these tools can demonstrate some important data science principles. Developing these tools was enjoyable and I’m working on making them better. I would appreciate any contributions (honestly, even if it’s a spelling mistake in a Jupyter Notebook, it helps) so check out the code if you want to help. Since this is my last article of the year, I would like to say thanks for reading — no matter how many stats you contributed to the totals, I could not have done this analysis without you! As we enter the new year, keep reading, keep writing code, keep doing data science, and keep making the world better. As always, I welcome feedback and discussion. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Scroll down to the bottom so all the stories’ stats are showing. Right click and save the page as stats.html in the toolkitdata/ directory Go to your Medium Stats Page.",Data Science with Medium Story Stats in Python,8,published,3910,2813,0.2531105581230004,1,1,1,1,0,1
9,505,145.3102408304977,47,https://towardsdatascience.com/announcing-the-reality-project-e16cc71abb64,2,Towards Data Science,2019-01-02 08:15:00,38.25,6,371,2018-12-31 20:09:00,"['Data', 'Education', 'Statistics', 'The Reality Project', 'Data Journalism']","Announcing the Reality Project Becoming less wrong about the world with data Strange and wonderful things happen when you put down the newspaper, turn off the tv, and decide to look at actual statistics about the world instead of relying on what you’ve been told. The ground begins to shift beneath your feet as you realize the worldview which the news has been feeding you — that the world is on a downward slide — is, based on almost every single factual measure, in direct opposition to reality. Once we look at the data on topics such as wealth, health, human rights, and environmental protection, we are left with no option but to conclude the world has been on an upward trajectory for most of recorded history, we live in the best time ever for human civilization, and the improvement shows no signs of stopping. Over the past five years, as I’ve slowly weaned myself from a debilitating news addiction, I’ve experienced what can only be described as an update to the worldview-generating software in my brain. Through tens of thousands of book pages, millions of data points, and countless charts, my view of the world as a dark, dangerous place and humanity as an evil force has reversed. Based on the data, the story of the world has flipped: the history of humanity is not one of decline, but a gradual ascent continuing to today. I did not start out with the aim of becoming an optimist, but a long-term positive outlook is the inevitable conclusion from a fact-based examination of the world. As a new year begins, and now that I’ve assimilated enough independent sources of data to be confident reality is really better than we think, I’m starting The Reality Project, a data-driven endeavor to present the world as it is through a series of weekly articles. The stated goal is to become less wrong about the world through data, disposing of our distorted worldviews. If, along the way, the data happen to give you a more optimistic view of humanity, then consider it as a beneficial side effect. The first article in The Reality Project, “The Disappearing Poor” is here. You can find all the published articles in The Reality Project here. Basics of The Reality Project Each week, I’ll be writing about statistics concerning different aspects of our world, selected based on the views where I was initially most wrong. These will cover topics from the environment to democracy to nuclear weapons — if humans are involved and there is reliable data, it falls in this purview although the focus will generally be on the big picture instead of daily minutiae. The articles will be short and informative, and each piece of data must be confirmed by independent sources to make it into The Reality Project. Upfront, I’d like to make clear I make no claims to have a monopoly on the truth. I’m aware that data is subject to the biases of those collecting and reporting it. However, just the possibility we may be proved wrong is not a reason to prevent us from trying to get at the truth. Science, and by extension humanity, advances through a process of gathering data, questioning theories, and discarding ideas when they are no longer supported by data. My beliefs are never set in stone, and when the evidence changes, or when I find data contradicting my existing ideas, it will not be suppressed but reported! This project has allegiance to a single idea: we should rely on evidence — data — and not opinions. What distinguishes The Reality Project from the news is a willingness to overturn worldviews when they are not in line with reality. Why this Matters Being less wrong about the world has not just been a vanity project. My fact-based worldview has altered the way I live for the better and I believe an accurate conception of the world is vital for society. Prior to this worldview software upgrade, I never volunteered or contributed to charitable causes based on simple reasoning: the world is a terrible place, you have to look out for yourself, and there will always be poor/sick/oppressed/hungry people so there’s no use trying to make a difference. The single most important effect of learning the real state of the world is seeing that worldwide efforts in areas such as poverty reduction, human rights expansion, and disease eradication have been overwhelming successes. These programs make a difference and prove that people acting together can reduce the suffering of conscious beings on a global scale. The facts show that trying to improve your own neighborhood or even the entire world is not futile. This realization flipped a switch in my brain: it is indefensible to not give time or money to making the world better. The United Nations, nonprofits, and yes, even individual volunteers, contribute to the flourishing of humans. Every bit of work you do may seem inconsequential, but it adds up until we live in a world where since 1990 over 1 billion people have escaped extreme poverty and the number of children who have received vaccines is over 85%. In short, much as discovering the plasticity of the brain makes you realize you can change your habits for the better (at any point in life), taking a data-driven worldview means seeing that humanity as a whole is capable of improvement. People assume that reading the news is a civic duty, but it wasn’t until I started reading the facts that I began contributing in a meaningful way. Those who incorrectly assume the world is getting worse are probably not among the many people actually making it better. If we want to adopt a fact-based worldview then we need some facts. Fortunately, for those looking to be less wrong about the world, there are more resources for data (often free) than ever before. If you want to replace a news habit (evidence suggests we don’t get rid of bad habits but replace them with positive ones) or just check the facts, then start with these: These are just a handful of the sites and books which present data about the world as it really is. There will be additional resources provided over the course of the project and I’m always looking for more recommendations. Conclusions The Reality Project will be a once-a-week series of articles dedicated to presenting a fact-based picture of the world with an aim to become less wrong about the world with data. Every week we will be exploring a different topic covering the entire range of human activities. This project was borne out of my “enlightenment” that began once I started to base my worldview on facts rather than what I was told through the news. Although the intention is not to make you more optimistic, that may be an inevitable side effect of becoming less wrong about the world. There have been many individuals and institutions that have guided me along my journey and will be with us throughout The Reality Project. Prominent among them is the incredible late Hans Rosling who made it his goal to spread a data-centered worldview, and by extension, work to ensure that things continue to get better. His work is carried on by his wife and the Gapminder Foundation. I’d like to leave you with a signature video from him, one of many inspirations I’ve had on my path out of the darkness. As always, I welcome feedback, constructive criticism, and, actively encourage dissension throughout the Reality Project. I ask only that rebuttals are rooted in data. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Gapminder.org: The Gapminder Foundation is an organization dedicated to improving the world by helping society adopt a fact-based worldview. At the least, take the Gapminder Test to assess your world knowledge. Worldbank open data: if you like your data directly from the source, this is the place to get it. Explore the data either through charts online or by downloading the data. Factfulness: Ten Reasons We’re Wrong About the World — and Why Things Are Better Than You Think by Hans Rosling (good review here): an accessible book explaining why we don’t understand the world and what to do about it. A joy to read and a good starting point for a fact-based worldview. The Rational Optimist: How Prosperity Evolves by Matt Ridley: an explanation of what drives human progress and prosperity increases. The Better Angels of Our Nature: Why Violence has Declined by Steven Pinker (good overview) and Enlightenment Now: The Case for Reason, Science, Humanism, and Progress by Steven Pinker (nice podcast here): these are magnificent works that will fundamentally alter how you view the world. Together, they consist of 1000+ pages of rigorously researched statistics, graphs, and stories, showing not just why we live in the most peaceful time in human history, but in the best time for human civilization. Our World in Data: visualizations, raw data, in-depth analysis — you’ll find it all at this comprehensive resource compiling data from many sources.",Announcing the Reality Project,4,published,970,1698,0.29740871613663133,1,1,0,0,0,0
15,822,145.2592832192477,64,https://towardsdatascience.com/the-disappearing-poor-6c68789e5a53,3,Towards Data Science,2019-01-02 09:29:00,17.49,10,498,2019-01-01 09:57:00,"['Data', 'Education', 'Statistics', 'The Reality Project', 'Data Journalism']","The Disappearing Poor Exploring the incredible worldwide gains in prosperity: Reality Project Episode 1 Every day for 25 years, newspapers could have correctly run the headline “130,000 People Escape Extreme Poverty Yesterday” (source). Instead, as far as I can tell, this has appeared on the front page of a major newspaper precisely 0 times. Maybe this explains the incredibly terrible results recorded by people on this simple question (from Gapminder): When this question was asked in the United States, 5% of humans got the correct answer, C: almost halved. Animals, on the other hand, answer this question with 33% accuracy (because they randomly guess). How is it possible for people in a supposedly literate society like the US to be so wrong about reality? I don’t like to place blame, but in this case, it’s likely justified: the media has succeeded in a campaign to misinform us about the world. Instead of reporting facts, they have chosen to activate our natural inclination towards xenophobia (fear of outsiders) by convincing us there is a “developed world” with well-off people, and then there is everyone else, a mass of poor hordes that will never be able to rise from poverty. Fortunately, armed with the right data, we can correct our wrong beliefs about poverty. On doing so, we discoverable a remarkable fact: over the past few decades, people all over the world have undergone a remarkable increase in prosperity, resulting in measurably better living standards for billions of people. This is episode one of the Reality Project, a weekly series dedicated to becoming less wrong about the world with data. The Facts about Global Poverty and Income First, we need to look at the statistics. The single best resource on this topic is the Our World in Data “Global Extreme Poverty” page. You can view all the numbers there, but one chart suffices to show the drastic decline in poverty: We can see that the definition of poverty varies by institution, but no matter which definition we use, the rates have drastically increased. Sure, the rate is decreasing, but because there are more people, doesn’t that mean more people are still poor? Nice try nightly news, but the number of people living in poverty has also declined precipitously, by over 1.2 billion since 1990. Where do these people go as they rise from extreme poverty? Simple: into the global middle class. As of this writing, about 85% of people in the world (6 billion out of 7 billion) are at or above the middle part of the global income scale. Making a distinction between “developed” and “developing” is the wrong way of looking at the world: there are not two income levels, but rather a smooth scale which people everywhere are rapidly climbing. We can observe the drastic increase in worldwide wealth in the numbers. As global poverty declines, incomes increase as shown in GDP per capita over time (from the Our World in Data “Economic Growth” page): So, the facts are unequivocal that poverty is declining and incomes are increasing at an extremely rapid pace, but does this actually matter? It is possible that people are making more money but not having better lives? All of this may look nice, but you may be wondering if the rise in wealth actually means that people are better off. Well, based on the following chart showing life dissatisfaction vs GDP per capita, the answer is a resounding yes. As wealth increases, people become more satisfied with life. Money is not a magical cure for all ills, and the effects only persist up to a certain income level but wealth is positive because of the side effects it brings with it: lower infant mortality, more educational opportunities, and fewer working hours. It can be easy to lose sight of the humans in all the numbers so it’s critical to remember that behind every one of these data points is a person. Billions of people rising out of poverty means mothers who don’t have to bury their children because care improves with wealth, girls who are able to attend high school, and families that are living healthier, happier lives. I’m not very good at talking about individual humans, but Hans Rosling provides stories (in addition to facts) in his excellent book Factfulness. As for me, I’ll end this section with another chart from Our World in Data which demonstrates why less poverty is positive: The data could not be more clear: around the world, humans are rising out of poverty and into the middle class. As a consequence, they are living longer, going to school more, working less, and leading more enjoyable lives. Cause of the global decline in poverty There are likely many factors at play behind this decline (as with any human phenomenon) but I’ll focus on the most compelling argument: commerce. (For a full treatment of this subject, see Matt Ridley’s The Rational Optimist). For hundreds of thousands of years, humans engaged in zero-sum interactions: one person stole or killed another person, so someone’s gain was offset by someone else’s loss. In this system, the overall state of humanity could improve only glacially if at all (new tools that raised living standards spread very slowly when communication was limited to walking speed). However, beginning around 1000 AD in Europe, a new form of interaction emerged: economic exchanges in which both parties came out better off. This invention, known as commerce, completely changed the game: people were now able to enter into positive-sum exchanges where both parties gained. This is a fundamental building block of civilization: in commerce, both the person selling a good and the person buying a good come out ahead. For most of history, humanity had a constant amount of material wealth which could change hands but did not increase overall. With the invention of commerce, the size of the economic pie itself increases as people trade goods and specialize. Both within and between country exchanges lead to a division of labor wherein people or countries specialize in fulfilling one role, leading to massive increases in efficiency. It took humans a long time to figure out, but once we created exchanges and market economies, we started on the incredible upward journey in wealth that continues to this day. We can see the beneficial effects of trade in recent years on the following chart. As countries trade more, they grow wealthier: Don’t mistake me for a free-market evangelist. Before I began my efforts to get less wrong about the world through data, I was an avowed Socialist who upheld the Nordic countries as paragons to emulate. However, my exploration of the numbers has led to the conclusion that free markets and the exchange they enable are the drivers for escaping poverty. I also still believe in the role of government: tight government controls in areas like environmental protection and worker rights are critical for making sure that increased wealth does not come at the cost of environmental devastation. The remarkable ability of commerce to lift people out of poverty can be seen in China, which over the past 40 years has undergone the most incredible wealth increase in history. As described in How China Escaped the Poverty Trap, China’s extraordinary escape from poverty occurred as the communist leaders gradually opened the nation to commerce, crucially, trade with other nations. In 1978, China allowed foreign trade, and in the 40 years since, has once again become a world power. Along the way, the living standards of hundreds of millions of people has been raised. (Not only does commerce lead to rising income, but it also leads to decreasing rates of violence within and between nations. As documented by Steven Pinker in The Better Angels of Our Nature, when it is cheaper to buy something from someone than to steal it, economics wins out. Our neighbors become more valuable to us alive than dead which means less killing and more trading. This is known as the theory of gentle commerce.) Caveats While the long-term picture is overwhelmingly positive, it’s important to also focus on the realities that still need to be improved. At the moment, there are remain hundreds of millions of people in extreme poverty around the world and every one of these people deserves to rise into the middle class. The unfortunate reality is that the vast proportion of the world’s poor are in Africa where groups like the United Nations are working to end poverty through targeted spending programs and the institution of social safety nets. There are around 750 million people still in extreme poverty today. Nonetheless, considering 1.2 billion people moved out of extreme poverty from 1990–2015, there is every indication that extreme poverty can be ended. Furthermore, any time the issue of income is brought up, we have to mention income inequality. If countries are getting richer, but the wealth is going to fewer people, then is humanity really better off? Once again, it’s worth looking at the data, this time on inequality both within and between countries. We’ll turn to the Our World in Data articles “Income Inequality” and “Global Economic Inequality” for the facts. Fortunately, the numbers again provide an optimistic answer. While within-country inequality has increased in a few countries, overall, global inequality has decreased and is projected to continue to decrease. From the following plot, we can see that some countries have experienced an increase in inequality within their borders if we define inequality as the share of total income going to the top 1%. (This definition may have some issues). On the other hand, global economic inequality has declined: In summary, when looking at inequalities, the wealth distribution may be getting more skewed within some countries, but overall, because overall levels of wealth have increased most rapidly in the poorest nations, even those at the bottom of the income level are better off. I won’t go into the debate about whether inequality is even negative, but, taking a long-term worldwide view of the situation, it does not appear to be worse now. Conclusions The news is not only misinforming you about the world but, by promoting a false picture, it’s stealing something valuable: the joy that comes with seeing the world is getting better. While it’s important to be realistic about the challenges we face in the short term, it’s also critical to take a look at the big picture and realize that all of our collective efforts are making a difference. People around the world are moving out of poverty into the middle class, and, as a result, living longer, healthier, more enjoyable lives. I would like to end on one final chart which highlights the most important part of The Reality Project: when we realize that worldwide poverty is decreasing it should give us confidence that efforts to eliminate poverty are not foolish. Although the greatest contributor to declining poverty is (most likely) commerce, poverty reduction efforts such as those advocated by the United Nations are also vital. If you think poverty rates are increasing, then your only conclusion must be that these programs don’t work. If, on the other hand, you have dug into the data as we did and discovered the true state of the world, you realize these efforts have made a difference. The poverty reduction program delivered benefits to the recipients far exceeding costs (read the details in “A Multifaceted Program Causes Lasting Progress for the Very Poor: Evidence from Six Countries”). The next time someone claims there will always be poor people and there’s no use in helping them, gently correct them, not with opinions, but with data. Sources In addition to the sources listed throughout the article, here are some resources for your own research. I welcome any discussion. As always, I welcome feedback, conversation, and constructive dissent. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Our World in Data: Economic Growth Raw Data: Worldbank Factfulness: Ten Reasons We’re Wrong About the World — and Why Things Are Better Than You Think by Hans Rosling Our World in Data: Extreme Poverty",The Disappearing Poor,3,published,2847,2278,0.3608428446005268,0,1,0,0,0,0
10,474,141.77644618413194,65,https://towardsdatascience.com/has-global-violence-declined-a-look-at-the-data-5af708f47fba,4,Towards Data Science,2019-01-05 21:04:00,25.88,15,398,2019-01-03 21:36:00,"['Data', 'Education', 'Statistics', 'The Reality Project', 'Data Journalism']","Has Global Violence Declined? A Look at the Data Are we really living in the most peaceful time in human history? It all depends on how you measure: Reality Project Episode 2 In his sweeping 2011 work The Better Angels of Our Nature: Why Violence has Declined, Steven Pinker makes the audacious claim, “today we may be living in the most peaceable era in our species’ existence.” To someone who pays even a little attention to constant news reports of violence, both local and international, this statement sounds ludicrous. The grandeur of this assertion should immediately raise our suspicions; as the science popularizer Carl Sagan often said, “extraordinary claims require extraordinary evidence.” Does Pinker have that evidence and does he interpret it correctly? Over the course of 800 pages, Pinker makes a convincing argument with numbers, figures, and references to many books and articles. Nonetheless, it’s critical that we don’t rely on a single argument to form our worldview. We must remember that data is never objective, and therefore we need to look at the data itself as well as multiple expert interpretations. In this article, we’ll examine the global violent conflict data and different stances on the same numbers. (As a note, Pinker’s work deals with all forms of violence from the individual — homicides, abuse — to the worldwide, but here we’ll focus on international violent conflicts — war — and leave the rest for future articles). This is Episode 2 of the Reality Project, an endeavor with the aim of becoming less wrong about the world with data. You can find all the articles here. (I’d like to emphasize I’m not trying to minimize the atrocities of war by presenting only data. It can be easy to lose track that we are talking about people when looking at numbers and every loss of human life is a tragedy.) The Data Our primary sources of derived data on the more positive side will be The Better Angels of Our Nature by Steven Pinker and the “War and Peace” page on Our World in Data. For a contrasting viewpoint, we’ll use “Steven Pinker is Wrong” by John Gray and “The Decline of Conflict: What Do the Data Really Say?” by Pasquale Cirillo and Nassim Nicholas Taleb. You can find raw data for your own analysis at The Conflict Catalog or the World Bank. The first question we need to ask to assess violence today is how violent was the past. While extremely reliable data is difficult to come by, anthropologists use archaeological evidence to determine causes of death and therefore approximate the rates of violence in a society. These results are shown in the following graphs of percentages of deaths that come from violent conflict in prehistoric and non-state societies (from Our World in Data). The percentages of individuals killed by violence ranges from about 60% to less than 5% in both prehistoric and nonstate societies. Those numbers are almost meaningless until we put them in perspective with modern figures: The most relevant entries are the final two. The US and Europe from 1900–1960, even with two world wars, saw less than 1% of their population perish in armed conflicts. In 2007, just 0.04% of deaths in the world were from international violence. If this data is correct, the world in 2007 was at least an order of magnitude safer than most prehistoric societies. We can also look at the same data in a slightly different manner, as the violent deaths per 100,000 citizens per year. Here we again see the same pattern of lower violence in modern times. On the left are non-state societies and on the right are state societies. Focusing on the bottom 3 rows of the right-hand table, we can see the strikingly low rates of violent death among modern societies, even accounting for two world wars. Again, it’s worth stressing the data here is not complete, but what is available suggests the following hypothesis: prehistoric and nonstate societies experienced much higher rates of violence than modern state societies. As Pinker and others (notably Jared Diamond) have made clear, the “noble savage” idea is entirely false. People did not live peacefully with one another when they were organized in tribes and then become more violent as they were civilized, but the opposite: they were extremely violent in tribes and gradually became less so as larger civilizations were built and commerce started to connect the world (this highlights two of the drivers in the decline of global war — trade and powerful states — that we’ll look at shortly). Even accounting for the atrocities wrought by nation-states in the 20th century, rates of violent death appear to be lower now than at any previous point. It’s worth looking into the period of time for which we have the best data: the modern age. In particular, we’ll zoom in on the post-1945 era, a time that Pinker has classified as “The Long Peace”. From this vantage point, things look very good. As noted by Pinker, since 1953, there have been zero conflicts between major world powers (the exception since WW2 was the Korean War) and there have been zero internationally recognized states that have gone out of existence through conquest since 1945. When we look at the data on battle deaths per 100,000 people in state-based conflicts, we see a significant decline since World War Two. Since the end of the second world war, the rate of deaths from all conflicts has decreased, with the most noticeable decline in conflicts between states. Most of the remaining battle deaths occur in civil conflicts, such as the tragic civil war in Syria that has been ongoing since 2011. Moreover, many of these civil wars involve foreign states, and the argument could be made major states have stopped fighting each other directly, but engage through other conflicts. One discouraging bit of news in contrast to the above graph is the number of overall state conflicts since the second world war: While the number of battle-related deaths (in the rate of people killed / 100,000 / year) has decreased, the total number of conflict is rising. This tells us that, contrary to what we might think as weapon technology advances, violent conflicts are resulting in fewer battle deaths. While, there are more civil conflicts around the globe, these tend to be less catastrophic than wars between major powers. Fortunately, the wars between major states have been on the decline — even including the entire 20th century — and show no signs of reversal. We can see this in the final chart of section showing the percent of the time major powers have been at war over the past 500 years (grouped into 25-year periods). There has been a clear decline in the percentage of years in which the major powers fought with one another since at least 1600. At one point the rate reached 100% — indicating there was at least one conflict between world powers for the entire 25-year period. Contrast that with 2000, when there were 0 conflicts between world powers. The current period of 66 years without a conflict between major powers is the longest time since at least the Roman Empire. After an exploration of the data thus far, Pinker’s argument looks to be holding up: the rate of death in battle for modern state societies in war is much lower than prehistoric and non-state societies even accounting for the world wars. Furthermore, the major powers no longer fight one another and while the number of civil conflicts has increased, these result in fewer deaths than conflicts between major states. However, before we consider this case to be closed we need to talk about one fundamental issue. The Critical Question: Relative vs Absolute Consider the following two scenarios: Which is worse? The question depends on your observation point. On the ground, observing from the location of the conflict itself, the bar is much worse. An individual in the bar has a 1 in 2 chance of being killed. From an outsider’s vantage point the civil war is much worse with a total loss of life 10 times greater. This is one of the central arguments that Pinker’s critics make: measuring violence in rates ignores the absolute numbers which means it does not account for the actual amount of human suffering. One of Pinker’s harshest critics, Nassim Taleb, brings up the point of units in his response. He illustrates it with the following pair of plots: The graph on the left shows the raw number of casualties versus time for major conflicts. The graph on the right shows the rescaled version where the populations have been standardized across all times to have the same base population. As an example, if we have a population 100 society that suffers 2 deaths and a society with 1000 people that suffers 10, we would multiply 2 * 10 to get 20 deaths to account for the population difference. These figures encapsulate the rates/absolute numbers argument: the rates of death in violent conflicts have clearly been decreasing and are at a historical low, but the actual number of deaths has increased over time. Although, it’s worth pointing out the actual number has decreased since 1950. I don’t know if I have an answer for you on this problem of relative/absolute. As a utilitarian, I believe in bringing the greatest amount of total good to the greatest amount of people, and therefore, I see the total number of humans dead as the mark of how our civilization has failed. On the other side of the argument, a human alive today has less probability of being killed in a violent conflict than at any point in the past which is no doubt progress. I’ll let the facts stand and allow you to decide for yourself. Even if we don’t want to make a judgment on the overall outcome, it’s worth taking a look at the potential causes for the reduction in rates of violence. In particular, we might want to think about why the total number of deaths (in addition to the relative measures) have fallen since 1945. The Drivers of the Decline in Violence For this section, we will look at Steven Pinker’s ideas as outlined in The Better Angels of Our Nature. (This work is the best look at potential reasons why violence has declined — partly because others refuse to concede this point). Pinker explains five forces, intended to account for the decline of violence on all scales, not just state-based conflicts, but we’ll focus on the first two which are most relevant to international conflict. The five historical forces behind the reduction in violence are: A powerful central government that can make and enforce laws using violence means that citizens are less likely to take punishment into their own hands, a concept known as the Leviathon theory. For all its romantic appeal, vigilante justice only leads to unending cycles of revenge violence. When citizens can count on a government to deliver fair punishments, they let the judicial system enforce the rules. Also, a state can prevent crime by making the penalty far greater than any potential payoff. Humans have the potential for both evil and good, and a strong central government with a fair criminal justice system with disincentives can steer them on the right path. A strong government alone is not enough for keeping the peace between nations though. On an international scale, the evidence suggests that democracies don’t fight each other possibly because of the ideals they share in common. Looking at some of the data, it’s clear that democracies just don’t fight one another directly anymore (although they may engage in proxy war). (For more on this concept see “Democratic Peace Theory.”) The idea that one democratic nation will not fight another is good news as democracies on the rise in the long run worldwide as shown by Pinker: (One counterpoint is the recent decline in Democratic Index which measures not just the government but also freedom of the press and voting rights. Although the long-term trends are positive, the recent declines are worrying.) The theory of gentle commerce can be summed up in two statements: Why don’t you steal a loaf of bread from the grocery store? Although most people believe they don’t steal because it’s the wrong thing to do, the real reason is more mundane: in our society, the potential cost of getting caught stealing — jail time and social ostracization — are higher than the cost to purchase items. When people are provided a legal means to obtain their goods that is cheaper than violence, they will take the legal option. We explored the second point in “The Disappearing Poor”, but it’s worth reiterating. Without exchanges in a market economy, all interactions between humans are zero-sum: you can steal something from me, but your gain is offset by my loss so humanity is no better off; the size of the economy stays the same. However, in an exchange of goods, both parties come out better ahead. Exchange also allows for specialization so different individuals/countries can make the goods for which they are best suited. Moreover, as trade continues, countries become dependent on one another because they no longer make all the goods they need. The end result of exchange is more plentiful goods at cheaper prices for all parties and improved international relations. Over the past several hundred years, we have slowly built up an international marketplace where all players are heavily dependent on one another. The US will not fight with another major power, not because she couldn’t defeat them, but because the economic losses would far outweigh the gains. As discussed by Pinker, countries that depend more on each other for trade are less likely to have a violent conflict controlling for other factors. In summary, the financial incentives have switched from war to trade for most nations. The Leviathon and gentle commerce actually work together — a strong government is able to create markets that allow for safe exchanges and enforce laws regulating trade. In other words, once you have a strong state, trading is easier, and more trading between nations means they are less likely to attack one another. As trade between world powers has increased, conflicts between major powers has also decreased to 0 since 1953. There are doubtless other factors that have played a role in the decline of war between nations. We discussed external historical factors behind the decrease, but Pinker and others (John Horgan in The End of War) have also outlined internal factors, like The Better Angels of Our Nature : empathy, self-control, morals, reason. By examining the data and the reasons, it’s clear that the major powers no longer see war as a feasible choice. This is not necessarily because they are ethical, but because the economics of war no longer makes sense. At the end of the day, humans — and nations — are driven by incentives, and through international trade, we have built a world where the incentives favor peace. Caveats and Conclusions In a narrow point of view, Pinker is correct: rates of violent conflict have substantially decreased and there is reason to believe they are at the lowest point in human history (if only because data is limited). Also, at least among modern democracies, violence is no longer seen as the default option for solving problems as it has been for almost all of human history. That being said, it’s worth thinking about how Pinker’s argument made be misleading. The first point is that by focusing on rates rather than numbers, we are neglecting the actual human suffering. More people dying from war is more people dying from war even if the percentage is decreasing. The numbers can be used to support multiple conclusions depending on one’s argument. A second central issue is, especially for pre-historic societies, reliable data is hard to get. Most of the numbers for conflict rates and deaths in prehistoric societies come by examining archaeological sites and looking at artifacts for evidence of violent death. However, it could be possible these marks have been misinterpreted and anthropologists could not possibly have studied all prehistoric societies. Simple extrapolation what has been found is the only option, but it’s also important to not draw immature conclusions. The last major issue we’ll cover here (for more see Taleb’s article) is that we could be living in an anomalous age. Pinker calls the most recent 70 years of peace (no wars between modern nations), “The Long Peace”, but it could be really only the temporary peace. Furthermore, as pointed out by Taleb, wars tend to be power law distributed, that is, they have long tails skewed to the right. The vast majority of deaths in armed conflicts are caused by a small number of the conflicts as can be seen below: One single conflict can completely outweigh all others in terms of deaths. Most of the armed conflicts since 1945 have fallen on the low end of the scale, but it only takes a single major battle to overshadow the peace since 1945. It’s possible a new conflict, one caused by global warming, resource shortages, or territorial disputes, could entirely undo the millennia of progress humans have made on violence reduction efforts. Initially, after reading Pinker’s book, I was convinced by his thesis. However, after taking the time to dig through the facts myself, a more nuanced picture emerged, one where sensationalist conflicts on both sides are not warranted. I see reasons for optimism: decreasing rates of deaths in armed conflicts mean a human alive today is less likely to die in battle than in recorded history, no battles between major powers since 1953, and a reduction in overall battle deaths since the 1950s, as well as reasons for pessimism: democracy and international trade, so important for reducing major conflicts, appear to be on a decline in recent years. Moreover, while major powers no longer fight directly with each other, they are involved through proxy wars. The exploration of violent conflict highlights the objective of the Reality Project. It’s not designed to be a feel-good effort but a fact-based endeavor, and, thus, any time I encounter an argument that is appealing to me this should only tell me to be more skeptical. The Reality Project has allegiances to no single interest, except getting at the actual statistics behind our world. It’s nice when the facts make us optimistic, but even when they don’t, it’s critical to understand the data so we can work to improve things. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. A minor civil war rages in a country of 1 million for several months. The total death count is 10,000 for a rate of 1%. Commerce: trade between nations allows us to engage in positive-sum interactions meaning our neighbors are worth more to us alive than dead Feminization: increased role of women in society and positions of power Rationality: Increased reliance on logical thinking instead of tradition for making policies and interacting with other humans Mass media and communication: made it possible for people to see outsiders not as dangerous, but as humans with a common humanity Our neighbors — both within and outside our country — are worth more to us alive than dead because exchanges are positive-sum Two men are alone in a bar. After a brief disagreement, one pulls out a pistol and murders the other. The total is 1 dead for a rate of 50%. Nation-states: the rise of societies ruled by a central government, in particular democracies, with a “monopoly on the legitimate use of force” When it’s cheaper to buy something than to steal something, the economics wins",Has Global Violence Declined? A Look at the Data,10,published,1538,3678,0.12887438825448613,1,1,0,0,0,0
18,10200,138.73140784412038,1755,https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e,44,Towards Data Science,2019-01-08 22:09:00,33.46,8,25317,2019-01-06 07:58:00,"['Data Science', 'Data Visualization', 'Python', 'Education', 'Towards Data Science']","The Next Level of Data Visualization in Python How to make great-looking, fully-interactive plots with a single line of Python The sunk-cost fallacy is one of many harmful cognitive biases to which humans fall prey. It refers to our tendency to continue to devote time and resources to a lost cause because we have already spent — sunk — so much time in the pursuit. The sunk-cost fallacy applies to staying in bad jobs longer than we should, slaving away at a project even when it’s clear it won’t work, and yes, continuing to use a tedious, outdated plotting library — matplotlib — when more efficient, interactive, and better-looking alternatives exist. Over the past few months, I’ve realized the only reason I use matplotlib is the hundreds of hours I’ve sunk into learning the convoluted syntax. This complication leads to hours of frustration on StackOverflow figuring out how to format dates or add a second y-axis. Fortunately, this is a great time for Python plotting, and after exploring the options, a clear winner — in terms of ease-of-use, documentation, and functionality — is the plotly Python library. In this article, we’ll dive right into plotly, learning how to make better plots in less time — often with one line of code. All of the code for this article is available on GitHub. The charts are all interactive and can be viewed on NBViewer here. The plotly Python package is an open-source library built on plotly.js which in turn is built on d3.js. We’ll be using a wrapper on plotly called cufflinks designed to work with Pandas dataframes. So, our entire stack is cufflinks > plotly > plotly.js > d3.js which means we get the efficiency of coding in Python with the incredible interactive graphics capabilities of d3. (Plotly itself is a graphics company with several products and open-source tools. The Python library is free to use, and we can make unlimited charts in offline mode plus up to 25 charts in online mode to share with the world.) All the work in this article was done in a Jupyter Notebook with plotly + cufflinks running in offline mode. After installing plotly and cufflinks with pip install cufflinks plotly import the following to run in Jupyter: Single Variable Distributions: Histograms and Boxplots Single variable — univariate — plots are a standard way to start an analysis and the histogram is a go-to plot (although it has some issues) for graphing a distribution. Here, using my Medium article statistics (you can see how to get your own stats here or use mine here) let’s make an interactive histogram of the number of claps for articles ( df is a standard Pandas dataframe): For those used to matplotlib, all we have to do is add one more letter ( iplot instead of plot) and we get a much better-looking and interactive chart! We can click on the data to get more details, zoom into sections of the plot, and as we’ll see later, select different categories to highlight. If we want to plot overlaid histograms, that’s just as simple: With a little bit of pandas manipulation, we can do a barplot: s we saw, we can combine the power of pandas with plotly + cufflinks. For a boxplot of the fans per story by publication, we use a pivot and then plot: The benefits of interactivity are that we can explore and subset the data as we like. There’s a lot of information in a boxplot, and without the ability to see the numbers, we’ll miss most of it! Scatterplots The scatterplot is the heart of most analyses. It allows us to see the evolution of a variable over time or the relationship between two (or more) variables. A considerable portion of real-world data has a time element. Luckily, plotly + cufflinks was designed with time-series visualizations in mind. Let’s make a dataframe of my TDS articles and look at how the trends have changed. Here we are doing quite a few different things all in one line: For more information, we can also add in text annotations quite easily: For a two-variable scatter plot colored by a third categorical variable we use: Let’s get a little more sophisticated by using a log axis — specified as a plotly layout — (see the Plotly documentation for the layout specifics) and sizing the bubbles by a numeric variable: With a little more work (see notebook for details), we can even put four variables (this is not advised) on one graph! As before, we can combine pandas with plotly+cufflinks for useful plots See the notebook or the documentation for more examples of added functionality. We can add in text annotations, reference lines, and best-fit lines to our plots with a single line of code, and still with all the interaction. Advanced Plots Now we’ll get into a few plots that you probably won’t use all that often, but which can be quite impressive. We’ll use the plotly figure_factory, to keep even these incredible plots to one line. When we want to explore relationships among many variables, a scattermatrix (also called a splom) is a great option: Even this plot is completely interactive allowing us to explore the data. To visualize the correlations between numeric variables, we calculate the correlations and then make an annotated heatmap: The list of plots goes on and on. Cufflinks also has several themes we can use to get completely different styling with no effort. For example, below we have a ratio plot in the “space” theme and a spread plot in “ggplot”: We also get 3D plots (surface and bubble): For those who are so inclined, you can even make a pie chart: When you make these plots in the notebook, you’ll notice a small link on the lower right-hand side on the graph that says “Export to plot.ly”. If you click that link, you are then taken to the chart studio where you can touch up your plot for a final presentation. You can add annotations, specify the colors, and generally clean everything up for a great figure. Then, you can publish your figure online so anyone can find it with the link. Below are two charts I touched up in Chart Studio: With everything mentioned here, we are still not exploring the full capabilities of the library! I’d encourage you to check out both the plotly and the cufflinks documentation for more incredible graphics. Conclusions The worst part about the sunk cost fallacy is you only realize how much time you’ve wasted after you’ve quit the endeavor. Fortunately, now that I’ve made the mistake of sticking with matploblib for too long, you don’t have to! When thinking about plotting libraries, there are a few things we want: As of right now, the best option for doing all of these in Python is plotly. Plotly allows us to make visualizations quickly and helps us get better insight into our data through interactivity. Also, let’s admit it, plotting should be one of the most enjoyable parts of data science! With other libraries, plotting turned into a tedious task, but with plotly, there is again joy in making a great figure! Now that it’s 2019, it is time to upgrade your Python plotting library for better efficiency, functionality, and aesthetics in your data science visualizations. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Adding a secondary y-axis because our variables have different ranges Adding in the title of the articles as hover information Interactive elements for subsetting/investigating data Option to dig into details as needed Easy customization for final presentation Getting a nicely formatted time-series x-axis automatically One-line charts for rapid exploration",The Next Level of Data Visualization in Python,8,published,75673,1457,7.0006863417982155,2,1,1,1,0,1
14,2400,137.01930229927083,387,https://towardsdatascience.com/a-non-technical-reading-list-for-data-science-d72451429a70,13,Towards Data Science,2019-01-10 15:14:00,18.44,15,2940,2019-01-10 09:42:00,"['Data Science', 'Reading', 'Education', 'Towards Data Science', 'Psychology']","A Non-Technical Reading List for Data Science Books that will make you a better data scientist without delving into the technical details Contrary to what some of data scientists may like to believe, we can never reduce the world to mere numbers and algorithms. When it comes down to it, decisions are made by humans, and being an effective data scientist means understanding both people and data. Consider the following real-life example: When OPower, a software company, wanted to get people to use less energy, they provided customers with plenty of stats about their electricity usage and cost. However, the data alone were not enough to get people to change. In addition, OPower needed to take advantage of behavioral science, namely, studies showing people were driven to reduce energy when they received smiley emoticons on their bills showing how they compare to their neighbors! The simple intervention of putting a 😃 on people’s electricity bills when they used less than their neighbors, and a 😦 face when they could do better ended up reducing electricity consumption 2–3%, in the process saving millions of dollars and preventing emissions of millions of pounds of CO2 🏆! To a data scientist, this may be a shock — you mean people don’t respond to pure data !— but this was no surprise to the chief science officer of OPower, Robert Cialdini, a former psychology professor who wrote a book about the human behavior. The takeaway is you can have any data you want but you still need an understanding of how humans work to effect real change. In our daily work and formal education as data scientists, it’s difficult to get a glimpse into the workings of humans or to take a step back and think about the social implications of our work. Therefore, it’s critical to read not only technical articles and textbooks but also to branch out into works that look at how people make choices and how data can be used to improve these choices. In this article, I’ll highlight 6 books that are non-technical — in the sense that they don’t delve into the math and algorithms — but critical reads for data scientists. These books are necessary for anyone who wants to accomplish the objective of data science: enable better real-world decisions through data. The 6 books are listed here with brief reviews and takeaways following: Predictions about the future — in areas like elections, economics, national politics, and tech advances — are often hilariously wrong. These forecasts are less humorful when they have actual real-world consequences, and in this work, Silver explains why people tend to be terrible at making predictions and examines the few who have managed to break the trend in a number of different fields. It turns out there is no one magic rule for being right about the future, just a handle of basic rules practiced by great predictors. Anyone can benefit from the simple advice offered throughout the book: We can never be completely right in our predictions about the world, but that should not stop us trying to be less wrong by relying on well-proved principles for making superior forecasts. Two great additional books in this same category are Superforecasting and Expert Political Judgement both by Philip Tetlock. 2. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy by Cathy O’Neill Weapons of Math Destruction should be mandatory reading for students pursuing a degree in stats, machine learning, or data science, and for anyone who will have to make the decision to deploy a model. A “weapon of math destruction” is any algorithm that is opaque — can’t be easily explained; works on a mass scale — affects millions or even billions of people, and has the potential to cause serious damage — such as undermining democratic elections or keeping whole swathes of our population imprisoned. The central premise is these weapons of math destruction have the ability to create feedback loops that spread inequality and because we can’t see inside these algorithms, we’ll have no idea how to correct them. It’s only after we look back and observe the immense damage — such as in the 2016 election or 2008 financial crises (both caused by algorithms harnessed for negative ends) — the harm our blind trust in these models can cause. Furthermore, our models are only as good as the data that are put in, and when that data is biased, then the predictions from the model will be as well. Consider a model for sentencing offenders that takes into account a person’s time of first encounter with law enforcement. Due to unjust policing strategies such as stop and frisk, a black man is likely to have a run-in with the police at a much younger age than a white man even accounting for other factors. This will result in the model recommending a longer prison sentence for blacks, during which these individuals will be losing out on economic opportunities and becoming ostracized from society. Those individuals are then more likely to re-offend, leading to a vicious cycle of imprisonment all started because of data generated by an unjust policy and then fed into a black box. O’Neil’s book, released in early 2016, is needed now more than ever before. The end of 2016 saw the devastation wreaked on the American democratic process by Russian actors who took advantage of Facebook’s algorithms to spread propaganda. Far from being an academic exercise, these actions had real-world consequences, raising into question the legitimacy of elections in the United States. Far-right advertisements continue to plague Facebook, driven by an algorithm that (most likely, we don’t really know) considers engagement as the top priority. Algorithms are only going to play a larger role in our daily lives moving forward. Already, where we go to school, what we read, whether we are approved for a loan, if we get a job, and what we buy are all decided to a significant extent by algorithms we have no control over and cannot query for an explanation. O’Neil’s book may seem pessimistic about machine learning models, but I like to think of it more as a necessary criticism: with so much unbridled enthusiasm surrounding machine learning, we need people who are willing to take a step back and ask: are these tools really improving peoples’ lives and how should we as a society adopt them? Machine learning algorithms are just tools, and as with any tools, they can be used for good and bad. Fortunately, we are still at an early stage which means we can shape the use of models to ensure they work towards making objective decisions and creating the best outcomes for the greatest number of people. The choices we make now in this regard will shape the future of data science in the decades to come, and it’s best to go into these debates well-informed. Data science may be a young field but is already having an immense impact for both good and bad in millions of individual’s lives. As pioneers on this new frontier, those of us working now have an obligation to ensure our algorithms don’t turn into weapons of math destruction. 3. (Tie) Algorithms to Live By: The Computer Science of Human Decisions by Brian Christian and Tom Griffiths and How Not to be Wrong: The Power of Mathematical Thinking by Jordan Ellenberg Computer science and statistics (and every other field of study) suffer from one problem when they are taught in school: they are boring in the abstract. It’s only when they are applied to real-world problems that they become interesting enough to make us want to understand. Both of these books do an incredible job of transforming dry subjects into entertaining and informative narratives about how to use algorithms, stats, and maths in our daily lives. For example, in Algorithms to Live By, the authors show how we can use the idea of the explore vs exploit tradeoff and optimal stopping to find out how long we should spend searching for a spouse (or new employee, restaurant for dinner, etc.). Likewise, we can use sorting algorithms to organize our belongings most efficiently for retrieving what you need quickly. You thought you knew these ideas, and you may even be able to write them in code, but you’ve probably never applied them to optimize your life. The main idea of How Not to be Wrong is similar as Ellenburg takes us through stories showing both the use and misuse of statistical concepts like linear regression, inference, Bayesian inference, and probability. Applying the laws of probability show us that playing the lottery is always a losing proposition, — except in the rare cases where the payoff is actually positive (as was discovered by a group of MIT students). Ellenburg does not shy away from showing us equations, but he applies them to real-world situations. The central quote of Ellenburg’s book is mathematical thinking is “the extension of common sense by other means.” In many situations, primarily in the distant past, our intuitions serve us well, but, now in the modern world, there are many cases where our initial response is completely false (see the next book). In these circumstances, we need to rely not on instincts but can instead use probability and statistics in order to arrive at the best decision. Both books are at the exact right level of rigor — mixing in a few equations with plenty of stories — and they are both enjoyable to read. Throughout these books, I found plenty of data science concepts I had never quite grasped in a classroom finally clicking and over and over again I experienced the joy of an “aha” moment. Math, statistics, and computer science are only useful to the extent they can affect your life for the better, and both of these books demonstrate all the uses of these subjects you’ve never stopped to consider. 4. Thinking, Fast and Slow by Daniel Kahneman If you haven’t realized it yet, then here’s a useful lesson: humans are irrational and we routinely make terrible decisions in all aspects of life. However, there is reason for hope: once we understand why we don’t act optimally, we can start to alter our behavior for better outcomes. This is the central premise of Kahneman’s masterwork documenting decades of experimental findings. Kahneman (the 2002 Nobel Prize winner in Economics) along with his research partner Amos Tversky (in addition to others such as Richard Thaler), created the highly influential field of behaviorial economics which treats people not as rational utility maximizers, but as the irrational decision-makers we actually are. This has created a real shift in thinking and design choices not only in economics, but in areas of life as diverse as medicine, sports, business practices, energy savings, and retirement funding. We can also apply many of the findings to data science, such as how to present study results. In this rigorous yet highly enjoyable work, Kahneman outlines all the ways in which we don’t act logically including the anchoring, availability, and substitution heuristics (rules of thumb) or our tendency towards loss aversion and falling prey to the sunk cost fallacy. He also outlines what is perhaps the most important takeaway: that we have two different systems of thought. Using System 1 is natural, and we have to overcome millions of years of evolution to employ System 2. Even though it’s difficult, in our data-rich world, we need to spend time honing our System 2 thinking. Sure, we may sometimes run into problems with overthinking, but underthinking — using System 1 instead of System 2 — is a far more serious problem. This book is crucial for understanding how people make decisions and what we as data scientists can do to help people make better choices. This book also has conclusions that apply outside of data science like the idea of two selves: experiencing and remembering. The experiencing self is the moment-to-moment feelings we have during an event but is much less important than the remembering self which is our perception of the event afterwards. The remembering self rates an experience according to the Peak-End rule which has profound implications for medicine, life satisfaction, and forcing ourselves to do unpleasant tasks. We will remember events for far longer than we experience them, so it’s crucial that during an experience, we try to maximize the future satisfaction of our remembering self. If you want to understand actual human psychology, not the idealized version presented in traditional classrooms, then this book is the best place to start. 5. (Dark horse): The Black Swan: The Impact of the Highly Improbable by Nassim Nicholas Taleb There is only one place for Taleb to occupy on a list, and it’s that of an outsider. Taleb, a former quantitative trader who made substantial sums during market downturns in 2000 and 2007, has made himself into a vocal scholar-researcher achieving worldwide acclaim and criticism for his works. Primarily, Taleb is occupied with one idea: the failure of contemporary ways of thinking especially in times of great uncertainty. In The Black Swan, Taleb puts forth the concept that we are blind to the randomness that rules human activities, and, as a result, are devastated when things do not turn out as expected. Originally published in 2007, The Black Swan has become more relevant since the unexpected events of 2008 and 2016 which completely upended traditional models. Of course, the question that immediately comes up based on the central premise, is: well don’t improbable events by definition not happen very often and so we shouldn’t be worried about them? The critical point is that while each improbable event by itself is unlikely to happen, taken together, there is a near certainty that many unexpected events will occur in your lifetime, or even in a single year. The chance of an economic crash occurring in any one year is miniscule, but the probabilities add up until an economic downturn every decade somewhere in the world is a near possibility. We should not only expect world-changing events to happen with high frequency, but we should not listen to experts who are constrained by what has occurred in the past. As anyone who invests in the stock market should know, past performance is no predictor of future performance, a lesson we’d be wise to consider in our data science models (which use past data). Also, our world is not normally distributed, but instead fat-tailed, with a few extreme events — the Great Recession — or a few wealthy individuals — Bill Gates — overshadowing all the others. When extreme events occur, no one is prepared because they far exceed the magnitude of any previous ones. The Black Swan is important for data scientists because it shows that any models based only on past performance will often be wrong with disastrous consequences. All machine learning models are built only with past data which means we don’t want to trust them too much. Models (Taleb’s included) are flawed approximations of reality, and we should make sure we have systems in place to deal with their inevitable failures. As a note, Taleb has gained a reputation not only for his novel ideas, but also for being extremely combative. He is willing to take on all comers and regularly criticizes scholars like Steven Pinker, or public figures like Nate Silver. His ideas are helpful to understand in our heavily skewed times, but his attitude can be slightly off-putting. Nonetheless, I think this book is a worthwhile read because it offers a non-mainstream thought system. (This book is the second in Taleb’s five-part Incerto laying out his complete philosophy. The Black Swan discusses the concept of highly improbable events, while the fourth book in the Incerto, Antifragile: Things That Gain from Disorder discusses ways in which you can make yourself not just robust to disruption, but set yourself up to be better off because of it. I think The Black Swan is the most relevant of the bunch for data science). Conclusions After a long day of staring at a computer screen, I can think of no better way to end the day than with a book (print, ebook, or audiobook it doesn’t matter). Data science requires constantly expanding the tools in your toolkit, and, even when we want something to relax and take our minds off work, that doesn’t mean we can’t be learning. These books are all engrossing reads that also teach us lessons about data science and life. The 6 works described here will provide a helpful supplement to more technical works by demonstrating what actually drives humans. Understanding how people think in reality — as opposed to idealized models — is just as critical as statistics for enabling better data-driven decisions. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy by Cathy O’Neill (Tie) Algorithms to Live By: The Computer Science of Human Decisions by Brian Christian and Tom Griffiths and How Not to be Wrong: The Power of Mathematical Thinking by Jordan Ellenberg Thinking, Fast and Slow by Daniel Kahneman Make lots of predictions and get rapid feedback: we are much better at making estimates for frequently occurring events, primarily because of the feedback and improvement cycle. Every day a weather forecast is wrong, that information goes into the model and makes the forecast for tomorrow better (one reason why weather forecasts have improved substantially over the decades). We are worst at making choices in situations we experience rarely, and in these cases, using as much data as possible is key Draw on as many diverse sources as possible: each provider of data has their own biases but by aggregating different estimates you can average out the mistakes — a point well made by Silver’s FiveThirtyEight website. This wisdom of the crowd approach means using sources that disagree with your views and not relying solely on “experts” in a field. Always include uncertainty intervals and don’t be afraid to update your views when the evidence changes: the single biggest mistake people make when forecasting is providing one number. Although one answer may be what the public wants to hear, the world is never strictly black or white but instead exists in shades of gray which we have a responsibility to represent in our predictions. Expressing uncertainty may seem cowardly — saying Hilary has a 70% chance of winning means no matter the outcome, you will be right — but it’s more realistic than a single yes/no. Moreover, people assume it’s a weakness to change one’s opinion, but updating your beliefs when the facts on the ground change is actually an important strength, in data science and forming a worldview. System 2 Slow and Rational: We need to use this mode in situations with many options and different sources of evidence to consider. It takes effort to employ System 2, but that effort is well-rewarded in the form of better decisions and consequently outcomes. The Signal and the Noise: Why So Many Predictions Fail — but Some Don’t by Nate Silver Think like a fox (not a hedgehog): have lots of small ideas (the fox) instead of one big idea (the hedgehog). If you have only a single idea, you will tend to seek out confirming evidence and ignore anything that contradicts your views (the confirmation bias). If you have lots of little ideas, you will be more concerned with what’s right rather than what supports your current beliefs and you can abandon any of the ideas when the evidence no longer supports them. These two different ways of thinking also explain why people who are more confident in their predictions (like TV pundits) tend to be wrong more of the time. System 1 Fast and Intuitive: This mode was designed by evolution to make rapid decisions without considering the evidence. While this served us well in our hunter-gatherer past, it often gets us into trouble in our information-rich world when we don’t take time to look at the data.",A Non-Technical Reading List for Data Science,8,published,15945,3715,0.6460296096904441,0,1,1,1,0,0
19,391,135.08982852275463,50,https://towardsdatascience.com/the-reality-of-global-nuclear-weapons-and-how-russian-nukes-turned-on-your-lights-6d55e056b516,1,Towards Data Science,2019-01-12 13:33:00,24.96,11,350,2019-01-03 21:39:00,"['The Reality Project', 'Data Journalism', 'Data', 'Education', 'Science']","The Reality of Global Nuclear Weapons and How Russian Nukes Turned On Your Lights Exploring the data on the decline in worldwide nuclear stockpiles and the most intriguing government program you’ve never heard of: Reality Project Episode 3 On a warm Boston summer in 2018, I was just settling onto the lawn at the Hatch Memorial Shell for a performance of my favorite symphony — Holst’s The Planets — when I saw a tent set up by the Union of Concerned Scientists. As someone generally concerned about the state of science, I wandered over and as I got closer, was drawn to a crowd gathered around a man discussing the threat of nuclear weapons. Having just finished Enlightenment Now by Steven Pinker, I was hoping to hear more positive news about the massive reductions in nuclear weapon stockpiles that occurred over the past 30 years. Instead, the man’s speech — as Pinker tells us to expect from academics — was entirely negative. The gist was that human folly led us to create weapons which could wipe our species off the face of the Earth and we were in grave danger. Perplexed, when the man paused to take a breathe, I raised my hand and asked if he knew how many nukes there were worldwide and how this compared to numbers in the past. Confidently he replied: “There’s more now than ever before although I don’t know the exact number.” At this point, buoyed by the confidence (and arrogance) that comes with possessing data someone else doesn’t, I pressed my factual advantage for all its worth stating: “In 1985, there were approximately 70,000 nuclear weapons in the world, and today, in 2018, there are less than 15,000. That represents a reduction of nearly 80%, and what’s more, there are 4 fewer countries with nuclear weapons today.” Surprised, the man asked for a fact check, and after an acceptable source was consulted, he acknowledged the optimistic numbers were correct. While my intention was not to defang the man, this was the unintended effect, and the crowd slowly began to disperse, the gusto gone from the man’s proclamations. Although I had inadvertently cost the man his entire audience, he agreed to have a discussion with me (after I apologized for the intellectual ambush) and we had a fruitful debate with both of us making concessions: I agreed that 0 nuclear weapons was optimal (although not realistic at the moment) and he said he would reframe his message to emphasize the progress we’ve made in nuclear weapons reduction. Later, as I sat listening to the sounds of Holst’s magnificent work, I thought about what this experience had taught me: In this article, we are going to examine the data concerning nuclear weapons around the world. The basic stats above are correct — the number of nuclear weapons and the number of countries holding them have both declined drastically in the past 30 years. However, there are also points about which we should rightly worry, including tension between the US and Russia which threatens arms control treaties and the possession of nuclear weapons by unstable states. In addition to the numbers, we’ll also learn about Megatons to Megawatts, the most intriguing government program you’ve probably never heard of, in which Russian bombs literally turned on your lights. This article is the third episode of The Reality Project, an effort aimed at becoming less wrong about the world with data. You can find the previous article here and all of the articles by searching for the Reality Project tag. The Facts Our data in this article will come from Our World in Data: Nuclear Weapons, Federation of American Scientists: Status of World Nuclear Forces, and The Better Angels of Our Nature by Steven Pinker (who cites Scott Sagan). We will also talk about nuclear arms treaties with information from the Arms Control Association and the State Department . Information on the Megatons to Megawatts program is from the Arms Control Association. Exact data on the size of nuclear weapon arsenals is a closely held secret, nonetheless, we can get fairly reliable estimates due to international agreements and monitoring programs. From the objective data, we clearly see a decreasing trend in the number of nuclear weapons worldwide. Here is the global total of nuclear weapons over time (from Our World in Data): The global nuclear stockpile peaked in 1985 and has been on a rapid decline ever since. Again, exact estimates differ, but the Federation of American Scientists states numbers went from 70,300 to 14,485 as of 2018. This respresents nearly an 80% decline in total nuclear weapons! Today, there are 9 countries with confirmed nuclear weapons (it is unlikely there are additional countries with weapons due to the technology/material needs and international monitoring): Russia currently has more overall nuclear weapons, while the United States has more deployed weapons. The state and type of nuclear weapons are just as important as the sheer numbers: The total in military use (deployed + stockpiled) is just under 10,000 with about 1,800 weapons on high alert. For all types of nuclear weapons, the worldwide total has declined substantially. For example, in 1986, Russia along had 40,000 nukes and now has under 7,000. The United States went from 31,000 in 1966 to under 7,000 in 2018. Another surprising piece of news is there are now fewer countries with nuclear weapons. 4 countries once had nuclear weapons but no longer do: South Africa, Belarus, Kazakhstan (which had 1,400), and Ukraine, which willingly gave up 5,000 nuclear weapons, the third-largest arsenal in the world. Moreover, many countries that were pursuing nuclear weapons at one point have since given up their nuclear weapon development. In the below chart, we see when nations started and gave up trying to develop nuclear weapons (preceded with a — ). Not only are countries with existing nuclear weapons working to reduce the number they have, but numerous other countries have decided that pursuing nuclear weapons is not something they want to do. In 1990, there were 13 countries in the world with nuclear weapons, a number that has dropped to 9 countries today. This is certainly positive, but three of the countries with nuclear weapons are concerning. India and Pakistan have fought 3 wars against one another, and North Korea is a complete unknown. The decline in nuclear tests has been even more impressive than that of weapons. Since the start of the Comprehensive Nuclear-Test-Ban-Treaty (which has not entered into force but is followed by the major world powers) in 1996, the US and Russian have not conducted a single test. Moreover, since 1999, only one country, North Korea, has done any nuclear testing at all. The cumulative world total of nuclear tests is over 2000, but only 6 (0.3%) occurred after 2000. We can see where these tests happened in this map: In summary, today there are around 15,000 nuclear weapons in the world (9,300 of which are in military use) held by 9 nations. 4 nations have given up their weapons while at least a dozen others have ceased development. The number of nuclear tests has declined to nearly 0 with only North Korea conducting any since 1999. Finally, projections for 2020 suggest there will be around 8,000 total nuclear weapons in the world. How Did We Get Here? Arms Reduction Treaties The decline in the size of global nuclear arsenals is undoubtedly a positive and as with any statistic, it’s helpful to understand why this happened so we can figure out if it will hold and what we can do to keep the trend on track. In short, the reduction in nuclear weapons has come about because of international treaties designed to reduce arsenals. We don’t have the space to address all of them (see here for a comprehensive list) but we’ll talk about several between the US and Russia. Together, these two countries control over 90% of nuclear weapons, so these agreements have a significant influence. An overview of nuclear reduction treaties between the US and Russia is here: From the table, we can see that not every negotiation was successful. Nonetheless, the treaties that have been agreed to have been effective. For instance, START I (Strategic Arms Reduction Treaty), proposed by Ronald Reagan in the 1980s, (it was delayed by the collapse of the USSR) achieved all objectives, with Russia and the US reducing weapons to 6,000 each by 2001. START I expired in 2009, but by that point, the SORT (Strategic Offensive Reductions Treaty) agreement was in place which further limited both US and Russian stockpiles. As a note of optimism, this treaty was passed in the Senate by a vote of 95–0 indicating that both parties can at least agree we don’t need the threat of annihilation hanging over our heads like a Sword of Damacles. The current treaty limiting nuclear weapons, New START, was ratified in 2010 with reductions completed by 2018. We can examine effect of this treaty by comparing the number of warheads before (left) and after (right) completion: Based on the final numbers (far right column), but the US and Russia have fewer deployed warheads than required by the Treaty (1,550)! The New Start Treaty officially expires in 2021, with options to extend it for 5 more years. The current President of the United States has voiced disdain for New Start, but there are no indications that the treaty will end early. (For a discussion of current developments, refer to this page from the arms control association.) With the present political climate, it is unclear what future arms control reduction measures will be put in place. Given the success of previous efforts, further arms reduction treaties must be a priority for both the US and Russia. Verification of these treaties involves on-site inspections, exchanges of information, remote monitoring, and surveillance including satellites. To prove the US had followed START I, 365 B-52 bombers were flown to Arizona, stripped of usable parts, and then chopped into pieces. They were left out for 3 months to allow Russian satellites to confirm the destruction. Other reductions of nuclear war threats have come without a bilateral agreement, including in 1991 when George H.W. Bush announced the US would remove almost all US nonstrategic (these are also called tactical nuclear weapons and are designed to be used on a battlefield) nuclear arms from deployment as the Soviet Union dissolved. As planned, the Soviet Union responded in kind, with Mikhail Gorbachev promising to withdraw all tactical Soviet nuclear weapons from naval deployment. This act alone, officially known as the Presidential Nuclear Initiatives (PNI) reduced the total number of nuclear weapons in the world by about 18,000. Although Russian and the US still possess both strategic and non-strategic nuclear weapons, the totals are at all-time lows (for a discussion of whether this will hold, see here). Megatons to Megawatts: The Coolest Government Program You’ve Never Heard Of Sometimes, especially when the fate of the world hangs in the balance, the government can do some amazing things. One lesser-known case is the “Megatons to Megawatts” program in which 500 tons of highly enriched uranium (HEU) from Russian nuclear weapons was converted into low-enriched uranium (LEU) and sold to United States companies to be used as fuel in nuclear power plants. The total impact of this astonishing program was that 500 metric tons of HEU — enough for 20,000 nuclear warheads — was converted into 14,000 tons of LEU and used to power nuclear reactors around the United States. (The cost of this 20-year program to taxpayers was $0 because the $12 billion of uranium was bought by the United States Enrichment Corporation.) Over the 18 years of the program (1995–2013), the LEU fuel provided about 50% of all uranium consumed by US nuclear reactors, meaning almost 10% of electricity generated in the United States was from old Russian nuclear weapons! If you get any of your power from a nuclear station, then your lights were literally turned on by Russian nuclear weapons: The total benefits of Megatons to Megawatts were enormous to both countries. The United States needed fuel for its nuclear reactors, and Russia needed cash after the fall of the Soviet Union. Both parties got exactly what they needed with the program successfully ending in 2013. Since then, the US has voluntary converted some of its HEU to LEU so it cannot be used in weapons and an agreement was signed in 2008 allowing the Russian nuclear industry to supply up to 20% of the US demand for uranium products until 2020. The importance of the Megatons to Megawatts program — in addition to material benefits — is it shows that countries can overcome their differences, particularly when it is in both parties economic interest to do so. It also proved that former enemies can work together and resolve disputes peacefully. As we think of how to resolve situations in our personal lives and on an international scale, we’d be wise to keep this program in mind: compromise means that both sides don’t get everything they want, but everyone eventually benefits. Conclusions Nuclear weapons are not going away for the foreseeable future. While in a perfect world there would be 0 nukes, we live in reality, not in a utopia. Therefore, as a short-term realist and long-term optimist, I appreciate the significant progress we have made — massive reductions in the number of nuclear weapons — while acknowledging the need for continued reductions. Furthermore, while I am glad there are concerned scientists, I would like them more if they had their facts straight before they speak from a position of prominence. Telling people the world is worse than it is will not lead to action, but to disillusionment and the belief that things can never get better. It’s worth remembering that progress is never directly linear, but has been trending positively for most of human history and has sped up in the past several decades. As nuclear weapons demonstrate, things could be both much worse and much better, and international efforts do work on a global scale. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. People assume the worst of humanity in the absence of data. Don’t act superior to someone when you are correcting them. Remember that you were ignorant as well before you read the statistics. Stockpiled: in storage with the military and “earmarked for use by commissioned delivered vehicles” (may be deployed) Decommissioned: retired from service and awaiting dismantlement. Strategic Weapons: larger weapons — such as ICBMs — which can be launched at a country from 1000s of miles away or on a bomber Non-Strategic (Tactical) Weapons: smaller yield, designed to be used on a battlefield, potentially in the vicinity of friendlies Even experts are seriously wrong in their view of the world. Deployed: on ICBMs or heavy bombers, can be launched instantaneously",The Reality of Global Nuclear Weapons and How Russian Nukes Turned On Your Lights,14,published,1402,2795,0.13989266547406082,8,1,0,0,0,0
11,451,130.95698995185185,49,https://towardsdatascience.com/the-myth-of-us-vs-them-e0bfccb62f41,0,Towards Data Science,2019-01-16 16:44:00,27.11,10,286,2019-01-01 16:00:00,"['Data', 'Education', 'The Reality Project', 'Data Journalism', 'World']","The Myth of Us vs Them Why the “Developed and Developing world” distinction no longer works: Reality Project Episode 4 Of all the myths spread by the media, perhaps none is more detrimental than Us vs Them: the idea that the world is divided into two groups — one good, one evil — and all events can be viewed as a struggle between the two. This two-sided view of the world takes advantage of our natural tendency to form tribes and is applied in many situations, from political parties to economic systems, and, to countries in the form of “developed vs. developing world.” In the last case, this takes the form: developed = rich with low birth rates (assumed to be us) vs. developing = poor with high birth rates (them). A binary outlook may make for compelling news (conflicts and tribalism are sure ways to get our attention) but it’s false. All of human life, from the personal — income, height, sexuality — to the international — systems of government, economic systems, national wealth — exists not in two states, but along a continuum. Moreover, we dehumanize people by splitting them into two groups letting our instincts and cognitive biases do the thinking for us instead of using our rationality. In this article, the fourth episode of the Reality Project — an effort dedicated to becoming less wrong about the world with data — we’ll see why a separation between developing and developed countries no longer applies and look at factful ways to view nations instead. The previous episode of The Reality Project, on global nuclear weapons is here. You can find all The Reality Project by searching on Medium for the tag. The Data For this article, we’ll rely primarily on Hans Rosling’s book: Factfulness: Ten Reasons We’re Wrong About the World. Other sources are the World Bank Poverty Page, the YouTube video “Debunking Third World Myths with the Best Stats You’ve Never Seen”, and the article “Should We Continue to Use the Term ‘Developing World’?”. I made the graphs with the Gapminder Data Exploration tool which you can download for free (it includes data). To start, it’s worth looking at where this binary idea applied to countries originated. The term “developing country” probably evolved out of “Third World”, which originally referred to countries aligned with neither the Communist Soviet bloc or Capitalist NATO in the 1950s. These countries tended to be former colonies struggling to provide citizens with a decent standard of living, and “Third World” soon came to mean poor countries with high birth rates, what are now referred to as developing nations. Admittedly, at one point, during the 1960s, this distinction actually did make sense — or at least was in line with the existing data. In the below chart, we see the fertility rate (births per woman) on the y-axis versus income (GDP per capita) on the x-axis in 1960 for all countries where we have data. (The color represents the world region: yellow = Europe, blue = Africa, green = Americas, and Red = Asia + Australia). There is a relatively clear grouping as countries in Europe plus a few in the Americas and Asia tended to be both richer and have fewer babies per woman (lower right). On the other hand, African countries almost exclusively had high birth rates and low income. Development aid took off in the post-WWII years, and so, at the time, this classification allowed NGOs and governments to focus their efforts. However, since 1960, the picture of the world has changed dramatically. Let’s look at the same graph in 1990, just after the fall of the Soviet Union. Now we see not a grouping but a linear transition. There are still differences between countries, but the noticeable gap that existed has given way to a smooth line. The global fertility rate, at an extremely high 5 babies per woman in 1960, had fallen to 3 in 1990. The extreme poverty rate — less than $2 /day — likewise fell, from over 60% in 1960 to less than 40% in 1990. These trends — decreasing fertility and decreasing poverty — have only accelerated since 1990. Below is the most up-to-date picture of the world: What we see here has been named “The Great Convergence.” While there are still differences, there is no one gap that divides countries; instead, they are arrayed along a continuous line of income. Drawing a single separating line of this graph is impossible because there is only a smooth transition. Countries all over the world have gotten richer and, as a result, have longer lives, reduced childhood mortality, and fewer children per women. Notably, China and India (visible as the large red bubbles) have experienced extreme reductions in poverty, resulting in a new world low of under 9% extreme poverty in 2018. The new global birth rate is at 2.5 babies per woman, drawing near to the replacement rate of 2.1. When we look at a similar graph of childhood survival versus fertility rate, we can see the same trend. The developing/developed gap here is even clearer in 1960 (left) but again, the world has since converged (right). The old boxes clearly no longer fit. We’ll get into a more accurate characteristic in the next section. We’ll end this part with one final visual: the changing worldwide income distribution since 1800. It’s one thing to get your picture of the world from the news, but completely different to watch the transformations take place in front of your eyes with real data. As incomes around the world increase, the global extreme poverty rate (vertical line) decreases from 86% to under 15%(and, in 2018, the World Bank reported it was under 10%). If you watch closely, you can see the emergence of a gap right around 1960 — this is precisely the time at which the terms “developing” and “developed” first came into use. However, as the animation continues, the gap slowly disappears until incomes exist along a continuous distribution. The separation of incomes — called the Great Divergence — was a real phenomenon, but the new picture is a Great Convergence as incomes increase nearly everywhere. This disappearing of the “gap” between two distinct sets of countries means the terms “developing” and “developed” no longer accurately represent countries. If we use this distinction, it means placing Mexico, GDP per capita $17,000, and Mozambique, GDP per capita $1,200, in the same category. In other words, a binary distinction is too broad. Nonetheless, as in the 1960s, some form of breaking up countries is useful as it allows us to find countries with similar needs. If the two-part model is broken, what fixes it? A Better System In Factfulness, Hans Rosling lays out a four-tiered approach for grouping countries based on similar needs derived from income. These are outlined as follows (from Bill Gates’ excellent post on the subject). If you must think of it this way, Level One is “lower-class”, Levels Two and Three are the global “middle-class”, and Level Four is the “upper-class”. In this system, the vast majority of the world’s population are correctly classified as middle class or above reflecting the recent substantial gains in income. Below is a breakdown of the world population into each level over time: In 1800, close to the entire world population (85%) existed on Level One. In 2018, only 15% of the world’s population is still there and the middle classes are growing far faster than the extremes. The actual levels and dollar amounts, with each person representing approximately 1 billion people, are below: The four-tier system is an improvement over a binary split. Rosling’s approach is useful because it captures needs of different countries more accurately than binary categories, it allows us to clearly see progress over time, and it prevents us from falling into the Us vs. Them type of thinking. (Not to mention the numbers work out very well). While this argument may seem like semantics, it actually does matter because how we measure determines how we act. (There are other systems to approach the same problem. The International Monetary Fund still uses a binary advanced vs emerging economies, the United Nations Development Program uses a continuous Human Development Index, and the World Bank has recently stopped using the terms “developing” and “developed”. There is also a compelling argument for expanding the four categories to five with the fifth representing abundance.) Why This Matters Why does any measurement matter? The answer: it helps us to figure out what approaches work to solve a problem by observing causes and effects. Consider a typical continuous human measurement, weight. If you are trying to lose weight, one of the best places to start is simply weigh yourself every day so you can see if you are going in the right direction. How else are you supposed to know if your (non)-interventions are having any impact? A process of act — measure results — adjust actions is how we personally improve in any pursuit. The exact same is true of global development: measuring along a continuous or four-tier scale allows us to clearly see progress over time and relate back specific improvements to causes. The weight metaphor also holds up when we start to think about using binary versus multiple categories. If there are only two states — fat and thin — we are inaccurate categorizing a lot of people and it’s going to be hard to stay motivated if your status never changes even from year-to-year. We are much better off thinking in terms of categories — obese, overweight, healthy, underweight — so we can make salient recommendations to individuals (or ourselves) to guide their actions for losing weight. When it comes to global poverty, it can seem like there are reasons to despair. There are 750 million people — mostly in Sub-Saharan Africa — currently in extreme poverty. However, we shouldn’t stop there, and, once we start digging into the data, the picture becomes much better: over 1.2 billion people lifted out of extreme poverty in the past 30 years and the real possibility of eliminating extreme poverty within the next several decades. With a binary worldview, we risk missing all of that by lumping together everyone not at our income level as developing. In the four-tier framework, we can see that actually about 85% (6/7 billion) people live on the second level or higher and the largest segment of the population lives in the global middle class (levels 2 and 3). If we look again at the graph of the distribution of levels over time, we see lots of reason for hope: The middle classes are growing and the first level (the “lower class”) is shrinking rapidly. This is good not only for people in those countries but also for everyone around the world. As countries get wealthier, citizens report increased life satisfaction (a better measure than day-to-day happiness): Furthermore, as incomes increase, fertility rates decrease as better health care leads to fewer children dying and subsequently, women have fewer babies: (There are many other reasons for the decline in fertility which you can read about on the Our World in Data fertility page.) There are people worried about overpopulation (I don’t happen to be one of them based on the data and best estimates) and, if you are one of them, the best way to limit population growth is to bring people in all countries out of poverty. As families have fewer children, they can spend more resources on each child, giving them a better chance to succeed, which they can then pass on to their children and the virtuous cycle of decreasing poverty and decreasing fertility continues. Conclusions You might argue that at the end of the day, this is only semantics. However, the “developed vs developing” dichotomy is more than a choice of words, it’s a mindset. At its core, a binary worldview is intellectually lazy because it means not looking at the data and it’s inhumane because it leads us to see individuals as the dreaded other rather than as people fighting for the same things that we are — health, prosperity, and a better life for those in the next generation. The Reality Project isn’t about knowing facts so you can feel superior, it’s about using facts to encourage people to contribute to the long ascent of humankind. The “Developed vs Developing” myth is a great place to start because it offers an opportunity to show optimistic stats, talk about how our view of the world is wrong, and make gentle corrections. Once we understand why and how the world is getting better, we can make sure it continues that way. As a first step, start by viewing matters — in your personal life and on an international level — not as black and white but on a scale, that, even when divided, shows progress and its causes. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. Level Two: between $2 and $8 per day. Around 3 billion people are on this level where kids can go to school instead of working. Level Three: between $8 and $32 per day. Around 2 billion people are on this level that includes motorized transportation and high school. Level Four: more than $32 per day. Around 1 billion people are on this level with labor-saving appliances and reliable health care. Level One: less than $2 per day. Around 1 billion people are on this level and getting food every day can be a struggle.",The Myth of Us vs Them,6,published,1055,2532,0.17812006319115323,15,1,0,0,0,0
16,3700,126.96810344840279,442,https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459,16,Towards Data Science,2019-01-20 16:28:00,26.87,14,3158,2019-01-19 18:55:00,"['Statistics', 'Data Science', 'Towards Data Science', 'Education', 'Probability']","The Poisson Distribution and Poisson Process Explained A straightforward walk-through of a useful statistical concept A tragedy of statistics in most schools is how dull it’s made. Teachers spend hours wading through derivations, equations, and theorems, and, when you finally get to the best part — applying concepts to actual numbers — it’s with irrelevant, unimaginative examples like rolling dice. This is a shame as stats can be enjoyable if you skip the derivations (which you’ll likely never need) and focus on using the ideas to solve interesting problems. In this article, we’ll cover Poisson Processes and the Poisson distribution, two important probability concepts. After highlighting only the relevant theory, we’ll work through a real-world example, showing equations and graphs to put the ideas in a proper context. Poisson Process A Poisson Process is a model for a series of discrete event where the average time between events is known, but the exact timing of events is random. The arrival of an event is independent of the event before (waiting time between events is memoryless). For example, suppose we own a website which our content delivery network (CDN) tells us goes down on average once per 60 days, but one failure doesn’t affect the probability of the next. All we know is the average time between failures. This is a Poisson process that looks like: The important point is we know the average time between events but they are randomly spaced (stochastic). We might have back-to-back failures, but we could also go years between failures due to the randomness of the process. A Poisson Process meets the following criteria (in reality many phenomena modeled as Poisson processes don’t meet these exactly): The last point — events are not simultaneous — means we can think of each sub-interval of a Poisson process as a Bernoulli Trial, that is, either a success or a failure. With our website, the entire interval may be 600 days, but each sub-interval — one day — our website either goes down or it doesn’t. Common examples of Poisson processes are customers calling a help center, visitors to a website, radioactive decay in atoms, photons arriving at a space telescope, and movements in a stock price. Poisson processes are generally associated with time, but they do not have to be. In the stock case, we might know the average movements per day (events per time), but we could also have a Poisson process for the number of trees in an acre (events per area). (One instance frequently given for a Poisson Process is bus arrivals (or trains or now Ubers). However, this is not a true Poisson process because the arrivals are not independent of one another. Even for bus systems that do not run on time, whether or not one bus is late affects the arrival time of the next bus. Jake VanderPlas has a great article on applying a Poisson process to bus arrival times which works better with made-up data than real-world data.) Poisson Distribution The Poisson Process is the model we use for describing randomly occurring events and by itself, isn’t that useful. We need the Poisson Distribution to do interesting things like finding the probability of a number of events in a time period or finding the probability of waiting some time until the next event. The Poisson Distribution probability mass function gives the probability of observing k events in a time period given the length of the period and the average events per time: This is a little convoluted, and events/time * time period is usually simplified into a single parameter, λ, lambda, the rate parameter. With this substitution, the Poisson Distribution probability function now has one parameter: Lambda can be thought of as the expected number of events in the interval. (We’ll switch to calling this an interval because remember, we don’t have to use a time period, we could use area or volume based on our Poisson process). I like to write out lambda to remind myself the rate parameter is a function of both the average events per time and the length of the time period but you’ll most commonly see it as directly above. As we change the rate parameter, λ, we change the probability of seeing different numbers of events in one interval. The below graph is the probability mass function of the Poisson distribution showing the probability of a number of events occurring in an interval with different rate parameters. The most likely number of events in the interval for each curve is the rate parameter. This makes sense because the rate parameter is the expected number of events in the interval and therefore when it’s an integer, the rate parameter will be the number of events with the greatest probability. When it’s not an integer, the highest probability number of events will be the nearest integer to the rate parameter, since the Poisson distribution is only defined for a discrete number of events. The discrete nature of the Poisson distribution is also why this is a probability mass function and not a density function. (The rate parameter is also the mean and standard deviation of the distribution, which do not need to be integers.) We can use the Poisson Distribution mass function to find the probability of observing a number of events over an interval generated by a Poisson process. Another use of the mass function equation — as we’ll see later — is to find the probability of waiting some time between events. A Worked-Out Example For the problem we’ll solve with a Poisson distribution, we could continue with website failures, but I propose something grander. In my childhood, my father would often take me into our yard to observe (or try to observe) meteor showers. We were not space geeks, but watching objects from outer space burn up in the sky was enough to get us outside even though meteor showers always seemed to occur in the coldest months. The number of meteors seen can be modeled as a Poisson distribution because the meteors are independent, the average number of meteors per hour is constant (in the short term), and — this is an approximation — meteors don’t occur simultaneously. To characterize the Poisson distribution, all we need is the rate parameter which is the number of events/interval * interval length. From what I remember, we were told to expect 5 meteors per hour on average or 1 every 12 minutes. Due to the limited patience of a young child (especially on a freezing night), we never stayed out more than 60 minutes, so we’ll use that as the time period. Putting the two together, we get: What exactly does “5 meteors expected” mean? Well, according to my pessimistic dad, that meant we’d see 3 meteors in an hour, tops. At the time, I had no data science skills and trusted his judgment. Now that I’m older and have a healthy amount of skepticism towards authority figures, it’s time to put his statement to the test. We can use the Poisson distribution to find the probability of seeing exactly 3 meteors in one hour of observation: 14% or about 1/7. If we went outside every night for one week, then we could expect my dad to be right precisely once! While that is nice to know, what we are after is the distribution, the probability of seeing different numbers of meteors. Doing this by hand is tedious, so we’ll use Python — which you can see in this Jupyter Notebook — for calculation and visualization. The below graph shows the Probability Mass Function for the number of meteors in an hour with an average time between meteors of 12 minutes (which is the same as saying 5 meteors expected in an hour). This is what “5 expected events” means! The most likely number of meteors is 5, the rate parameter of the distribution. (Due to a quirk of the numbers, 4 and 5 have the same probability, 18%). As with any distribution, there is one most likely value, but there are also a wide range of possible values. For example, we could go out and see 0 meteors, or we could see more than 10 in one hour. To find the probabilities of these events, we use the same equation but this time calculate sums of probabilities (see notebook for details). We already calculated the chance of seeing exactly 3 meteors as about 14%. The chance of seeing 3 or fewer meteors in one hour is 27% which means the probability of seeing more than 3 is 73%. Likewise, the probability of more than 5 meteors is 38.4% while we could expect to see 5 or fewer meteors in 61.6% of observation hours. Although it’s small, there is a 1.4% chance of observing more than 10 meteors in an hour! To visualize these possible scenarios, we can run an experiment by having our sister record the number of meteors she sees every hour for 10,000 hours. The results are shown in the histogram below: (This is obviously a simulation. No sisters were employed for this article.) Looking at the possible outcomes reinforces that this is a distribution and the expected outcome does not always occur. On a few lucky nights, we’d witness 10 or more meteors in an hour, although we’d usually see 4 or 5 meteors. The rate parameter, λ, is the only number we need to define the Poisson distribution. However, since it is a product of two parts (events/interval * interval length) there are two ways to change it: we can increase or decrease the events/interval and we can increase or decrease the interval length. First, let’s change the rate parameter by increasing or decreasing the number of meteors per hour to see how the distribution is affected. For this graph, we are keeping the time period constant at 60 minutes (1 hour). In each case, the most likely number of meteors over the hour is the expected number of meteors, the rate parameter for the Poisson distribution. As one example, at 12 meteors per hour (MPH), our rate parameter is 12 and there is an 11% chance of observing exactly 12 meteors in 1 hour. If our rate parameter increases, we should expect to see more meteors per hour. Another option is to increase or decrease the interval length. Below is the same plot, but this time we are keeping the number of meteors per hour constant at 5 and changing the length of time we observe. It’s no surprise that we expect to see more meteors the longer we stay out! Whoever said “he who hesitates is lost” clearly never stood around watching meteor showers. Waiting Time An intriguing part of a Poisson process involves figuring out how long we have to wait until the next event (this is sometimes called the interarrival time). Consider the situation: meteors appear once every 12 minutes on average. If we arrive at a random time, how long can we expect to wait to see the next meteor? My dad always (this time optimistically) claimed we only had to wait 6 minutes for the first meteor which agrees with our intuition. However, if we’ve learned anything, it’s that our intuition is not good at probability. I won’t go into the derivation (it comes from the probability mass function equation), but the time we can expect to wait between events is a decaying exponential. The probability of waiting a given amount of time between successive events decreases exponentially as the time increases. The following equation shows the probability of waiting more than a specified time. With our example, we have 1 event/12 minutes, and if we plug in the numbers we get a 60.65% chance of waiting > 6 minutes. So much for my dad’s guess! To show another case, we can expect to wait more than 30 minutes about 8.2% of the time. (We need to note this is between each successive pair of events. The waiting times between events are memoryless, so the time between two events has no effect on the time between any other events. This memorylessness is also known as the Markov property). A graph helps us to visualize the exponential decay of waiting time: There is a 100% chance of waiting more than 0 minutes, which drops off to a near 0% chance of waiting more than 80 minutes. Again, since this is a distribution, there are a wide range of possible interarrival times. Conversely, we can use this equation to find the probability of waiting less than or equal to a time: We can expect to wait 6 minutes or less to see a meteor 39.4% of the time. We can also find the probability of waiting a period of time: there is a 57.72% probability of waiting between 5 and 30 minutes to see the next meteor. To visualize the distribution of waiting times, we can once again run a (simulated) experiment. We simulate watching for 100,000 minutes with an average rate of 1 meteor / 12 minutes. Then, we find the waiting time between each meteor we see and plot the distribution. The most likely waiting time is 1 minute, but that is not the average waiting time. Let’s get back to the original question: how long can we expect to wait on average to see the first meteor if we arrive at a random time? To answer the average waiting time question, we’ll run 10,000 separate trials, each time watching the sky for 100,000 minutes. The graph below shows the distribution of the average waiting time between meteors from these trials: The average of the 10,000 averages turns out to be 12.003 minutes. Even if we arrive at a random time, the average time we can expect to wait for the first meteor is the average time between occurrences. At first, this may be difficult to understand: if events occur on average every 12 minutes, then why should we have to wait the entire 12 minutes before seeing one event? The answer is this is an average waiting time, taking into account all possible situations. If the meteors came exactly every 12 minutes, then the average time we’d have to wait to see the first one would be 6 minutes. However, because this is an exponential distribution, sometimes we show up and have to wait an hour, which outweighs the greater number of times when we wait fewer than 12 minutes. This is called the Waiting Time Paradox and is a worthwhile read. As a final visualization, let’s do a random simulation of 1 hour of observation. Well, this time we got exactly what we expected: 5 meteors. We had to wait 15 minutes for the first one, but then had a good stretch of shooting stars. At least in this case, it’d be worth going out of the house for celestial observation! A Binomial Distribution is used to model the probability of the number of successes we can expect from n trials with a probability p. The Poisson Distribution is a special case of the Binomial Distribution as n goes to infinity while the expected number of successes remains fixed. The Poisson is used as an approximation of the Binomial if n is large and p is small. As with many ideas in statistics, “large” and “small” are up to interpretation. A rule of thumb is the Poisson distribution is a decent approximation of the Binomial if n > 20 and np < 10. Therefore, a coin flip, even for 100 trials, should be modeled as a Binomial because np =50. A call center which gets 1 call every 30 minutes over 120 minutes could be modeled as a Poisson distribution as np = 4. One important distinction is a Binomial occurs for a fixed set of trials (the domain is discrete) while a Poisson occurs over a theoretically infinite number of trials (continuous domain). This is only an approximation; remember, all models are wrong, but some are useful! For more on this topic, see the Related Distribution section on Wikipedia for the Poisson Distribution. There is also a good Stack Exchange answer here. Meteors are the streaks of light you see in the sky that are caused by pieces of debris called meteoroids burning up in the atmosphere. A meteoroid can come from an asteroid, a comet, or a piece of a planet and is usually millimeters in diameter but can be up to a kilometer. If the meteoroid survives its trip through the atmosphere and impacts Earth, it’s called a meteorite. Asteroids are much larger chunks of rock orbiting the sun in the asteroid belt. Pieces of asteroids that break off become meteoroids. The more you know!. To summarize, a Poisson Distribution gives the probability of a number of events in an interval generated by a Poisson process. The Poisson distribution is defined by the rate parameter, λ, which is the expected number of events in the interval (events/interval * interval length) and the highest probability number of events. We can also use the Poisson Distribution to find the waiting time between events. Even if we arrive at a random time, the average waiting time will always be the average time between events. The next time you find yourself losing focus in statistics, you have my permission to stop paying attention to the teacher. Instead, find the relevant equations and apply them to an interesting problem. You can learn the material and you’ll have an appreciation for how stats helps us to understand the world. Above all, stay curious: there are many amazing phenomenon in the world, and we can use data science is a great tool for exploring them, As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes. The average rate (events per time period) is constant. Two events cannot occur at the same time. Events are independent of each other. The occurrence of one event does not affect the probability another event will occur.",The Poisson Distribution and Poisson Process Explained,7,published,11752,3371,1.0975971521803618,0,1,1,1,0,0
17,82,122.29968599763889,18,https://towardsdatascience.com/a-great-public-health-conspiracy-73f7ac6fb4e0,3,Towards Data Science,2019-01-25 08:30:00,18.13,13,118,2019-01-23 18:51:00,"['Health', 'Data', 'Data Journalism', 'The Reality Project', 'Education']","A Great Public Health Conspiracy? The collusion of science, medicine, and the government to improve public health. Episode Five of The Reality Project. In January 2013, the city council of Windsor, Ontario made a curious choice. By a vote of 8–3, they implemented a plan to increase rates of tooth decay and cavities among the town’s children by more than 50%. What evil action did these councilors impose? Holding Halloween multiple times per year? Letting a candy company make school lunches? No, they did something far more disastrous: they deliberately choose to ignore mountains of medical advice, give in to public hysteria, and undo one of the greatest public health achievements of the last century, the fluoridation of public water. Water fluoridation, adding the mineral fluoride to public water supplies to protect against tooth decay, has been a common practice since 1945. About 400 million people worldwide drink water with added fluoride, including at least 66% of the US population. Thousands of studies have found water fluoridation to be beneficial and risk-free, with the Centers for Disease Control and Prevention Concluding: “fluoride is both safe and effective in preventing and controlling dental caries” (caries being the medical term for tooth decay/cavities). Nonetheless, the government of one small Canadian town thought they knew better than the entire medical community. After 6 years, a 51% increase in cavities among children, a 300% increase in low-income families needing financial support for dental care, and untold suffering for the town’s children, the Windsor council reluctantly reversed its decision in a December 2018 meeting. At the meeting, 5 dental experts testified, giving their full support to water fluoridation in accordance with all the medical evidence. On the other hand, 20 citizens— none with a medical degree — voiced their opposition based on unfounded fears and a belief government should not “medicate” the public even when that medication prevents children from going to the hospital. Fortunately, at the end of the day, reason, and a poll showing 80% of the citizens wanted water fluoridation prevailed, and Windsor will once again enjoy the benefits. This story immediately caught my attention due to the intriguing elements: a great medical achievement undone by conspiracy theorists, people refusing to acknowledge a substantial body of evidence, and finally, a triumph of science over superstition. Further, it illustrates why The Reality Project, an effort to become less wrong about the world with data, is important: it’s only by examining the data that we can figure out which policies work and then — even as individual citizens — campaign for their implementation. Being right when it comes to science, medicine, and our view of the world is not just about vanity, it’s about ensuring we make the best decisions for society. In this fifth episode of The Reality Project, we’ll dive into the evidence on public water fluoridation. While previous episodes have focused on large-scale ideas, this one is more personal: it involves your health and that of your family. We need to know the facts, because while we might initially laugh off conspiracy theorists, it’s only funny until they start eroding trust in governments, and with it, degrading the well-being of society. Perhaps we have a natural inclination to seek out insane theories (you did click on this because of “conspiracy” in the title right?) but the truth is often more amazing as is demonstrated by the data on public water fluoridation. The Facts on Public Water Fluoridation With any health topic, especially one that has attracted controversy, we must be careful about where we get our data. Even studies in peer-reviewed journals can have biases — intentional or not. Therefore, the best practice for reviewing medical evidence is to look at meta-analyses, reviews that evaluate results from dozens or hundreds of studies. The Cochrane Organization, a British charity, was founded to carry out systematic reviews of medical literature and make objective recommendations for the good of the public. We’ll be relying on their meta-analysis of public water fluoridation as well as one from the Australian National Health and Medical Research Council (NHMRC). For an economic analysis, we’ll use data from “Economic Evaluation of Community Water Fluoridation”. None of these sources are captivating reading, but relying on entertainment (that is, the news) as a source of facts is disastrous when it comes to health. Reading the actual evidence is difficult, but it’s the only way to not be fooled by special interests. Wikipedia articles can be a good place to start, but we should always follow through to the citations. Here I’ll summarize the relevant parts, but if you’re skeptical, I encourage you to read the articles. Let’s start off with an easy question: does water fluoridation work to prevent cavities and tooth decay? The unequivocal answer: yes, at least in children. According to the Cochrane review (based on 107 studies) public water fluoridation reduces cavities by 35% in baby teeth and 26% in permanent teeth. Other meta-analyses have found similar results. The report from the NHMRC found a risk reduction between 9% and 35% across studies. The same review found the number of people needed to treat (NNT) to prevent a cavity ranged from 3–14. An NNT of 3 means that for every 3 people receiving fluoridated water, 1 person will not develop a cavity as a result. The relevant findings of the NHMRC review are pictured below. Dental caries is the scientific term for tooth decay (which is the same thing as cavities caused by acid produced by bacteria). The risk difference and the NNT are the two most helpful columns. In all studies reviewed, the risk of tooth decay was decreased by the fluoridation of public water supplies. The Cochrane review is careful to point out there are not enough studies to confirm water fluoridation reduces the prevalence of tooth decay in adults. However, the Centers for Disease Control and Prevention (CDC) cites at least one study that showed a 20–40% reduction in cavities for adults suggesting benefits may persist throughout life. It’s important to acknowledge both the evidence we have — water fluoridation improves dental health in children — and admit where we need more research before drawing conclusions. The efficacy of a medical intervention is not the only critical factor: there need to be no harmful side effects, or at least the benefits must be greater than the adverse outcomes. The second question is therefore: Are there any side effects to fluoridation? Opponents of water fluoridation list a litany of side effects (including cancer and lower IQ), but, in study after study, the medical evidence points to only one that occurs infrequently: a mild discoloration of the teeth. Known as dental fluorosis, this condition is purely aesthetic meaning it only affects how the teeth appear. In other news, drinking coffee and tea can also stain your teeth! Any studies trumpeted by the conspiracy theorists claiming to have found negative effects examined doses far beyond what any community water fluoridation plan calls for (which is 0.7 parts per million). What this means is the only possible side effect of public water fluoridation is a slight stain similar to what you might get from too much tea. So, it comes down to what you value: if you want your teeth to rot but look impeccable, stop drinking fluoridated water (and tea)! For those of us who dislike dental pain and tooth decay, the risk of mild discoloration is well worth the benefits of fluoride. In case you don’t believe me, perhaps the following statement from the American Dental Association (ADA) can sway you: “[the ADA] unreservedly endorses the fluoridation of community water supplies as safe, effective and necessary in preventing tooth decay”. Mind you, the ADA stands to lose from fluoridated water because of the resulting reduction in patients. Maybe the only person who benefitted from the situation? A local dentist who now has bookings through 2020 because of the cessation of water fluoridation. Economics An effective treatment is one thing, but the most important question for communities before implementation may be: is it worth it? We can’t put a price on the human suffering prevented, but we can look at the cost of fluoridation compared to the reduction in treatment costs for tooth decay. In the review “Economic Evaluation of Community Water Fluoridation”, 6 different studies across various population sizes were examined and the cost for water fluoridation per person per year was found to range from $0.24 to $4.85 depending on community size (larger populations mean lower per capita costs). To put these numbers in context, the average benefit provided per person per year ranged from $5.49 to $93.19. Comparing the costs and benefits side by side, community water fluoridation delivered up to $135 for every dollar invested. In every study, the benefits exceeded the costs as can be seen in the table below: As a note of interest, the cost to benefit ratio generally increases as the size of the community increases. This makes sense: the infrastructure is the most expensive part and the fluoride itself is relatively cheap. Therefore, the more people served from the same water system, the cheaper the costs per person. We expect our retirement funds to deliver positive returns, and we should do the same from our public health interventions. You could do worse than put your money in public water fluoridation: it not only reduces suffering of the children in your community but can also return 135x the investment! Any community that chooses to fluoridate its water is not only improving the dental health of its citizens but doing so at a societal profit. Alternatives Are there other ways to get the same benefits without putting fluoride in the water? The answer is a definite yes: milk and salt can both be fluoridated and the most common alternatives are topical options including gels and toothpaste. About 90% of the toothpaste in the United States now contains fluoride. What’s more, the concentrations in toothpaste are nearly 1,000 times that of water fluoridation (1000+ ppm compared to 0.7 ppm for water)! For anyone who may be concerned with potential health effects of fluoridation, the clear solution is not to avoid tap water, but to stop brushing one’s teeth. The prevalence of other sources of fluoride could be why the effects of public water fluoridation have decreased over the past several decades. Initial studies conducted in the 1950s and 1960s found reductions of 50–70% in tooth decay when fluoridation was introduced. However, as we learned above, the present evidence suggests a more modest reduction of around 20–35%. This is good news as the overall rate of tooth decay in the US decreases and the gaps in dental health between fluoridated and non-fluoridated communities shrinks. It should be noted however, that access to fluoridated toothpaste and alternative sources are limited in low socioeconomic areas, precisely the places where they are needed most. The CDC sums it up well: Water fluoridation is especially beneficial for communities of low socioeconomic status. These communities have a disproportionate burden of dental caries and have less access than higher income communities to dental-care services and other sources of fluoride… Although other fluoride-containing products are available, water fluoridation remains the most equitable and cost-effective method of delivering fluoride to all members of most communities, regardless of age, educational attainment, or income level. The best part about public water fluoridation, as opposed to the alternatives, is that it does not discriminate based on wealth. It’s also much cheaper compared to the application of fluoride gel which can cost $100 per person per year. A common refrain among opponents to public water fluoridation is that all the other options mean it is no longer needed. However, if that were the case, what can explain the rapid rise in tooth decay seen in Windsor in just a few years? Granted that is only an anecdote, but other studies have found similar adverse effects once water fluoridation is stopped. Yes, there are alternatives, but they require people to be able to afford them and know they should be using them. It’s much more effective to fluoridate public water, cover all the bases, and reach those who are most vulnerable. Why the Pushback? Given the above evidence: fluoride is effective, safe, and the cheapest option for preventing tooth decay especially for children, it can be hard to imagine anyone opposing this measure. Nonetheless, there is vociferous pushback to water fluoridation, often from those who oppose government intervention of any kind. The Rational Wiki has a great page that debunks many of the illogical arguments against water fluoridation, but here I’ll only mention two. It is true the WHO has put out publications about removing fluoride from drinking water. However, the WHO is not talking about controlled public water fluoridation but only excessive natural fluoridation. This only applies to a very narrow section of the world population that lives in areas where the natural supply of fluoride is extremely high. Anything is dangerous in extreme amounts, including many vitamins and comparing these locations to public water fluoridation is faulty logic. The 0.7 parts per million used in the United States to fluoridate public water is well below the 4 ppm the National Resource council considers safe. The WHO is, in fact, one of the strongest supporters of water fluoridation, unequivocally stating: “ fluoridation of water supplies, where possible, is the most effective public health measure for the prevention of dental decay.” Citing the ability of water fluoridation to reduce inequalities in health, the WHO advocates adopting this intervention wherever possible. The use of one limited example to stand in for all instances is a classic tactic used by conspiracy theorists. It represents a fallacy of over-generalization and a comparison of two entirely different situations. This is probably the most common argument against water fluoridation and comes in many forms: the government shouldn’t medicate its citizens, consumers are not given a choice to receive fluoridation, we should be able to live our lives free from tyranny. The argument from choice breaks down on several levels. Given that governments — at least in the US where the opposition is strong — are elected by the people, you do have a choice about water fluoridation. If you don’t like it, there is nothing preventing you from running for office to change things. Also, you have the choice to only drink bottled water, which may or may not be fluoridated (since the FDA is somewhat lax over enforcing regulations around bottled water). Furthermore, children, the ones who stand to benefit most from water fluoridation, can’t vote and therefore do not have a choice either way. If things were really up to the people, then everyone would have an equal say. Also, the government does plenty of other things for public safety, like plow roads in the winter and regulate medicines. Should we have a choice about whether we want to drive on a highway covered in snow or give our children medication that could contain anything companies want (before medicine was regulated, it often killed more people than it helped). When it comes to the safety of the public, we often don’t really have a choice, which is a good thing given our predilection for irrational, harmful behavior. The critics of water fluoridation are not numerous, they just tend to be loud. In Windsor, where 4/5 residents were for water fluoridation, a vocal minority were the ones causing most of the damage. Likewise, a 1998 Gallup Poll found that 70% of respondents in the United States were in favor of the practice indicating a small fraction of the population is responsible for the opposition. The problem appears to be not one of public opinion, but one of who is the loudest. People are much more likely to speak out when they believe (falsely) they are being hurt by the government than when they know they are being helped and it makes for more compelling news. I’ll end this section on one telling story. When Kuopio, a Finnish city, stopped water fluoridation in 1992, they implemented the change one month before the announced cessation without telling the public. This means the public was drinking un-fluoridated water for one whole month without knowing it. Researchers used this offset to survey residents about symptoms supposedly associated with water fluoridation and even the taste of the water. The results of the research showed the residents had no clue the water fluoridation had stopped early: they reported no change in symptoms or taste of the water even after the fluoridation had been stopped. It was only when the residents were told they were no longer drinking fluoridated water that they miraculously reported reductions in symptoms and stated they could notice a change in taste. In other words, the entire effect was in their heads. The studies’ authors nicely sum it up: “it seems likely that the prevalence of the symptoms … is connected with the psychological rather than with the physical effects of being exposed to fluoridated water.” Those who regarded fluoridation as a negative practice were most likely to report feeling better once they knew it had stopped (but noticed no change when it actually occurred). The human mind is very good at justifying irrational beliefs, and the concept that public water fluoridation is harmful has to rank as one of the most illogical theories. Whatever people’s motivation for opposing water fluoridation, one thing is clear: they have not examined the data themselves and thought critically about the evidence. Conclusions The story of water fluoridation is a classic tale of how medicine should work: it was intensively studied, and, after being proven safe and effective, implemented at a national scale in the United States. While the article I started with may make you question humanity’s future, I view the tale as a positive. The important lesson is that science and medicine won out in the end. Windsor is once again getting fluoridated water, and the children in the town (and potentially adults) will experience the relief. It’s worth pointing out that the truth won not by out-shouting the other side, but by presenting clear evidence. The problem is not a lack of public support for beneficial measures, it’s that the opposition is so loud. To paraphrase a famous quote, all that is necessary for superstition to win is for science to sit quietly by. Now that you know the facts: water fluoridation clearly delivers health benefits, has no adverse side effects, and is the most cost-effective method to prevent tooth decay — you are prepared to use this information to continue making the world a better place. If humans have the tendency to seek out the sensational, then present water fluoridation as sensational. It’s a scheme carried out by science, medicine, and government on the grandest scale that has actually managed to improve public health while delivering a profit. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",A Great Public Health Conspiracy?,6,published,651,3547,0.023118127995489145,1,1,0,0,0,0
8,4500,119.97129475101852,354,https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6,16,Towards Data Science,2019-01-27 16:23:00,28.88,6,2175,2019-01-27 06:26:00,"['Data Science', 'Towards Data Science', 'Education', 'Programming', 'Data Analysis']","Interactive Controls in Jupyter Notebooks How to use interactive IPython widgets to enhance data exploration and analysis There are few actions less efficient in data exploration than re-running the same cell over and over again, each time slightly changing the input parameters. Despite knowing this, I still find myself repeatedly executing cells just to make the slightest change, for example, choosing a different value for a function, selecting various date ranges for analysis, or even adjusting the theme of a plotly visualization. Not only is this inefficient, but it’s also frustrating, disrupting the flow of an exploratory data analysis. The ideal solution to this issue would be interactive controls to change inputs without needing to rewrite or rerun code. Fortunately, as is often the case in Python, someone has already run into this problem and developed a great tool to solve it. In this article, we’ll see how to get started with IPython widgets ( ipywidgets), interactive controls you can build with one line of code. This library allows us to turn Jupyter Notebooks from static documents into interactive dashboards, perfect for exploring and visualizing data. You can view a completely interactive running notebook with the widgets in this article on mybinder by clicking the image below. IPython widgets, unfortunately, do not render on GitHub or nbviewer but you can still access the notebook and run locally. Getting Started with IPywidgets The first step, as usual, is installing the library: pip install ipywidgets . Once that finishes, you can activate widgets for Jupyter Notebook with To use with JupyterLab, run: To import the ipywidgetslibrary in a notebook, run Let’s say we have the following dataframe with Medium article statistics (these are my actual stats, you can see how to get them in this article): How can we view all articles with more than 1000 reads? Here’s one way: But if we want to show articles with more than 500 claps, we have to write another line of code: Wouldn’t it be nice if we could just rapidly change these parameters — both the column and threshold — without writing more code? Try this: With the @interact decorator, the IPywidgets library automatically gives us a text box and a slider for choosing a column and number! It looks at the inputs to our function and creates interactive controls based on the types. Now we can segment the data using the controls (widgets) without writing code. You may have noticed some problems with the widgets — x can go negative and we had to type in the correct column name. We can fix these by providing specific arguments to the function parameters: Now we get a dropdown for the column (with the options in the list) and an integer slider limited to a range (the format is (start, stop, step) ). Read through the documentation for the full details of how function parameters are mapped to widgets. We can use this same @interact decorator to quickly turn any ordinary function into an interactive widget. For example, we may have a lot of images in a directory we want to quickly look through: Now we can quickly cycle through all the images without re-running the cell. This might actually be useful if you were building a convolutional neural network and wanted to examine the images your network had missclassified. The uses of widgets for data exploration are boundless. Another simple example is finding correlations between two columns: There are numerous helpful examples on the ipywidgets GitHub. Interactive widgets are especially helpful for selecting data to plot. We can use the same @interact decorator with functions that visualize our data: Here we are using the amazing cufflinks+plotly combination to make an interactive plot with interactive IPython widget controls. You may have noticed the plot was a little slow to update. If that is the case, we can use @interact_manual which requires a button for updating. Now the plot will only be updated when the button is pressed. This is useful for functions that take a while to return an output. To get more from the IPywidgets library, we can make the widgets ourselves and use them int the interact function. One of my favorite widgets is the DatePicker. Say we have a function, stats_for_article_published_between, that takes a start and end date and prints stats for all the articles published between them. We can make this interactive using the following code Now we get two interactive date selection widgets and the values are passed into the function (see notebook for details): Similarly, we can make a function that plots the cumulative total of a column up until a date using the same DataPicker interactive widget. If we want to make the options for one widget dependent on the value of another, we use the observe function. Here, we alter the image browser function to choose both the directory and image. The list of images displayed is updated based on the directory we select. When we want to reuse widgets across cells, we just need to assign them to the output of theinteract function. Now, to reuse the stats widget, we can just call stats.widget in a cell. This lets us reuse our widgets across a notebook. As a note, the widgets are tied to one another meaning the value in one cell will be automatically updated to the value you select for the same widget in another cell. We haven’t gotten close to covering all the capabilities of IPywidgets. For instance, we can link values together, create custom widgets, make buttons, build animations, create a dashboard with tabs, and so on. Take a look at the documentation for further uses. Even with the small amount covered here, I hope you see how interactive controls can enhance a notebook workflow! Conclusions The Jupyter Notebook is a great data exploration and analysis environment. However, by itself, it doesn’t offer the best functionality. Using tools like notebooks extensions and interactive widgets make the notebook come to life and make our jobs as data scientists more efficient. Furthermore, building widgets and using them in a notebook is simply fun! Writing lots of code to do the same task repeatedly is not enjoyable, but using interactive controls creates a more natural flow for our data explorations and analyses. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will. Will Koehrsen Data Scientist at Cortex Intel, Data Science Communicator Towards Data Science Sharing concepts, ideas, and codes.",Interactive Controls in Jupyter Notebooks,5,published,7532,1191,3.7783375314861463,0,1,1,1,0,0
